classification.ops.fisher_factors.homog|append def _append_homog(tensor): """Appends a homogeneous coordinate to the last dimension of a Tensor. Args: tensor: A Tensor. Returns: A Tensor identical to the input but one larger in the last dimension.  The new entries are filled with ones. """ rank = len(tensor.shape.as_list()) shape = array_ops.concat([array_ops.shape(tensor)[:-1], [1]], axis=0) ones = array_ops.ones(shape, dtype=tensor.dtype) return array_ops.concat([tensor, ones], axis=rank - 1) 
classification.ops.fisher_factors.ScaleFactor.ops|make|update|inverse def make_inverse_update_ops(self): raise NotImplementedError 
text_gcn-master.layers.Layer.call def __call__(self, inputs): with tf.name_scope(self.name): if self.logging and not self.sparse_inputs: tf.summary.histogram(self.name + '/inputs', inputs) outputs = self._call(inputs) if self.logging: tf.summary.histogram(self.name + '/outputs', outputs) return outputs 
GPSig.kernels.SequentialMatern32.Matern def _Matern32(self, X, X2=None): r = self._euclid_dist(X, X2) return (1.0 + np.sqrt(3.0) * r) * tf.exp(-np.sqrt(3.0) * r) 
common.CustomResize.Custom|Resize def __init__(self, short_edge_length, max_size, interp=cv2.INTER_LINEAR): """ Args: short_edge_length ([int, int]): a [min, max] interval from which to sample the shortest edge length. max_size (int): maximum allowed longest edge length. """ super(CustomResize, self).__init__() if isinstance(short_edge_length, int): short_edge_length = short_edge_length, short_edge_length self._init(locals()) 
gan.factorVAE.FactorVAE.save def save(self, global_id, saver=None, checkpoint_dir=None): if saver is None: saver = self.saver if checkpoint_dir is None: checkpoint_dir = self.checkpoint_dir saver.save(self.sess, os.path.join(checkpoint_dir, 'model'), global_step=global_id) 
embeddings_concatenate.main def main(): parser = argparse.ArgumentParser() parser.add_argument('-i', '--embeddings', nargs='+', required=True) parser.add_argument('-o', '--output', required=True) parser.add_argument('-v', '--vocabulary', default=None) parser.add_argument('-b', '--batch_size', type=int, default=1024) parser.add_argument('-k', '--num_nearest_neighbor', type=int, default=10) parser.add_argument('-oov', '--generate_oov_words', action='store_false') args = parser.parse_args() if args.generate_oov_words: concatenate_embeddings_generate(embeddings_path=args.embeddings, out_path=args.output, vocab=vocab_from_path(args.vocabulary) if args.vocabulary else None, batch_size=args.batch_size, k=args. num_nearest_neighbor) else: concatenate_embeddings(embeddings_path=args.embeddings, out_path= args.output, vocab=vocab_from_path(args.vocabulary) if args. vocabulary else None) 
batch_test.BatchTest.broadcast|pmap|or|test|jit def test_jit_or_pmap_broadcast(self):  def kernel_fn(x1, x2, do_flip, keys, do_square, params, _unused=None, p =0.65): res = np.abs(np.matmul(x1, x2)) if do_square: res *= res if do_flip: res = -res res *= random.uniform(keys) * p return [res, params] params = np.array([1.0, 0.3]), (np.array([1.2]), np.array([0.5])) x2 = np.arange(0, 10).reshape((10,)) keys = random.PRNGKey(1) kernel_fn_pmapped = batch._jit_or_pmap_broadcast(kernel_fn, device_count=0) x1 = np.arange(0, 10).reshape((1, 10)) for do_flip in [True, False]: for do_square in [True, False]: with self.subTest(do_flip=do_flip, do_square=do_square, device_count=0): res_1 = kernel_fn(x1, x2, do_flip, keys, do_square, params, _unused=True, p=0.65) res_2 = kernel_fn_pmapped(x1, x2, do_flip, keys, do_square, params, _unused=True) self.assertAllClose(res_1, res_2, True) utils.stub_out_pmap(batch, 1) x1 = np.arange(0, 10).reshape((1, 10)) kernel_fn_pmapped = batch._jit_or_pmap_broadcast(kernel_fn, device_count=1) for do_flip in [True, False]: for do_square in [True, False]: with self.subTest(do_flip=do_flip, do_square=do_square, device_count=1): res_1 = kernel_fn(x1, x2, do_flip, keys, do_square, params, _unused=False, p=0.65) res_2 = kernel_fn_pmapped(x1, x2, do_flip, keys, do_square, params, _unused=None) self.assertAllClose(res_1[0], res_2[0], True) self.assertAllClose(tree_map(partial(np.expand_dims, axis=0 ), res_1[1]), res_2[1], True) kernel_fn_pmapped = batch._jit_or_pmap_broadcast(kernel_fn, device_count=2) x1 = np.arange(0, 20).reshape((2, 10)) utils.stub_out_pmap(batch, 2)  def broadcast(arg): return np.broadcast_to(arg, (2,) + arg.shape) for do_flip in [True, False]: for do_square in [True, False]: with self.subTest(do_flip=do_flip, do_square=do_square, device_count=2): res_1 = kernel_fn(x1, x2, do_flip, keys, do_square, params, p=0.2) res_2 = kernel_fn_pmapped(x1, x2, do_flip, keys, do_square, params, _unused=None, p=0.2) self.assertAllClose(res_1[0][0], res_2[0][0], True) self.assertAllClose(res_1[0][1], res_2[0][1], True) self.assertAllClose(tree_map(broadcast, res_1[1]), res_2[1], True) 
bert-master.run_pretraining.main def main(_): tf.logging.set_verbosity(tf.logging.INFO) if not FLAGS.do_train and not FLAGS.do_eval: raise ValueError( 'At least one of `do_train` or `do_eval` must be True.') bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file) tf.gfile.MakeDirs(FLAGS.output_dir) input_files = [] for input_pattern in FLAGS.input_file.split(','): input_files.extend(tf.gfile.Glob(input_pattern)) tf.logging.info('*** Input Files ***') for input_file in input_files: tf.logging.info('  %s' % input_file) tpu_cluster_resolver = None if FLAGS.use_tpu and FLAGS.tpu_name: tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver( FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project) is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2 run_config = tf.contrib.tpu.RunConfig(cluster=tpu_cluster_resolver, master=FLAGS.master, model_dir=FLAGS.output_dir, save_checkpoints_steps=FLAGS.save_checkpoints_steps, tpu_config=tf. contrib.tpu.TPUConfig(iterations_per_loop=FLAGS.iterations_per_loop, num_shards=FLAGS.num_tpu_cores, per_host_input_for_training= is_per_host)) model_fn = model_fn_builder(bert_config=bert_config, init_checkpoint= FLAGS.init_checkpoint, learning_rate=FLAGS.learning_rate, num_train_steps=FLAGS.num_train_steps, num_warmup_steps=FLAGS. num_warmup_steps, use_tpu=FLAGS.use_tpu, use_one_hot_embeddings= FLAGS.use_tpu) estimator = tf.contrib.tpu.TPUEstimator(use_tpu=FLAGS.use_tpu, model_fn =model_fn, config=run_config, train_batch_size=FLAGS. train_batch_size, eval_batch_size=FLAGS.eval_batch_size) if FLAGS.do_train: tf.logging.info('***** Running training *****') tf.logging.info('  Batch size = %d', FLAGS.train_batch_size) train_input_fn = input_fn_builder(input_files=input_files, max_seq_length=FLAGS.max_seq_length, max_predictions_per_seq= FLAGS.max_predictions_per_seq, is_training=True) estimator.train(input_fn=train_input_fn, max_steps=FLAGS. num_train_steps) if FLAGS.do_eval: tf.logging.info('***** Running evaluation *****') tf.logging.info('  Batch size = %d', FLAGS.eval_batch_size) eval_input_fn = input_fn_builder(input_files=input_files, max_seq_length=FLAGS.max_seq_length, max_predictions_per_seq= FLAGS.max_predictions_per_seq, is_training=False) result = estimator.evaluate(input_fn=eval_input_fn, steps=FLAGS. max_eval_steps) output_eval_file = os.path.join(FLAGS.output_dir, 'eval_results.txt') with tf.gfile.GFile(output_eval_file, 'w') as writer: tf.logging.info('***** Eval results *****') for key in sorted(result.keys()): tf.logging.info('  %s = %s', key, str(result[key])) writer.write('%s = %s\n' % (key, str(result[key]))) 
graphsage.minibatch.NodeMinibatchIterator.end def end(self): return self.batch_num * self.batch_size >= len(self.train_nodes) 
texar.modules.memory.memory_network.MemNetBase.get_default_embed_fn.fn|embed def _embed_fn(memory, soft_memory, mode=None): if memory is None and soft_memory is None: raise ValueError('Either `memory` or `soft_memory` is required.') if memory is not None and soft_memory is not None: raise ValueError( 'Must not specify `memory` and `soft_memory` at the same time.') embedded_memory = embedder(ids=memory, soft_ids=soft_memory, mode=mode) temporal_embedded = temporal_embedder(sequence_length=tf.constant([ memory_size]), mode=mode) temporal_embedded = tf.tile(temporal_embedded, [tf.shape( embedded_memory)[0], 1, 1]) if combine == 'add': return tf.add(embedded_memory, temporal_embedded) elif combine == 'concat': return tf.concat([embedded_memory, temporal_embedded], axis=-1) else: raise ValueError('Unknown combine method: {}'.format(combine)) 
attributionpriors.mnist.mnist_train.batch|standardize def batch_standardize(frames): return tf.map_fn(lambda frame: tf.image.per_image_standardization(frame ), frames) 
embedding.Embedding.to|index|list def list_to_index(self, words): try: indexes = [] for w in words: indexes.append(self.vocabulary.word_to_index(w)) return indexes except KeyError as err: raise 
optimizer.Optimizer.deepzono_forward_pass.get|index def get_index(active_abstracts, in_name, index_store): index = 0 while True: index = index + active_abstracts[index:].index(in_name) if not index in index_store: break index += 1 return index 
util.model|details|print def print_model_details(locnet, model): print('Localization Network Summary:') print(locnet.summary()) print('\nFull Network Summary:') print(model.summary()) 
dsnt.softmax|d def _softmax2d(target, axes): """ A softmax implementation which can operate across more than one axis - as this isn't provided by Tensorflow Arguments: targets - The tensor on which to apply softmax axes - An integer or list of integers across which to apply softmax """ max_axis = tf.reduce_max(target, axes, keepdims=True) target_exp = tf.exp(target - max_axis) normalize = tf.reduce_sum(target_exp, axes, keepdims=True) softmax = target_exp / normalize return softmax 
bow_seq2seq.mix|softmax|bow|predict def bow_predict_mix_softmax(enc_batch_size, vocab_size, max_enc_bow, enc_state ): """bow prediction with mixture of softmax""" bow_topk_prob = tf.zeros([enc_batch_size, vocab_size]) for i in range(max_enc_bow): bow_proj = tf.layers.Dense(vocab_size, name='bow_proj_%d' % i, kernel_initializer=tf.random_normal_initializer(stddev=0.05), bias_initializer=tf.constant_initializer(0.0)) bow_logits = bow_proj(enc_state[1].h) bow_prob_i = tf.nn.softmax(bow_logits, axis=1) bow_topk_prob += bow_prob_i return bow_topk_prob 
bnn_trainer.Trainer.adagrad|optimize def optimize_adagrad(self, loss, train_vars=None, lr=0.01): optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate, decay=0.9) if train_vars is None: train_op = optimizer.minimize(loss, global_step=self.global_step, gate_gradients=optimizer.GATE_NONE) else: train_op = optimizer.minimize(loss, var_list=train_vars, global_step=self.global_step, gate_gradients=optimizer.GATE_NONE) return train_op 
networks.decoder def decoder2(inputs, is_training=True, scope='decoder2', reuse=None): """Decoder Post-processing net = CBHG Args: inputs: A 3d tensor with shape of [N, T_y/r, n_mels*r]. Log magnitude spectrogram of sound files. It is recovered to its original shape. is_training: Whether or not the layer is in training mode. scope: Optional scope for `variable_scope` reuse: Boolean, whether to reuse the weights of a previous layer by the same name.  Returns Predicted linear spectrogram tensor with shape of [N, T_y, 1+n_fft//2]. """ with tf.variable_scope(scope, reuse=reuse): inputs = tf.reshape(inputs, [tf.shape(inputs)[0], -1, hp.n_mels]) dec = conv1d_banks(inputs, K=hp.decoder_num_banks, is_training= is_training) dec = tf.layers.max_pooling1d(dec, pool_size=2, strides=1, padding= 'same') dec = conv1d(dec, filters=hp.embed_size // 2, size=3, scope='conv1d_1') dec = bn(dec, is_training=is_training, activation_fn=tf.nn.relu, scope='conv1d_1') dec = conv1d(dec, filters=hp.n_mels, size=3, scope='conv1d_2') dec = bn(dec, is_training=is_training, scope='conv1d_2') dec = tf.layers.dense(dec, hp.embed_size // 2) for i in range(4): dec = highwaynet(dec, num_units=hp.embed_size // 2, scope= 'highwaynet_{}'.format(i)) dec = gru(dec, hp.embed_size // 2, bidirection=True) outputs = tf.layers.dense(dec, 1 + hp.n_fft // 2) return outputs 
fasterai.loss.WassFeatureLoss.wass|loss|single def _single_wass_loss(self, pred, targ): mean_test, tr_cov_test, root_cov_test = targ mean_synth, cov_synth = self._calc_2_moments(pred) loss = self._calc_l2wass_dist(mean_test, tr_cov_test, root_cov_test, mean_synth, cov_synth) return loss 
elpips.networks.squeezenet1_1.get|slice def get_slice2(self, input): with tf.name_scope('slice2'): net = self._pool(input, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='VALID', name='max_pool_slice2', data_format='NHWC') net = self.fire_module(input=net, index=3, ch_in=64, ch_out_squeeze =16, ch_out_expand=64) return self.fire_module(input=net, index=4, ch_in=128, ch_out_squeeze=16, ch_out_expand=64) 
train_LA_meanteacher_certainty.get|weight|consistency|current def get_current_consistency_weight(epoch): return args.consistency * ramps.sigmoid_rampup(epoch, args. consistency_rampup) 
utils.download def download(directory, filename, url): """Download filename from url unless it's already in directory.""" if not tf.gfile.Exists(directory): print('Creating directory %s' % directory) os.mkdir(directory) path = os.path.join(directory, filename) if not tf.gfile.Exists(path): print('Downloading %s to %s' % (url, path)) path, _ = urllib.request.urlretrieve(url, path) statinfo = os.stat(path) print('Successfully downloaded', filename, statinfo.st_size, 'bytes') return path 
HistogramCalculator.HistogramCalculator.Histogram|Calculator def __init__(self, hps, logdir): self.hps = hps self.logdir = logdir 
avod.core.models.avod_model.AvodModel.Avod|Model def __init__(self, model_config, train_val_test, dataset): """ Args: model_config: configuration for the model train_val_test: "train", "val", or "test" dataset: the dataset that will provide samples and ground truth """ super(AvodModel, self).__init__(model_config) self.dataset = dataset self._num_final_classes = self.dataset.num_classes + 1 input_config = self._config.input_config self._bev_pixel_size = np.asarray([input_config.bev_dims_h, input_config.bev_dims_w]) self._bev_depth = input_config.bev_depth self._img_pixel_size = np.asarray([input_config.img_dims_h, input_config.img_dims_w]) self._img_depth = [input_config.img_depth] avod_config = self._config.avod_config self._proposal_roi_crop_size = [avod_config.avod_proposal_roi_crop_size ] * 2 self._positive_selection = avod_config.avod_positive_selection self._nms_size = avod_config.avod_nms_size self._nms_iou_threshold = avod_config.avod_nms_iou_thresh self._path_drop_probabilities = self._config.path_drop_probabilities self._box_rep = avod_config.avod_box_representation if self._box_rep not in ['box_3d', 'box_8c', 'box_8co', 'box_4c', 'box_4ca' ]: raise ValueError('Invalid box representation', self._box_rep) self._rpn_model = RpnModel(model_config, train_val_test, dataset) if train_val_test not in ['train', 'val', 'test']: raise ValueError( 'Invalid train_val_test value,should be one of ["train", "val", "test"]' ) self._train_val_test = train_val_test self._is_training = self._train_val_test == 'train' self.sample_info = {} 
prune_vgg16.PruneVgg16.get|pruned|weights def get_pruned_weights(self, cut_channels): for name, cut_channel in cut_channels.items(): _, next_layer_name = self._get_last_and_next_layer_name(name) cut_content = ['kernel', 'bias', 'beta', 'gamma', 'moving_mean', 'moving_variance'] assert next_layer_name is not None weight = self.pruned_weights_dict[name] weight = prune_channel(weight, cut_channel, cut_type='output') self.pruned_weights_dict[name] = weight name = name.rstrip(cut_content[0]) for content in cut_content[1:]: if self.pruned_weights_dict.get(name + content) is not None: weight = self.pruned_weights_dict[name + content] weight = prune_channel(weight, cut_channel, cut_type='flatten') self.pruned_weights_dict[name + content] = weight weight = self.pruned_weights_dict[next_layer_name] weight = prune_channel(weight, cut_channel, cut_type='input') self.pruned_weights_dict[next_layer_name] = weight return self.pruned_weights_dict 
borealisflows.noise_flow_model.NoiseFlow.forward def forward(self, z, eps_std, yy=None, nlf0=None, nlf1=None, iso=None, cam=None ): x = z for i in reversed(range(self.n_levels)): if i < self.n_levels - 1: x = split2d_reverse('pool{}'.format(i), x, eps_std) for bijector in reversed(self.model[i]): if type(bijector) in [AffineCouplingCondY, AffineCouplingCondXY, AffineCouplingFitSdnGain2, AffineCouplingCondYG, AffineCouplingCamSdn, AffineCouplingCondXYG, AffineCouplingSdnGain, AffineCouplingSdn, AffineCouplingGain, AffineCouplingGainEx1, AffineCouplingGainEx2, AffineCouplingGainEx3, AffineCouplingSdnEx1, AffineCouplingSdnEx2, AffineCouplingSdnEx3, AffineCouplingSdnEx4, AffineCouplingGainEx4, AffineCouplingSdnEx5, AffineCouplingSdnEx6]: x = bijector._forward(x, yy, nlf0, nlf1, iso, cam) else: x = bijector._forward(x) x = unsqueeze2d(x, self.hps.squeeze_factor, self.hps.squeeze_type) return x 
dsnt.kl|d def _kl_2d(p, q, eps=24): unsummed_kl = p * (tf.log(p + eps) - tf.log(q + eps)) kl_values = tf.reduce_sum(unsummed_kl, [-1, -2]) return kl_values 
sidd_utils.kl|data|div def kl_div_3_data(p_data, q_data, bin_edges=None, left_edge=0.0, right_edge =1.0, n_bins=1000): """Returns forward, inverse, and symmetric KL divergence between two sets of data points p and q""" if bin_edges is None: data_range = right_edge - left_edge bin_width = data_range / n_bins bin_edges = np.arange(left_edge, right_edge + bin_width, bin_width) p, _ = get_histogram(p_data, bin_edges, left_edge, right_edge, n_bins) q, _ = get_histogram(q_data, bin_edges, left_edge, right_edge, n_bins) idx = (p > 0) & (q > 0) p = p[idx] q = q[idx] logp = np.log(p) logq = np.log(q) kl_fwd = np.sum(p * (logp - logq)) kl_inv = np.sum(q * (logq - logp)) kl_sym = (kl_fwd + kl_inv) / 2.0 return kl_fwd, kl_inv, kl_sym 
query_methods.UncertaintySampling.Sampling|Uncertainty def __init__(self, model, input_shape, num_labels, gpu): super().__init__(model, input_shape, num_labels, gpu) 
bim.metrics.indices|corr def corr_indices(model, data): """Given the name of a model and a set of data, return the indices of the images that are correctly classified by the model.""" label_fpath = os.path.join(BASE_DIR, 'data', data, 'val.txt') labels = [int(l.split(' ')[-1]) for l in tf.gfile.Open(label_fpath). readlines()] img_fnames = tf.gfile.ListDirectory(os.path.join(BASE_DIR, ATTR_DIR[ FLAGS.scratch], model + '-' + data)) preds = [int(p.split('_')[-1]) for p in sorted(img_fnames)] attr_indices = range(len(preds)) if FLAGS.scratch: attr_indices = range(FLAGS.num_imgs) global_indices = get_global_indices() corr = [i for i in attr_indices if preds[i] == labels[global_indices[i]]] return corr 
predictor.Predictor.disparity|predict def predict_disparity(self): if self.settings.predict: if self.network is None: self.network = DenseMapNet(settings=self.settings) self.model = self.network.build_model() if self.settings.images: self.get_epe(use_train_data=False, get_performance=True) else: for i in range(4): self.get_epe(use_train_data=False, get_performance=True) else: self.get_epe(use_train_data=False, get_performance=True) if self.settings.notrain: self.get_epe() 
cleverhans.attacks.CarliniWagnerL2.Carlini|Wagner|L def __init__(self, model, back='tf', sess=None): """ Note: the model parameter should be an instance of the cleverhans.model.Model abstraction provided by CleverHans. """ super(CarliniWagnerL2, self).__init__(model, back, sess) import tensorflow as tf self.feedable_kwargs = {'y': tf.float32, 'y_target': tf.float32} self.structural_kwargs = ['batch_size', 'confidence', 'targeted', 'learning_rate', 'binary_search_steps', 'max_iterations', 'abort_early', 'initial_const', 'clip_min', 'clip_max'] if not isinstance(self.model, Model): self.model = CallableModelWrapper(self.model, 'logits') 
utils.gather|from|dict def gather_from_dict(tensor_dict, choice): """Chooses tensor values along first dimension using given choice.  If `tensor_dict` = { 0: zeros(shape=(6)), 1: ones(shape=(6)), 2: twos(shape=(6)), 3: threes(shape=(6)) } and choice = [0, 0, 2, 2, 1, 0] then returned tensor is [0., 0., 2., 2., 1., 0.]  Args: tensor_dict: A dict with int keys and tensor values. All tensor values must be of same type and shape. choice: A 1-d int tensor with number of elements equal to first dimension of tensors in `tensor_dict`. The values in the tensor must be valid keys in `tensor_dict`.  Returns: A tensor of same type and shape as tensors in `tensor_dict`. """ one_tensor = next(iter(tensor_dict.values())) tf.debugging.assert_rank(choice, rank=1) tf.debugging.assert_equal(tf.size(choice), tf.shape(one_tensor)[0]) zeros_tensor = tf.zeros_like(one_tensor) final_tensor = zeros_tensor for c, t in tensor_dict.items(): tf.debugging.assert_equal(tf.shape(t), tf.shape(one_tensor)) tf.debugging.assert_type(t, tf_type=one_tensor.dtype) final_tensor += tf.compat.v1.where(tf.equal(choice, c), t, zeros_tensor ) return final_tensor 
AffineCouplingGainEx4.AffineCouplingGainEx4.log|and|forward|det|jacobian def _forward_and_log_det_jacobian(self, x, yy, nlf0=None, nlf1=None, iso= None, cam=None): if self._last_layer: x = tf.reshape(x, (-1, self.i0, self.i1, self.ic)) scale = gain_model_params_ex4(iso, self.gain_init) scale = scale + x * 0.0 y = x if scale is not None: y *= scale if scale is None: log_abs_det_J = tf.constant(0.0, dtype=x.dtype, name='fldj') else: log_abs_det_J = tf.reduce_sum(tf.log(scale), axis=[1, 2, 3]) return y, log_abs_det_J 
neural_tangents.stax.get|kernel|erf def _get_erf_kernel(ker_mat, prod, do_backprop, ntk=None): dot_sigma = 4 / (np.pi * np.sqrt(prod - 4 * ker_mat ** 2)) ker_mat = _arcsin(2 * ker_mat / np.sqrt(prod), do_backprop) * 2 / np.pi if ntk is not None: ntk *= dot_sigma return ker_mat, ntk 
deepzono_nodes.get|xpp def get_xpp(matrix): """ Arguments --------- matrix : numpy.ndarray must be a 2D array  Return ------ output : numpy.ndarray contains pointers to the rows of matrix """ return (matrix.__array_interface__['data'][0] + np.arange(matrix.shape[ 0]) * matrix.strides[0]).astype(np.uintp) 
attributionpriors.mnist.mnist_train.train def train(save_dir, sess, cond_input_op, y, train_pl, loss, train_step, eg_loss, eg_train, train_eg, accuracy_op, accuracy_update_op, reset_metrics_op, training_iterator, validation_iterator, test_iterator, handle): init = tf.group(tf.global_variables_initializer(), tf. local_variables_initializer()) sess.run(init) training_handle = sess.run(training_iterator.string_handle()) validation_handle = sess.run(validation_iterator.string_handle()) test_handle = sess.run(test_iterator.string_handle()) max_acc = 0.0 sess.run(training_iterator.initializer) global_step = tf.train.get_or_create_global_step() saver = tf.train.Saver() validation_accuracies = [] validation_total_variances = [] while True: try: _, _, i = sess.run([train_step, accuracy_update_op, global_step ], feed_dict={handle: training_handle, train_pl: True}) if FLAGS.lamb > 0.0: _, train_var_loss = sess.run([eg_train, eg_loss], feed_dict ={handle: training_handle, train_eg: True}) except tf.errors.OutOfRangeError: break if i % validation_step == 0: sess.run(reset_metrics_op) sess.run(validation_iterator.initializer) while True: try: sess.run(accuracy_update_op, feed_dict={handle: validation_handle}) except tf.errors.OutOfRangeError: break sess.run(validation_iterator.initializer) vald_var_loss = sess.run(eg_loss, feed_dict={handle: validation_handle, train_eg: True}) validation_accuracy = sess.run(accuracy_op, feed_dict={handle: validation_handle}) validation_total_variances.append(vald_var_loss) validation_accuracies.append(validation_accuracy) print( 'Iteration: {}, validation accuracy: {:.6f}, e-variation vald (batch): {:.6f}' .format(i, validation_accuracy, vald_var_loss), end='\r') sess.run(reset_metrics_op) if validation_accuracy > max_acc: max_acc = validation_accuracy save_path = saver.save(sess, save_dir) saver.restore(sess, save_dir) sess.run(reset_metrics_op) sess.run(test_iterator.initializer) while True: try: sess.run(accuracy_update_op, feed_dict={handle: test_handle, train_pl: False}) except tf.errors.OutOfRangeError: break test_accuracy = sess.run(accuracy_op) print('Test accuracy: {:.6f}'.format(test_accuracy)) return validation_total_variances, validation_accuracies, test_accuracy 
opt.warmup|constant def warmup_constant(x, warmup=0.002): s = 1 if x <= warmup else 0 return s * (x / warmup) + (1 - s) * 1 
attributionpriors.mnist.mnist_download.feature|bytes def _bytes_feature(value): return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value])) 
utils.images|save def save_images(images, size, image_path): return imsave(inverse_transform(images), size, image_path) 
build_imagenet_data.ImageCoder.png|to|jpeg def png_to_jpeg(self, image_data): return self._sess.run(self._png_to_jpeg, feed_dict={self._png_data: image_data}) 
bert-master.tokenization.is|whitespace def _is_whitespace(char): """Checks whether `chars` is a whitespace character.""" if char == ' ' or char == '\t' or char == '\n' or char == '\r': return True cat = unicodedata.category(char) if cat == 'Zs': return True return False 
tf_utils.distributions.repeat def repeat(x, n): if n == 1: return x shape = map(int, x.get_shape().as_list()) shape[0] *= n idx = tf.range(tf.shape(x)[0]) idx = tf.reshape(idx, [-1, 1]) idx = tf.tile(idx, [1, n]) idx = tf.reshape(idx, [-1]) x = tf.gather(x, idx) x.set_shape(shape) return x 
official.utils.misc.model_helpers_test.PastStopThresholdTest.false|threshold|past|test|stop|none def test_past_stop_threshold_none_false(self): """Tests that check None returns false.""" self.assertFalse(model_helpers.past_stop_threshold(None, -1.5)) self.assertFalse(model_helpers.past_stop_threshold(None, None)) self.assertFalse(model_helpers.past_stop_threshold(None, 1.5)) self.assertTrue(model_helpers.past_stop_threshold(0, 1.5)) 
average_checkpoints.main def main(): tf.logging.set_verbosity(tf.logging.INFO) parser = argparse.ArgumentParser(formatter_class=argparse. ArgumentDefaultsHelpFormatter) parser.add_argument('--model_dir', required=True, help= 'The model directory containing the checkpoints.') parser.add_argument('--output_dir', required=True, help= 'The output directory where the averaged checkpoint will be saved.') parser.add_argument('--max_count', type=int, default=8, help= 'The maximal number of checkpoints to average.') args = parser.parse_args() if args.model_dir == args.output_dir: raise ValueError('Model and output directory must be different') checkpoints_path = tf.train.get_checkpoint_state(args.model_dir ).all_model_checkpoint_paths if len(checkpoints_path) > args.max_count: checkpoints_path = checkpoints_path[-args.max_count:] num_checkpoints = len(checkpoints_path) tf.logging.info('Averaging %d checkpoints...' % num_checkpoints) tf.logging.info('Listing variables...') var_list = tf.train.list_variables(checkpoints_path[0]) avg_values = {} for name, shape in var_list: if not name.startswith('global_step'): avg_values[name] = np.zeros(shape) for checkpoint_path in checkpoints_path: tf.logging.info('Loading checkpoint %s' % checkpoint_path) reader = tf.train.load_checkpoint(checkpoint_path) for name in avg_values: avg_values[name] += reader.get_tensor(name) / num_checkpoints tf_vars = [] for name, value in six.iteritems(avg_values): tf_vars.append(tf.get_variable(name, shape=value.shape)) placeholders = [tf.placeholder(v.dtype, shape=v.shape) for v in tf_vars] assign_ops = [tf.assign(v, p) for v, p in zip(tf_vars, placeholders)] latest_step = int(checkpoints_path[-1].split('-')[-1]) out_base_file = os.path.join(args.output_dir, 'model.ckpt') global_step = tf.get_variable('global_step', initializer=tf.constant( latest_step, dtype=tf.int64), trainable=False) saver = tf.train.Saver(tf.global_variables()) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for p, assign_op, (name, value) in zip(placeholders, assign_ops, six.iteritems(avg_values)): sess.run(assign_op, {p: value}) tf.logging.info('Saving averaged checkpoint to %s-%d' % ( out_base_file, latest_step)) saver.save(sess, out_base_file, global_step=global_step) 
sidd_utils.mini|sample|tr|batch|dict def sample_mini_batch_dict_tr(n_patches, tr_dict, hps, mb_pool=None, n_target_ch=4): t0 = time.time() x_batch = np.zeros((n_patches, hps.patch_height, hps.patch_height, n_target_ch), dtype=float) y_batch = np.zeros((n_patches, hps.patch_height, hps.patch_height, n_target_ch), dtype=float) nlf_batch = [None] * n_patches print('sample_mini_batch_dict: from image %d to ' % hps.cur_tr_im_idx, end='') for i in range(n_patches): input_patch, gt_patch, var_patch = sample_one_patch(tr_dict[ 'tr_in_ims'][hps.cur_tr_im_idx], tr_dict['tr_gt_ims'][hps. cur_tr_im_idx], [], hps.patch_height, hps.patch_height) x_batch[(i), :, :, :] = input_patch y_batch[(i), :, :, :] = gt_patch nlf_batch[i] = tr_dict['tr_nl_ims'][hps.cur_tr_im_idx] hps.cur_tr_im_idx += 1 print('%3d  in  %.2f  sec.' % (hps.cur_tr_im_idx, time.time() - t0)) return x_batch, y_batch, [], nlf_batch 
deepMOT-master.models.siamrpn.SiamRPN.forward def forward(self, x, r1_kernel, cls1_kernel): x_f = self.featureExtract(x) return self.regress_adjust(F.conv2d(self.conv_r2(x_f), r1_kernel) ), F.conv2d(self.conv_cls2(x_f), cls1_kernel) 
utils.get|image def get_image(image_path, input_height, input_width, resize_height=64, resize_width=64, is_crop=True, is_grayscale=False): image = imread(image_path, is_grayscale) return transform(image, input_height, input_width, resize_height, resize_width, is_crop) 
plato.agent.component.dialogue_policy.deep_learning.reinforce_policy.ReinforcePolicy.save def save(self, path=None): """ Saves the policy model to the provided path  :param path: path to save the model to :return: """ if not self.is_training: return if not path: path = 'models/policies/reinforce.pkl' print('No policy file name provided. Using default: {0}'.format(path)) if not os.path.exists(os.path.dirname(path)): os.makedirs(os.path.dirname(path), exist_ok=True) obj = {'weights': self.weights, 'alpha': self.alpha, 'alpha_decay_rate': self.alpha_decay_rate, 'epsilon': self.epsilon, 'exploration_decay_rate': self.exploration_decay_rate} with open(path, 'wb') as file: pickle.dump(obj, file, pickle.HIGHEST_PROTOCOL) 
DataProcessing.v|All|Data|collect def collectAllData_v2(path): df = pd.read_csv(path) df = df.set_index('Filename') return df 
clr_callback.CyclicLR.clr def clr(self): cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size)) x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1) if self.scale_mode == 'cycle': return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, 1 - x) * self.scale_fn(cycle) else: return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, 1 - x) * self.scale_fn(self.clr_iterations) 
avod.core.anchor_projector_test.AnchorProjectorTest.project|to|tensors|test|bev def test_project_to_bev_tensors(self): anchors = np.asarray([[0, 0, 3, 2, 0, 6], [3, 0, 3, 2, 0, 2]], dtype=np .float64) tf_anchors = tf.convert_to_tensor(anchors, dtype=tf.float32) bev_extents = [[-5, 5], [0, 10]] tf_bev_extents = tf.convert_to_tensor(bev_extents, dtype=tf.float32) bev_extents_range = np.diff(bev_extents, axis=1) bev_extents_range = np.stack([bev_extents_range, bev_extents_range] ).flatten() expected_boxes = np.asarray([[0 - -5 - 1, 4, 0 - -5 + 1, 10], [3 - -5 - 1, 6, 3 - -5 + 1, 8]], dtype=np.float64) expected_boxes_norm = expected_boxes / bev_extents_range tf_boxes, tf_boxes_norm = anchor_projector.project_to_bev(tf_anchors, tf_bev_extents) np_boxes, np_boxes_norm = anchor_projector.project_to_bev(anchors, bev_extents) sess = tf.Session() with sess.as_default(): tf_boxes_out = tf_boxes.eval() tf_boxes_norm_out = tf_boxes_norm.eval() np.testing.assert_allclose(tf_boxes_out, expected_boxes) np.testing.assert_allclose(tf_boxes_norm_out, expected_boxes_norm) np.testing.assert_allclose(tf_boxes_out, np_boxes) np.testing.assert_allclose(tf_boxes_norm_out, np_boxes_norm) 
similarity_datasets.get|Turk|M def get_MTurk771(): data = pd.read_csv(Mturk771_path, sep=' ', header=None).values return Bunch(X=data[:, 0:2].astype('object'), y=2 * data[:, (2)].astype (np.float)) 
slac.utils.gif_utils.gif_summary_v2.event|py|gif def py_gif_event(step, tag, tensor, max_outputs, fps): summary = py_gif_summary(tag, tensor, max_outputs, fps) if isinstance(summary, bytes): summ = summary_pb2.Summary() summ.ParseFromString(summary) summary = summ event = event_pb2.Event(summary=summary) event.wall_time = time.time() event.step = step event_pb = event.SerializeToString() return event_pb 
archs.maml2.MAML.conv|layer def conv_layer(self, x, W, b, name, strides=1): with tf.variable_scope(name, reuse=tf.AUTO_REUSE): x = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME') x = tf.nn.bias_add(x, b) return x 
NLIN.Constants|calc|Intra def calcIntraConstants(param): param = normalizeParameters(param) N = param.N_mc R = 2 * np.pi * (np.random.rand(5, N) - 0.5 * np.ones((5, N))) w0 = R[(0), :] - R[(1), :] + R[(2), :] argInB = (w0 < np.pi) * (w0 > -np.pi) return _calcIntra(param, argInB, R) 
nets.mobilenet.mobilenet_v2_test.MobilenetV2Test.Classes|test|Creation|No def testCreationNoClasses(self): spec = copy.deepcopy(mobilenet_v2.V2_DEF) net, ep = mobilenet.mobilenet(tf.placeholder(tf.float32, (10, 224, 224, 16)), conv_defs=spec, num_classes=None) self.assertIs(net, ep['global_pool']) 
vkge.training.constraints.linear|update|pseudoboolean def pseudoboolean_linear_update(var_matrix): pseudoboolean_linear = tf.minimum(1.0, tf.maximum(var_matrix, 0.0)) return tf.assign(var_matrix, pseudoboolean_linear) 
build_imagenet_data.cmyk|is def _is_cmyk(filename): """Determine if file contains a CMYK JPEG format image.  Args: filename: string, path of the image file.  Returns: boolean indicating if the image is a JPEG encoded with CMYK color space. """ blacklist = ['n01739381_1309.JPEG', 'n02077923_14822.JPEG', 'n02447366_23489.JPEG', 'n02492035_15739.JPEG', 'n02747177_10752.JPEG', 'n03018349_4028.JPEG', 'n03062245_4620.JPEG', 'n03347037_9675.JPEG', 'n03467068_12171.JPEG', 'n03529860_11437.JPEG', 'n03544143_17228.JPEG', 'n03633091_5218.JPEG', 'n03710637_5125.JPEG', 'n03961711_5286.JPEG', 'n04033995_2932.JPEG', 'n04258138_17003.JPEG', 'n04264628_27969.JPEG', 'n04336792_7448.JPEG', 'n04371774_5854.JPEG', 'n04596742_4225.JPEG', 'n07583066_647.JPEG', 'n13037406_4650.JPEG'] return filename.split('/')[-1] in blacklist 
r2r_problem.R2RProblem.get|optimizer def get_optimizer(self, learning_rate): return tf.keras.optimizers.Adam(learning_rate=learning_rate) 
utils.data_utils.dataLoaderUSR.get|hr|lr|paths def get_lr_hr_paths(self, data_dir): lr_path = sorted(os.listdir(data_dir + self.low_res_folder_)) hr_path = sorted(os.listdir(data_dir + 'high_res/')) num_paths = min(len(lr_path), len(hr_path)) all_lr_paths, all_hr_paths = [], [] for f in lr_path[:num_paths]: all_lr_paths.append(os.path.join(data_dir + self.low_res_folder_, f)) all_hr_paths.append(os.path.join(data_dir + 'high_res/', f)) return num_paths, all_lr_paths, all_hr_paths 
facenet-master.contributed.batch_represent.main def main(args): with tf.Graph().as_default(): with tf.Session() as sess: output_dir = os.path.expanduser(args.output_dir) if not os.path.isdir(output_dir): os.makedirs(output_dir) print('Loading trained model...\n') meta_file, ckpt_file = facenet.get_model_filenames(os.path. expanduser(args.trained_model_dir)) facenet.load_model(args.trained_model_dir, meta_file, ckpt_file) print('Finding image paths and targets...\n') data = load_files(args.data_dir, load_content=False, shuffle=False) labels_array = data['target'] paths = data['filenames'] images_placeholder = tf.get_default_graph().get_tensor_by_name( 'input:0') embeddings = tf.get_default_graph().get_tensor_by_name( 'embeddings:0') phase_train_placeholder = tf.get_default_graph( ).get_tensor_by_name('phase_train:0') image_size = images_placeholder.get_shape()[1] embedding_size = embeddings.get_shape()[1] print('Generating embeddings from images...\n') start_time = time.time() batch_size = args.batch_size nrof_images = len(paths) nrof_batches = int(np.ceil(1.0 * nrof_images / batch_size)) emb_array = np.zeros((nrof_images, embedding_size)) for i in xrange(nrof_batches): start_index = i * batch_size end_index = min((i + 1) * batch_size, nrof_images) paths_batch = paths[start_index:end_index] images = facenet.load_data(paths_batch, do_random_crop= False, do_random_flip=False, image_size=image_size, do_prewhiten=True) feed_dict = {images_placeholder: images, phase_train_placeholder: False} emb_array[start_index:end_index, :] = sess.run(embeddings, feed_dict=feed_dict) time_avg_forward_pass = (time.time() - start_time) / float( nrof_images) print( 'Forward pass took avg of %.3f[seconds/image] for %d images\n' % (time_avg_forward_pass, nrof_images)) print('Finally saving embeddings and gallery to: %s' % output_dir) np.save(os.path.join(output_dir, 'gallery.npy'), labels_array) np.save(os.path.join(output_dir, 'signatures.npy'), emb_array) 
deepctr.layers.interaction.InteractingLayer.get|config def get_config(self): config = {'att_embedding_size': self.att_embedding_size, 'head_num': self.head_num, 'use_res': self.use_res, 'seed': self.seed} base_config = super(InteractingLayer, self).get_config() return dict(list(base_config.items()) + list(config.items())) 
classification.ops.fisher_blocks.tracenorm|pi|compute def _compute_pi_tracenorm(left_cov, right_cov): left_norm = math_ops.trace(left_cov) * right_cov.shape.as_list()[0] right_norm = math_ops.trace(right_cov) * left_cov.shape.as_list()[0] return math_ops.sqrt(left_norm / right_norm) 
official.utils.data.file_io_test.BaseTest.deserialize|test|serialize def test_serialize_deserialize_1(self): self._serialize_deserialize(num_cores=2) 
models.attacks_tf.jsma def jsma(sess, x, predictions, grads, sample, target, theta, gamma, clip_min, clip_max, feed=None): """ TensorFlow implementation of the JSMA (see https://arxiv.org/abs/1511.07528 for details about the algorithm design choices). :param sess: TF session :param x: the input placeholder :param predictions: the model's symbolic output (the attack expects the probabilities, i.e., the output of the softmax, but will also work with logits typically) :param grads: symbolic gradients :param sample: numpy array with sample input :param target: target class for sample input :param theta: delta for each feature adjustment :param gamma: a float between 0 - 1 indicating the maximum distortion percentage :param clip_min: minimum value for components of the example returned :param clip_max: maximum value for components of the example returned :return: an adversarial sample """ adv_x = copy.copy(sample) nb_features = np.product(adv_x.shape[1:]) original_shape = adv_x.shape adv_x = np.reshape(adv_x, (1, nb_features)) max_iters = np.floor(nb_features * gamma / 2) nb_classes = len(grads) increase = bool(theta > 0) if increase: search_domain = set([i for i in xrange(nb_features) if adv_x[0, i] < clip_max]) else: search_domain = set([i for i in xrange(nb_features) if adv_x[0, i] > clip_min]) iteration = 0 adv_x_original_shape = np.reshape(adv_x, original_shape) current = utils_tf.model_argmax(sess, x, predictions, adv_x_original_shape, feed=feed) _logger.debug('Starting JSMA attack up to {} iterations'.format(max_iters)) while current != target and iteration < max_iters and len(search_domain ) > 1: adv_x_original_shape = np.reshape(adv_x, original_shape) grads_target, grads_others = jacobian(sess, x, grads, target, adv_x_original_shape, nb_features, nb_classes, feed=feed) if iteration % ((max_iters + 1) // 5) == 0 and iteration > 0: _logger.debug('Iteration {} of {}'.format(iteration, int( max_iters))) i, j, search_domain = saliency_map(grads_target, grads_others, search_domain, increase) adv_x = apply_perturbations(i, j, adv_x, increase, theta, clip_min, clip_max) current = utils_tf.model_argmax(sess, x, predictions, adv_x_original_shape, feed=feed) iteration = iteration + 1 if current == target: _logger.info('Attack succeeded using {} iterations'.format(iteration)) else: _logger.info(('Failed to find adversarial example ' + 'after {} iterations').format(iteration)) percent_perturbed = float(iteration * 2) / nb_features if current == target: return np.reshape(adv_x, original_shape), 1, percent_perturbed else: return np.reshape(adv_x, original_shape), 0, percent_perturbed 
pytorch_pretrained_bert.modeling.BertPredictionHeadTransform.forward def forward(self, hidden_states): hidden_states = self.dense(hidden_states) hidden_states = self.transform_act_fn(hidden_states) hidden_states = self.LayerNorm(hidden_states) return hidden_states 
texar.data.data.scalar_data.ScalarData.make|data def _make_data(self): dataset_hparams = self._hparams.dataset dataset = MonoTextData._make_mono_text_dataset(dataset_hparams) dataset, dataset_size = self._shuffle_dataset(dataset, self._hparams, self._hparams.dataset.files) self._dataset_size = dataset_size data_spec = dsutils._DataSpec(dataset=dataset, dataset_size=self. _dataset_size) dataset, data_spec = self._process_dataset(dataset, self._hparams, data_spec) self._data_spec = data_spec self._decoder = data_spec.decoder dataset = self._make_batch(dataset, self._hparams) if self._hparams.prefetch_buffer_size > 0: dataset = dataset.prefetch(self._hparams.prefetch_buffer_size) self._dataset = dataset 
ant_maze_env.AntMazeEnv.render def render(self, *args, **kwargs): """Render the environment.""" return self.wrapped_env.render(*args, **kwargs) 
facenet-master.tmp.vggface16.load def load(filename, images): vgg16 = io.loadmat(filename) vgg16Layers = vgg16['net'][0][0]['layers']  def vbbWeights(layerNumber): W = vgg16Layers[0][layerNumber][0][0][2][0][0] W = tf.constant(W) return W  def vbbConstants(layerNumber): b = vgg16Layers[0][layerNumber][0][0][2][0][1].T b = tf.constant(np.reshape(b, b.size)) return b modelGraph = {} modelGraph['input'] = images modelGraph['conv1_1'] = tf.nn.conv2d(modelGraph['input'], filter= vbbWeights(0), strides=[1, 1, 1, 1], padding='SAME') modelGraph['relu1_1'] = tf.nn.relu(modelGraph['conv1_1'] + vbbConstants(0)) modelGraph['conv1_2'] = tf.nn.conv2d(modelGraph['relu1_1'], filter= vbbWeights(2), strides=[1, 1, 1, 1], padding='SAME') modelGraph['relu1_2'] = tf.nn.relu(modelGraph['conv1_2'] + vbbConstants(2)) modelGraph['pool1'] = tf.nn.max_pool(modelGraph['relu1_2'], ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') modelGraph['conv2_1'] = tf.nn.conv2d(modelGraph['pool1'], filter= vbbWeights(5), strides=[1, 1, 1, 1], padding='SAME') modelGraph['relu2_1'] = tf.nn.relu(modelGraph['conv2_1'] + vbbConstants(5)) modelGraph['conv2_2'] = tf.nn.conv2d(modelGraph['relu2_1'], filter= vbbWeights(7), strides=[1, 1, 1, 1], padding='SAME') modelGraph['relu2_2'] = tf.nn.relu(modelGraph['conv2_2'] + vbbConstants(7)) modelGraph['pool2'] = tf.nn.max_pool(modelGraph['relu2_2'], ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') modelGraph['conv3_1'] = tf.nn.conv2d(modelGraph['pool2'], filter= vbbWeights(10), strides=[1, 1, 1, 1], padding='SAME') modelGraph['relu3_1'] = tf.nn.relu(modelGraph['conv3_1'] + vbbConstants(10) ) modelGraph['conv3_2'] = tf.nn.conv2d(modelGraph['relu3_1'], filter= vbbWeights(12), strides=[1, 1, 1, 1], padding='SAME') modelGraph['relu3_2'] = tf.nn.relu(modelGraph['conv3_2'] + vbbConstants(12) ) modelGraph['conv3_3'] = tf.nn.conv2d(modelGraph['relu3_2'], filter= vbbWeights(14), strides=[1, 1, 1, 1], padding='SAME') modelGraph['relu3_3'] = tf.nn.relu(modelGraph['conv3_3'] + vbbConstants(14) ) modelGraph['pool3'] = tf.nn.max_pool(modelGraph['relu3_3'], ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') modelGraph['conv4_1'] = tf.nn.conv2d(modelGraph['pool3'], filter= vbbWeights(17), strides=[1, 1, 1, 1], padding='SAME') modelGraph['relu4_1'] = tf.nn.relu(modelGraph['conv4_1'] + vbbConstants(17) ) modelGraph['conv4_2'] = tf.nn.conv2d(modelGraph['relu4_1'], filter= vbbWeights(19), strides=[1, 1, 1, 1], padding='SAME') modelGraph['relu4_2'] = tf.nn.relu(modelGraph['conv4_2'] + vbbConstants(19) ) modelGraph['conv4_3'] = tf.nn.conv2d(modelGraph['relu4_2'], filter= vbbWeights(21), strides=[1, 1, 1, 1], padding='SAME') modelGraph['relu4_3'] = tf.nn.relu(modelGraph['conv4_3'] + vbbConstants(21) ) modelGraph['pool4'] = tf.nn.max_pool(modelGraph['relu4_3'], ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') modelGraph['conv5_1'] = tf.nn.conv2d(modelGraph['pool4'], filter= vbbWeights(24), strides=[1, 1, 1, 1], padding='SAME') modelGraph['relu5_1'] = tf.nn.relu(modelGraph['conv5_1'] + vbbConstants(24) ) modelGraph['conv5_2'] = tf.nn.conv2d(modelGraph['relu5_1'], filter= vbbWeights(26), strides=[1, 1, 1, 1], padding='SAME') modelGraph['relu5_2'] = tf.nn.relu(modelGraph['conv5_2'] + vbbConstants(26) ) modelGraph['conv5_3'] = tf.nn.conv2d(modelGraph['relu5_2'], filter= vbbWeights(28), strides=[1, 1, 1, 1], padding='SAME') modelGraph['relu5_3'] = tf.nn.relu(modelGraph['conv5_3'] + vbbConstants(28) ) modelGraph['pool5'] = tf.nn.max_pool(modelGraph['relu5_3'], ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') modelGraph['resh1'] = tf.reshape(modelGraph['pool5'], [-1, 25088]) modelGraph['fc6'] = tf.nn.relu_layer(modelGraph['resh1'], tf.reshape( vbbWeights(31), [25088, 4096]), vbbConstants(31)) modelGraph['dropout1'] = tf.nn.dropout(modelGraph['fc6'], 0.5) modelGraph['fc7'] = tf.nn.relu_layer(modelGraph['dropout1'], tf.squeeze (vbbWeights(34), [0, 1]), vbbConstants(34)) modelGraph['dropout2'] = tf.nn.dropout(modelGraph['fc7'], 0.5) modelGraph['fc8'] = tf.nn.relu_layer(modelGraph['dropout2'], tf.squeeze (vbbWeights(37), [0, 1]), vbbConstants(37)) return modelGraph 
nets.mobilenet.mobilenet.global|pool def global_pool(input_tensor, pool_op=tf.nn.avg_pool): """Applies avg pool to produce 1x1 output.  NOTE: This function is funcitonally equivalenet to reduce_mean, but it has baked in average pool which has better support across hardware.  Args: input_tensor: input tensor pool_op: pooling op (avg pool is default) Returns: a tensor batch_size x 1 x 1 x depth. """ shape = input_tensor.get_shape().as_list() if shape[1] is None or shape[2] is None: kernel_size = tf.convert_to_tensor([1, tf.shape(input_tensor)[1], tf.shape(input_tensor)[2], 1]) else: kernel_size = [1, shape[1], shape[2], 1] output = pool_op(input_tensor, ksize=kernel_size, strides=[1, 1, 1, 1], padding='VALID') output.set_shape([None, 1, 1, None]) return output 
util.read|pfm def read_pfm(fpath, expected_identifier='Pf'):  def _get_next_line(f): next_line = f.readline().decode('utf-8').rstrip() while next_line.startswith('#'): next_line = f.readline().rstrip() return next_line with open(fpath, 'rb') as f: identifier = _get_next_line(f) if identifier != expected_identifier: raise Exception( 'Unknown identifier. Expected: "%s", got: "%s".' % ( expected_identifier, identifier)) try: line_dimensions = _get_next_line(f) dimensions = line_dimensions.split(' ') width = int(dimensions[0].strip()) height = int(dimensions[1].strip()) except: raise Exception( 'Could not parse dimensions: "%s". Expected "width height", e.g. "512 512".' % line_dimensions) try: line_scale = _get_next_line(f) scale = float(line_scale) assert scale != 0 if scale < 0: endianness = '<' else: endianness = '>' except: raise Exception( 'Could not parse max value / endianess information: "%s". Should be a non-zero number.' % line_scale) try: data = np.fromfile(f, '%sf' % endianness) data = np.reshape(data, (height, width)) data = np.flipud(data) with np.errstate(invalid='ignore'): data *= abs(scale) except: raise Exception( 'Invalid binary values. Could not create %dx%d array from input.' % (height, width)) return data 
graphsage.supervised_models.SupervisedGraphsage.Graphsage|Supervised def __init__(self, num_classes, placeholders, features, adj, degrees, layer_infos, concat=True, aggregator_type='mean', model_size='small', sigmoid_loss=False, identity_dim=0, **kwargs): """ Args: - placeholders: Stanford TensorFlow placeholder object. - features: Numpy array with node features. - adj: Numpy array with adjacency lists (padded with random re-samples) - degrees: Numpy array with node degrees. - layer_infos: List of SAGEInfo namedtuples that describe the parameters of all the recursive layers. See SAGEInfo definition above. - concat: whether to concatenate during recursive iterations - aggregator_type: how to aggregate neighbor information - model_size: one of "small" and "big" - sigmoid_loss: Set to true if nodes can belong to multiple classes """ models.GeneralizedModel.__init__(self, **kwargs) if aggregator_type == 'mean': self.aggregator_cls = MeanAggregator elif aggregator_type == 'seq': self.aggregator_cls = SeqAggregator elif aggregator_type == 'meanpool': self.aggregator_cls = MeanPoolingAggregator elif aggregator_type == 'maxpool': self.aggregator_cls = MaxPoolingAggregator elif aggregator_type == 'gcn': self.aggregator_cls = GCNAggregator else: raise Exception('Unknown aggregator: ', self.aggregator_cls) self.inputs1 = placeholders['batch'] self.model_size = model_size self.adj_info = adj if identity_dim > 0: self.embeds = tf.get_variable('node_embeddings', [adj.get_shape(). as_list()[0], identity_dim]) else: self.embeds = None if features is None: if identity_dim == 0: raise Exception( 'Must have a positive value for identity feature dimension if no input features given.' ) self.features = self.embeds else: self.features = tf.Variable(tf.constant(features, dtype=tf.float32), trainable=False) if not self.embeds is None: self.features = tf.concat([self.embeds, self.features], axis=1) self.degrees = degrees self.concat = concat self.num_classes = num_classes self.sigmoid_loss = sigmoid_loss self.dims = [(0 if features is None else features.shape[1]) + identity_dim] self.dims.extend([layer_infos[i].output_dim for i in range(len( layer_infos))]) self.batch_size = placeholders['batch_size'] self.placeholders = placeholders self.layer_infos = layer_infos self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate) self.build() 
plato.agent.component.user_simulator.agenda_based_user_simulator.agenda.Agenda.pop def pop(self): """ Pop top item from the agenda.  :return: top item """ if self.agenda: return self.agenda.pop() else: print('Warning! Attempted to pop an empty agenda.') return None 
download.maybe|extract|download|and def maybe_download_and_extract(url, download_dir): """ Download and extract the data if it doesn't already exist. Assumes the url is a tar-ball file.  :param url: Internet URL for the tar-file to download. Example: "https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"  :param download_dir: Directory where the downloaded file is saved. Example: "data/CIFAR-10/"  :return: Nothing. """ filename = url.split('/')[-1] file_path = os.path.join(download_dir, filename) if not os.path.exists(file_path): if not os.path.exists(download_dir): os.makedirs(download_dir) file_path, _ = urllib.request.urlretrieve(url=url, filename= file_path, reporthook=_print_download_progress) print() print('Download finished. Extracting files.') if file_path.endswith('.zip'): zipfile.ZipFile(file=file_path, mode='r').extractall(download_dir) elif file_path.endswith(('.tar.gz', '.tgz')): tarfile.open(name=file_path, mode='r:gz').extractall(download_dir) print('Done.') else: print('Data has apparently already been downloaded and unpacked.') 
pvae.pixelvae_bbans_two_layer.post|elem|fn|param def post1_elem_param_fn(z2, mu1_post, sig1_post):  def g(z1_idxs, params=None, idx=None): _, _, mu1_prior, sig1_prior = np.moveaxis(params, -1, 0) z1 = mu1_prior + sig1_prior * bb_ans.std_gaussian_centres( prior_precision)[z1_idxs] mu1_prior, sig1_prior = gen_net1(z1, z2) return np.stack((mu1_post, sig1_post, mu1_prior, sig1_prior), axis=-1) return g 
amb_dir_def.get|task|dir def get_task_dir(hparams): if hparams.measurement_type in ['drop_independent', 'drop_row', 'drop_col', 'drop_rowcol']: task_dir = '{}_p{}/'.format(hparams.measurement_type, hparams.drop_prob ) elif hparams.measurement_type in ['drop_patch', 'keep_patch', 'extract_patch']: task_dir = '{}_k{}/'.format(hparams.measurement_type, hparams. patch_size) elif hparams.measurement_type in ['blur_addnoise']: task_dir = '{}_br{}_bfs{}_anstd{}/'.format(hparams.measurement_type, hparams.blur_radius, hparams.blur_filter_size, hparams. additive_noise_std) elif hparams.measurement_type in ['pad_rotate_project', 'pad_rotate_project_with_theta']: task_dir = '{}_na{}/'.format(hparams.measurement_type, hparams. num_angles) else: raise NotImplementedError return task_dir 
deepctr.layers.interaction.InnerProductLayer.get|config def get_config(self): config = {'reduce_sum': self.reduce_sum} base_config = super(InnerProductLayer, self).get_config() return dict(list(base_config.items()) + list(config.items())) 
run.finetune def finetune(trial, version, retrain_epochs, density, base, iteration_count): path = _get_path('finetune', trial, version, 'retrain_{}/density_{}'. format(retrain_epochs, density)) base = os.path.join(base, trial) _create_initial_state_at_checkpoint(path, base, 'final') run(path, **{'--lottery_pruning_method': 'prune_all_to_global_{}'. format(density), '--lottery_reset_to': '{}/lottery/checkpoint_iter_final'.format(base), '--lottery_prune_at': '{}/lottery/checkpoint_iter_final'.format( base), '--lottery_force_learning_rate': '0.00025', '--max_train_epochs': 10 + retrain_epochs * iteration_count}) 
nets.inception_v4_test.InceptionTest.All|test|Points|Shapes|End def testAllEndPointsShapes(self): batch_size = 5 height, width = 299, 299 num_classes = 1000 inputs = tf.random_uniform((batch_size, height, width, 3)) _, end_points = inception.inception_v4(inputs, num_classes) endpoints_shapes = {'Conv2d_1a_3x3': [batch_size, 149, 149, 32], 'Conv2d_2a_3x3': [batch_size, 147, 147, 32], 'Conv2d_2b_3x3': [ batch_size, 147, 147, 64], 'Mixed_3a': [batch_size, 73, 73, 160], 'Mixed_4a': [batch_size, 71, 71, 192], 'Mixed_5a': [batch_size, 35, 35, 384], 'Mixed_5b': [batch_size, 35, 35, 384], 'Mixed_5c': [ batch_size, 35, 35, 384], 'Mixed_5d': [batch_size, 35, 35, 384], 'Mixed_5e': [batch_size, 35, 35, 384], 'Mixed_6a': [batch_size, 17, 17, 1024], 'Mixed_6b': [batch_size, 17, 17, 1024], 'Mixed_6c': [ batch_size, 17, 17, 1024], 'Mixed_6d': [batch_size, 17, 17, 1024], 'Mixed_6e': [batch_size, 17, 17, 1024], 'Mixed_6f': [batch_size, 17, 17, 1024], 'Mixed_6g': [batch_size, 17, 17, 1024], 'Mixed_6h': [ batch_size, 17, 17, 1024], 'Mixed_7a': [batch_size, 8, 8, 1536], 'Mixed_7b': [batch_size, 8, 8, 1536], 'Mixed_7c': [batch_size, 8, 8, 1536], 'Mixed_7d': [batch_size, 8, 8, 1536], 'AuxLogits': [ batch_size, num_classes], 'global_pool': [batch_size, 1, 1, 1536], 'PreLogitsFlatten': [batch_size, 1536], 'Logits': [batch_size, num_classes], 'Predictions': [batch_size, num_classes]} self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys()) for endpoint_name in endpoints_shapes: expected_shape = endpoints_shapes[endpoint_name] self.assertTrue(endpoint_name in end_points) self.assertListEqual(end_points[endpoint_name].get_shape().as_list( ), expected_shape) 
data_learning.main def main(): args = get_input_arguments() training_mode = args.mode nn_model = NeuralNetworkModel(conf.data_shape_to_nn, conf. abs_shape_to_nn, conf.phase_shape_to_nn, conf.total_classes) nn_model.get_data_from_file(conf.data_folder, np.float32, training_mode) if training_mode: nn_model.cnn_model_abs_phase() nn_model.fit_data(conf.epochs) nn_model.save_model(conf.model_name) else: nn_model.load_model(conf.model_name) result = nn_model.get_test_result() nn_model.end() 
classification.ops.fisher_blocks.EigenCorrectedKroneckerProductFB.multiply|inverse def multiply_inverse(self, vector): left_basis = self._input_factor.get_eigen_basis(self._input_damping) right_basis = self._output_factor.get_eigen_basis(self._output_damping) scaling = self._scale_factor.get_cov() reshaped_vector = utils.layer_params_to_mat2d(vector) reshaped_out = utils.eigen_basis_kron_product_2d(left_basis, right_basis, reshaped_vector, transpose=True) damped_scale_factor = scaling + self._damping reshaped_out = reshaped_out / damped_scale_factor reshaped_out = utils.eigen_basis_kron_product_2d(left_basis, right_basis, reshaped_out, transpose=False) return utils.mat2d_to_layer_params(vector, reshaped_out) 
commons.measure_utils.get_inpaint_func_opencv.func|inpaint def inpaint_func(image, mask): mask = np.prod(mask, axis=2, keepdims=True) unknown = (1 - mask).astype(np.uint8) image = 255 * (image - x_min) / (x_max - x_min) image = image.astype(np.uint8) inpainted = cv2.inpaint(image, unknown, 3, inpaint_type) inpainted = inpainted.astype(np.float32) inpainted = inpainted / 255.0 * (x_max - x_min) + x_min inpainted = np.reshape(inpainted, image.shape) return inpainted 
xlnet-master.run_classifier.file_based_input_fn_builder.fn|input def input_fn(params, input_context=None): """The actual input function.""" if FLAGS.use_tpu: batch_size = params['batch_size'] elif is_training: batch_size = FLAGS.train_batch_size elif FLAGS.do_eval: batch_size = FLAGS.eval_batch_size else: batch_size = FLAGS.predict_batch_size d = tf.data.TFRecordDataset(input_file) if input_context is not None: tf.logging.info('Input pipeline id %d out of %d', input_context. input_pipeline_id, input_context.num_replicas_in_sync) d = d.shard(input_context.num_input_pipelines, input_context. input_pipeline_id) if is_training: d = d.shuffle(buffer_size=FLAGS.shuffle_buffer) d = d.repeat() d = d.apply(tf.contrib.data.map_and_batch(lambda record: _decode_record (record, name_to_features), batch_size=batch_size, drop_remainder= drop_remainder)) return d 
train_critic.main.model|test def test_model(x, **kwargs): return model(x, train=False, **kwargs) 
facenet-master.tmp.deepdream.main.resize def resize(img, size): img = tf.expand_dims(img, 0) return tf.image.resize_bilinear(img, size)[(0), :, :, :] 
model.PCGN_beamsearch.PCGNBeamSearchDecoder.finalize def finalize(self, outputs, final_state, sequence_lengths): """Finalize and return the predicted_ids.  Args: outputs: An instance of BeamSearchDecoderOutput. final_state: An instance of BeamSearchDecoderState. Passed through to the output. sequence_lengths: An `int64` tensor shaped `[batch_size, beam_width]`. The sequence lengths determined for each beam during decode.  Returns: outputs: An instance of FinalBeamSearchDecoderOutput where the predicted_ids are the result of calling _gather_tree. final_state: The same input instance of BeamSearchDecoderState. """ predicted_ids = beam_search_ops.gather_tree(outputs.predicted_ids, outputs.parent_ids, sequence_length=sequence_lengths) outputs = FinalBeamSearchDecoderOutput(beam_search_decoder_output= outputs, predicted_ids=predicted_ids) return outputs, final_state 
run_openai_gpt.main.tokenize|encode|and def tokenize_and_encode(obj): """ Tokenize and encode a nested object """ if isinstance(obj, str): return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj)) elif isinstance(obj, int): return obj return list(tokenize_and_encode(o) for o in obj) 
hbaselines.goal_conditioned.policy.ActorCriticPolicy.store|transition def store_transition(self, obs0, context0, action, reward, obs1, context1, done, evaluate=False): """Store a transition in the replay buffer.  Parameters ---------- obs0 : array_like the last observation context0 : array_like or None the last contextual term. Set to None if no context is provided by the environment. action : array_like the action reward : float the reward obs1 : array_like the current observation context1 : array_like or None the current contextual term. Set to None if no context is provided by the environment. done : float is the episode done evaluate : bool whether the sample is being provided by the evaluation environment. If so, the data is not stored in the replay buffer. """ raise NotImplementedError 
clr_callback.CyclicLR.on|begin|train def on_train_begin(self, logs={}): logs = logs or {} if self.clr_iterations == 0: K.set_value(self.model.optimizer.lr, self.base_lr) else: K.set_value(self.model.optimizer.lr, self.clr()) 
layers.BinaryFullyConnectedLayer.rho|op|setter def _rho_setter_op(self): """Op to set regularizer rho """ self.rho_ph = utils.get_ph(self.rho) self.set_rho_op = tf.assign(self.rho, self.rho_ph) 
model.l|loss|smooth def smooth_l1_loss(y_true, y_pred): """Implements Smooth-L1 loss. y_true and y_pred are typicallly: [N, 4], but could be any shape. """ diff = K.abs(y_true - y_pred) less_than_one = K.cast(K.less(diff, 1.0), 'float32') loss = less_than_one * 0.5 * diff ** 2 + (1 - less_than_one) * (diff - 0.5) return loss 
deepctr.layers.interaction.FM.call def call(self, inputs, **kwargs): if K.ndim(inputs) != 3: raise ValueError( 'Unexpected inputs dimensions %d, expect to be 3 dimensions' % K.ndim(inputs)) concated_embeds_value = inputs square_of_sum = tf.square(tf.reduce_sum(concated_embeds_value, axis=1, keep_dims=True)) sum_of_square = tf.reduce_sum(concated_embeds_value * concated_embeds_value, axis=1, keep_dims=True) cross_term = square_of_sum - sum_of_square cross_term = 0.5 * tf.reduce_sum(cross_term, axis=2, keep_dims=False) return cross_term 
facenet-master.src.generative.models.dfc_vae_resnet.relu|leaky def leaky_relu(x): return tf.maximum(0.1 * x, x) 
dnc.jobs|l|add def add_l4_jobs(jobs): optimizers = 'l4adam', 'l4mom' fraction_list = list(np.round(np.arange(0.05, 1, 0.05), 2)) for optimizer in optimizers: for fraction in fraction_list: jobs.append( 'python train.py --optimizer {optimizer} --fraction {fraction}' .format(optimizer=optimizer, fraction=fraction)) 
official.utils.data.file_io.shard|dataframe|iter def iter_shard_dataframe(df, rows_per_core=1000): """Two way shard of a dataframe.  This function evenly shards a dataframe so that it can be mapped efficiently. It yields a list of dataframes with length equal to the number of CPU cores, with each dataframe having rows_per_core rows. (Except for the last batch which may have fewer rows in the dataframes.) Passing vectorized inputs to a multiprocessing pool is much more effecient than iterating through a dataframe in serial and passing a list of inputs to the pool.  Args: df: Pandas dataframe to be sharded. rows_per_core: Number of rows in each shard.  Returns: A list of dataframe shards. """ n = len(df) num_cores = min([multiprocessing.cpu_count(), n]) num_blocks = int(np.ceil(n / num_cores / rows_per_core)) max_batch_size = num_cores * rows_per_core for i in range(num_blocks): min_index = i * max_batch_size max_index = min([(i + 1) * max_batch_size, n]) df_shard = df[min_index:max_index] n_shard = len(df_shard) boundaries = np.linspace(0, n_shard, num_cores + 1, dtype=np.int64) yield [df_shard[boundaries[j]:boundaries[j + 1]] for j in range( num_cores)] 
enas.controller.Controller.trainer|build def _build_trainer(self): raise NotImplementedError('Abstract method.') 
gym_pycolab.protocols.scrolling.permit def permit(entity, the_plot, motions, scrolling_group=''): """Indicate next permissible motions for the egocentric entity `entity`.  Although it's mentioned in the argument description, it's worth pointing out that these are motions that will be permissible for `entity` in the next game iteration, not in the current one.  It is fine for the same entity to call this function more than once in the same game iteration, as long as `scrolling_group` is always the same.  See the section "On the loose interpretation of 'legal' scrolling motions" for a disappointing but necessary complication of the semantics of this function.  Args: entity: the egocentric pycolab game entity giving permission for certain scrolling motions during the next game iteration. the_plot: the pycolab game's `Plot` object. motions: an iterable of scrolling motions that will be allowable *during the next game iteration*. These motions are 2-tuples which can be interpreted as the (possibly negative) number of rows/columns that the game window is allowed to move downward/rightward over the game board; or, conveniently, this is a (sub?)set of valid (row, column) motions that `entity` will be able to make at the next iteration (the numbers are the same either way). scrolling_group: a string identifier for the scrolling group for which `entity` is granting scrolling permission.  Raises: TypeError: `entity` is not a pycolab entity. Error: `entity` is known to belong to a scrolling group distinct from `scrolling_group`, or `entity` is not registered as egocentric within `scrolling_group`. """ _check_scrolling_group(entity, the_plot, scrolling_group) egocentrists = the_plot.setdefault('scrolling_{}_egocentrists'.format( scrolling_group), set()) if entity not in egocentrists: raise Error( '{} is not registered as an egocentric entity in scrolling group {}' .format(_entity_string_for_errors(entity), repr(scrolling_group))) my_permit_frame = the_plot.frame + 1 all_permit_frames = the_plot.setdefault('scrolling_{}_permitted_frame'. format(scrolling_group), dict()) all_permits = the_plot.setdefault('scrolling_{}_permitted'.format( scrolling_group), dict()) my_permits = all_permits.setdefault(entity, set()) if all_permit_frames.setdefault(entity, my_permit_frame ) != my_permit_frame: all_permit_frames[entity] = my_permit_frame my_permits.clear() my_permits.update(motions) 
commons.utils.get|uncond|phs def get_phs_uncond(hparams): z_ph = tf.placeholder(tf.float32, [None, hparams.z_dim], name='z_ph') x_ph = tf.placeholder(tf.float32, [hparams.batch_size] + hparams. image_dims, name='x_ph') return z_ph, x_ph 
nets.resnet_v2_test.ResnetCompleteNetworkTest.Convolutional|Endpoint|Fully|test|Shapes def testFullyConvolutionalEndpointShapes(self): global_pool = False num_classes = 10 inputs = create_test_input(2, 321, 321, 3) with slim.arg_scope(resnet_utils.resnet_arg_scope()): _, end_points = self._resnet_small(inputs, num_classes, global_pool =global_pool, spatial_squeeze=False, scope='resnet') endpoint_to_shape = {'resnet/block1': [2, 41, 41, 4], 'resnet/block2': [2, 21, 21, 8], 'resnet/block3': [2, 11, 11, 16], 'resnet/block4': [2, 11, 11, 32]} for endpoint in endpoint_to_shape: shape = endpoint_to_shape[endpoint] self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape) 
train_app.eval_loop.metric|k|cmc|at def cmc_metric_at_k(k): return metrics.streaming_mean_cmc_at_k(probe_x_var, probe_y_var, gallery_x_var, gallery_y_var, k=k, measure=distance_measure) 
nets.mobilenet_v1_test.MobilenetV1Test.Network|test|Base|Build def testBuildBaseNetwork(self): batch_size = 5 height, width = 224, 224 inputs = tf.random_uniform((batch_size, height, width, 3)) net, end_points = mobilenet_v1.mobilenet_v1_base(inputs) self.assertTrue(net.op.name.startswith('MobilenetV1/Conv2d_13')) self.assertListEqual(net.get_shape().as_list(), [batch_size, 7, 7, 1024]) expected_endpoints = ['Conv2d_0', 'Conv2d_1_depthwise', 'Conv2d_1_pointwise', 'Conv2d_2_depthwise', 'Conv2d_2_pointwise', 'Conv2d_3_depthwise', 'Conv2d_3_pointwise', 'Conv2d_4_depthwise', 'Conv2d_4_pointwise', 'Conv2d_5_depthwise', 'Conv2d_5_pointwise', 'Conv2d_6_depthwise', 'Conv2d_6_pointwise', 'Conv2d_7_depthwise', 'Conv2d_7_pointwise', 'Conv2d_8_depthwise', 'Conv2d_8_pointwise', 'Conv2d_9_depthwise', 'Conv2d_9_pointwise', 'Conv2d_10_depthwise', 'Conv2d_10_pointwise', 'Conv2d_11_depthwise', 'Conv2d_11_pointwise', 'Conv2d_12_depthwise', 'Conv2d_12_pointwise', 'Conv2d_13_depthwise', 'Conv2d_13_pointwise'] self.assertItemsEqual(end_points.keys(), expected_endpoints) 
mnist.gen.gan_def.fc|discriminator def discriminator_fc(hparams, x, scope_name, train, reuse): with tf.variable_scope(scope_name) as scope: if reuse: scope.reuse_variables() d_bn0 = ops.batch_norm(name='d_bn0') h0 = ops.linear(x, 25, 'd_h0_lin') h0 = ops.lrelu(d_bn0(h0, train=train)) d_bn1 = ops.batch_norm(name='d_bn1') h1 = ops.linear(h0, 25, 'd_h1_lin') h1 = ops.lrelu(d_bn1(h1, train=train)) d_logit = ops.linear(h1, 1, 'd_h2_lin') d = tf.nn.sigmoid(d_logit) return d, d_logit 
bert-master.tokenization.BasicTokenizer.Tokenizer|Basic def __init__(self, do_lower_case=True): """Constructs a BasicTokenizer.  Args: do_lower_case: Whether to lower case the input. """ self.do_lower_case = do_lower_case 
pathfinder.graph_construction.load|resources def load_resources(): global concept2id, relation2id, id2relation, id2concept concept2id = {} id2concept = {} with open(config['paths']['concept_vocab'], 'r', encoding='utf8') as f: for w in f.readlines(): concept2id[w.strip()] = len(concept2id) id2concept[len(id2concept)] = w.strip() print('concept2id done') id2relation = {} relation2id = {} with open(config['paths']['relation_vocab'], 'r', encoding='utf8') as f: for w in f.readlines(): id2relation[len(id2relation)] = w.strip() relation2id[w.strip()] = len(relation2id) print('relation2id done') 
tflib.ops.linear.Linear.uniform def uniform(stdev, size): if _weights_stdev is not None: stdev = _weights_stdev return np.random.uniform(low=-stdev * np.sqrt(3), high=stdev * np.sqrt( 3), size=size).astype('float32') 
test_envs.TestEfficientHRLEnvironments.utils|test|maze|env def test_maze_env_utils(self): """Test hbaselines/envs/efficient_hrl/maze_env_utils.py.""" for maze_id in ['Maze', 'Push', 'Fall', 'Block', 'BlockMaze']: construct_maze(maze_id) self.assertRaises(NotImplementedError, construct_maze, maze_id='error') p1 = 0, 0 p2 = 2, 2 self.assertAlmostEqual(point_distance(p1, p2), np.sqrt(2 ** 2 + 2 ** 2)) p1 = 0, 0 p2 = 2, 2 p3 = 0, 2 p4 = 2, 0 x, y, *_ = line_intersect(p1, p2, p3, p4) self.assertAlmostEqual(x, 1) self.assertAlmostEqual(y, 1) 
embeddings.OpenKE.config.Config.Config.prediction|link|init def init_link_prediction(self): """ import essential files and set essential interfaces for link prediction """ self.lib.importTestFiles() self.lib.importTypeFiles() self.test_h = np.zeros(self.lib.getEntityTotal(), dtype=np.int64) self.test_t = np.zeros(self.lib.getEntityTotal(), dtype=np.int64) self.test_r = np.zeros(self.lib.getEntityTotal(), dtype=np.int64) self.test_h_addr = self.test_h.__array_interface__['data'][0] self.test_t_addr = self.test_t.__array_interface__['data'][0] self.test_r_addr = self.test_r.__array_interface__['data'][0] 
avod.core.model.DetectionModel.model|config @property def model_config(self): return self._config 
nets.gen_models.ResNetSR.model|create def create_model(self): init = Input(shape=self.lr_shape) x0 = Convolution2D(64, (3, 3), activation='relu', padding='same')(init) x1 = Convolution2D(64, (3, 3), activation='relu', padding='same', strides=1 )(x0) x2 = Convolution2D(64, (3, 3), activation='relu', padding='same', strides=1 )(x1) x = self._residual_block(x2, 1) for i in range(self.n_residual_blocks - 1): x = self._residual_block(x, i + 2) x = Add()([x, x0]) x = self._upscale_block(x) x = x if self.SCALE < 4 else self._upscale_block(x) x = x if self.SCALE < 8 else self._upscale_block(x) out = Convolution2D(3, (3, 3), activation='tanh', padding='same')(x) return Model(init, out) 
td_or_not_td.alg.replaymemory.ReplayMemory.state|next def next_state(self): return [self.next_s] 
deepctr.layers.utils.Hash.build def build(self, input_shape): super(Hash, self).build(input_shape) 
prune_common.get|channels|nums def get_channels_nums(weights_dict, channel_type='output'): dim = 3 if channel_type == 'output' else 2 channels_nums = collections.OrderedDict() for key, weight in weights_dict.items(): if len(weight.shape) == 2: weight = np.reshape(weight, [1, 1] + list(weight.shape)) channels_nums[key] = weight.shape[dim] return channels_nums 
utils.load|data def load_data(image_path, flip=True, is_test=False): img_A, img_B = load_image(image_path) img_A, img_B = preprocess_A_and_B(img_A, img_B, flip=flip, is_test=is_test) img_A = img_A / 127.5 - 1.0 img_B = img_B / 127.5 - 1.0 img_AB = np.concatenate((img_A, img_B), axis=2) return img_AB 
utils.Aggregator.add @tf.Module.with_name_scope def add(self, actor_ids, values): """In-place adds values to the state associated to the given actors.  Args: actor_ids: 1D tensor with the list of actor IDs we want to add values to. values: A structure of tensors following the input spec, with an added first dimension that must either have the same size as 'actor_ids', or should not exist (in which case, the value is broadcasted to all actor ids). """ tf.nest.assert_same_structure(values, self._state) for s, v in zip(tf.nest.flatten(self._state), tf.nest.flatten(values)): s.scatter_add(tf.IndexedSlices(v, actor_ids)) 
bert.tokenization.FullTokenizer.Full|Tokenizer def __init__(self, vocab_file, do_lower_case=True): self.vocab = load_vocab(vocab_file) self.inv_vocab = {v: k for k, v in self.vocab.items()} self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case) self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab) 
densefuse_net.DenseFuseNet.transform|recons def transform_recons(self, img): enc = self.encoder.encode(img) target_features = enc self.target_features = target_features generated_img = self.decoder.decode(target_features) return generated_img 
losses.dice|loss def dice_loss1(score, target): target = target.float() smooth = 1e-05 intersect = torch.sum(score * target) y_sum = torch.sum(target) z_sum = torch.sum(score) loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth) loss = 1 - loss return loss 
texar.core.layers_test.MergeLayerTest.shape|test|output def test_output_shape(self): """Tests MergeLayer.compute_output_shape function. """ input_shapes = [[None, 1, 2], [64, 2, 2], [None, 3, 2]] concat_layer = layers.MergeLayer(mode='concat', axis=1) concat_output_shape = concat_layer.compute_output_shape(input_shapes) self.assertEqual(concat_output_shape, [64, 6, 2]) sum_layer = layers.MergeLayer(mode='sum', axis=1) sum_output_shape = sum_layer.compute_output_shape(input_shapes) self.assertEqual(sum_output_shape, [64, 2]) input_shapes = [[None, 5, 2], [64, None, 2], [2]] esum_layer = layers.MergeLayer(mode='elemwise_sum') esum_output_shape = esum_layer.compute_output_shape(input_shapes) self.assertEqual(esum_output_shape, [64, 5, 2]) 
GPSig.low_rank_calculations.Nystrom|map def Nystrom_map(X, kern, num_components=None, inducing_samples=None, return_inducing_samples=False): """ Computes the Nystrom features with uniform sampling given a kernel and num_components See e.g. https://dl.acm.org/citation.cfm?id=2343678 ------------------------------------------------------------------- Input :X:             input data points with size (num_samples, num_dims) :kern:          function handle to a kernel function that takes two matrices as input e.g. X1 (num_samples1, num_dims) and X2 (num_samples2, num_dim), and computes the matrix k(X1, X2) matrix of size (num_samples1, num_samples2) :num_components:number of components to take, i.e. the rank of the low-rank kernel matrix Output :X_nys:         Nystrom features of size (num_samples, num_components) """ num_samples = tf.shape(X)[0] if inducing_samples is None: idx, idx_not, rev_map = draw_indices(num_samples, num_components, need_inv=True) X_sampled = tf.gather(X, idx, axis=-2) X_not_sampled = tf.gather(X, idx_not, axis=-2) W = kern(X_sampled, X_sampled) + tf.diag(settings.numerics. jitter_level * tf.random_uniform([num_components], dtype= settings.float_type)) K21 = kern(X_not_sampled, X_sampled) C = tf.concat((W, K21), axis=-2) S, U = tf.self_adjoint_eig(W) D = tf.sqrt(tf.maximum(tf.cast(settings.numerics.jitter_level, settings.float_type) ** 2, S)) X_nys = tf.matmul(C, U) / tf.expand_dims(D, axis=-2) X_nys = tf.gather(X_nys, rev_map, axis=-2) else: num_components = tf.shape(inducing_samples)[0] W = kern(inducing_samples, inducing_samples) + tf.diag(settings. numerics.jitter_level * tf.random_uniform([num_components], dtype=settings.float_type)) Kxy = kern(tf.reshape(X, [-1, tf.shape(X)[-1]]), inducing_samples) S, U = tf.self_adjoint_eig(W) D = tf.sqrt(tf.maximum(tf.cast(settings.numerics.jitter_level, settings.float_type) ** 2, S)) X_nys = tf.matmul(Kxy, U) / tf.expand_dims(D, axis=-2) X_nys = tf.reshape(X_nys, tf.concat((tf.shape(X)[:-1], [ num_components]), axis=0)) if return_inducing_samples: return X_nys, X_sampled else: return X_nys 
plot_gain_params.get|params|gain def get_gain_params(df_gain_params, gain_param_name): iso_vals = [100, 400, 800, 1600, 3200] xtr = df_gain_params['epoch'] ytr = [] for i in range(5): if gain_param_name == 'g': ytr.append(df_gain_params['g' + str(iso_vals[i])]) else: ytr.append(df_gain_params['gain_params' + str(i)]) return xtr, ytr 
func_generate_traindata_noise.generate|traindata def generate_traindata512(traindata_all, traindata_label, Setting02_AngualrViews): """ Generate validation or test set( = full size(512x512) LF images)  input: traindata_all   (16x512x512x9x9x3) uint8 traindata_label (16x512x512x9x9)   float32 Setting02_AngualrViews [0,1,2,3,4,5,6,7,8] for 9x9   output: traindata_batch_list   (batch_size x 512 x 512 x len(Setting02_AngualrViews)) float32 traindata_label_batchNxN (batch_size x 512 x 512 )               float32 """ input_size = 512 label_size = 512 traindata_batch = np.zeros((len(traindata_all), input_size, input_size, len(Setting02_AngualrViews), len(Setting02_AngualrViews)), dtype=np .float32) traindata_label_batchNxN = np.zeros((len(traindata_all), label_size, label_size)) """ inital setting """ crop_half1 = int(0.5 * (input_size - label_size)) for ii in range(0, len(traindata_all)): R = 0.299 G = 0.587 B = 0.114 image_id = ii ix_rd = 0 iy_rd = 0 idx_start = 0 idy_start = 0 traindata_batch[(ii), :, :, :, :] = np.squeeze(R * traindata_all[ image_id:image_id + 1, idx_start:idx_start + input_size, idy_start:idy_start + input_size, :, :, (0)].astype('float32') + G * traindata_all[image_id:image_id + 1, idx_start:idx_start + input_size, idy_start:idy_start + input_size, :, :, (1)].astype ('float32') + B * traindata_all[image_id:image_id + 1, idx_start:idx_start + input_size, idy_start:idy_start + input_size, :, :, (2)].astype('float32')) if len(traindata_all) >= 12 and traindata_label.shape[-1] == 9: traindata_label_batchNxN[(ii), :, :] = traindata_label[( image_id), idx_start + crop_half1:idx_start + crop_half1 + label_size, idy_start + crop_half1:idy_start + crop_half1 + label_size, (4 + ix_rd), (4 + iy_rd)] elif len(traindata_label.shape) == 5: traindata_label_batchNxN[(ii), :, :] = traindata_label[( image_id), idx_start + crop_half1:idx_start + crop_half1 + label_size, idy_start + crop_half1:idy_start + crop_half1 + label_size, (0), (0)] else: traindata_label_batchNxN[(ii), :, :] = traindata_label[( image_id), idx_start + crop_half1:idx_start + crop_half1 + label_size, idy_start + crop_half1:idy_start + crop_half1 + label_size] traindata_batch = np.float32(1 / 255 * traindata_batch) traindata_batch = np.minimum(np.maximum(traindata_batch, 0), 1) traindata_batch_list = [] for i in range(traindata_batch.shape[3]): for j in range(traindata_batch.shape[4]): traindata_batch_list.append(np.expand_dims(traindata_batch[:, :, :, (i), (j)], axis=-1)) return traindata_batch_list, traindata_label_batchNxN 
regression.misc.eval_utils.likelihood|log def log_likelihood(log_py_xw, std_y_train): """ Log Likelihood. :param log_py_xw: [n_particles, batch_size] or [batch_size] :param std_y_train: float :return : tensor of shape []. RMSE. """ rank = len(log_py_xw.get_shape()) if rank == 1: log_py_xw = tf.expand_dims(log_py_xw, [0]) ll = tf.reduce_mean(zs.log_mean_exp(log_py_xw, 0)) - tf.log(std_y_train) return ll 
src.utilities.batch|norm|distances|L def batch_L_norm_distances(X: np.array, Y: np.array, ord=2) ->np.array: """ Takes 2 arrays of N x d examples and calculates the p-norm between them. Result is dimension N. If the inputs are N x h x w etc, they are first flattened to be N x d """ assert X.shape == Y.shape, 'X and Y must have the same dimensions' N = X.shape[0] rest = X.shape[1:] d = reduce(operator.mul, rest, 1) x = X.reshape(N, d) y = Y.reshape(N, d) if ord == 2: return np.sum((x - y) ** 2, axis=1) elif ord == 1: return np.sum(np.abs(x - y), axis=1) elif ord == 0: return np.isclose(x, y).astype(np.float).sum(axis=1) elif ord == np.inf: return np.max(np.abs(x - y), axis=1) else: raise NotImplementedError( 'Norms other than 0, 1, 2, inf not implemented') 
bert-master.run_classifier.ColaProcessor.get|examples|test def get_test_examples(self, data_dir): """See base class.""" return self._create_examples(self._read_tsv(os.path.join(data_dir, 'test.tsv')), 'test') 
optimizers.get|optimizers def get_optimizers(w_list, z_list, losses, args): """ create a tuple of optimizer operations depending on settings. also return global steps to track learning rate decay and w-learning rate for logging :param w_list: weights per layer :param z_list: zs per layer :param losses: tuple of all losses :param args: run arguments :return: """ w_lr, z_lr, global_w_step = get_learning_rates(args) w_optim = get_w_optim_sgd(losses, w_list, w_lr, global_w_step, args) z_optim = get_z_optim_sgd(losses, z_lr, z_list, args) optim_collect = namedtuple('optim', ['w', 'z']) optims = optim_collect(w_optim, z_optim) return optims, global_w_step, w_lr 
gym_pycolab.tests.test_things.TestSprite.update def update(self, actions, board, layers, backdrop, things, the_plot): """This `update` implementation is "final". Do not override.""" pre_update_callable = get_pre_update(self, the_plot) pre_update_callable(actions, board, layers, backdrop, things, the_plot) self.real_update(actions, board, layers, backdrop, things, the_plot) post_update_callable = get_post_update(self, the_plot) post_update_callable(actions, board, layers, backdrop, things, the_plot) 
neural_tangents.predict._eigen_fns.transform def transform(fn): """Generates a transform given a function on the eigenvalues."""  def _(vec, dt): return np.einsum('ji,i,ki,k...->j...', evecs, fn(evals, dt), evecs, vec, optimize=True) return _ 
plato.agent.component.dialogue_policy.deep_learning.supervised_policy.SupervisedPolicy.Policy|Supervised def __init__(self, args): """ Initialize parameters and internal structures  :param args: dictionary containing the policy's arguments """ super(SupervisedPolicy, self).__init__() self.ontology = None if 'ontology' in args: ontology = args['ontology'] if isinstance(ontology, Ontology): self.ontology = ontology else: raise ValueError( 'SupervisedPolicy Unacceptable ontology type %s ' % ontology) else: raise ValueError('SupervisedPolicy: No ontology provided') self.database = None if 'database' in args: database = args['database'] if isinstance(database, DataBase): self.database = database else: raise ValueError( 'SupervisedPolicy: Unacceptable database type %s ' % database) else: raise ValueError('SupervisedPolicy: No database provided') self.agent_id = args['agent_id'] if 'agent_id' in args else 0 self.agent_role = args['agent_role'] if 'agent_role' in args else 'system' domain = args['domain'] if 'domain' in args else None self.IS_GREEDY_POLICY = False self.policy_path = None self.policy_net = None self.tf_scope = 'policy_' + self.agent_role + '_' + str(self.agent_id) self.sess = None self.warmup_policy = None self.warmup_simulator = None self.is_training = True self.informable_slots = deepcopy(list(self.ontology.ontology[ 'informable'].keys())) self.requestable_slots = deepcopy(self.ontology.ontology['requestable'] + ['this', 'signature']) self.system_requestable_slots = deepcopy(self.ontology.ontology[ 'system_requestable']) self.dstc2_acts = None if not domain: self.NStateFeatures = 56 self.dstc2_acts = ['repeat', 'canthelp', 'affirm', 'negate', 'deny', 'ack', 'thankyou', 'bye', 'reqmore', 'hello', 'welcomemsg', 'expl-conf', 'select', 'offer', 'reqalts', 'confirm-domain', 'confirm'] else: if domain in ['SlotFilling', 'CamRest']: d_state = SlotFillingDialogueState({'slots': self. system_requestable_slots}) if domain == 'CamRest': self.dstc2_acts_sys = ['offer', 'canthelp', 'affirm', 'deny', 'ack', 'bye', 'reqmore', 'welcomemsg', 'expl-conf', 'select', 'repeat', 'confirm-domain', 'confirm'] self.dstc2_acts_usr = ['affirm', 'negate', 'deny', 'ack', 'thankyou', 'bye', 'reqmore', 'hello', 'expl-conf', 'repeat', 'reqalts', 'restart', 'confirm'] if self.agent_role == 'system': self.dstc2_acts = self.dstc2_acts_sys elif self.agent_role == 'user': self.dstc2_acts = self.dstc2_acts_usr else: print( 'Warning! domain has not been defined. Using Slot-Filling dialogue State' ) d_state = SlotFillingDialogueState({'slots': self.informable_slots} ) d_state.initialize() self.NStateFeatures = len(self.encode_state(d_state)) print( 'Supervised dialogue policy automatically determined number of state features: {0}' .format(self.NStateFeatures)) if domain == 'CamRest': self.NActions = len(self.dstc2_acts) + len(self.requestable_slots) if self.agent_role == 'system': self.NActions += len(self.system_requestable_slots) else: self.NActions += len(self.requestable_slots) else: self.NActions = 5 self.policy_alpha = 0.05 self.tf_saver = None 
slac.environments.video_wrapper.VideoWrapper.step def _step(self, action): time_step = self._env.step(action) if self._rendering: self._frames.append(self._env.render()) return time_step 
utils.bool|str def str2bool(x): return x.lower() in 'true' 
w1_model.W1.get|networks def get_networks(self): nets = OrderedDict([('phi', self.phi)]) nets['gen'] = self.g return nets 
ml.CV|kfold def kfoldCV(sc, pairs_all, classes_all, embedding_df, clfs, n_run, n_fold, n_proportion, n_seed): scores_df = pd.DataFrame() bc_embedding_df = sc.broadcast(embedding_df) print(type(bc_embedding_df)) for r in range(n_run): n_seed += r random.seed(n_seed) np.random.seed(n_seed) n_proportion = 1 pairs, classes = balance_data(pairs_all, classes_all, n_proportion) skf = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state= n_seed) cv = skf.split(pairs, classes) print('run', r) bc_pairs_classes = sc.broadcast((pairs, classes)) cv_list = [(train, test, k) for k, (train, test) in enumerate(cv)] scores = cvSpark(sc, r, bc_pairs_classes.value[0], bc_pairs_classes .value[1], cv_list, bc_embedding_df.value, clfs) scores_df = scores_df.append(scores) return scores_df 
data_prep_util.load|h def load_h5(h5_filename): f = h5py.File(h5_filename) data = f['data'][:] label = f['label'][:] return data, label 
celebA.gen.wgan_utils.conv|strided|d def conv2d_strided(x, W, b): conv = tf.nn.conv2d(x, W, strides=[1, 2, 2, 1], padding='SAME') return tf.nn.bias_add(conv, b) 
ops.tanh def tanh(x): return tf.tanh(x) 
seld_dcase2019_master.seld.main def main(argv): """ Main wrapper for training sound event localization and detection network.  :param argv: expects two optional inputs. first input: task_id - (optional) To chose the system configuration in parameters.py. (default) 1 - uses default parameters second input: job_id - (optional) all the output files will be uniquely represented with this. (default) 1  """ if len(argv) != 3: print('\n\n') print( '-------------------------------------------------------------------------------------------------------' ) print('The code expected two optional inputs') print('\t>> python seld.py <task-id> <job-id>') print( '\t\t<task-id> is used to choose the user-defined parameter set from parameter.py' ) print('Using default inputs for now') print( '\t\t<job-id> is a unique identifier which is used for output filenames (models, training plots). You can use any number or string for this.' ) print( '-------------------------------------------------------------------------------------------------------' ) print('\n\n') task_id = '1' if len(argv) < 2 else argv[1] params = parameter.get_params(task_id) job_id = 1 if len(argv) < 3 else argv[-1] train_splits, val_splits, test_splits = None, None, None if params['mode'] == 'dev': test_splits = [1, 2, 3, 4] val_splits = [2, 3, 4, 1] train_splits = [[3, 4], [4, 1], [1, 2], [2, 3]] elif params['mode'] == 'eval': test_splits = [0] val_splits = [1] train_splits = [[2, 3, 4]] avg_scores_val = [] avg_scores_test = [] for split_cnt, split in enumerate(test_splits): print( """  ---------------------------------------------------------------------------------------------------""" ) print( '------------------------------------      SPLIT {}   -----------------------------------------------' .format(split)) print( '---------------------------------------------------------------------------------------------------' ) cls_feature_class.create_folder(params['model_dir']) unique_name = '{}_{}_{}_{}_split{}'.format(task_id, job_id, params[ 'dataset'], params['mode'], split) unique_name = os.path.join(params['model_dir'], unique_name) model_name = '{}_model.h5'.format(unique_name) print('unique_name: {}\n'.format(unique_name)) print('Loading training dataset:') data_gen_train = cls_data_generator.DataGenerator(dataset=params[ 'dataset'], split=train_splits[split_cnt], batch_size=params[ 'batch_size'], seq_len=params['sequence_length'], feat_label_dir=params['feat_label_dir']) print('Loading validation dataset:') data_gen_val = cls_data_generator.DataGenerator(dataset=params[ 'dataset'], split=val_splits[split_cnt], batch_size=params[ 'batch_size'], seq_len=params['sequence_length'], feat_label_dir=params['feat_label_dir'], shuffle=False) data_in, data_out = data_gen_train.get_data_sizes() print('FEATURES:\n\tdata_in: {}\n\tdata_out: {}\n'.format(data_in, data_out)) gt = collect_test_labels(data_gen_val, data_out, params['quick_test']) sed_gt = evaluation_metrics.reshape_3Dto2D(gt[0]) doa_gt = evaluation_metrics.reshape_3Dto2D(gt[1]) nb_classes = data_gen_train.get_nb_classes() def_elevation = data_gen_train.get_default_elevation() doa_gt[:, nb_classes:] = doa_gt[:, nb_classes:] / (180.0 / def_elevation) print( """MODEL: dropout_rate: {} CNN: nb_cnn_filt: {}, pool_size{} rnn_size: {}, fnn_size: {} """ .format(params['dropout_rate'], params['nb_cnn2d_filt'], params ['pool_size'], params['rnn_size'], params['fnn_size'])) model = keras_model.get_model(data_in=data_in, data_out=data_out, dropout_rate=params['dropout_rate'], nb_cnn2d_filt=params[ 'nb_cnn2d_filt'], pool_size=params['pool_size'], rnn_size= params['rnn_size'], fnn_size=params['fnn_size'], weights=params ['loss_weights']) best_seld_metric = 99999 best_epoch = -1 patience_cnt = 0 seld_metric = np.zeros(params['nb_epochs']) tr_loss = np.zeros(params['nb_epochs']) val_loss = np.zeros(params['nb_epochs']) doa_metric = np.zeros((params['nb_epochs'], 6)) sed_metric = np.zeros((params['nb_epochs'], 2)) nb_epoch = 2 if params['quick_test'] else params['nb_epochs'] for epoch_cnt in range(nb_epoch): start = time.time() hist = model.fit_generator(generator=data_gen_train.generate(), steps_per_epoch=2 if params['quick_test'] else data_gen_train.get_total_batches_in_data(), validation_data =data_gen_val.generate(), validation_steps=2 if params[ 'quick_test'] else data_gen_val.get_total_batches_in_data(), epochs=params['epochs_per_fit'], verbose=2) tr_loss[epoch_cnt] = hist.history.get('loss')[-1] val_loss[epoch_cnt] = hist.history.get('val_loss')[-1] pred = model.predict_generator(generator=data_gen_val.generate( ), steps=2 if params['quick_test'] else data_gen_val. get_total_batches_in_data(), verbose=2) sed_pred = evaluation_metrics.reshape_3Dto2D(pred[0]) > 0.5 doa_pred = evaluation_metrics.reshape_3Dto2D(pred[1]) doa_pred[:, nb_classes:] = doa_pred[:, nb_classes:] / (180.0 / def_elevation) sed_metric[(epoch_cnt), :] = evaluation_metrics.compute_sed_scores( sed_pred, sed_gt, data_gen_val.nb_frames_1s()) doa_metric[(epoch_cnt), : ] = evaluation_metrics.compute_doa_scores_regr(doa_pred, doa_gt, sed_pred, sed_gt) seld_metric[epoch_cnt] = evaluation_metrics.compute_seld_metric( sed_metric[(epoch_cnt), :], doa_metric[(epoch_cnt), :]) plot_functions(unique_name, tr_loss, val_loss, sed_metric, doa_metric, seld_metric) patience_cnt += 1 if seld_metric[epoch_cnt] < best_seld_metric: best_seld_metric = seld_metric[epoch_cnt] best_epoch = epoch_cnt model.save(model_name) patience_cnt = 0 print( """epoch_cnt: %d, time: %.2fs, tr_loss: %.2f, val_loss: %.2f, ER_overall: %.2f, F1_overall: %.2f, doa_error_pred: %.2f, good_pks_ratio:%.2f, seld_score: %.2f, best_seld_score: %.2f, best_epoch : %d """ % (epoch_cnt, time.time() - start, tr_loss[epoch_cnt], val_loss[epoch_cnt], sed_metric[epoch_cnt, 0], sed_metric[ epoch_cnt, 1], doa_metric[epoch_cnt, 0], doa_metric[ epoch_cnt, 1], seld_metric[epoch_cnt], best_seld_metric, best_epoch)) if patience_cnt > params['patience']: break avg_scores_val.append([sed_metric[best_epoch, 0], sed_metric[ best_epoch, 1], doa_metric[best_epoch, 0], doa_metric[ best_epoch, 1], best_seld_metric]) print('\nResults on validation split:') print('\tUnique_name: {} '.format(unique_name)) print('\tSaved model for the best_epoch: {}'.format(best_epoch)) print('\tSELD_score: {}'.format(best_seld_metric)) print('\tDOA Metrics: DOA_error: {}, frame_recall: {}'.format( doa_metric[best_epoch, 0], doa_metric[best_epoch, 1])) print('\tSED Metrics: ER_overall: {}, F1_overall: {}\n'.format( sed_metric[best_epoch, 0], sed_metric[best_epoch, 1])) print('Loading testing dataset:') data_gen_test = cls_data_generator.DataGenerator(dataset=params[ 'dataset'], split=split, batch_size=params['batch_size'], seq_len=params['sequence_length'], feat_label_dir=params[ 'feat_label_dir'], shuffle=False, per_file=params[ 'dcase_output'], is_eval=True if params['mode'] is 'eval' else False) print( '\nLoading the best model and predicting results on the testing split' ) model = load_model('{}_model.h5'.format(unique_name)) pred_test = model.predict_generator(generator=data_gen_test. generate(), steps=2 if params['quick_test'] else data_gen_test. get_total_batches_in_data(), verbose=2) test_sed_pred = evaluation_metrics.reshape_3Dto2D(pred_test[0]) > 0.5 test_doa_pred = evaluation_metrics.reshape_3Dto2D(pred_test[1]) test_doa_pred[:, nb_classes:] = test_doa_pred[:, nb_classes:] / ( 180.0 / def_elevation) if params['dcase_output']: dcase_dump_folder = os.path.join(params['dcase_dir'], '{}_{}_{}'.format(task_id, params['dataset'], params['mode'])) cls_feature_class.create_folder(dcase_dump_folder) print('Dumping recording-wise results in: {}'.format( dcase_dump_folder)) test_filelist = data_gen_test.get_filelist() max_frames_with_content = data_gen_test.get_nb_frames() frames_per_file = data_gen_test.get_frame_per_file() for file_cnt in range(test_sed_pred.shape[0] // frames_per_file): output_file = os.path.join(dcase_dump_folder, test_filelist [file_cnt].replace('.npy', '.csv')) dc = file_cnt * frames_per_file output_dict = (evaluation_metrics. regression_label_format_to_output_format(data_gen_test, test_sed_pred[dc:dc + max_frames_with_content, :], test_doa_pred[dc:dc + max_frames_with_content, :] * 180 / np.pi)) evaluation_metrics.write_output_format_file(output_file, output_dict) if params['mode'] is 'dev': test_data_in, test_data_out = data_gen_test.get_data_sizes() test_gt = collect_test_labels(data_gen_test, test_data_out, params['quick_test']) test_sed_gt = evaluation_metrics.reshape_3Dto2D(test_gt[0]) test_doa_gt = evaluation_metrics.reshape_3Dto2D(test_gt[1]) test_doa_gt[:, nb_classes:] = test_doa_gt[:, nb_classes:] / ( 180.0 / def_elevation) test_sed_loss = evaluation_metrics.compute_sed_scores(test_sed_pred , test_sed_gt, data_gen_test.nb_frames_1s()) test_doa_loss = evaluation_metrics.compute_doa_scores_regr( test_doa_pred, test_doa_gt, test_sed_pred, test_sed_gt) test_metric_loss = evaluation_metrics.compute_seld_metric( test_sed_loss, test_doa_loss) avg_scores_test.append([test_sed_loss[0], test_sed_loss[1], test_doa_loss[0], test_doa_loss[1], test_metric_loss]) print('Results on test split:') print('\tSELD_score: {},  '.format(test_metric_loss)) print('\tDOA Metrics: DOA_error: {}, frame_recall: {}'.format( test_doa_loss[0], test_doa_loss[1])) print('\tSED Metrics: ER_overall: {}, F1_overall: {}\n'.format( test_sed_loss[0], test_sed_loss[1])) print('\n\nValidation split scores per fold:\n') for cnt in range(len(val_splits)): print( '\tSplit {} - SED ER: {} F1: {}; DOA error: {} frame recall: {}; SELD score: {}' .format(cnt, avg_scores_val[cnt][0], avg_scores_val[cnt][1], avg_scores_val[cnt][2], avg_scores_val[cnt][3], avg_scores_val[ cnt][4])) if params['mode'] is 'dev': print('\n\nTesting split scores per fold:\n') for cnt in range(len(val_splits)): print( '\tSplit {} - SED ER: {} F1: {}; DOA error: {} frame recall: {}; SELD score: {}' .format(cnt, avg_scores_test[cnt][0], avg_scores_test[cnt][ 1], avg_scores_test[cnt][2], avg_scores_test[cnt][3], avg_scores_test[cnt][4])) 
train.sequence|decode def decode_sequence(input_seq): states_value = encoder_model.predict(input_seq) target_seq = np.zeros((1, 1, num_decoder_tokens)) target_seq[0, 0, target_token_index['\t']] = 1.0 stop_condition = False decoded_sentence = '' while not stop_condition: output_tokens, h, c = decoder_model.predict([target_seq] + states_value ) sampled_token_index = np.argmax(output_tokens[(0), (-1), :]) sampled_char = reverse_target_char_index[sampled_token_index] decoded_sentence += sampled_char if sampled_char == '\n' or len(decoded_sentence ) > max_decoder_seq_length: stop_condition = True target_seq = np.zeros((1, 1, num_decoder_tokens)) target_seq[0, 0, sampled_token_index] = 1.0 states_value = [h, c] return decoded_sentence 
deepctr.layers.activation.Dice.call def call(self, inputs, training=None, **kwargs): inputs_normed = self.bn(inputs, training=training) x_p = tf.sigmoid(inputs_normed) return self.alphas * (1.0 - x_p) * inputs + x_p * inputs 
setup_cifar.CIFARModel.__init__.bounded|relu def bounded_relu(x): return K.relu(x, max_value=1) 
plato.dialogue.action.DialogueAct.eq def __eq__(self, other): """ Equality operator.  :param other: the dialogue Act to compare against :return: True of False """ return (self.funcName == other.funcName and self.intent == other.intent and self.name == other.name and [s for s in self.params if s not in other.params] == []) 
AffineCouplingSdnEx2.AffineCouplingSdnEx2.det|jacobian|log|inverse def _inverse_log_det_jacobian(self, z, yy, nlf0=None, nlf1=None, iso=None, cam=None): scale = sdn_model_params_ex2(yy, iso, self.gain_init) if scale is None: return tf.constant(0.0, dtype=z.dtype, name='ildj') return -tf.reduce_sum(tf.log(scale), axis=[1, 2, 3]) 
regression.ops.mvg_optimizer.MVGOptimizer.momentum|init def _init_momentum(self): self._momentum = tf.get_variable(self.w_name + '_momentum', shape=self. shape, initializer=tf.constant_initializer(0.0), trainable=False) 
gpt2_train_main.main.epoch|dev def _dev_epoch(sess): """Evaluates on the dev set. """ iterator.restart_dataset(sess, 'dev') cum_loss = 0.0 cum_ppl = 0.0 nsamples = 0 fetches = {'loss': loss, 'ppl': ppl, 'batch_size': batch_size} while True: try: feed_dict = {iterator.handle: iterator.get_handle(sess, 'dev'), tx.context.global_mode(): tf.estimator.ModeKeys.EVAL} rets = sess.run(fetches, feed_dict) cum_loss += rets['loss'] * rets['batch_size'] cum_ppl += rets['ppl'] * rets['batch_size'] nsamples += rets['batch_size'] except tf.errors.OutOfRangeError: break avg_loss = cum_loss / nsamples avg_ppl = cum_ppl / nsamples tf.logging.info('dev loss: {}; ppl: {}; nsamples: {}'.format(avg_loss, avg_ppl, nsamples)) if FLAGS.do_train and avg_loss < dev_best['loss']: dev_best['loss'] = avg_loss dev_best['ppl'] = avg_ppl ckpt_fn = os.path.join(FLAGS.output_dir, 'model_best.ckpt') ckpt_fn = saver_best.save(sess, ckpt_fn) tf.logging.info('Checkpoint best to {}'.format(ckpt_fn)) 
utils.utils.seed|random|set def set_random_seed(myseed): tf.set_random_seed(myseed) np.random.seed(myseed) random.seed(myseed) 
test_policy.TestGoalConditionedPolicy.test|init def test_init(self): """Validate that the graph and variables are initialized properly.""" policy = GoalConditionedPolicy(**self.policy_params) self.assertEqual(policy.meta_period, self.policy_params['meta_period']) self.assertEqual(policy.relative_goals, self.policy_params[ 'relative_goals']) self.assertEqual(policy.off_policy_corrections, self.policy_params[ 'off_policy_corrections']) self.assertEqual(policy.use_fingerprints, self.policy_params[ 'use_fingerprints']) self.assertEqual(policy.centralized_value_functions, self.policy_params ['centralized_value_functions']) self.assertEqual(policy.connected_gradients, self.policy_params[ 'connected_gradients']) self.assertListEqual(sorted([var.name for var in get_trainable_vars()]), ['Manager/model/pi/fc0/bias:0', 'Manager/model/pi/fc0/kernel:0', 'Manager/model/pi/fc1/bias:0', 'Manager/model/pi/fc1/kernel:0', 'Manager/model/pi/output/bias:0', 'Manager/model/pi/output/kernel:0', 'Manager/model/qf_0/fc0/bias:0', 'Manager/model/qf_0/fc0/kernel:0', 'Manager/model/qf_0/fc1/bias:0', 'Manager/model/qf_0/fc1/kernel:0', 'Manager/model/qf_0/qf_output/bias:0', 'Manager/model/qf_0/qf_output/kernel:0', 'Manager/model/qf_1/fc0/bias:0', 'Manager/model/qf_1/fc0/kernel:0', 'Manager/model/qf_1/fc1/bias:0', 'Manager/model/qf_1/fc1/kernel:0', 'Manager/model/qf_1/qf_output/bias:0', 'Manager/model/qf_1/qf_output/kernel:0', 'Manager/target/pi/fc0/bias:0', 'Manager/target/pi/fc0/kernel:0', 'Manager/target/pi/fc1/bias:0', 'Manager/target/pi/fc1/kernel:0', 'Manager/target/pi/output/bias:0', 'Manager/target/pi/output/kernel:0', 'Manager/target/qf_0/fc0/bias:0', 'Manager/target/qf_0/fc0/kernel:0', 'Manager/target/qf_0/fc1/bias:0', 'Manager/target/qf_0/fc1/kernel:0', 'Manager/target/qf_0/qf_output/bias:0', 'Manager/target/qf_0/qf_output/kernel:0', 'Manager/target/qf_1/fc0/bias:0', 'Manager/target/qf_1/fc0/kernel:0', 'Manager/target/qf_1/fc1/bias:0', 'Manager/target/qf_1/fc1/kernel:0', 'Manager/target/qf_1/qf_output/bias:0', 'Manager/target/qf_1/qf_output/kernel:0', 'Worker/model/pi/fc0/bias:0', 'Worker/model/pi/fc0/kernel:0', 'Worker/model/pi/fc1/bias:0', 'Worker/model/pi/fc1/kernel:0', 'Worker/model/pi/output/bias:0', 'Worker/model/pi/output/kernel:0', 'Worker/model/qf_0/fc0/bias:0', 'Worker/model/qf_0/fc0/kernel:0', 'Worker/model/qf_0/fc1/bias:0', 'Worker/model/qf_0/fc1/kernel:0', 'Worker/model/qf_0/qf_output/bias:0', 'Worker/model/qf_0/qf_output/kernel:0', 'Worker/model/qf_1/fc0/bias:0', 'Worker/model/qf_1/fc0/kernel:0', 'Worker/model/qf_1/fc1/bias:0', 'Worker/model/qf_1/fc1/kernel:0', 'Worker/model/qf_1/qf_output/bias:0', 'Worker/model/qf_1/qf_output/kernel:0', 'Worker/target/pi/fc0/bias:0', 'Worker/target/pi/fc0/kernel:0', 'Worker/target/pi/fc1/bias:0', 'Worker/target/pi/fc1/kernel:0', 'Worker/target/pi/output/bias:0', 'Worker/target/pi/output/kernel:0', 'Worker/target/qf_0/fc0/bias:0', 'Worker/target/qf_0/fc0/kernel:0', 'Worker/target/qf_0/fc1/bias:0', 'Worker/target/qf_0/fc1/kernel:0', 'Worker/target/qf_0/qf_output/bias:0', 'Worker/target/qf_0/qf_output/kernel:0', 'Worker/target/qf_1/fc0/bias:0', 'Worker/target/qf_1/fc0/kernel:0', 'Worker/target/qf_1/fc1/bias:0', 'Worker/target/qf_1/fc1/kernel:0', 'Worker/target/qf_1/qf_output/bias:0', 'Worker/target/qf_1/qf_output/kernel:0']) self.assertAlmostEqual(policy.worker_reward_fn(states=np.array([1, 2, 3 ]), goals=np.array([3, 2, 1]), next_states=np.array([0, 0, 0])), - 3.7416573867873044) tf.compat.v1.reset_default_graph() 
utils.ElapsedTimer.time|elapsed|print def print_elapsed_time(self): print('Speed: %s ' % self.elapsed(self.elapsed_time())) 
utils.file_operation.file|retain def retain_file(path, tagger, retain_file): files = glob.glob(os.path.join(path, tagger + '*')) for _file in files: if not _file.endswith(retain_file): os.remove(_file) 
pytorch_pretrained_bert.modeling.BertPredictionHeadTransform.Transform|Prediction|Head|Bert def __init__(self, config): super(BertPredictionHeadTransform, self).__init__() self.dense = nn.Linear(config.hidden_size, config.hidden_size) if isinstance(config.hidden_act, str) or sys.version_info[0 ] == 2 and isinstance(config.hidden_act, unicode): self.transform_act_fn = ACT2FN[config.hidden_act] else: self.transform_act_fn = config.hidden_act self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12) 
svae_dc.utils.bo_plot_results.all|plot|algos def plot_all_algos(env_name, algos, y_lims, prefix, baselines_prefix=None, nruns=50, ntrials=22, ninit=2, names=None, markers=None, colors=None, debug=False): if names is None: names = NAMES if markers is None: markers = MARKERS if colors is None: colors = COLORS if baselines_prefix is None: baselines_prefix = prefix hs = [] labels = [] fig = plt.figure(figsize=(10, 10)) for algo_id, algo in enumerate(algos): not_baseline = 'SVAE' in algo or 'KL' in algo prfx = os.path.expanduser(prefix if not_baseline else baselines_prefix) y_all_runs = [] rn = -1 while len(y_all_runs) < nruns: rn += 1 assert rn < 100 algo_str = 'output_' + env_name + '_' + algo + '_UCB1.0_run' + str( rn) fname = os.path.join(prfx, algo_str, 'x_y_all_run' + str(rn) + '.npz') print('looking for ', fname) if not os.path.exists(fname): continue print('loading', fname) data = np.load(fname) y_all_runs.append(data['y_all'][0:ntrials].squeeze()) y_all_runs = np.vstack(y_all_runs) labels.append(names[algo]) hs.append(plot_algo(y_all_runs, marker=markers[algo], color=colors[ algo], label=names[algo], ylims=y_lims)) xticks = [] xlbls = [] for i in range(ninit): if ntrials < 20 or i > 0: xticks.append(i) xlbls.append('init\n' + str(i + 1)) for i in range(y_all_runs.shape[1]): if ntrials < 20 or (i + 1) % 5 == 0: xticks.append(i + ninit) xlbls.append(str(i + 1)) plt.xticks(xticks, xlbls, rotation=0, fontsize=25) if debug: print(algo, 'y_all_runs', y_all_runs) plt.xlabel('trial', fontsize=25, labelpad=-20) plt.ylabel('best reward so far', fontsize=30) loc = 'upper left' if ntrials <= 20 else 'lower right' plt.legend(hs, labels, loc=loc, fontsize=20) plt.tight_layout() fig.savefig('bo_plot_' + env_name + '.png') plt.close(fig) 
train_noise_flow.thread|sample def sample_thread(thr_id, niter, sess, ts_batch_que, loss, sd_z, x, x_sample, y, nlf0, nlf1, iso, cam, is_training, sample_epoch_loss_que, sd_z_que, kldiv_que, requeue=False, sc_sd=1, epoch=0): is_fix = True iso_vals = [100, 400, 800, 1600, 3200] iso_fix = [100] cam_fix = [['IP', 'GP', 'S6', 'N6', 'G4'].index('S6')] nlf_s6 = [[0.000479, 2e-06], [0.001774, 2e-06], [0.003696, 2e-06], [ 0.008211, 2e-06], [0.01993, 2e-06]] for k in range(niter): ts_mb_dict = ts_batch_que.get() _x = ts_mb_dict['_x'] _y = ts_mb_dict['_y'] if is_fix: _iso = iso_fix _cam = cam_fix _nlf0 = [nlf_s6[iso_vals.index(iso_fix[0])][0]] _nlf1 = [nlf_s6[iso_vals.index(iso_fix[0])][0]] else: _iso = ts_mb_dict['iso'] _cam = ts_mb_dict['cam'] _nlf0 = ts_mb_dict['nlf0'] _nlf1 = ts_mb_dict['nlf1'] x_sample_val = sess.run(x_sample, feed_dict={y: _y, nlf0: _nlf0, nlf1: _nlf1, iso: _iso, cam: _cam, is_training: False}) kldiv3 = kl_div_3_data(_x, x_sample_val) sample_loss, sd_z_val = sess.run([loss, sd_z], feed_dict={x: x_sample_val, y: _y, nlf0: _nlf0, nlf1: _nlf1, iso: _iso, cam: _cam, is_training: False}) vis_mbs_dir = os.path.join(hps.logdir, 'samples_epoch_%04d' % epoch, 'samples_%.1f' % hps.temp) kldiv3 = calc_kldiv_mb(ts_mb_dict, x_sample_val, vis_mbs_dir, sc_sd) if requeue: ts_batch_que.put(ts_mb_dict) sample_epoch_loss_que.put(sample_loss) sd_z_que.put(sd_z_val) kldiv_que.put(kldiv3) 
plyfile.PlyElement.count @property def count(self): return self._count 
texar.modules.encoders.bert_encoders_test.BertEncoderTest.test|trainable|variables def test_trainable_variables(self): """Tests the functionality of automatically collecting trainable variables. """ inputs = tf.placeholder(dtype=tf.int32, shape=[None, None]) encoder = BertEncoder() _, _ = encoder(inputs) self.assertEqual(len(encoder.trainable_variables), 3 + 2 + 12 * 16 + 2) hparams = {'pretrained_model_name': 'bert-large-uncased'} encoder = BertEncoder(hparams=hparams) _, _ = encoder(inputs) self.assertEqual(len(encoder.trainable_variables), 3 + 2 + 24 * 16 + 2) hparams = {'encoder': {'num_blocks': 6}, 'pretrained_model_name': None} encoder = BertEncoder(hparams=hparams) _, _ = encoder(inputs) self.assertEqual(len(encoder.trainable_variables), 3 + 2 + 6 * 16 + 2) 
utils.Rto|MI|SN def SNRtoMI(N, effSNR, constellation): N = int(N) SNRlin = 10 ** (effSNR / 10) constellation = constellation / np.sqrt(np.mean(np.abs(constellation) ** 2) ) M = constellation.size x_id = np.random.randint(0, M, (N,)) x = constellation[:, (x_id)] z = 1 / np.sqrt(2) * (np.random.normal(size=x.shape) + 1.0j * np.random .normal(size=x.shape)) y = x + z * np.sqrt(1 / SNRlin) return calcMI_MC(x, y, constellation) 
src.util.FileUtil.write|File def writeFile(context, filename, append=False): if not append: with open(filename, 'w+') as fout: for co in context: fout.write(co + '\n') else: with open(filename, 'a+') as fout: for co in context: fout.write(co + '\n') 
texar.core.layers_test.MergeLayerTest.test|trainable|variables def test_trainable_variables(self): """Test the trainable_variables of the layer. """ layers_ = [] layers_.append(tf.layers.Conv1D(filters=200, kernel_size=3)) layers_.append(tf.layers.Conv1D(filters=200, kernel_size=4)) layers_.append(tf.layers.Conv1D(filters=200, kernel_size=5)) layers_.append(tf.layers.Dense(200)) layers_.append(tf.layers.Dense(200)) m_layer = layers.MergeLayer(layers_) inputs = tf.zeros([64, 16, 1024], dtype=tf.float32) _ = m_layer(inputs) num_vars = sum([len(layer.trainable_variables) for layer in layers_]) self.assertEqual(num_vars, len(m_layer.trainable_variables)) 
indoor3d_util.bounding|point|box|collect def collect_point_bounding_box(anno_path, out_filename, file_format): """ Compute bounding boxes from each instance in original dataset files on one room. **We assume the bbox is aligned with XYZ coordinate.** Save both the point XYZRGB and the bounding box for the point's parent element.  Args: anno_path: path to annotations. e.g. Area_1/office_2/Annotations/ out_filename: path to save instance bounding boxes for each point, plus the point's XYZRGBL each line is XYZRGBL offsetX offsetY offsetZ a b c, where cx = X+offsetX, cy=X+offsetY, cz=Z+offsetZ where (cx,cy,cz) is center of the box, a,b,c are distances from center to the surfaces of the box, i.e. x1 = cx-a, x2 = cx+a, y1=cy-b etc. file_format: output file format, txt or numpy Returns: None  Note: room points are shifted, the most negative point is now at origin. """ point_bbox_list = [] for f in glob.glob(os.path.join(anno_path, '*.txt')): cls = os.path.basename(f).split('_')[0] if cls not in g_classes: cls = 'clutter' points = np.loadtxt(f) label = g_class2label[cls] xyz_min = np.amin(points[:, 0:3], axis=0) xyz_max = np.amax(points[:, 0:3], axis=0) xyz_center = (xyz_min + xyz_max) / 2 dimension = (xyz_max - xyz_min) / 2 xyz_offsets = xyz_center - points[:, 0:3] dimensions = np.ones((points.shape[0], 3)) * dimension labels = np.ones((points.shape[0], 1)) * label point_bbox_list.append(np.concatenate([points, labels, xyz_offsets, dimensions], 1)) point_bbox = np.concatenate(point_bbox_list, 0) room_xyz_min = np.amin(point_bbox[:, 0:3], axis=0) point_bbox[:, 0:3] -= room_xyz_min if file_format == 'txt': fout = open(out_filename, 'w') for i in range(point_bbox.shape[0]): fout.write('%f %f %f %d %d %d %d %f %f %f %f %f %f\n' % ( point_bbox[i, 0], point_bbox[i, 1], point_bbox[i, 2], point_bbox[i, 3], point_bbox[i, 4], point_bbox[i, 5], point_bbox[i, 6], point_bbox[i, 7], point_bbox[i, 8], point_bbox[i, 9], point_bbox[i, 10], point_bbox[i, 11], point_bbox[i, 12])) fout.close() elif file_format == 'numpy': np.save(out_filename, point_bbox) else: print('ERROR!! Unknown file format: %s, please use txt or numpy.' % file_format) exit() 
thumt.layers.attention.attention|multihead def multihead_attention(queries, memories, bias, num_heads, key_size, value_size, output_size, keep_prob=None, output=True, state=None, summary=True, dtype=None, scope=None, max_relative_dis=None): """ Multi-head scaled-dot-product attention with input/output transformations.  :param queries: A tensor with shape [batch, length_q, depth_q] :param memories: A tensor with shape [batch, length_m, depth_m] :param bias: A tensor (see attention_bias) :param num_heads: An integer dividing key_size and value_size :param key_size: An integer :param value_size: An integer :param output_size: An integer :param keep_prob: A floating point number in (0, 1] :param output: Whether to use output transformation :param state: An optional dictionary used for incremental decoding :param summary: Use image summary :param dtype: An optional instance of tf.DType :param scope: An optional string :param max_relative_dis: An integer  :returns: A dict with the following keys: weights: A tensor with shape [batch, heads, length_q, length_kv] outputs: A tensor with shape [batch, length_q, depth_v] """ if key_size % num_heads != 0: raise ValueError( 'Key size (%d) must be divisible by the number of attention heads (%d).' % (key_size, num_heads)) if value_size % num_heads != 0: raise ValueError( 'Value size (%d) must be divisible by the number of attention heads (%d).' % (value_size, num_heads)) with tf.variable_scope(scope, default_name='multihead_attention', values=[queries, memories], dtype=dtype): next_state = {} if memories is None: size = key_size * 2 + value_size combined = linear(queries, size, True, True, scope='qkv_transform') q, k, v = tf.split(combined, [key_size, key_size, value_size], axis=-1) if state is not None: k = tf.concat([state['key'], k], axis=1) v = tf.concat([state['value'], v], axis=1) next_state['key'] = k next_state['value'] = v else: q = linear(queries, key_size, True, True, scope='q_transform') combined = linear(memories, key_size + value_size, True, scope= 'kv_transform') k, v = tf.split(combined, [key_size, value_size], axis=-1) q = split_heads(q, num_heads) k = split_heads(k, num_heads) v = split_heads(v, num_heads) length_q = tf.shape(q)[2] length_kv = tf.shape(k)[2] key_depth_per_head = key_size // num_heads q *= key_depth_per_head ** -0.5 if max_relative_dis and memories is None: rpr_k = tf.get_variable('rpr_k', [2 * max_relative_dis + 1, key_size // num_heads]) rpr_v = tf.get_variable('rpr_v', [2 * max_relative_dis + 1, value_size // num_heads]) rpr_k = create_rpr(rpr_k, length_q, length_kv, max_relative_dis) rpr_v = create_rpr(rpr_v, length_q, length_kv, max_relative_dis) rpr = {'rpr_k': rpr_k, 'rpr_v': rpr_v} results = multiplicative_attention(q, k, v, bias, keep_prob, rpr=rpr) else: results = multiplicative_attention(q, k, v, bias, keep_prob) weights = results['weights'] x = combine_heads(results['outputs']) if output: outputs = linear(x, output_size, True, True, scope= 'output_transform') else: outputs = x if should_generate_summaries() and summary: attention_image_summary(weights) outputs = {'weights': weights, 'outputs': outputs} if state is not None: outputs['state'] = next_state return outputs 
gym_pycolab.envs.pycolab_grid_worlds_env.PycolabGridWorldsLevel14Env.Grid|Worlds|Pycolab|Level|Env def __init__(self): super(PycolabGridWorldsEnv, self).__init__() 
texar.modules.encoders.multihead_attention.MultiheadAttentionEncoder.build def _build(self, queries, memory, memory_attention_bias, cache=None, mode=None ): """Encodes the inputs.  Args: queries: A 3d tensor with shape of [batch, length_query, depth_query]. memory: A 3d tensor with shape of [batch, length_key, depth_key]. memory_attention_bias: A 3d tensor with shape of [batch, length_key, num_units]. cache: Memory cache only when inferencing the sentence from sractch. mode (optional): A tensor taking value in :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, including `TRAIN`, `EVAL` and `PREDICT`. Controls dropout mode. If `None` (default), :func:`texar.global_mode` is used.  Returns: A Tensor of shape `[batch_size, max_time, dim]` containing the encoded vectors. """ with tf.variable_scope(self.variable_scope): num_heads = self._hparams.num_heads num_units = self._hparams.num_units if num_units % num_heads: raise ValueError( 'Value depth (%d) must be divisible by the number of attention heads (%d).' % (num_units, num_heads))  def _update_and_return(layer, key): if memory is None: out = layer(queries) if cache is not None: key = 'self_{}'.format(key) res = cache[key] if isinstance(res, tf.TensorArray): res = res.write(res.size(), tf.squeeze(out, axis=[1])) out = transpose_batch_time(res.stack()) else: res = tf.concat([res, out], axis=1) out = res cache[key] = res elif cache is not None: key = 'memory_{}'.format(key) res = cache[key] if isinstance(res, tf.TensorArray): size = res.size() false_fn = lambda : transpose_batch_time(res.stack()) else: size = tf.shape(res)[1] false_fn = lambda : res out = tf.cond(tf.equal(size, 0), true_fn=lambda : layer( memory), false_fn=false_fn) else: out = layer(memory) return out Q = self.Q_dense(queries) K = _update_and_return(self.K_dense, 'keys') V = _update_and_return(self.V_dense, 'values') Q_ = self._split_heads(Q) K_ = self._split_heads(K) V_ = self._split_heads(V) key_depth_per_head = num_units // num_heads Q_ *= key_depth_per_head ** -0.5 logits = tf.matmul(Q_, K_, transpose_b=True) if memory_attention_bias is not None: logits += memory_attention_bias weights = tf.nn.softmax(logits, name='attention_weights') weights = tf.layers.dropout(weights, rate=self._hparams. dropout_rate, training=is_train_mode(mode)) outputs = tf.matmul(weights, V_) outputs = self._combine_heads(outputs) outputs = self.O_dense(outputs) if not self._built: self._add_internal_trainable_variables() self._built = True return outputs 
t_sgan_sn_celeba_128.data|generator def data_generator(batch_size=32): X = [] while True: np.random.shuffle(imgs) for f in imgs: X.append(imread(f)) if len(X) == batch_size: X = np.array(X) yield X X = [] 
graphsage.utils.load|data def load_data(prefix, normalize=True, load_walks=False): G_data = json.load(open(prefix + '-G.json')) G = json_graph.node_link_graph(G_data) if isinstance(G.nodes()[0], int): conversion = lambda n: int(n) else: conversion = lambda n: n if os.path.exists(prefix + '-feats.npy'): feats = np.load(prefix + '-feats.npy') else: print('No features present.. Only identity features will be used.') feats = None id_map = json.load(open(prefix + '-id_map.json')) id_map = {conversion(k): int(v) for k, v in id_map.items()} walks = [] class_map = json.load(open(prefix + '-class_map.json')) if isinstance(list(class_map.values())[0], list): lab_conversion = lambda n: n else: lab_conversion = lambda n: int(n) class_map = {conversion(k): lab_conversion(v) for k, v in class_map.items() } broken_count = 0 for node in G.nodes(): if not 'val' in G.node[node] or not 'test' in G.node[node]: G.remove_node(node) broken_count += 1 print( 'Removed {:d} nodes that lacked proper annotations due to networkx versioning issues' .format(broken_count)) print('Loaded data.. now preprocessing..') for edge in G.edges(): if G.node[edge[0]]['val'] or G.node[edge[1]]['val'] or G.node[edge[0]][ 'test'] or G.node[edge[1]]['test']: G[edge[0]][edge[1]]['train_removed'] = True else: G[edge[0]][edge[1]]['train_removed'] = False if normalize and not feats is None: from sklearn.preprocessing import StandardScaler train_ids = np.array([id_map[n] for n in G.nodes() if not G.node[n] ['val'] and not G.node[n]['test']]) train_feats = feats[train_ids] scaler = StandardScaler() scaler.fit(train_feats) feats = scaler.transform(feats) if load_walks: with open(prefix + '-walks.txt') as fp: for line in fp: walks.append(map(conversion, line.split())) return G, feats, id_map, walks, class_map 
seed_rl-master.grpc.python.ops_test.OpsTest.two|test|clients @parameterized.parameters(([], False), ([1], True), ([2], True)) def test_two_clients(self, dim, batched): address = self.get_unix_address() server = ops.Server([address])  @tf.function(input_signature=[tf.TensorSpec(dim, tf.int32)]) def foo(x): return x + 1 server.bind(foo, batched=batched) server.start() client = ops.Client(address) client2 = ops.Client(address) with futures.ThreadPoolExecutor(max_workers=2) as executor: f0 = executor.submit(client.foo, 42) f1 = executor.submit(client2.foo, 44) self.assertAllEqual(43, f0.result()) self.assertAllEqual(45, f1.result()) server.shutdown() 
classification.ops.estimator.FisherEstimator.op|kfac|re|scale|init def re_init_kfac_scale_op(self): reinit_ops = [factor.init_kfac_scale_factor_op() for factor in self. _layers.get_scale_factors()] return control_flow_ops.group(*reinit_ops) 
deepctr.contrib.rnn._rnn_step.maybe|through|copy|some def _maybe_copy_some_through(): """Run RNN step.  Pass through either no or some past state.""" new_output, new_state = call_cell() nest.assert_same_structure(state, new_state) flat_new_state = nest.flatten(new_state) flat_new_output = nest.flatten(new_output) return control_flow_ops.cond(time < min_sequence_length, lambda : flat_new_output + flat_new_state, lambda : _copy_some_through( flat_new_output, flat_new_state)) 
nets.cyclegan_test.CycleganTest.four|multiple|test|of|width|if|not|error def test_error_if_width_not_multiple_of_four_width31(self): self._error_if_width_not_multiple_of_four_helper(31) 
data_utils.Dataset.print_bow.decode|set def _decode_set(s, shared): output = [] for si in s: if si in shared: output.append('[' + self.id2word[si] + ']') else: output.append(self.id2word[si]) return 
gym_pycolab.tests.maze_walker_test.MazeWalkerTest.testNotConfinedToBoard.check|positions def check_positions(actions, board, layers, backdrop, things, the_plot): del actions, board, layers, backdrop self.assertEqual(the_plot['machinima_args'][0], things['P'].position) self.assertEqual(the_plot['machinima_args'][1], things['P']. virtual_position) 
ant_maze_env.AntMazeEnv.get|ori def get_ori(self): """Return the orientation of the ant.""" return self.wrapped_env.get_ori() 
facenet-master.contributed.cluster.align|data def align_data(image_list, image_size, margin, pnet, rnet, onet): minsize = 20 threshold = [0.6, 0.7, 0.7] factor = 0.709 img_list = [] for x in xrange(len(image_list)): img_size = np.asarray(image_list[x].shape)[0:2] bounding_boxes, _ = align.detect_face.detect_face(image_list[x], minsize, pnet, rnet, onet, threshold, factor) nrof_samples = len(bounding_boxes) if nrof_samples > 0: for i in xrange(nrof_samples): if bounding_boxes[i][4] > 0.95: det = np.squeeze(bounding_boxes[(i), 0:4]) bb = np.zeros(4, dtype=np.int32) bb[0] = np.maximum(det[0] - margin / 2, 0) bb[1] = np.maximum(det[1] - margin / 2, 0) bb[2] = np.minimum(det[2] + margin / 2, img_size[1]) bb[3] = np.minimum(det[3] + margin / 2, img_size[0]) cropped = image_list[x][bb[1]:bb[3], bb[0]:bb[2], :] aligned = misc.imresize(cropped, (image_size, image_size), interp='bilinear') prewhitened = facenet.prewhiten(aligned) img_list.append(prewhitened) if len(img_list) > 0: images = np.stack(img_list) return images else: return None 
GPSig.training.save_snapshot.run def run(self, ctx): current_iter = ctx.iteration self.save_dict[current_iter] = {} self.save_dict[current_iter]['time'] = ctx.time_spent + self.start_time likelihood = ctx.session.run(self.model.likelihood_tensor) self.save_dict[current_iter]['elbo'] = likelihood print('ELBO: {:.2f}'.format(likelihood), end='\t|\t') if self.save_params: save_trainables = {} for param in self.model.trainable_parameters: save_trainables[param.pathname] = ctx.session.run(param. constrained_tensor) self.save_dict[current_iter]['params'] = save_trainables if self.val_scorer is not None: val_score = self.val_scorer(self.model) print('Val. accuracy: {:.3f}'.format(val_score), end='\t|\t') self.save_dict[current_iter]['val'] = val_score print() 
models.NodeApplyModule.forward def forward(self, node): h = self.linear(node.data['h']) h = self.activation(h) return {'h': h} 
datasets.market1501.split|train|to|read|str def read_train_split_to_str(dataset_dir): """Read training data to list of filenames.  Parameters ---------- dataset_dir : str Path to the Market 1501 dataset directory.  Returns ------- (List[str], List[int], List[int]) Returns a tuple with the following values:  * List of image filenames (full path to image files). * List of unique IDs for the individuals in the images. * List of camera indices.  """ filenames, ids, camera_indices = [], [], [] image_dir = os.path.join(dataset_dir, 'bounding_box_train') for filename in sorted(os.listdir(image_dir)): meta_data = _parse_filename(filename) if meta_data is None: continue filenames.append(os.path.join(image_dir, filename)) ids.append(meta_data[0]) camera_indices.append(meta_data[1]) return filenames, ids, camera_indices 
cpplint.Check|Comma|Spacing def CheckCommaSpacing(filename, clean_lines, linenum, error): """Checks for horizontal spacing near commas and semicolons.  Args: filename: The name of the current file. clean_lines: A CleansedLines instance containing the file. linenum: The number of the line to check. error: The function to call with any errors found. """ raw = clean_lines.lines_without_raw_strings line = clean_lines.elided[linenum] if Search(',[^,\\s]', ReplaceAll('\\boperator\\s*,\\s*\\(', 'F(', line) ) and Search(',[^,\\s]', raw[linenum]): error(filename, linenum, 'whitespace/comma', 3, 'Missing space after ,' ) if Search(';[^\\s};\\\\)/]', line): error(filename, linenum, 'whitespace/semicolon', 3, 'Missing space after ;') 
xlnet-master.modeling.embedding|positional def positional_embedding(pos_seq, inv_freq, bsz=None): sinusoid_inp = tf.einsum('i,d->id', pos_seq, inv_freq) pos_emb = tf.concat([tf.sin(sinusoid_inp), tf.cos(sinusoid_inp)], -1) pos_emb = pos_emb[:, (None), :] if bsz is not None: pos_emb = tf.tile(pos_emb, [1, bsz, 1]) return pos_emb 
pytorch_pretrained_bert.file_utils.s|request def s3_request(func): """ Wrapper function for s3 requests in order to create more helpful error messages. """  @wraps(func) def wrapper(url, *args, **kwargs): try: return func(url, *args, **kwargs) except ClientError as exc: if int(exc.response['Error']['Code']) == 404: raise EnvironmentError('file {} not found'.format(url)) else: raise return wrapper 
translate.utils.warn def warn(msg): log(msg, level=logging.WARN) 
translate.models.attention_execution_decoder.look def look(time, state, input_, prev_weights=None, pos=None, context=None): prev_weights_ = [(prev_weights if i == align_encoder_id else None) for i in range(len(encoders))] pos_ = None if decoder.pred_edits: pos_ = [(pos if i == align_encoder_id else None) for i in range(len (encoders))] if decoder.attn_prev_word: state = tf.concat([state, input_], axis=1) if decoder.attn_prev_attn and context is not None: state = tf.concat([state, context], axis=1) if decoder.hidden_state_scaling: attention_states_ = [(states * decoder.hidden_state_scaling) for states in attention_states] else: attention_states_ = attention_states parameters = dict(hidden_states=attention_states_, encoder_input_length =encoder_input_length, encoders=encoders, aggregation_method= decoder.aggregation_method) context, new_weights = multi_attention(state, time=time, pos=pos_, prev_weights=prev_weights_, **parameters) if decoder.context_mapping: with tf.variable_scope(scope_name): activation = (tf.nn.tanh if decoder.context_mapping_activation == 'tanh' else None) use_bias = not decoder.context_mapping_no_bias context = dense(context, decoder.context_mapping, use_bias= use_bias, activation=activation, name='context_mapping') return context, new_weights[align_encoder_id] 
run.lr|finetune def lr_finetune(trial, version, retrain_epochs, density, base): rewind_point = 863 * (10 - retrain_epochs) path = _get_path('lr_finetune', trial, version, 'retrain_{}/density_{}' .format(retrain_epochs, density)) base = os.path.join(base, trial) _create_initial_state_at_checkpoint(path, base, rewind_point) run(path, **{'--lottery_pruning_method': 'prune_all_to_global_{}'. format(density), '--lottery_reset_to': '{}/lottery/checkpoint_iter_final'.format(base), '--lottery_prune_at': '{}/lottery/checkpoint_iter_final'.format( base), '--lottery_reset_global_step_to': '{}/lottery/checkpoint_iter_{}'.format(base, rewind_point), '--max_train_epochs': 10}) 
latent_plots.make_interactive_plot.on|click def on_click(click): global last_sample if click.xdata != None and click.ydata != None and click.inaxes == ax[0]: z1 = click.xdata z2 = click.ydata dream = decoder.predict(np.array([[z1, z2]])) pred, entropy, bald = model.get_results(dream) print('Predicted Class: {}, prob: {}'.format(pred.argmax(axis=1), pred.max(axis=1))) print('Predictive Entropy: {}'.format(entropy[0])) print('MI Score:         {}'.format(bald[0])) proj.set_data(dream.squeeze()) print(z1, z2) plt.draw() last_sample = dream 
run_eval.eval|run def run_eval(params): graph_config = params['graph_config'] data_config = params['data_config'] attack_config = params['attack_config'] attack_params = params['attack_params'] batch_size = data_config['batch_size'] gpu_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True)) session = tf.Session(config=gpu_config) logger = logging.getLogger('logger') network_fn = nets_factory.get_network_fn(graph_config['model'], num_classes=graph_config['num_classes'], is_training=False) eval_image_size = network_fn.default_image_size preprocessing_name = graph_config['model'] image_preprocessing_fn = preprocessing_factory.get_preprocessing( preprocessing_name, is_training=False) data_loader = DataLoader(data_config, image_preprocessing_fn, eval_image_size) logits, normal_network = network_fn(data_loader.input_images) net_varlist = [v for v in tf.get_collection(tf.GraphKeys.VARIABLES)] saver = tf.train.Saver(var_list=net_varlist) saver.restore(session, graph_config['checkpoint_file']) input_placeholder = tf.placeholder(shape=[None, eval_image_size, eval_image_size, 3], dtype=tf.float32, name='Input_placeholder') logits_frm_placeholder, network_frm_placeholder = network_fn( input_placeholder, reuse=tf.AUTO_REUSE) logger.info('Loaded the networks') clever_model = chm.CallableModelWrapper(network_fn, 'logits') attacker_class = getattr(attacks, attack_config['attack_name']) attacker = attacker_class(clever_model, sess=session) if attack_config['attack_type'] == 'GT': outputs = tf.placeholder(shape=[None, graph_config['num_classes']], dtype=tf.float32, name='GT_placeholder') adversarial_x_tensor = attacker.generate(data_loader.input_images, y=outputs, **attack_params) elif attack_config['attack_type'] == 'self': adversarial_x_tensor = attacker.generate(data_loader.input_images, **attack_params) logger.info('Loaded the attack') im_list = open(data_config['image_list']).readlines() gt_list = open(data_config['gt_list']).readlines() im_list = [x.strip() for x in im_list] gt_list = [x.strip() for x in gt_list] total_iter = len(im_list) / batch_size metrics = metric_functions.init_metrics() for i in range(int(total_iter)): if i % 10 == 0: logger.info('iter' + str(i) + ' of ' + str(total_iter)) im_batch = im_list[i * batch_size:(i + 1) * batch_size] gt_real = gt_list[i * batch_size:(i + 1) * batch_size] gt_real = [int(x) for x in gt_real] gt_batch = utils.get_gt_batch(gt_real, graph_config) if attack_config['attack_type'] == 'GT': feed_dict = {data_loader.image_path: im_batch, outputs: gt_batch} elif attack_config['attack_type'] == 'self': feed_dict = {data_loader.image_path: im_batch} else: raise ValueError('attack type has to be GT or self') adversarial_x = session.run(adversarial_x_tensor, feed_dict=feed_dict) feed_dict = {data_loader.image_path: im_batch, input_placeholder: adversarial_x} normal_out, adv_out = session.run([logits, logits_frm_placeholder], feed_dict=feed_dict) metrics = metric_functions.update_metrics(metrics, normal_out, adv_out, gt_real, graph_config['offset']) normal_out = metrics['normal_prediction'][-1] adv_out = metrics['adv_prediction'][-1] log_txt = ' '.join(['Adversarial prediction', str(adv_out), 'true_predictions', str(normal_out), 'true_gt ', str(gt_real[-1])]) logger.info(log_txt) logger.info('======== ITER ' + str(i) + '========') logger.info('Real Top-1 Accuracy = {:.2f}'.format(metrics[ 'real_acc'][-1])) logger.info('Top-1 Accuracy = {:.2f}'.format(metrics['adv_acc'][-1])) logger.info('Fooling Rate = {:.2f}'.format(metrics['fr'][-1])) logger.info('Old Label New Rank = {:.2f}'.format(np.mean(metrics[ 'old_label_rank_new']))) logger.info('New Label Old Rank = {:.2f}'.format(np.mean(metrics[ 'new_label_rank_old']))) if i % 10 == 0: normal_x = session.run(data_loader.input_images, feed_dict) pert = normal_x - adversarial_x log_txt = ' '.join(['Image min max', str(np.max(normal_x)), str (np.min(normal_x))]) logger.info(log_txt) log_txt = ' '.join(['Perturbation min max', str(np.max(pert)), str(np.min(pert))]) logger.info(log_txt) result = open(params['results_file'], 'w') result.write('Real Top-1 Accuracy = {:.4f}'.format(metrics['real_acc'][ -1]) + '\n') result.write('Top-1 Accuracy = {:.4f}'.format(metrics['adv_acc'][-1]) + '\n') result.write('Fooling Rate = {:.4f}'.format(metrics['fr'][-1]) + '\n') result.write('Old Label New Rank = {:.4f}'.format(np.mean(metrics[ 'old_label_rank_new'])) + '\n') result.write('New Label Old Rank = {:.4f}'.format(np.mean(metrics[ 'new_label_rank_old'])) + '\n') result.close() 
controller.Controller.eval|lm def eval_lm(self, model, dset, sess, mode, decoding_mode='greedy'): print('Evaluating the language model ... ') start_time = time.time() batch_size = self.batch_size model_name = self.model_name num_batches = dset.num_batches(batch_size, mode) print('%d batches in total' % num_batches) metrics_dict = {'loss': [], 'ppl': []} for bi in range(num_batches): batch_dict = dset.next_batch(mode, batch_size, model_name) output_dict = model.eval_step(sess, batch_dict) for m in output_dict: if m in metrics_dict: metrics_dict[m].append(-output_dict[m]) for m in metrics_dict: metrics_dict[m] = np.average(metrics_dict[m]) print('%s: %.4f' % (m, metrics_dict[m])) print('time cost: %.2fs' % (time.time() - start_time)) print('') return metrics_dict 
SMILESX_utils.DataSequence.Sequence|Data def __init__(self, smiles_set, vocab, max_length, props_set, batch_size, soft_padding=False): self.smiles_set = smiles_set self.vocab = vocab self.max_length = max_length self.props_set = props_set self.batch_size = batch_size self.iepoch = 0 self.soft_padding = soft_padding 
classification.ops.fisher_blocks.EigenCorrectedConvKFCBasicFB.minibatches|num|registered @property def num_registered_minibatches(self): return len(self._inputs) 
bow_seq2seq.sample|topk|enc|with def _sample_topk_with_enc(batch_ind, batch_prob, enc_inputs, sample_size, stop_words, source_sample_ratio): """add the content words from the original sentence""" enc_sample_size = int(source_sample_ratio * sample_size) topk_sample_size = sample_size - enc_sample_size sample_ind = [] for ei, bi, bp in zip(enc_inputs, batch_ind, batch_prob): enc_bow = list(set(list(ei)) - stop_words) if len(enc_bow) < enc_sample_size: enc_sample_size = len(enc_bow) topk_sample_size = sample_size - enc_sample_size enc_sample = np.random.choice(a=enc_bow, size=enc_sample_size, replace=False) topk_sample = np.random.choice(a=bi, size=topk_sample_size, replace =False, p=bp) sample = list(enc_sample) + list(topk_sample) sample_ind.append(sample) return np.array(sample_ind) 
data_utils.init def init(max_length=cnf.bin_max_len): train_set.clear() test_set.clear() for some_task in cnf.all_tasks: train_set[some_task] = [] test_set[some_task] = [] for all_max_len in range(max_length): train_set[some_task].append([]) test_set[some_task].append([]) 
official.utils.accelerator.tpu.construct_scalar_host_call.host|fn|call def host_call_fn(global_step, *args): """Training host call. Creates scalar summaries for training metrics.  This function is executed on the CPU and should not directly reference any Tensors in the rest of the `model_fn`. To pass Tensors from the model to the `metric_fn`, provide as part of the `host_call`. See https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec for more information.  Arguments should match the list of `Tensor` objects passed as the second element in the tuple passed to `host_call`.  Args: global_step: `Tensor with shape `[batch]` for the global_step *args: Remaining tensors to log.  Returns: List of summary ops to run on the CPU host. """ step = global_step[0] with tf.contrib.summary.create_file_writer(logdir=model_dir, filename_suffix='.host_call').as_default(): with tf.contrib.summary.always_record_summaries(): for i, name in enumerate(metric_names): tf.contrib.summary.scalar(prefix + name, args[i][0], step=step) return tf.contrib.summary.all_summary_ops() 
pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLModel.mems|init def init_mems(self, data): if self.mem_len > 0: mems = [] param = next(self.parameters()) for i in range(self.n_layer): empty = torch.zeros(self.mem_len, data.size(1), self.config. d_model, dtype=param.dtype, device=param.device) mems.append(empty) return mems else: return None 
attacks.parsimonious_attack.ParsimoniousAttack.perturb def perturb(self, image, label, index, sess): """Perturb an image.  Args: image: numpy array of size [1, 32, 32, 3], an original image label: numpy array of size [1], the label of the image (or target label) index: int, the index of the image sess: TensorFlow session  Returns: adv_image: numpy array of size [1, 32, 32, 3], an adversarial image num_queries: int, the number of queries success: bool, True if attack is successful """ np.random.seed(index) adv_image = np.copy(image) num_queries = 0 block_size = self.block_size upper_left = [0, 0] lower_right = [32, 32] blocks = self._split_block(upper_left, lower_right, block_size) noise = -self.epsilon * np.ones_like(image, dtype=np.int32) num_blocks = len(blocks) batch_size = self.batch_size if self.batch_size > 0 else num_blocks curr_order = np.random.permutation(num_blocks) while True: num_batches = int(math.ceil(num_blocks / batch_size)) for i in range(num_batches): bstart = i * batch_size bend = min(bstart + batch_size, num_blocks) blocks_batch = [blocks[curr_order[idx]] for idx in range(bstart, bend)] noise, queries, loss, success = self.local_search.perturb(image, noise, label, sess, blocks_batch) num_queries += queries tf.logging.info( 'Block size: {}, batch: {}, loss: {:.4f}, num queries: {}'. format(block_size, i, loss, num_queries)) if num_queries > self.max_queries: return adv_image, num_queries, False adv_image = self._perturb_image(image, noise) if success: return adv_image, num_queries, True if not self.no_hier and block_size >= 2: block_size //= 2 blocks = self._split_block(upper_left, lower_right, block_size) num_blocks = len(blocks) batch_size = self.batch_size if self.batch_size > 0 else num_blocks curr_order = np.random.permutation(num_blocks) else: curr_order = np.random.permutation(num_blocks) 
borealisflows.layers.BatchNorm.det|jacobian|log|inverse def _inverse_log_det_jacobian(self, y): return tf.cond(tf.equal(self.is_training, tf.constant(True)), lambda : self._log_det_jacobian(y, True), lambda : self._log_det_jacobian(y, False)) 
data.eval|file|save def save_eval_file(opt, stats, eval_type='losses', split='dev', ext='pickle'): if cfg.test_save: name = '{}/{}.{}'.format(utils.make_name(opt, prefix='garbage/{}/'. format(eval_type), is_dir=True, eval_=True), split, ext) else: name = '{}/{}.{}'.format(utils.make_name(opt, prefix='results/{}/'. format(eval_type), is_dir=True, eval_=True), split, ext) print('Saving {} {} to {}'.format(split, eval_type, name)) if ext == 'pickle': with open(name, 'wb') as f: pickle.dump(stats, f) elif ext == 'txt': with open(name, 'w') as f: f.write(stats) elif ext == 'json': with open(name, 'w') as f: json.dump(stats, f) else: raise 
iterative_nvidia.main def main(): parser = argparse.ArgumentParser() parser.add_argument('--trial', required=True) parser.add_argument('--version', required=True) parser.add_argument('--retrain-epochs', required=True) parser.add_argument('--base-dir', required=True) parser.add_argument('--method', required=True) args = parser.parse_args() run_iterative(args.method, args.trial, args.version, args. retrain_epochs, args.base_dir) 
model.PCGN_beamsearch.maybe|check def _check_maybe(t): if isinstance(t, tensor_array_ops.TensorArray): raise TypeError( 'TensorArray state is not supported by PCGNBeamSearchDecoder: %s' % t.name) if t.shape.ndims is None: raise ValueError( 'Expected tensor (%s) to have known rank, but ndims == None.' % t) 
neural_style.main def main(): global args args = parse_args() if args.video: render_video() else: render_single_image() 
AffineCouplingGainEx3.AffineCouplingGainEx3.det|jacobian|log|inverse def _inverse_log_det_jacobian(self, z, yy, nlf0=None, nlf1=None, iso=None, cam=None): scale = gain_model_params_ex3(iso) if scale is None: return tf.constant(0.0, dtype=z.dtype, name='ildj') return -tf.log(scale) 
thumt.utils.weight_ratio.weight|maxpool|ratio def weight_ratio_maxpool(input, output, maxnum, flatten=False): """ inputs: (..., dim) output: (..., dim/maxpart) weight_ratios: (..., dim, dim/maxnum) """ maxnum = tf.constant(maxnum, dtype=tf.int32) weight_shape = tf.concat([tf.shape(input), tf.shape(output)[-1:]], axis=-1) input = tf.reshape(input, [-1, input.shape[-1].value]) output = tf.reshape(output, [-1, output.shape[-1].value]) shape_inp = tf.shape(input) batch = shape_inp[0] dim_in = shape_inp[-1] shape = tf.concat([shape_inp[:-1], [shape_inp[-1] // maxnum, maxnum]], axis=0) dim_out = shape[-2] value = tf.reshape(input, shape) pos = tf.argmax(value, -1) pos = tf.cast(pos, tf.int32) pos = tf.reshape(pos, [-1]) if flatten: indices = tf.range(tf.shape(pos)[0]) * maxnum + pos weight_ratio = tf.sparse_to_dense(indices, [batch * dim_in], tf. ones(tf.shape(indices))) weight_ratio = tf.reshape(weight_ratio, weight_shape[:-1]) else: indices = dim_out * pos + dim_in * tf.range(batch * dim_out, dtype= tf.int32) indices = tf.reshape(indices, [-1, dim_out]) indices += tf.expand_dims(tf.range(dim_out, dtype=tf.int32), 0) indices = tf.reshape(indices, [-1]) weight_ratio = tf.sparse_to_dense(indices, [batch * dim_in * dim_out], tf.ones(tf.shape(indices))) weight_ratio = tf.reshape(weight_ratio, weight_shape) return weight_ratio 
extract_template_cpnet.tempalte|extract def extract_tempalte(): """ Reads original conceptnet csv file and extracts all English relations (head and tail are both English entities) into a new file, with the following format for each line: <relation> <head> <tail> <weight>. :return: """ config = configparser.ConfigParser() config.read('paths.cfg') only_english = [] with open(config['paths']['conceptnet'], encoding='utf8') as f: for line in f.readlines(): ls = line.split('\t') if ls[2].startswith('/c/en/') and ls[3].startswith('/c/en/'): """ Some preprocessing: - Remove part-of-speech encoding. - Split("/")[-1] to trim the "/c/en/" and just get the entity name, convert all to - Lowercase for uniformity. """ rel = ls[1].split('/')[-1].lower() head = del_pos(ls[2]).split('/')[-1].lower() tail = del_pos(ls[3]).split('/')[-1].lower() data = json.loads(ls[4]) if 'surfaceText' in data: subj = data['surfaceStart'].lower() obj = data['surfaceEnd'].lower() surface = data['surfaceText'].lower() temp = surface.replace('[[%s]]' % subj, '#SUBJ#').replace( '[[%s]]' % obj, '#OBJ#') temp = temp.replace('[', '').replace(']', '') if '#SUBJ#' not in temp or '#OBJ#' not in temp: continue weight = data['weight'] if rel not in rel_templates_dict: rel_templates_dict[rel] = dict() if temp not in rel_templates_dict[rel]: rel_templates_dict[rel][temp] = 0.0 rel_templates_dict[rel][temp] += weight with open(config['paths']['conceptnet_en'], 'w', encoding='utf8') as f: f.write('\n'.join(only_english)) 
interpolation_main.model|build def build_model(batch, train_data, lambdas): """ This function is basically the same as build_model() in baseline_seq2seq_attn.py, except the InterpolateDecoder and InterpolateHelper. """ batch_size = tf.shape(batch['target_length'])[0] source_embedder = tx.modules.WordEmbedder(vocab_size=train_data. source_vocab.size, hparams=config_model.embedder) encoder = tx.modules.BidirectionalRNNEncoder(hparams=config_model.encoder) enc_outputs, _ = encoder(source_embedder(batch['source_text_ids'])) target_embedder = tx.modules.WordEmbedder(vocab_size=train_data. target_vocab.size, hparams=config_model.embedder) decoder = InterpolationDecoder(memory=tf.concat(enc_outputs, axis=2), memory_sequence_length=batch['source_length'], vocab_size= train_data.target_vocab.size, hparams=config_model.decoder) start_tokens = tf.ones_like(batch['target_length'] ) * train_data.target_vocab.bos_token_id helper = InterpolationHelper(embedding=target_embedder, start_tokens= start_tokens, end_token=train_data.target_vocab.eos_token_id, reward_metric=config_data.eval_metric, vocab=train_data. target_vocab, ground_truth=batch['target_text_ids'][:, 1:], ground_truth_length=batch['target_length'] - 1, lambdas=lambdas) training_outputs, _, training_length = decoder(helper=helper, initial_state=decoder.zero_state(batch_size=batch_size, dtype=tf. float32), max_decoding_length=60) train_op = tx.core.get_train_op(tx.losses. sequence_sparse_softmax_cross_entropy(labels=training_outputs. sample_id, logits=training_outputs.logits, sequence_length= training_length), hparams=config_model.opt) beam_search_outputs, _, _ = tx.modules.beam_search_decode(decoder_or_cell =decoder, embedding=target_embedder, start_tokens=start_tokens, end_token=train_data.target_vocab.eos_token_id, beam_width= config_model.beam_width, max_decoding_length=60) return train_op, beam_search_outputs 
graphsage.layers.Layer.call def _call(self, inputs): return inputs 
functions.help|category|print def print_category_help(data): print('') if data == 'atomic': print('Enter a possible effect type from the following effect types:') print( 'all - compute the output for all effect types {{oEffect, oReact, oWant, xAttr, xEffect, xIntent, xNeed, xReact, xWant}}' ) print( 'oEffect - generate the effect of the event on participants other than PersonX' ) print( 'oReact - generate the reactions of participants other than PersonX to the event' ) print( 'oEffect - generate what participants other than PersonX may want after the event' ) elif data == 'conceptnet': print('Enter a possible relation from the following list:') print('') print('AtLocation') print('CapableOf') print('Causes') print('CausesDesire') print('CreatedBy') print('DefinedAs') print('DesireOf') print('Desires') print('HasA') print('HasFirstSubevent') print('HasLastSubevent') print('HasPainCharacter') print('HasPainIntensity') print('HasPrerequisite') print('HasProperty') print('HasSubevent') print('InheritsFrom') print('InstanceOf') print('IsA') print('LocatedNear') print('LocationOfAction') print('MadeOf') print('MotivatedByGoal') print('NotCapableOf') print('NotDesires') print('NotHasA') print('NotHasProperty') print('NotIsA') print('NotMadeOf') print('PartOf') print('ReceivesAction') print('RelatedTo') print('SymbolOf') print('UsedFor') print('') print('NOTE: Capitalization is important') else: raise print('') 
eranlayers.eran|dense def eran_dense(inputs, units, activation='relu', name=None): """ adds a dense layer to the graph, including bias  Arguments --------- inputs : tf.Tensor the preceding layer units : int the number of neurons this dense layer maps to activation : str one of ['relu', 'sigmoid', 'tanh'], default is 'relu' name : str optional name for the non-linearity operation at the end  Return ------ output : tf.Tensor tensor associated with the non-linearity operation """ tmp = eran_affine(inputs, units) return eran_activation(tmp, activation, name=name) 
deepctr.layers.sequence.BiasEncoding.call def call(self, inputs, mask=None): """ :param concated_embeds_value: None * field_size * embedding_size :return: None*1 """ transformer_out = [] for i in range(self.sess_max_count): transformer_out.append(inputs[i] + self.item_bias_embedding + self. seq_bias_embedding + self.sess_bias_embedding[i]) return transformer_out 
darkflow.net.ops.convolution.local.speak def speak(self): l = self.lay args = [l.ksize] * 2 + [l.pad] + [l.stride] args += [l.activation] msg = 'loca {}x{}p{}_{}  {}'.format(*args) return msg 
poisson-performance.PoissonLIF.poisson|math|step def _poisson_step_math(self, dt, rates, spiked): spiked[...] = 0 next_spikes = np.zeros_like(spiked) to_process = np.ones_like(spiked, dtype=bool) while np.any(to_process): next_spikes[to_process] += self._sample_exponential(rates[to_process]) to_process &= next_spikes < dt spiked[to_process] += self.amplitude / dt 
test_darkflow.Ov|CLI|test|SAVEPB|YOL def test_CLI_SAVEPB_YOLOv2(): testString = ( 'flow --model {0} --load {1} --config {2} --threshold 0.4 --savepb' .format(yolo_CfgPath, yolo_WeightPath, generalConfigPath)) with pytest.raises(SystemExit): executeCLI(testString) assert os.path.exists(pbPath ), 'Expected output .pb file: {0} was not found.'.format(pbPath) assert os.path.exists(metaPath ), 'Expected output .meta file: {0} was not found.'.format(metaPath) 
AffineCouplingFitSdnGain2.AffineCouplingFitSdnGain2.forward def _forward(self, x, yy, nlf0=None, nlf1=None, iso=None, cam=None): if self._last_layer: x = tf.reshape(x, (-1, self.i0, self.i1, self.ic)) yy = tf.reshape(yy, (-1, self.i0, self.i1, self.ic)) if yy.shape[1] == 2 * x.shape[1]: yy = squeeze2d(yy, 2) beta1, beta2 = sdn_iso_model_params_2(iso) scale = tf.sqrt(beta1 * yy + beta2) shift = 0.0 y = x if scale is not None: y *= scale if shift is not None: y += shift return y 
fasterai.unet.CustomPixelShuffle_ICNR.ICNR|Pixel|Custom|Shuffle def __init__(self, ni: int, nf: int=None, scale: int=2, blur: bool=False, leaky: float=None, **kwargs): super().__init__() nf = ifnone(nf, ni) self.conv = custom_conv_layer(ni, nf * scale ** 2, ks=1, use_activ= False, **kwargs) icnr(self.conv[0].weight) self.shuf = nn.PixelShuffle(scale) self.pad = nn.ReplicationPad2d((1, 0, 1, 0)) self.blur = nn.AvgPool2d(2, stride=1) self.relu = relu(True, leaky=leaky) 
SPH3D_modelnet.normalize|xyz def normalize_xyz(points): points -= tf.reduce_mean(points, axis=1, keepdims=True) scale = tf.reduce_max(tf.reduce_sum(tf.square(points), axis=-1, keepdims=True), axis=1, keepdims=True) scale = tf.sqrt(scale, name='normalize') points /= scale return points 
vae_train._main.generate def _generate(sess, saver, fname=None): if tf.train.checkpoint_exists(FLAGS.model): saver.restore(sess, FLAGS.model) else: raise ValueError('cannot find checkpoint model') batch_size = train_data.batch_size dst = tfd.MultivariateNormalDiag(loc=tf.zeros([batch_size, config. latent_dims]), scale_diag=tf.ones([batch_size, config.latent_dims])) dcdr_states, latent_z = connector_stoch(dst) vocab = train_data.vocab start_tokens = tf.ones(batch_size, tf.int32) * vocab.bos_token_id end_token = vocab.eos_token_id if config.decoder_type == 'lstm':  def _cat_embedder(ids): """Concatenates latent variable to input word embeddings """ embedding = decoder_w_embedder(ids) return tf.concat([embedding, latent_z], axis=1) outputs, _, _ = decoder(initial_state=dcdr_states, decoding_strategy='infer_sample', embedding=_cat_embedder, max_decoding_length=100, start_tokens=start_tokens, end_token= end_token) else:  def _embedding_fn(ids, times): w_embed = decoder_w_embedder(ids) p_embed = decoder_p_embedder(times) return w_embed * config.hidden_size ** 0.5 + p_embed outputs, _ = decoder(memory=dcdr_states, decoding_strategy= 'infer_sample', memory_sequence_length=tf.ones(tf.shape( dcdr_states)[0]), embedding=_embedding_fn, max_decoding_length= 100, start_tokens=start_tokens, end_token=end_token) sample_tokens = vocab.map_ids_to_tokens(outputs.sample_id) sess.run(tf.tables_initializer()) feed = {tx.global_mode(): tf.estimator.ModeKeys.PREDICT} sample_tokens_ = sess.run(sample_tokens, feed_dict=feed) if fname is None: fh = sys.stdout else: fh = open(fname, 'w', encoding='utf-8') for sent in sample_tokens_: sent = tx.utils.compat_as_text(list(sent)) end_id = len(sent) if vocab.eos_token in sent: end_id = sent.index(vocab.eos_token) fh.write(' '.join(sent[:end_id + 1]) + '\n') print('Output done') fh.close() 
hbaselines.goal_conditioned.policy.FeedForwardPolicy.get|td|map def get_td_map(self): """See parent class.""" if not self.replay_buffer.can_sample(): return {} obs0, actions, rewards, obs1, terminals1 = self.replay_buffer.sample() return self.get_td_map_from_batch(obs0, actions, rewards, obs1, terminals1) 
layers.AbstractLayer.backward def backward(self, x, p): """Backward propagation of MSA co-state eqn  Arguments: x {tf tensor} -- state at t p {tf tensor} -- co-state at t+1  Returns: tf tensor -- costate p at t """ x = tf.stop_gradient(x) p = tf.stop_gradient(p) H = tf.reduce_sum(p * self.forward(x)) p_next = tf.gradients(H, x)[0] return p_next 
cpplint.Close|Expression|Reverse def ReverseCloseExpression(clean_lines, linenum, pos): """If input points to ) or } or ] or >, finds the position that opens it.  If lines[linenum][pos] points to a ')' or '}' or ']' or '>', finds the linenum/pos that correspond to the opening of the expression.  Args: clean_lines: A CleansedLines instance containing the file. linenum: The number of the line to check. pos: A position on the line.  Returns: A tuple (line, linenum, pos) pointer *at* the opening brace, or (line, 0, -1) if we never find the matching opening brace.  Note we ignore strings and comments when matching; and the line we return is the 'cleansed' line at linenum. """ line = clean_lines.elided[linenum] if line[pos] not in ')}]>': return line, 0, -1 start_pos, stack = FindStartOfExpressionInLine(line, pos, []) if start_pos > -1: return line, linenum, start_pos while stack and linenum > 0: linenum -= 1 line = clean_lines.elided[linenum] start_pos, stack = FindStartOfExpressionInLine(line, len(line) - 1, stack) if start_pos > -1: return line, linenum, start_pos return line, 0, -1 
word_chatbot.WordChatbot.samples|generate def generate_samples(self, data_dir, tmp_dir, data_split): """ The function assumes that if you have data at one level of the pipeline, you don't want to re-generate it, so for example if the 4 txt files exist, the function continues by generating the t2t-datagen format files. So if you want to re-download or re-generate data, you have to delete it first from the appropriate directories.  Params: :data_dir: Directory where the data will be generated The raw data has to be downloaded one directory level higher. :data_split: Which data split to generate samples for. """ self.data_dir = data_dir print('t2t_csaky_log: ' + self.mode[data_split] + ' data generation activated.') sPath = os.path.join(data_dir, self.mode[data_split] + 'Source.txt') tPath = os.path.join(data_dir, self.mode[data_split] + 'Target.txt') with tf.gfile.GFile(sPath, mode='r') as source_file: with tf.gfile.GFile(tPath, mode='r') as target_file: source, target = source_file.readline(), target_file.readline() while source and target: yield {'inputs': source.strip(), 'targets': target.strip()} source, target = source_file.readline(), target_file.readline() 
texar.modules.decoders.beam_search_decode_test.BeamSearchDecodeTest.attention|beam|test|decoder|search def test_attention_decoder_beam_search(self): """Tests beam search with RNNAttentionDecoder. """ seq_length = np.random.randint(self._max_time, size=[self._batch_size]) + 1 encoder_values_length = tf.constant(seq_length) hparams = {'attention': {'kwargs': {'num_units': self._attention_dim}}, 'rnn_cell': {'kwargs': {'num_units': self._cell_dim}}} decoder = tx.modules.AttentionRNNDecoder(vocab_size=self._vocab_size, memory=self._encoder_output, memory_sequence_length= encoder_values_length, hparams=hparams) self._test_beam_search(decoder) 
text_gcn-master.models.Model.loss def _loss(self): raise NotImplementedError 
kaffe.transformers.ReLUFuser.pair|eligible|is def is_eligible_pair(self, parent, child): return (self.allowed_parent_types is None or parent.kind in self. allowed_parent_types) and child.kind == NodeKind.ReLU 
predict_test.PredictTest.Momentum|test|Prediction|NTK @jtu.parameterized.named_parameters(jtu.cases_from_list({'testcase_name': '_train={}_test={}_network={}_logits={}_{}'.format(train, test, network, out_logits, name), 'train_shape': train, 'test_shape': test, 'network': network, 'out_logits': out_logits, 'fn_and_kernel': fn} for train, test, network in zip(TRAIN_SHAPES, TEST_SHAPES, NETWORK) for out_logits in OUTPUT_LOGITS for name, fn in KERNELS.items() if len(train) == 2)) def testNTKMomentumPrediction(self, train_shape, test_shape, network, out_logits, fn_and_kernel): key = random.PRNGKey(0) key, split = random.split(key) x_train = random.normal(split, train_shape) key, split = random.split(key) y_train = np.array(random.bernoulli(split, shape=(train_shape[0], out_logits)), np.float32) key, split = random.split(key) x_test = random.normal(split, test_shape) params, f, ntk = fn_and_kernel(key, train_shape[1:], network, out_logits) loss = lambda y, y_hat: 0.5 * np.mean((y - y_hat) ** 2) grad_loss = jit(grad(lambda params, x: loss(f(params, x), y_train))) g_dd = ntk(x_train, None, 'ntk') g_td = ntk(x_test, x_train, 'ntk') atol = ATOL rtol = RTOL step_size = 0.5 if len(train_shape) > 2: atol = ATOL * 2 rtol = RTOL * 2 step_size = 0.1 train_time = 100.0 steps = int(train_time / np.sqrt(step_size)) init, predictor, get = predict.momentum(g_dd, y_train, loss, step_size, g_td) opt_init, opt_update, get_params = momentum(step_size, 0.9) opt_state = opt_init(params) fx_initial_train = f(params, x_train) fx_initial_test = f(params, x_test) lin_state = init(fx_initial_train, fx_initial_test) fx_pred_train, fx_pred_test = get(lin_state) self.assertAllClose(fx_initial_train, fx_pred_train, True) self.assertAllClose(fx_initial_test, fx_pred_test, True) for i in range(steps): params = get_params(opt_state) opt_state = opt_update(i, grad_loss(params, x_train), opt_state) params = get_params(opt_state) fx_train = f(params, x_train) fx_test = f(params, x_test) lin_state = predictor(lin_state, train_time) fx_pred_train, fx_pred_test = get(lin_state) fx_disp_train = np.sqrt(np.mean((fx_train - fx_initial_train) ** 2)) fx_disp_test = np.sqrt(np.mean((fx_test - fx_initial_test) ** 2)) fx_error_train = (fx_train - fx_pred_train) / fx_disp_train fx_error_test = (fx_test - fx_pred_test) / fx_disp_test self.assertAllClose(fx_error_train, np.zeros_like(fx_error_train), True, rtol, atol) self.assertAllClose(fx_error_test, np.zeros_like(fx_error_test), True, rtol, atol) 
avod.core.losses.WeightedL2LocalizationLoss.loss|compute def _compute_loss(self, prediction_tensor, target_tensor, weights): """Compute loss function. Args: prediction_tensor: A float tensor of shape [batch_size, num_anchors, code_size] representing the (encoded) predicted locations of objects. target_tensor: A float tensor of shape [batch_size, num_anchors, code_size] representing the regression targets weights: a float tensor of shape [batch_size, num_anchors]  Returns: loss: a (scalar) tensor representing the value of the loss function or a float tensor of shape [batch_size, num_anchors] """ weighted_diff = (prediction_tensor - target_tensor) * tf.expand_dims( weights, 2) square_diff = 0.5 * tf.square(weighted_diff) return tf.reduce_sum(square_diff) 
deeppoly_nodes.DeeppolyReluNodeLast.Last|Relu|Node|Deeppoly def __init__(self, weights, bias, relu_present, input_names, output_name, output_shape): """ Arguments --------- weights : numpy.ndarray matrix of the fully connected layer (must be 2D) bias : numpy.ndarray bias of the fully connected layer relu_present : bool whether this layer has relu or not """ DeeppolyNode.__init__(self, weights, bias, input_names, output_name, output_shape) self.relu_present = relu_present 
official.resnet.imagenet_test.BaseTest.tear|Down def tearDown(self): super(BaseTest, self).tearDown() tf.gfile.DeleteRecursively(self.get_temp_dir()) 
AffineCouplingCondY.AffineCouplingCondY.log|and|inverse|det|jacobian def _inverse_and_log_det_jacobian(self, y, yy, nlf0=None, nlf1=None, iso= None, cam=None): shift, log_scale = self._shift_and_log_scale_fn(yy) tf.summary.histogram('cY shift', shift) tf.summary.histogram('cY log_scale', log_scale) log_scale = self.scale * tf.tanh(log_scale) x = y if log_scale is not None: x *= tf.exp(log_scale) if shift is not None: x += shift if log_scale is None: log_abs_det_J_inv = tf.constant(0.0, dtype=y.dtype, name='ildj') else: log_abs_det_J_inv = tf.reduce_sum(log_scale, axis=[1, 2, 3]) if self._last_layer: return tf.layers.flatten(x), log_abs_det_J_inv return x, log_abs_det_J_inv 
model.PCGN_beamsearch.search|beam|step def _beam_search_step(real_vocab_size, time, logits, next_cell_state, beam_state, batch_size, beam_width, end_token, length_penalty_weight): """Performs a single step of Beam Search Decoding.  Args: time: Beam search time step, should start at 0. At time 0 we assume that all beams are equal and consider only the first beam for continuations. logits: Logits at the current time step. A tensor of shape `[batch_size, beam_width, vocab_size]` next_cell_state: The next state from the cell, e.g. an instance of AttentionWrapperState if the cell is attentional. beam_state: Current state of the beam search. An instance of `BeamSearchDecoderState`. batch_size: The batch size for this input. beam_width: Python int.  The size of the beams. end_token: The int32 end token. length_penalty_weight: Float weight to penalize length. Disabled with 0.0.  Returns: A new beam state. """ static_batch_size = tensor_util.constant_value(batch_size) prediction_lengths = beam_state.lengths previously_finished = beam_state.finished step_log_probs = nn_ops.log_softmax(logits) step_log_probs = _mask_probs(step_log_probs, end_token, previously_finished ) total_probs = array_ops.expand_dims(beam_state.log_probs, 2 ) + step_log_probs vocab_size = logits.shape[-1].value or array_ops.shape(logits)[-1] lengths_to_add = array_ops.one_hot(indices=array_ops.tile(array_ops. reshape(end_token, [1, 1]), [batch_size, beam_width]), depth= vocab_size, on_value=constant_op.constant(0, dtype=dtypes.int64), off_value=constant_op.constant(1, dtype=dtypes.int64), dtype=dtypes .int64) add_mask = 1 - math_ops.to_int64(previously_finished) lengths_to_add = array_ops.expand_dims(add_mask, 2) * lengths_to_add new_prediction_lengths = lengths_to_add + array_ops.expand_dims( prediction_lengths, 2) scores = _get_scores(log_probs=total_probs, sequence_lengths= new_prediction_lengths, length_penalty_weight=length_penalty_weight) time = ops.convert_to_tensor(time, name='time') scores_shape = array_ops.shape(scores) scores_flat = control_flow_ops.cond(time > 0, lambda : array_ops. reshape(scores, [batch_size, -1]), lambda : scores[:, (0)]) num_available_beam = control_flow_ops.cond(time > 0, lambda : math_ops. reduce_prod(scores_shape[1:]), lambda : math_ops.reduce_prod( scores_shape[2:])) next_beam_size = math_ops.minimum(ops.convert_to_tensor(beam_width, dtype=dtypes.int32, name='beam_width'), num_available_beam) next_beam_scores, word_indices = nn_ops.top_k(scores_flat, k=next_beam_size ) next_beam_scores.set_shape([static_batch_size, beam_width]) word_indices.set_shape([static_batch_size, beam_width]) next_beam_probs = _tensor_gather_helper(gather_indices=word_indices, gather_from=total_probs, batch_size=batch_size, range_size= beam_width * vocab_size, gather_shape=[-1], name='next_beam_probs') raw_next_word_ids = math_ops.mod(word_indices, vocab_size, name= 'next_beam_word_ids') next_word_ids = math_ops.to_int32(raw_next_word_ids) next_word_ids = next_word_ids % real_vocab_size next_beam_ids = math_ops.to_int32(word_indices / vocab_size, name= 'next_beam_parent_ids') previously_finished = _tensor_gather_helper(gather_indices= next_beam_ids, gather_from=previously_finished, batch_size= batch_size, range_size=beam_width, gather_shape=[-1]) next_finished = math_ops.logical_or(previously_finished, math_ops.equal (next_word_ids, end_token), name='next_beam_finished') lengths_to_add = math_ops.to_int64(math_ops.not_equal(next_word_ids, end_token)) lengths_to_add = (1 - math_ops.to_int64(next_finished)) * lengths_to_add next_prediction_len = _tensor_gather_helper(gather_indices= next_beam_ids, gather_from=beam_state.lengths, batch_size= batch_size, range_size=beam_width, gather_shape=[-1]) next_prediction_len += lengths_to_add next_cell_state = nest.map_structure(lambda gather_from: _maybe_tensor_gather_helper(gather_indices=next_beam_ids, gather_from=gather_from, batch_size=batch_size, range_size= beam_width, gather_shape=[batch_size * beam_width, -1]), next_cell_state) next_state = BeamSearchDecoderState(cell_state=next_cell_state, log_probs=next_beam_probs, lengths=next_prediction_len, finished= next_finished) output = BeamSearchDecoderOutput(scores=next_beam_scores, predicted_ids =next_word_ids, parent_ids=next_beam_ids) return output, next_state 
xlnet-master.data_utils.split|b|and|a def _split_a_and_b(data, sent_ids, begin_idx, tot_len, extend_target=False): """Split two segments from `data` starting from the index `begin_idx`.""" data_len = data.shape[0] if begin_idx + tot_len >= data_len: tf.logging.info( '[_split_a_and_b] returns None: begin_idx %d + tot_len %d >= data_len %d' , begin_idx, tot_len, data_len) return None end_idx = begin_idx + 1 cut_points = [] while end_idx < data_len: if sent_ids[end_idx] != sent_ids[end_idx - 1]: if end_idx - begin_idx >= tot_len: break cut_points.append(end_idx) end_idx += 1 a_begin = begin_idx if len(cut_points) == 0 or random.random() < 0.5: label = 0 if len(cut_points) == 0: a_end = end_idx else: a_end = random.choice(cut_points) b_len = max(1, tot_len - (a_end - a_begin)) b_begin = random.randint(0, data_len - 1 - b_len) b_end = b_begin + b_len while b_begin > 0 and sent_ids[b_begin - 1] == sent_ids[b_begin]: b_begin -= 1 while b_end < data_len - 1 and sent_ids[b_end - 1] == sent_ids[b_end]: b_end += 1 new_begin = a_end else: label = 1 a_end = random.choice(cut_points) b_begin = a_end b_end = end_idx new_begin = b_end while a_end - a_begin + b_end - b_begin > tot_len: if a_end - a_begin > b_end - b_begin: a_end -= 1 else: b_end -= 1 ret = [data[a_begin:a_end], data[b_begin:b_end], label, new_begin] if extend_target: if a_end >= data_len or b_end >= data_len: tf.logging.info( '[_split_a_and_b] returns None: a_end %d or b_end %d >= data_len %d' , a_end, b_end, data_len) return None a_target = data[a_begin + 1:a_end + 1] b_target = data[b_begin:b_end + 1] ret.extend([a_target, b_target]) return ret 
neural_tangents.predict.cpu|jit def _jit_cpu(x):  def jit_cpu(f): if _is_on_cpu(x): return jit(f, backend='cpu') return jit(f) return jit_cpu 
sidd_utils.train|test|calc|stats def calc_train_test_stats(hps): hps.n_train_per_scene = hps.end_tr_im_idx - hps.start_tr_im_idx hps.n_test_per_scene = hps.end_ts_im_idx - hps.start_ts_im_idx hps.n_train = (hps.n_tr_inst * hps.n_train_per_scene * hps. n_patches_per_image) hps.n_test = hps.n_ts_inst * hps.n_test_per_scene * hps.n_patches_per_image hps.n_tr_bat_per_seq = int(np.ceil(hps.n_tr_inst * hps. n_patches_per_image / hps.n_batch_train)) hps.n_ts_bat_per_seq = int(np.ceil(hps.n_ts_inst * hps. n_patches_per_image / hps.n_batch_test)) 
stax_test.StaxTest.test|parameterizations @jtu.parameterized.named_parameters(jtu.cases_from_list({'testcase_name': '_{}_{}_{}_{}_{}_{}_{}'.format(model, width, 'same_inputs' if same_inputs else 'different_inputs', 'filter_size=%s' % str(filter_size ), proj_into_2d, 'NTK' if is_ntk else 'NNGP', 'parameterization=%s' % str(parameterization)), 'model': model, 'width': width, 'same_inputs': same_inputs, 'filter_size': filter_size, 'proj_into_2d': proj_into_2d, 'is_ntk': is_ntk, 'parameterization': parameterization} for model in MODELS for width in WIDTHS for same_inputs in [False, True] for is_ntk in [False, True] for filter_size in FILTER_SIZES for proj_into_2d in PROJECTIONS for parameterization in PARAMETERIZATIONS)) def test_parameterizations(self, model, width, same_inputs, is_ntk, filter_size, proj_into_2d, parameterization): is_conv = 'conv' in model W_std, b_std = 2.0 ** 0.5, 0.5 ** 0.5 padding = PADDINGS[0] strides = STRIDES[0] phi = stax.Relu() use_pooling, is_res = False, False layer_norm = None if is_conv: if xla_bridge.get_backend().platform == 'cpu': raise jtu.SkipTest('Not running CNN models on CPU to save time.') elif proj_into_2d != PROJECTIONS[0]: raise jtu.SkipTest('FC models do not have these parameters.') self._check_agreement_with_empirical(W_std, b_std, filter_size, is_conv, is_ntk, is_res, layer_norm, padding, phi, proj_into_2d, same_inputs, strides, use_pooling, width, parameterization) 
utils.entropy|cross|d def cross_entropy2d(logit, target, ignore_index=255, weight=None, size_average=True, batch_average=True): n, c, h, w = logit.size() target = target.squeeze(1) if weight is None: criterion = nn.CrossEntropyLoss(weight=weight, ignore_index= ignore_index, size_average=False) else: criterion = nn.CrossEntropyLoss(weight=torch.from_numpy(np.array( weight)).float().cuda(), ignore_index=ignore_index, size_average=False) loss = criterion(logit, target.long()) if size_average: loss /= h * w if batch_average: loss /= n return loss 
test.dis|network|trained def trained_dis_network(patch, reuse=False): """ Parameters: * patch - input image for the network * reuse - boolean variable to reuse weights Returns: * softmax of logits """ with tf.variable_scope('D') as scope: if reuse: scope.reuse_variables() h0 = lrelu(conv3d_WN(patch, 32, name='d_h0_conv')) h1 = lrelu(conv3d_WN(h0, 32, name='d_h1_conv')) p1 = avg_pool3D(h1) h2 = lrelu(conv3d_WN(p1, 64, name='d_h2_conv')) h3 = lrelu(conv3d_WN(h2, 64, name='d_h3_conv')) p3 = avg_pool3D(h3) h4 = lrelu(conv3d_WN(p3, 128, name='d_h4_conv')) h5 = lrelu(conv3d_WN(h4, 128, name='d_h5_conv')) p5 = avg_pool3D(h5) h6 = lrelu(conv3d_WN(p5, 256, name='d_h6_conv')) h7 = lrelu(conv3d_WN(h6, 256, name='d_h7_conv')) up1 = deconv3d_WN(h7, 256, name='d_up1_deconv') up1 = tf.concat([h5, up1], 4) h8 = lrelu(conv3d_WN(up1, 128, name='d_h8_conv')) h9 = lrelu(conv3d_WN(h8, 128, name='d_h9_conv')) up2 = deconv3d_WN(h9, 128, name='d_up2_deconv') up2 = tf.concat([h3, up2], 4) h10 = lrelu(conv3d_WN(up2, 64, name='d_h10_conv')) h11 = lrelu(conv3d_WN(h10, 64, name='d_h11_conv')) up3 = deconv3d_WN(h11, 64, name='d_up3_deconv') up3 = tf.concat([h1, up3], 4) h12 = lrelu(conv3d_WN(up3, 32, name='d_h12_conv')) h13 = lrelu(conv3d_WN(h12, 32, name='d_h13_conv')) h14 = conv3d_WN(h13, F.num_classes, name='d_h14_conv') return tf.nn.softmax(h14) 
thumt.utils.lrp.diagonal|v|n|create def create_diagonal_v2n(batchsize, length, dim): """ diagonal matrix (batchsize, len_src, len_src, dim) result[bs, len_src, len_src, dim] = 1 """ result = tf.diag(tf.ones([length], dtype=tf.float32)) result = tf.expand_dims(result, 0) result = tf.expand_dims(result, -1) result = tf.tile(result, [batchsize, 1, 1, dim]) return result 
extensions.u_hved.application.U_HVEDApplication.evaluator|initialise def initialise_evaluator(self, eval_param): self.eval_param = eval_param self.evaluator = SegmentationEvaluator(self.readers[0], self. segmentation_param, eval_param) 
craystack.codecs.buckets|gaussian|std def std_gaussian_buckets(precision): """ Return the endpoints of buckets partitioning the domain of the prior. Each bucket has mass 1 / (1 << precision) under the prior. """ if precision in std_gaussian_bucket_cache: return std_gaussian_bucket_cache[precision] else: buckets = norm.ppf(np.linspace(0, 1, (1 << precision) + 1)) std_gaussian_bucket_cache[precision] = buckets return buckets 
xlnet-master.train_gpu.core|graph|single def single_core_graph(is_training, features, mems): model_fn = get_model_fn() model_ret = model_fn(features=features, labels=None, mems=mems, is_training=is_training) return model_ret 
classification.ops.utils.LayerParametersDict.getitem def __getitem__(self, key): key = self._canonicalize_key(key) return super(LayerParametersDict, self).__getitem__(key) 
Init.ones def ones(shape, name=None): """All ones.""" initial = tf.ones(shape, dtype=tf.float32) return tf.Variable(initial, name=name) 
src.util.NN.lstm|short|bi|conn def short_conn_bi_lstm(x, x_lens, n_hidden, name, share_name, config=None): """ release first add then max res-Net :param x: :param x_lens: :param n_hidden: :param name: :param share_num: :param keep_prob: :param is_training: :return: query4relation embedding """ with tf.variable_scope('get_realtion_vec_network', reuse=tf.AUTO_REUSE): output1, output1_state = BiLSTM(x, x_lens, n_hidden, share_name) middle1 = tf.concat(output1, 2) output2, output2_state = BiLSTM(middle1, x_lens, n_hidden, name + '_2') output1 = tf.concat(output1, -1) output2 = tf.concat(output2, -1) if 'single_lstm' in config['model'] and config['model']['single_lstm']: query4relation = output1 print('single layer') else: query4relation = output2 + output1 return query4relation, output1, output2 
GPSig.kernels.SequentialPoly.poly @params_as_tensors def _poly(self, X, X2=None): if X2 is None: return (tf.matmul(X, X, transpose_b=True) + self.gamma) ** self.degree else: return (tf.matmul(X, X2, transpose_b=True) + self.gamma) ** self.degree 
neural_tangents.stax.get|relu|kernel|ab def _get_ab_relu_kernel(ker_mat, prod, a, b, do_backprop, ntk=None): cosines = ker_mat / np.sqrt(prod) angles = _arccos(cosines, do_backprop) dot_sigma = (a ** 2 + b ** 2 - (a - b) ** 2 * angles / np.pi) / 2 ker_mat = (a - b) ** 2 * _sqrt(prod - ker_mat ** 2, do_backprop) / (2 * np.pi) + dot_sigma * ker_mat if ntk is not None: ntk *= dot_sigma return ker_mat, ntk 
tflib.ops.conv2d.Conv|D def Conv2D(name, input_dim, output_dim, filter_size, inputs, he_init=True, mask_type=None, stride=1, weightnorm=None, biases=True, gain=1.0): """ inputs: tensor of shape (batch size, num channels, height, width) mask_type: one of None, 'a', 'b'  returns: tensor of shape (batch size, num channels, height, width) """ with tf.name_scope(name) as scope: if mask_type is not None: mask_type, mask_n_channels = mask_type mask = np.ones((filter_size, filter_size, input_dim, output_dim ), dtype='float32') center = filter_size // 2 mask[center + 1:, :, :, :] = 0.0 mask[(center), center + 1:, :, :] = 0.0 for i in xrange(mask_n_channels): for j in xrange(mask_n_channels): if (mask_type == 'a' and i >= j or mask_type == 'b' and i > j): mask[(center), (center), i::mask_n_channels, j:: mask_n_channels] = 0.0  def uniform(stdev, size): return np.random.uniform(low=-stdev * np.sqrt(3), high=stdev * np.sqrt(3), size=size).astype('float32') fan_in = input_dim * filter_size ** 2 fan_out = output_dim * filter_size ** 2 / stride ** 2 if mask_type is not None: fan_in /= 2.0 fan_out /= 2.0 if he_init: filters_stdev = np.sqrt(4.0 / (fan_in + fan_out)) else: filters_stdev = np.sqrt(2.0 / (fan_in + fan_out)) if _weights_stdev is not None: filter_values = uniform(_weights_stdev, (filter_size, filter_size, input_dim, output_dim)) else: filter_values = uniform(filters_stdev, (filter_size, filter_size, input_dim, output_dim)) filter_values *= gain filters = lib.param(name + '.Filters', filter_values) if weightnorm == None: weightnorm = _default_weightnorm if weightnorm: norm_values = np.sqrt(np.sum(np.square(filter_values), axis=(0, 1, 2))) target_norms = lib.param(name + '.g', norm_values) with tf.name_scope('weightnorm') as scope: norms = tf.sqrt(tf.reduce_sum(tf.square(filters), reduction_indices=[0, 1, 2])) filters = filters * (target_norms / norms) if mask_type is not None: with tf.name_scope('filter_mask'): filters = filters * mask result = tf.nn.conv2d(input=inputs, filter=filters, strides=[1, 1, stride, stride], padding='SAME', data_format='NCHW') if biases: _biases = lib.param(name + '.Biases', np.zeros(output_dim, dtype='float32')) result = tf.nn.bias_add(result, _biases, data_format='NCHW') return result 
official.utils.logs.mlperf_helper.Logger.ncf|print def ncf_print(self, key, value=None, stack_offset=_STACK_OFFSET, deferred= False, extra_print=False, prefix=_NCF_PREFIX): if self._mlperf_log is None or not self.enabled: return self._mlperf_log.ncf_print(key=key, value=value, stack_offset= stack_offset, deferred=deferred, extra_print=extra_print, prefix=prefix ) 
thumt.models.rnnsearch.decoder def _decoder(cell, inputs, memory, sequence_length, initial_state, dtype= None, scope=None): batch = tf.shape(inputs)[0] time_steps = tf.shape(inputs)[1] dtype = dtype or inputs.dtype output_size = cell.output_size zero_output = tf.zeros([batch, output_size], dtype) zero_value = tf.zeros([batch, memory.shape[-1].value], dtype) with tf.variable_scope(scope or 'decoder', dtype=dtype): inputs = tf.transpose(inputs, [1, 0, 2]) mem_mask = tf.sequence_mask(sequence_length['source'], maxlen=tf. shape(memory)[1], dtype=dtype) bias = layers.attention.attention_bias(mem_mask, 'masking', dtype=dtype ) bias = tf.squeeze(bias, axis=[1, 2]) cache = layers.attention.attention(None, memory, None, output_size) input_ta = tf.TensorArray(dtype, time_steps, tensor_array_name= 'input_array') output_ta = tf.TensorArray(dtype, time_steps, tensor_array_name= 'output_array') value_ta = tf.TensorArray(dtype, time_steps, tensor_array_name= 'value_array') alpha_ta = tf.TensorArray(dtype, time_steps, tensor_array_name= 'alpha_array') input_ta = input_ta.unstack(inputs) initial_state = layers.nn.linear(initial_state, output_size, True, False, scope='s_transform') initial_state = tf.tanh(initial_state)  def loop_func(t, out_ta, att_ta, val_ta, state, cache_key): inp_t = input_ta.read(t) results = layers.attention.attention(state, memory, bias, output_size, cache={'key': cache_key}) alpha = results['weight'] context = results['value'] cell_input = [inp_t, context] cell_output, new_state = cell(cell_input, state) cell_output = _copy_through(t, sequence_length['target'], zero_output, cell_output) new_state = _copy_through(t, sequence_length['target'], state, new_state) new_value = _copy_through(t, sequence_length['target'], zero_value, context) out_ta = out_ta.write(t, cell_output) att_ta = att_ta.write(t, alpha) val_ta = val_ta.write(t, new_value) cache_key = tf.identity(cache_key) return t + 1, out_ta, att_ta, val_ta, new_state, cache_key time = tf.constant(0, dtype=tf.int32, name='time') loop_vars = time, output_ta, alpha_ta, value_ta, initial_state, cache[ 'key'] outputs = tf.while_loop(lambda t, *_: t < time_steps, loop_func, loop_vars, parallel_iterations=32, swap_memory=True) output_final_ta = outputs[1] value_final_ta = outputs[3] final_output = output_final_ta.stack() final_output.set_shape([None, None, output_size]) final_output = tf.transpose(final_output, [1, 0, 2]) final_value = value_final_ta.stack() final_value.set_shape([None, None, memory.shape[-1].value]) final_value = tf.transpose(final_value, [1, 0, 2]) result = {'outputs': final_output, 'values': final_value, 'initial_state': initial_state} return result 
neural_tangents.stax.transform|kernels def _transform_kernels(kernels, fn, **fn_kwargs): """Apply transformation to kernels.  Args: kernels: a `Kernel` object. fn: nonlinearity function, can only be Relu, Erf or Identity. Returns: The transformed kernel. """ is_gaussian = kernels.is_gaussian if not is_gaussian: raise ValueError( 'An affine layer (i.e. dense or convolution) has to be applied before a nonlinearity layer.' ) if fn is _ab_relu: return _transform_kernels_ab_relu(kernels, **fn_kwargs) if fn is _erf: return _transform_kernels_erf(kernels, **fn_kwargs) raise NotImplementedError( 'Analaytic kernel for activiation {} is not implmented'.format(fn)) 
estimate_gradient_norm.EstimateLipschitz.load_model.cond def cond(it, randv, eig_est, eig_est_prev, tfconst): norm_diff = tf.norm(eig_est - eig_est_prev, axis=0) return tf.logical_and(it < 500, norm_diff > 0.001) 
utils_test.SplitStructureTest.test|basic def test_basic(self): prefix, suffix = utils.split_structure([tf.constant([1, 2, 3]), tf. constant([[4, 5], [6, 7], [8, 9]])], 1) self.assertAllEqual(prefix[0], tf.constant([1])) self.assertAllEqual(prefix[1], tf.constant([[4, 5]])) self.assertAllEqual(suffix[0], tf.constant([2, 3])) self.assertAllEqual(suffix[1], tf.constant([[6, 7], [8, 9]])) 
Micro-Net-master.Models._test_unet.unet|test def test_unet(): """ unet() test case :return: """ model = unet(inputs_shape=(500, 500, 3), nb_classes=FLAGS.nb_classes) model.summary() 
nmt.attention_utils._BaseAttentionMechanism.values @property def values(self): return self._values 
NLIN.Power|calc|Ase|Noise def calcAseNoisePower(param): G = param.L * param.alpha AseNoiseDensity = param.Nspan * (param.PolMux + 1 ) * h * c / param.lambda_ * 10 ** (param.NF / 10) / 2 * (10 ** (G / 10) - 1) AseNoisePower = param.BaudRate * 1000000000.0 * AseNoiseDensity return AseNoisePower 
craystack.codecs_test.discretized|test def test_discretized(): rng = np.random.RandomState() coding_prec = 16 bin_prec = 8 n = 10000 data = rng.randint(1 << bin_prec, size=10000) check_codec((n,), cs.codecs._discretize(expit, logit, -0.5, 0.5, bin_prec, coding_prec), data) 
gen.epoch|structured|plot|resnet|for|A def plot_resnet110_structured_A_epoch_for_epoch(): common.epoch_for_epoch_plot(network=common.RESNET110, is_iterative= False, prune_method=common.STRUCTURED_A, min_max_y=(-0.03, 0.01), comparison_points=[(0, 0.9325 - 0.9314), (160 * 0.1882836396, 0.9322 - 0.9314)], comparison_err=[0.0029, 0.0022], comparison_label=common.RETHINKING) 
darkflow.utils.box.BoundBox.Box|Bound def __init__(self, classes): self.x, self.y = float(), float() self.w, self.h = float(), float() self.c = float() self.class_num = classes self.probs = np.zeros((classes,)) 
enas.cifar10.general_controller.GeneralController.General|Controller def __init__(self, search_for='both', search_whole_channels=False, num_layers=4, num_branches=6, out_filters=48, lstm_size=32, lstm_num_layers=2, lstm_keep_prob=1.0, tanh_constant=None, temperature= None, lr_init=0.001, lr_dec_start=0, lr_dec_every=100, lr_dec_rate=0.9, l2_reg=0, entropy_weight=None, clip_mode=None, grad_bound=None, use_critic=False, bl_dec=0.999, optim_algo='adam', sync_replicas=False, num_aggregate=None, num_replicas=None, skip_target=0.8, skip_weight=0.5, name='controller', *args, **kwargs): print('-' * 80) print('Building ConvController') self.search_for = search_for self.search_whole_channels = search_whole_channels self.num_layers = num_layers self.num_branches = num_branches self.out_filters = out_filters self.lstm_size = lstm_size self.lstm_num_layers = lstm_num_layers self.lstm_keep_prob = lstm_keep_prob self.tanh_constant = tanh_constant self.temperature = temperature self.lr_init = lr_init self.lr_dec_start = lr_dec_start self.lr_dec_every = lr_dec_every self.lr_dec_rate = lr_dec_rate self.l2_reg = l2_reg self.entropy_weight = entropy_weight self.clip_mode = clip_mode self.grad_bound = grad_bound self.use_critic = use_critic self.bl_dec = bl_dec self.skip_target = skip_target self.skip_weight = skip_weight self.optim_algo = optim_algo self.sync_replicas = sync_replicas self.num_aggregate = num_aggregate self.num_replicas = num_replicas self.name = name self._create_params() self._build_sampler() 
enas.cifar10.models.Model.eval|once def eval_once(self, sess, eval_set, feed_dict=None, verbose=False): """Expects self.acc and self.global_step to be defined.  Args: sess: tf.Session() or one of its wrap arounds. feed_dict: can be used to give more information to sess.run(). eval_set: "valid" or "test" """ assert self.global_step is not None global_step = sess.run(self.global_step) print('Eval at {}'.format(global_step)) if eval_set == 'valid': assert self.x_valid is not None assert self.valid_acc is not None num_examples = self.num_valid_examples num_batches = self.num_valid_batches acc_op = self.valid_acc elif eval_set == 'test': assert self.test_acc is not None num_examples = self.num_test_examples num_batches = self.num_test_batches acc_op = self.test_acc else: raise NotImplementedError("Unknown eval_set '{}'".format(eval_set)) total_acc = 0 total_exp = 0 for batch_id in xrange(num_batches): acc = sess.run(acc_op, feed_dict=feed_dict) total_acc += acc total_exp += self.eval_batch_size if verbose: sys.stdout.write('\r{:<5d}/{:>5d}'.format(total_acc, total_exp)) if verbose: print('') print('{}_accuracy: {:<6.4f}'.format(eval_set, float(total_acc) / total_exp)) 
neural_tangents.stax.marg|point def _point_marg(x): n, X, Y, channels = x.shape x_flat = np.reshape(x, (n, -1, channels)) x_flat_t = np.transpose(x_flat, (0, 2, 1)) ret = np.matmul(x_flat, x_flat_t) return np.swapaxes(np.reshape(ret, (n, X, Y, X, Y)), 3, 2) 
nets.inception_v1.v|inception|base def inception_v1_base(inputs, final_endpoint='Mixed_5c', scope='InceptionV1'): """Defines the Inception V1 base architecture.  This architecture is defined in: Going deeper with convolutions Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich. http://arxiv.org/pdf/1409.4842v1.pdf.  Args: inputs: a tensor of size [batch_size, height, width, channels]. final_endpoint: specifies the endpoint to construct the network up to. It can be one of ['Conv2d_1a_7x7', 'MaxPool_2a_3x3', 'Conv2d_2b_1x1', 'Conv2d_2c_3x3', 'MaxPool_3a_3x3', 'Mixed_3b', 'Mixed_3c', 'MaxPool_4a_3x3', 'Mixed_4b', 'Mixed_4c', 'Mixed_4d', 'Mixed_4e', 'Mixed_4f', 'MaxPool_5a_2x2', 'Mixed_5b', 'Mixed_5c'] scope: Optional variable_scope.  Returns: A dictionary from components of the network to the corresponding activation.  Raises: ValueError: if final_endpoint is not set to one of the predefined values. """ end_points = {} with tf.variable_scope(scope, 'InceptionV1', [inputs]): with slim.arg_scope([slim.conv2d, slim.fully_connected], weights_initializer=trunc_normal(0.01)): with slim.arg_scope([slim.conv2d, slim.max_pool2d], stride=1, padding='SAME'): end_point = 'Conv2d_1a_7x7' net = slim.conv2d(inputs, 64, [7, 7], stride=2, scope=end_point ) end_points[end_point] = net if final_endpoint == end_point: return net, end_points end_point = 'MaxPool_2a_3x3' net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point) end_points[end_point] = net if final_endpoint == end_point: return net, end_points end_point = 'Conv2d_2b_1x1' net = slim.conv2d(net, 64, [1, 1], scope=end_point) end_points[end_point] = net if final_endpoint == end_point: return net, end_points end_point = 'Conv2d_2c_3x3' net = slim.conv2d(net, 192, [3, 3], scope=end_point) end_points[end_point] = net if final_endpoint == end_point: return net, end_points end_point = 'MaxPool_3a_3x3' net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point) end_points[end_point] = net if final_endpoint == end_point: return net, end_points end_point = 'Mixed_3b' with tf.variable_scope(end_point): with tf.variable_scope('Branch_0'): branch_0 = slim.conv2d(net, 64, [1, 1], scope= 'Conv2d_0a_1x1') with tf.variable_scope('Branch_1'): branch_1 = slim.conv2d(net, 96, [1, 1], scope= 'Conv2d_0a_1x1') branch_1 = slim.conv2d(branch_1, 128, [3, 3], scope ='Conv2d_0b_3x3') with tf.variable_scope('Branch_2'): branch_2 = slim.conv2d(net, 16, [1, 1], scope= 'Conv2d_0a_1x1') branch_2 = slim.conv2d(branch_2, 32, [3, 3], scope= 'Conv2d_0b_3x3') with tf.variable_scope('Branch_3'): branch_3 = slim.max_pool2d(net, [3, 3], scope= 'MaxPool_0a_3x3') branch_3 = slim.conv2d(branch_3, 32, [1, 1], scope= 'Conv2d_0b_1x1') net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3]) end_points[end_point] = net if final_endpoint == end_point: return net, end_points end_point = 'Mixed_3c' with tf.variable_scope(end_point): with tf.variable_scope('Branch_0'): branch_0 = slim.conv2d(net, 128, [1, 1], scope= 'Conv2d_0a_1x1') with tf.variable_scope('Branch_1'): branch_1 = slim.conv2d(net, 128, [1, 1], scope= 'Conv2d_0a_1x1') branch_1 = slim.conv2d(branch_1, 192, [3, 3], scope ='Conv2d_0b_3x3') with tf.variable_scope('Branch_2'): branch_2 = slim.conv2d(net, 32, [1, 1], scope= 'Conv2d_0a_1x1') branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope= 'Conv2d_0b_3x3') with tf.variable_scope('Branch_3'): branch_3 = slim.max_pool2d(net, [3, 3], scope= 'MaxPool_0a_3x3') branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope= 'Conv2d_0b_1x1') net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3]) end_points[end_point] = net if final_endpoint == end_point: return net, end_points end_point = 'MaxPool_4a_3x3' net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point) end_points[end_point] = net if final_endpoint == end_point: return net, end_points end_point = 'Mixed_4b' with tf.variable_scope(end_point): with tf.variable_scope('Branch_0'): branch_0 = slim.conv2d(net, 192, [1, 1], scope= 'Conv2d_0a_1x1') with tf.variable_scope('Branch_1'): branch_1 = slim.conv2d(net, 96, [1, 1], scope= 'Conv2d_0a_1x1') branch_1 = slim.conv2d(branch_1, 208, [3, 3], scope ='Conv2d_0b_3x3') with tf.variable_scope('Branch_2'): branch_2 = slim.conv2d(net, 16, [1, 1], scope= 'Conv2d_0a_1x1') branch_2 = slim.conv2d(branch_2, 48, [3, 3], scope= 'Conv2d_0b_3x3') with tf.variable_scope('Branch_3'): branch_3 = slim.max_pool2d(net, [3, 3], scope= 'MaxPool_0a_3x3') branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope= 'Conv2d_0b_1x1') net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3]) end_points[end_point] = net if final_endpoint == end_point: return net, end_points end_point = 'Mixed_4c' with tf.variable_scope(end_point): with tf.variable_scope('Branch_0'): branch_0 = slim.conv2d(net, 160, [1, 1], scope= 'Conv2d_0a_1x1') with tf.variable_scope('Branch_1'): branch_1 = slim.conv2d(net, 112, [1, 1], scope= 'Conv2d_0a_1x1') branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope ='Conv2d_0b_3x3') with tf.variable_scope('Branch_2'): branch_2 = slim.conv2d(net, 24, [1, 1], scope= 'Conv2d_0a_1x1') branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope= 'Conv2d_0b_3x3') with tf.variable_scope('Branch_3'): branch_3 = slim.max_pool2d(net, [3, 3], scope= 'MaxPool_0a_3x3') branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope= 'Conv2d_0b_1x1') net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3]) end_points[end_point] = net if final_endpoint == end_point: return net, end_points end_point = 'Mixed_4d' with tf.variable_scope(end_point): with tf.variable_scope('Branch_0'): branch_0 = slim.conv2d(net, 128, [1, 1], scope= 'Conv2d_0a_1x1') with tf.variable_scope('Branch_1'): branch_1 = slim.conv2d(net, 128, [1, 1], scope= 'Conv2d_0a_1x1') branch_1 = slim.conv2d(branch_1, 256, [3, 3], scope ='Conv2d_0b_3x3') with tf.variable_scope('Branch_2'): branch_2 = slim.conv2d(net, 24, [1, 1], scope= 'Conv2d_0a_1x1') branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope= 'Conv2d_0b_3x3') with tf.variable_scope('Branch_3'): branch_3 = slim.max_pool2d(net, [3, 3], scope= 'MaxPool_0a_3x3') branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope= 'Conv2d_0b_1x1') net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3]) end_points[end_point] = net if final_endpoint == end_point: return net, end_points end_point = 'Mixed_4e' with tf.variable_scope(end_point): with tf.variable_scope('Branch_0'): branch_0 = slim.conv2d(net, 112, [1, 1], scope= 'Conv2d_0a_1x1') with tf.variable_scope('Branch_1'): branch_1 = slim.conv2d(net, 144, [1, 1], scope= 'Conv2d_0a_1x1') branch_1 = slim.conv2d(branch_1, 288, [3, 3], scope ='Conv2d_0b_3x3') with tf.variable_scope('Branch_2'): branch_2 = slim.conv2d(net, 32, [1, 1], scope= 'Conv2d_0a_1x1') branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope= 'Conv2d_0b_3x3') with tf.variable_scope('Branch_3'): branch_3 = slim.max_pool2d(net, [3, 3], scope= 'MaxPool_0a_3x3') branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope= 'Conv2d_0b_1x1') net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3]) end_points[end_point] = net if final_endpoint == end_point: return net, end_points end_point = 'Mixed_4f' with tf.variable_scope(end_point): with tf.variable_scope('Branch_0'): branch_0 = slim.conv2d(net, 256, [1, 1], scope= 'Conv2d_0a_1x1') with tf.variable_scope('Branch_1'): branch_1 = slim.conv2d(net, 160, [1, 1], scope= 'Conv2d_0a_1x1') branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope ='Conv2d_0b_3x3') with tf.variable_scope('Branch_2'): branch_2 = slim.conv2d(net, 32, [1, 1], scope= 'Conv2d_0a_1x1') branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope ='Conv2d_0b_3x3') with tf.variable_scope('Branch_3'): branch_3 = slim.max_pool2d(net, [3, 3], scope= 'MaxPool_0a_3x3') branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope ='Conv2d_0b_1x1') net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3]) end_points[end_point] = net if final_endpoint == end_point: return net, end_points end_point = 'MaxPool_5a_2x2' net = slim.max_pool2d(net, [2, 2], stride=2, scope=end_point) end_points[end_point] = net if final_endpoint == end_point: return net, end_points end_point = 'Mixed_5b' with tf.variable_scope(end_point): with tf.variable_scope('Branch_0'): branch_0 = slim.conv2d(net, 256, [1, 1], scope= 'Conv2d_0a_1x1') with tf.variable_scope('Branch_1'): branch_1 = slim.conv2d(net, 160, [1, 1], scope= 'Conv2d_0a_1x1') branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope ='Conv2d_0b_3x3') with tf.variable_scope('Branch_2'): branch_2 = slim.conv2d(net, 32, [1, 1], scope= 'Conv2d_0a_1x1') branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope ='Conv2d_0a_3x3') with tf.variable_scope('Branch_3'): branch_3 = slim.max_pool2d(net, [3, 3], scope= 'MaxPool_0a_3x3') branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope ='Conv2d_0b_1x1') net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3]) end_points[end_point] = net if final_endpoint == end_point: return net, end_points end_point = 'Mixed_5c' with tf.variable_scope(end_point): with tf.variable_scope('Branch_0'): branch_0 = slim.conv2d(net, 384, [1, 1], scope= 'Conv2d_0a_1x1') with tf.variable_scope('Branch_1'): branch_1 = slim.conv2d(net, 192, [1, 1], scope= 'Conv2d_0a_1x1') branch_1 = slim.conv2d(branch_1, 384, [3, 3], scope ='Conv2d_0b_3x3') with tf.variable_scope('Branch_2'): branch_2 = slim.conv2d(net, 48, [1, 1], scope= 'Conv2d_0a_1x1') branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope ='Conv2d_0b_3x3') with tf.variable_scope('Branch_3'): branch_3 = slim.max_pool2d(net, [3, 3], scope= 'MaxPool_0a_3x3') branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope ='Conv2d_0b_1x1') net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3]) end_points[end_point] = net if final_endpoint == end_point: return net, end_points raise ValueError('Unknown final endpoint %s' % final_endpoint) 
vnet.UpsamplingDeconvBlock.Upsampling|Block|Deconv def __init__(self, n_filters_in, n_filters_out, stride=2, normalization='none' ): super(UpsamplingDeconvBlock, self).__init__() ops = [] if normalization != 'none': ops.append(nn.ConvTranspose3d(n_filters_in, n_filters_out, stride, padding=0, stride=stride)) if normalization == 'batchnorm': ops.append(nn.BatchNorm3d(n_filters_out)) elif normalization == 'groupnorm': ops.append(nn.GroupNorm(num_groups=16, num_channels=n_filters_out)) elif normalization == 'instancenorm': ops.append(nn.InstanceNorm3d(n_filters_out)) else: assert False else: ops.append(nn.ConvTranspose3d(n_filters_in, n_filters_out, stride, padding=0, stride=stride)) ops.append(nn.ReLU(inplace=True)) self.conv = nn.Sequential(*ops) 
base_model.Base.get|stats def get_stats(self, config): raise NotImplementedError('Please Implement this method') 
plyfile.PlyParseError.repr def __repr__(self): return ('PlyParseError(%r, element=%r, row=%r, prop=%r)' % self.message, self.element, self.row, self.prop) 
neural_tangents.stax.parallel.fn|init def init_fn(rng, input_shape): return list(init_fn_stax(rng, input_shape)) 
cdvae-cls-gan-mcc.CDVAECLSGAN.sp|encoder def sp_encoder(self, x): net = self.arch['encoder']['sp'] return self._encoder(x, net) 
regression.misc.layers.NormalPrior.Prior|Normal def __init__(self, n_in, n_out, w_name, params=None): super(NormalPrior, self).__init__(n_in, n_out, w_name) if params is None or isinstance(params, dict) and len(params) == 0: mean = 0.0 logstd = 0.0 elif isinstance(params, dict) and 'mean' in params and 'logstd' in params: mean = params['p_mean'] logstd = params['p_logstd'] else: raise ValueError('Not an appropriate param') self._p_mean = mean * tf.ones([self.n_in + 1, self.n_out]) self._p_logstd = logstd * tf.ones([self.n_in + 1, self.n_out]) 
official.utils.logs.metric_hook_test.LoggingMetricHookTest.n|print|every|validate|secs def _validate_print_every_n_secs(self, sess, at_end): t = tf.constant(42.0, name='foo') train_op = tf.constant(3) hook = metric_hook.LoggingMetricHook(tensors=[t.name], every_n_secs=1.0, at_end=at_end, metric_logger=self._logger) hook.begin() mon_sess = monitored_session._HookedSession(sess, [hook]) sess.run(tf.global_variables_initializer()) mon_sess.run(train_op) self.assertRegexpMatches(str(self._logger.logged_metric), t.name) self._logger.logged_metric = [] mon_sess.run(train_op) self.assertEqual(str(self._logger.logged_metric).find(t.name), -1) time.sleep(1.0) self._logger.logged_metric = [] mon_sess.run(train_op) self.assertRegexpMatches(str(self._logger.logged_metric), t.name) self._logger.logged_metric = [] hook.end(sess) if at_end: self.assertRegexpMatches(str(self._logger.logged_metric), t.name) else: self.assertEqual(str(self._logger.logged_metric).find(t.name), -1) 
europilot.screen.ScreenGrab.exit def __exit__(self): self.close() 
elpips.networks.vgg16.get|slice def get_slice4(self, input): with tf.name_scope('slice4'): net = self._pool(input, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID', name='max_pool_slice4', data_format='NHWC') net = self._conv(net, self.features['17.weight'], self.features[ '17.bias'], w_shape=[3, 3, 256, 512], padding='SAME', stride=[1, 1, 1, 1]) net = self._conv(net, self.features['19.weight'], self.features[ '19.bias'], w_shape=[3, 3, 512, 512], padding='SAME', stride=[1, 1, 1, 1]) return self._conv(net, self.features['21.weight'], self.features[ '21.bias'], w_shape=[3, 3, 512, 512], padding='SAME', stride=[1, 1, 1, 1]) 
parser.parser|parameter def parameter_parser(): """ A method to parse up command line parameters. By default it gives an embedding of the Facebook politicians network. The default hyperparameters give a good quality representation and good candidate cluster means without grid search. """ parser = argparse.ArgumentParser(description='Run GEMSEC.') parser.add_argument('--input', nargs='?', default= './data/politician_edges.csv', help='Input graph path.') parser.add_argument('--embedding-output', nargs='?', default= './output/embeddings/politician_embedding.csv', help='Embeddings path.' ) parser.add_argument('--cluster-mean-output', nargs='?', default= './output/cluster_means/politician_means.csv', help= 'Cluster means path.') parser.add_argument('--log-output', nargs='?', default= './output/logs/politician.json', help='Log path.') parser.add_argument('--assignment-output', nargs='?', default= './output/assignments/politician.json', help='Log path.') parser.add_argument('--dump-matrices', type=bool, default=True, help= 'Save the embeddings to disk or not. Default is not.') parser.add_argument('--model', nargs='?', default= 'GEMSECWithRegularization', help='The model type.') parser.add_argument('--P', type=float, default=1, help= 'Return hyperparameter. Default is 1.') parser.add_argument('--Q', type=float, default=1, help= 'In-out hyperparameter. Default is 1.') parser.add_argument('--walker', nargs='?', default='first', help= 'Random walker order. Default is first.') parser.add_argument('--dimensions', type=int, default=16, help= 'Number of dimensions. Default is 16.') parser.add_argument('--random-walk-length', type=int, default=80, help= 'Length of random walk per source. Default is 80.') parser.add_argument('--num-of-walks', type=int, default=5, help= 'Number of random walks per source. Default is 5.') parser.add_argument('--window-size', type=int, default=5, help= 'Window size for proximity statistic extraction. Default is 5.') parser.add_argument('--distortion', type=float, default=0.75, help= 'Downsampling distortion. Default is 0.75.') parser.add_argument('--negative-sample-number', type=int, default=10, help='Number of negative samples to draw. Default is 10.') parser.add_argument('--initial-learning-rate', type=float, default=0.01, help='Initial learning rate. Default is 0.01.') parser.add_argument('--minimal-learning-rate', type=float, default= 0.001, help='Minimal learning rate. Default is 0.001.') parser.add_argument('--annealing-factor', type=float, default=1, help= 'Annealing factor. Default is 1.0.') parser.add_argument('--initial-gamma', type=float, default=0.1, help= 'Initial clustering weight. Default is 0.1.') parser.add_argument('--final-gamma', type=float, default=0.5, help= 'Final clustering weight. Default is 0.5.') parser.add_argument('--lambd', type=float, default=2.0 ** -4, help= 'Smoothness regularization penalty. Default is 0.0625.') parser.add_argument('--cluster-number', type=int, default=20, help= 'Number of clusters. Default is 20.') parser.add_argument('--overlap-weighting', nargs='?', default= 'normalized_overlap', help= 'Weight construction technique for regularization.') parser.add_argument('--regularization-noise', type=float, default=10 ** -8, help='Uniform noise max and min on the feature vector distance.') return parser.parse_args() 
train_cvact.validate def validate(grd_descriptor, sat_descriptor): accuracy = 0.0 data_amount = 0.0 dist_array = 2 - 2 * np.matmul(sat_descriptor, np.transpose(grd_descriptor) ) top1_percent = int(dist_array.shape[0] * 0.01) + 1 for i in range(dist_array.shape[0]): gt_dist = dist_array[i, i] prediction = np.sum(dist_array[:, (i)] < gt_dist) if prediction < 1: accuracy += 1.0 data_amount += 1.0 accuracy /= data_amount return accuracy 
privacy_accountants.P|compute|mu def compute_muP(epoch, noise_multi, N, batch_size): T = epoch * N / batch_size return np.sqrt(np.exp(noise_multi ** -2) - 1) * np.sqrt(T) * batch_size / N 
baseline_seq2seq_attn_main.main def main(): """Entrypoint. """ train_data = tx.data.PairedTextData(hparams=config_data.train) val_data = tx.data.PairedTextData(hparams=config_data.val) test_data = tx.data.PairedTextData(hparams=config_data.test) data_iterator = tx.data.TrainTestDataIterator(train=train_data, val= val_data, test=test_data) batch = data_iterator.get_next() train_op, infer_outputs = build_model(batch, train_data)  def _train_epoch(sess, epoch_no): data_iterator.switch_to_train_data(sess) training_log_file = open(log_dir + 'training_log' + str(epoch_no) + '.txt', 'w', encoding='utf-8') step = 0 while True: try: loss = sess.run(train_op) print('step={}, loss={:.4f}'.format(step, loss), file= training_log_file) if step % config_data.observe_steps == 0: print('step={}, loss={:.4f}'.format(step, loss)) training_log_file.flush() step += 1 except tf.errors.OutOfRangeError: break  def _eval_epoch(sess, mode, epoch_no): if mode == 'val': data_iterator.switch_to_val_data(sess) else: data_iterator.switch_to_test_data(sess) refs, hypos = [], [] while True: try: fetches = [batch['target_text'][:, 1:], infer_outputs. predicted_ids[:, :, (0)]] feed_dict = {tx.global_mode(): tf.estimator.ModeKeys.EVAL} target_texts_ori, output_ids = sess.run(fetches, feed_dict= feed_dict) target_texts = tx.utils.strip_special_tokens(target_texts_ori .tolist(), is_token_list=True) target_texts = tx.utils.str_join(target_texts) output_texts = tx.utils.map_ids_to_strs(ids=output_ids, vocab=val_data.target_vocab) tx.utils.write_paired_text(target_texts, output_texts, log_dir + mode + '_results' + str(epoch_no) + '.txt', append=True, mode='h', sep=' ||| ') for hypo, ref in zip(output_texts, target_texts): if config_data.eval_metric == 'bleu': hypos.append(hypo) refs.append([ref]) elif config_data.eval_metric == 'rouge': hypos.append(tx.utils.compat_as_text(hypo)) refs.append(tx.utils.compat_as_text(ref)) except tf.errors.OutOfRangeError: break if config_data.eval_metric == 'bleu': return tx.evals.corpus_bleu_moses(list_of_references=refs, hypotheses=hypos) elif config_data.eval_metric == 'rouge': rouge = Rouge() return rouge.get_scores(hyps=hypos, refs=refs, avg=True)  def _calc_reward(score): """ Return the bleu score or the sum of (Rouge-1, Rouge-2, Rouge-L). """ if config_data.eval_metric == 'bleu': return score elif config_data.eval_metric == 'rouge': return sum([value['f'] for key, value in score.items()]) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) sess.run(tf.local_variables_initializer()) sess.run(tf.tables_initializer()) best_val_score = -1.0 scores_file = open(log_dir + 'scores.txt', 'w', encoding='utf-8') for i in range(config_data.num_epochs): _train_epoch(sess, i) val_score = _eval_epoch(sess, 'val', i) test_score = _eval_epoch(sess, 'test', i) best_val_score = max(best_val_score, _calc_reward(val_score)) if config_data.eval_metric == 'bleu': print_stdout_and_file( 'val epoch={}, BLEU={:.4f}; best-ever={:.4f}'.format(i, val_score, best_val_score), file=scores_file) print_stdout_and_file('test epoch={}, BLEU={:.4f}'.format(i, test_score), file=scores_file) print_stdout_and_file('=' * 50, file=scores_file) elif config_data.eval_metric == 'rouge': print_stdout_and_file('valid epoch {}:'.format(i), file= scores_file) for key, value in val_score.items(): print_stdout_and_file('{}: {}'.format(key, value), file =scores_file) print_stdout_and_file('fsum: {}; best_val_fsum: {}'.format( _calc_reward(val_score), best_val_score), file=scores_file) print_stdout_and_file('test epoch {}:'.format(i), file= scores_file) for key, value in test_score.items(): print_stdout_and_file('{}: {}'.format(key, value), file =scores_file) print_stdout_and_file('=' * 110, file=scores_file) scores_file.flush() 
neural_tangents.stax.marg|point def _point_marg(x): n, X, Y, channels = x.shape x_flat = np.reshape(x, (n, -1, channels)) x_flat_t = np.transpose(x_flat, (0, 2, 1)) ret = np.matmul(x_flat, x_flat_t) return np.swapaxes(np.reshape(ret, (n, X, Y, X, Y)), 3, 2) 
layers.BinaryFullyConnectedLayer.func|sign def _sign_func(self, x): """Modified sign function randomize if 0, else sign  Arguments: x {tf tensor} -- input tensor  Returns: tf tensor -- sign(x) """ cond = x == tf.zeros_like(x) return tf.where(cond, tf.ones_like(x), tf.sign(x)) 
xlnet-master.xlnet.XLNetModel.get|output|sequence def get_sequence_output(self): """ Returns: float32 Tensor in shape [len, bsz, d_model]. The last layer hidden representation of XLNet. """ return self.output 
neural_tangents.stax.flip|height|width def _flip_height_width(kernels): """Flips the order of spatial axes in the covariance matrices.  Args: kernels: a `Kernel` object.  Returns: A `Kernel` object with `height` and `width` axes order flipped in all covariance matrices. For example, if `kernels.nngp` has shape `[batch_size_1, batch_size_2, height, height, width, width]`, then `_flip_height_width(kernels).nngp` has shape `[batch_size_1, batch_size_2, width, width, height, height]`. """ var1, nngp, var2, ntk, is_height_width, marginal, cross = (kernels.var1, kernels.nngp, kernels.var2, kernels.ntk, kernels.is_height_width, kernels.marginal, kernels.cross)  def flip_5or6d(mat): return np.moveaxis(mat, (-2, -1), (-4, -3)) if marginal == M.OVER_PIXELS: var1 = np.transpose(var1, (0, 2, 1)) var2 = np.transpose(var2, (0, 2, 1)) if var2 is not None else var2 elif marginal in [M.OVER_POINTS, M.NO]: var1 = flip_5or6d(var1) var2 = flip_5or6d(var2) if var2 is not None else var2 else: raise NotImplementedError( 'Only implemented for `OVER_PIXELS`, `OVER_POINTS` and `NO`; supplied {}' .format(marginal)) if cross == M.OVER_PIXELS: nngp = np.moveaxis(nngp, -1, -2) ntk = np.moveaxis(ntk, -1, -2) if _is_array(ntk) else ntk elif cross in [M.OVER_POINTS, M.NO]: nngp = flip_5or6d(nngp) ntk = flip_5or6d(ntk) if _is_array(ntk) else ntk return kernels._replace(var1=var1, nngp=nngp, var2=var2, ntk=ntk, is_height_width=not is_height_width) 
neural_tangents.stax.preprocess|kernel|fn def _preprocess_kernel_fn(init_fn, kernel_fn):  def new_kernel_fn(x1_or_kernel, x2=None, get=None): """Returns the `Kernel` resulting from applying `ker_fun` to given inputs.  Args: x1_or_kernel: either a `np.ndarray` with shape `[batch_size_1] + input_shape`, or a `Kernel`. x2: an optional `np.ndarray` with shape `[batch_size_2] + input_shape`. `None` means `x2 == x1` or `x1_or_kernel is Kernel`. get: either `None`, a string, or a tuple of strings specifying which data should be returned by the kernel function. Can be "nngp", "ntk", "var1", "var2", "is_gaussian", "is_height_width", "marginal", "cross". Returns: If `get` is a string, returns the requested `np.ndarray`. If `get` is a tuple, returns an `AnalyticKernel` namedtuple containing only the requested information.  If `get` is None then a Kernel object is returned containing all the data. """ if isinstance(x1_or_kernel, Kernel) or isinstance(x1_or_kernel, list ) and all(isinstance(k, Kernel) for k in x1_or_kernel): return _apply_kernel(init_fn, kernel_fn, x1_or_kernel) return outer_kernel_fn(x1_or_kernel, x2, get)  @utils.get_namedtuple('AnalyticKernel') def outer_kernel_fn(x1, x2, get): if not isinstance(x1, np.ndarray): raise TypeError( 'Inputs to a kernel propagation function should be a `Kernel`, a `list` of `Kernel`s, or a (tuple of) `np.ndarray`(s), got %s.' % type(x1)) if not (x2 is None or isinstance(x2, np.ndarray)): raise TypeError( '`x2` to a kernel propagation function should be `None` or a `np.ndarray`, got %s.' % type(x2)) include_ntk = get is None or 'ntk' in get covs_req = getattr(kernel_fn, _COVARIANCES_REQ, {'marginal': M. OVER_ALL, 'cross': M.OVER_ALL}) kernel = _inputs_to_kernel(x1, x2, compute_ntk=include_ntk, **covs_req) return _apply_kernel(init_fn, kernel_fn, kernel) if hasattr(kernel_fn, _COVARIANCES_REQ): setattr(new_kernel_fn, _COVARIANCES_REQ, getattr(kernel_fn, _COVARIANCES_REQ)) return new_kernel_fn 
neural_style.file|read|flow def read_flow_file(path): with open(path, 'rb') as f: header = struct.unpack('4s', f.read(4))[0] w = struct.unpack('i', f.read(4))[0] h = struct.unpack('i', f.read(4))[0] flow = np.ndarray((2, h, w), dtype=np.float32) for y in range(h): for x in range(w): flow[0, y, x] = struct.unpack('f', f.read(4))[0] flow[1, y, x] = struct.unpack('f', f.read(4))[0] return flow 
facenet-master.contributed.real_time_face_recognition.main def main(args): frame_interval = 3 fps_display_interval = 5 frame_rate = 0 frame_count = 0 video_capture = cv2.VideoCapture(0) face_recognition = face.Recognition() start_time = time.time() if args.debug: print('Debug enabled') face.debug = True while True: ret, frame = video_capture.read() if frame_count % frame_interval == 0: faces = face_recognition.identify(frame) end_time = time.time() if end_time - start_time > fps_display_interval: frame_rate = int(frame_count / (end_time - start_time)) start_time = time.time() frame_count = 0 add_overlays(frame, faces, frame_rate) frame_count += 1 cv2.imshow('Video', frame) if cv2.waitKey(1) & 255 == ord('q'): break video_capture.release() cv2.destroyAllWindows() 
xlnet-master.tpu_estimator._StoppingPredictHook.before|run def before_run(self, run_context): return session_run_hook.SessionRunArgs(self._scalar_stopping_signal) 
thumt.layers.attention.bias|attention def attention_bias(inputs, mode, inf=-1000000000.0, dtype=None, name=None): """ A bias tensor used in attention mechanism :param inputs: A tensor :param mode: one of "causal", "masking", "proximal" or "distance" :param inf: A floating value :param dtype: An instance of tf.DType :param name: optional string :returns: A 4D tensor with shape [batch, heads, queries, memories] """ with tf.name_scope(name, default_name='attention_bias', values=[inputs]): if dtype is None: dtype = tf.float32 if dtype != tf.float32: inf = dtype.min if mode == 'causal': length = inputs lower_triangle = tf.matrix_band_part(tf.ones([length, length]), -1, 0) ret = inf * (1.0 - lower_triangle) ret = tf.reshape(ret, [1, 1, length, length]) elif mode == 'masking': mask = inputs ret = (1.0 - mask) * inf ret = tf.expand_dims(tf.expand_dims(ret, 1), 1) elif mode == 'proximal': length = inputs r = tf.to_float(tf.range(length)) diff = tf.expand_dims(r, 0) - tf.expand_dims(r, 1) ret = tf.expand_dims(tf.expand_dims(-tf.log(1 + tf.abs(diff)), 0), 0) elif mode == 'distance': length, distance = inputs distance = tf.where(distance > length, 0, distance) distance = tf.cast(distance, tf.int64) lower_triangle = tf.matrix_band_part(tf.ones([length, length]), -1, 0) mask_triangle = 1.0 - tf.matrix_band_part(tf.ones([length, length]), distance - 1, 0) ret = inf * (1.0 - lower_triangle + mask_triangle) ret = tf.reshape(ret, [1, 1, length, length]) else: raise ValueError('Unknown mode %s' % mode) return tf.cast(ret, dtype) 
nets.inception_v2_test.InceptionV2Test.Network|test|Build|Classification def testBuildClassificationNetwork(self): batch_size = 5 height, width = 224, 224 num_classes = 1000 inputs = tf.random_uniform((batch_size, height, width, 3)) logits, end_points = inception.inception_v2(inputs, num_classes) self.assertTrue(logits.op.name.startswith( 'InceptionV2/Logits/SpatialSqueeze')) self.assertListEqual(logits.get_shape().as_list(), [batch_size, num_classes]) self.assertTrue('Predictions' in end_points) self.assertListEqual(end_points['Predictions'].get_shape().as_list(), [ batch_size, num_classes]) 
learner.n|target|bellman|step def n_step_bellman_target(rewards, done, q_target, gamma, n_steps): """Computes n-step Bellman targets.  See section 2.3 of R2D2 paper (which does not mention the logic around end of episode).  Args: rewards: <float32>[time, batch_size] tensor. This is r_t in the equations below. done: <bool>[time, batch_size] tensor. This is done_t in the equations below. done_t should be true if the episode is done just after experimenting reward r_t. q_target: <float32>[time, batch_size] tensor. This is Q_target(s_{t+1}, a*) (where a* is an action chosen by the caller). gamma: Exponential RL discounting. n_steps: The number of steps to look ahead for computing the Bellman targets.  Returns: y_t targets as <float32>[time, batch_size] tensor. When n_steps=1, this is just:  $$r_t + gamma * (1 - done_t) * Q_{target}(s_{t+1}, a^*)$$  In the general case, this is:  $$(\\sum_{i=0}^{n-1} \\gamma ^ {i} * notdone_{t, i-1} * r_{t + i}) + \\gamma ^ n * notdone_{t, n-1} * Q_{target}(s_{t + n}, a^*) $$  where notdone_{t,i} is defined as:  $$notdone_{t,i} = \\prod_{k=0}^{k=i}(1 - done_{t+k})$$  The last n_step-1 targets cannot be computed with n_step returns, since we run out of Q_{target}(s_{t+n}). Instead, they will use n_steps-1, .., 1 step returns. For those last targets, the last Q_{target}(s_{t}, a^*) is re-used multiple times. """ bellman_target = tf.concat([tf.zeros_like(q_target[0:1]), q_target] + [ (q_target[-1:] / gamma ** k) for k in range(1, n_steps)], axis=0) done = tf.concat([done] + [tf.zeros_like(done[0:1])] * n_steps, axis=0) rewards = tf.concat([rewards] + [tf.zeros_like(rewards[0:1])] * n_steps, axis=0) for _ in range(n_steps): rewards = rewards[:-1] done = done[:-1] bellman_target = rewards + gamma * (1.0 - tf.cast(done, tf.float32) ) * bellman_target[1:] return bellman_target 
avod.core.models.rpn_model.RpnModel.anchor|inputs|fill|pl def _fill_anchor_pl_inputs(self, anchors_info, ground_plane, image_shape, stereo_calib_p2, sample_name, sample_augs): """ Fills anchor placeholder inputs with corresponding data  Args: anchors_info: anchor info from mini_batch_utils ground_plane: ground plane coefficients image_shape: image shape (h, w), used for projecting anchors sample_name: name of the sample, e.g. "000001" sample_augs: list of sample augmentations """ all_anchor_boxes_3d = [] anchors_ious = [] anchor_offsets = [] anchor_classes = [] if len(self.dataset.classes) > 1: for class_idx in range(len(self.dataset.classes)): grid_anchor_boxes_3d = self._anchor_generator.generate(area_3d= self._area_extents, anchor_3d_sizes=self._cluster_sizes[ class_idx], anchor_stride=self._anchor_strides[class_idx], ground_plane=ground_plane) all_anchor_boxes_3d.append(grid_anchor_boxes_3d) all_anchor_boxes_3d = np.concatenate(all_anchor_boxes_3d) else: class_idx = 0 grid_anchor_boxes_3d = self._anchor_generator.generate(area_3d=self ._area_extents, anchor_3d_sizes=self._cluster_sizes[class_idx], anchor_stride=self._anchor_strides[class_idx], ground_plane= ground_plane) all_anchor_boxes_3d = grid_anchor_boxes_3d sample_has_labels = True if self._train_val_test in ['train', 'val']: if anchors_info: (anchor_indices, anchors_ious, anchor_offsets, anchor_classes ) = anchors_info anchor_boxes_3d_to_use = all_anchor_boxes_3d[anchor_indices] else: train_cond = (self._train_val_test == 'train' and self. _train_on_all_samples) eval_cond = (self._train_val_test == 'val' and self. _eval_all_samples) if train_cond or eval_cond: sample_has_labels = False else: sample_has_labels = False if not sample_has_labels: voxel_grid_2d = self.dataset.kitti_utils.create_sliced_voxel_grid_2d( sample_name, self.dataset.bev_source, image_shape=image_shape) anchors_to_use = box_3d_encoder.box_3d_to_anchor(all_anchor_boxes_3d) empty_filter = anchor_filter.get_empty_anchor_filter_2d(anchors_to_use, voxel_grid_2d, density_threshold=1) anchor_boxes_3d_to_use = all_anchor_boxes_3d[empty_filter] anchor_boxes_3d_to_use = np.asarray(anchor_boxes_3d_to_use) anchors_ious = np.asarray(anchors_ious) anchor_offsets = np.asarray(anchor_offsets) anchor_classes = np.asarray(anchor_classes) if kitti_aug.AUG_FLIPPING in sample_augs: anchor_boxes_3d_to_use = kitti_aug.flip_boxes_3d(anchor_boxes_3d_to_use , flip_ry=False) if anchors_info: anchor_offsets[:, (0)] = -anchor_offsets[:, (0)] anchors_to_use = box_3d_encoder.box_3d_to_anchor(anchor_boxes_3d_to_use) num_anchors = len(anchors_to_use) bev_anchors, bev_anchors_norm = anchor_projector.project_to_bev( anchors_to_use, self._bev_extents) img_anchors, img_anchors_norm = anchor_projector.project_to_image_space( anchors_to_use, stereo_calib_p2, image_shape) self._bev_anchors_norm = bev_anchors_norm[:, ([1, 0, 3, 2])] self._img_anchors_norm = img_anchors_norm[:, ([1, 0, 3, 2])] self._placeholder_inputs[self.PL_ANCHORS] = anchors_to_use if self._train_val_test in ['train', 'val'] and len(anchors_ious) > 0: self._placeholder_inputs[self.PL_ANCHOR_IOUS] = anchors_ious self._placeholder_inputs[self.PL_ANCHOR_OFFSETS] = anchor_offsets self._placeholder_inputs[self.PL_ANCHOR_CLASSES] = anchor_classes elif self._train_val_test in ['test'] or len(anchors_ious) == 0: self._placeholder_inputs[self.PL_ANCHOR_IOUS] = np.zeros(num_anchors) self._placeholder_inputs[self.PL_ANCHOR_OFFSETS] = np.zeros([ num_anchors, 6]) self._placeholder_inputs[self.PL_ANCHOR_CLASSES] = np.zeros(num_anchors ) else: raise ValueError('Got run mode {}, and non-empty anchor info'. format(self._train_val_test)) self._placeholder_inputs[self.PL_BEV_ANCHORS] = bev_anchors self._placeholder_inputs[self.PL_BEV_ANCHORS_NORM] = self._bev_anchors_norm self._placeholder_inputs[self.PL_IMG_ANCHORS] = img_anchors self._placeholder_inputs[self.PL_IMG_ANCHORS_NORM] = self._img_anchors_norm 
network.NeuralNetwork.finalize def finalize(self): """Finalize graph for training """ out_dim = self.xs[-1].get_shape().as_list()[-1] assert out_dim == self.n_out_classes, 'Final layer output dimension should be equal to n_out_classes.' assert not self.finalized, 'Graph already finalized.' self.y = tf.placeholder(self.dtype, [None, self.n_out_classes], 'output_placeholder') self.loss = self.loss_func(self.y, self.out) correct_prediction = tf.equal(tf.argmax(self.out, 1), tf.argmax(self.y, 1)) self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, self.dtype)) pout = -tf.gradients(self.loss, self.out)[0] self.ps = [pout] for l in reversed(range(self.n_layers)): pout = self.layers[l].backward(self.xs[l], pout) self.ps.append(pout) self.ps.reverse() for l in range(self.n_layers): if self.layers[l].is_trainable: self.layers[l].set_ops(self.xs[l], self.ps[l + 1]) self.saver = tf.train.Saver(tf.global_variables()) self.init = tf.global_variables_initializer() tf.Graph.finalize(tf.get_default_graph()) self.finalized = True 
enas.ptb.ptb_enas_child.PTBEnasChild.train|build def _build_train(self): print('Build train graph') all_h, self.train_reset = self._model(self.x_train, True, False) log_probs = self._get_log_probs(all_h, self.y_train, batch_size=self. batch_size, is_training=True) self.loss = tf.reduce_sum(log_probs) / tf.to_float(self.batch_size) self.train_ppl = tf.exp(tf.reduce_mean(log_probs)) tf_variables = [var for var in tf.trainable_variables() if var.name. startswith(self.name)] self.num_vars = count_model_params(tf_variables) print('-' * 80) print('Model has {} parameters'.format(self.num_vars)) loss = self.loss if self.rnn_l2_reg is not None: loss += self.rnn_l2_reg * tf.reduce_sum(all_h ** 2) / tf.to_float(self .batch_size) if self.rnn_slowness_reg is not None: loss += self.rnn_slowness_reg * self.all_h_diff / tf.to_float(self. batch_size) self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name ='global_step') (self.train_op, self.lr, self.grad_norm, self.optimizer, self.grad_norms ) = (get_train_ops(loss, tf_variables, self.global_step, clip_mode= self.clip_mode, grad_bound=self.grad_bound, l2_reg=self.l2_reg, lr_warmup_val=self.lr_warmup_val, lr_warmup_steps=self. lr_warmup_steps, lr_init=self.lr_init, lr_dec_start=self. lr_dec_start, lr_dec_every=self.lr_dec_every, lr_dec_rate=self. lr_dec_rate, lr_dec_min=self.lr_dec_min, optim_algo=self.optim_algo, moving_average=self.optim_moving_average, sync_replicas=self. sync_replicas, num_aggregate=self.num_aggregate, num_replicas=self. num_replicas, get_grad_norms=True)) 
EndToEndClassification.Dataset.esc50_processor.processed|features|esc|combined|dump def _dump_features_processed_esc50_combined(load_parsed_esc50, save_folder_path, save_folder_path_raw, augmentations=4, frames=101, seed=41, batch_size=50): """ Generates ESC50 features from the 'processed' dataset. It does so according to the specifications in the paper. Each of the 2000 5sec clips is cut into 50% overlapping segments. 4 augmentations are made of each. Largely the same in implementation as the original Piczak code.  Args: load_parsed_esc50 (str): folder containing the esc50_meta.pkl and esc50_audio.dat files. save_folder_path (str): folder for saving logscaled mel features. save_folder_path_raw (str): folder for saving raw waveform features. augmentations (int): number of augmentations of each segment. frames (int): nr of frames of the mel features. seed (int): seed for pseudo RNG. batch_size (int): batch size for multiprocessing (note, this has nothing to do with the minibatch size). """ np.random.seed(seed) if isinstance(load_parsed_esc50, str): meta, audio = load_processed_esc50(load_parsed_esc50) else: raise ValueError('load_parsed_esc50 should be a to folder') if not (os.path.isdir(save_folder_path) and os.path.isdir( save_folder_path_raw)): raise ValueError('please provide valid folders for saving the features' ) segments = [] segments_raw = [] for b in range(len(audio) // batch_size + 1): print('b:{}'.format(b)) start = b * batch_size end = (b + 1) * batch_size if end > len(audio): end = len(audio) seg_combined = Parallel(n_jobs=CPU_COUNT)(delayed( _extract_segments_combined)((audio[(i), :], meta.loc[i, 'filename'], meta.loc[i, 'fold'], meta.loc[i, 'category'], meta .loc[i, 'category_name'], 0, frames)) for i in range(start, end)) segments_batch = [seg[0] for seg in seg_combined] segments_raw_batch = [seg[1] for seg in seg_combined] segments.extend(segments_batch) segments_raw.extend(segments_raw_batch) for _ in range(augmentations): seg_combined = Parallel(n_jobs=CPU_COUNT)(delayed( _extract_segments_combined)((_augment_esc50(audio[(i), :]), meta.loc[i, 'filename'], meta.loc[i, 'fold'], meta.loc[i, 'category'], meta.loc[i, 'category_name'], 1, frames)) for i in range(start, end)) segments_batch = [seg[0] for seg in seg_combined] segments_raw_batch = [seg[1] for seg in seg_combined] segments.extend(segments_batch) segments_raw.extend(segments_raw_batch) segments = [pd.concat(segments, ignore_index=True)] segments_raw = [pd.concat(segments_raw, ignore_index=True)] print('{} / {}'.format(end, len(audio))) segments[0][0:30000].to_pickle(os.path.join(save_folder_path, 'esc50_features_long_logspec0.pkl')) segments[0][30000:60000].to_pickle(os.path.join(save_folder_path, 'esc50_features_long_logspec1.pkl')) segments[0][60000:].to_pickle(os.path.join(save_folder_path, 'esc50_features_long_logspec2.pkl')) segments_raw[0][0:30000].to_pickle(os.path.join(save_folder_path_raw, 'esc50_features_long_raw0.pkl')) segments_raw[0][30000:60000].to_pickle(os.path.join( save_folder_path_raw, 'esc50_features_long_raw1.pkl')) segments_raw[0][60000:].to_pickle(os.path.join(save_folder_path_raw, 'esc50_features_long_raw2.pkl')) 
gym_pycolab.things.Drape.curtain @property def curtain(self): return self._c_u_r_t_a_i_n 
nmt.attention_utils.LuongAttention.call def __call__(self, query, state): """Score the query based on the keys and values.  Args: query: Tensor of dtype matching `self.values` and shape `[batch_size, query_depth]`. state: Tensor of dtype matching `self.values` and shape `[batch_size, alignments_size]` (`alignments_size` is memory's `max_time`).  Returns: alignments: Tensor of dtype matching `self.values` and shape `[batch_size, alignments_size]` (`alignments_size` is memory's `max_time`). """ with variable_scope.variable_scope(None, 'luong_attention', [query]): score = _luong_score(query, self._keys, self._scale) alignments = self._probability_fn(score, state) next_state = alignments return alignments, next_state 
svae_dc.svae_dc_args.get|args|all def get_all_args(): parser = argparse.ArgumentParser(description='SVAE') parser.add_argument('--run_id', type=int, default=0, help='Run ID') parser.add_argument('--seed', type=int, default=0, help='Random seed') parser.add_argument('--gpu', type=int, default=0, help='GPU id') parser.add_argument('--learning_rate', type=float, default=0.0001, help ='Learning rate') parser.add_argument('--batch_size', type=int, default=32, help='Batch size' ) parser.add_argument('--debug', type=int, default=0, help='Debug level') parser.add_argument('--output_prefix', type=str, default=os.path. expanduser('svaedata/')) parser.add_argument('--log_interval', type=int, default=100, help= 'Log every k train updates (epochs)') parser.add_argument('--save_interval', type=int, default=1000, help= 'Save every k train updates (epochs)') parser.add_argument('--video_steps_interval', type=int, default=10, help='Interval for steps to sim and render for eval') parser.add_argument('--svae_dc_load', type=str, default='', help= 'Checkpoint to load to continue training') parser.add_argument('--svae_dc_hidden_size', type=int, default=64, help ='Hidden size for all NNs') parser.add_argument('--svae_dc_tau_size', type=int, default=7, help= 'Size of each tau_k (in tau_{1:K})') parser.add_argument('--svae_dc_K_tau_scale', type=float, default=0.05, help='How much to shrink K=scale*T (T in xi_{1:T})') parser.add_argument('--svae_dc_logvar_patience', type=int, default=None, help='Number of epochs for logvar increase stages') parser.add_argument('--svae_dc_num_data_passes', type=int, default= 10000, help='Number of passes over training data') parser.add_argument('--svae_dc_use_laplace', type=int, default=1, help= 'Whether to use Laplace instead of Gaussian') parser.add_argument('--svae_dc_coder_type', type=str, default='conv', choices=['conv'], help='Decoder/Encoder NN type') parser.add_argument('--svae_dc_latent_type', type=str, default='conv', choices=['conv', 'mlp'], help='NN type for latent dynamics') parser.add_argument('--svae_dc_good_th', type=float, default=0.35, help ='Threshold for traj to be considered acceptable') parser.add_argument('--svae_dc_gen_beta', type=float, default=None, help='How important to sync with gen/latent part.') parser.add_argument('--env_name', type=str, default='YumiVel-v2', help= 'Gym env name string') parser.add_argument('--controller_class', type=str, default= 'WaypointsVelPolicy', choices=['DaisyGait11DPolicy', 'DaisyGait27DPolicy', 'DaisyTripod27DPolicy', 'WaypointsPosPolicy', 'WaypointsMinJerkPolicy', 'WaypointsEEPolicy', 'WaypointsVelPolicy' ], help='Controller class name') parser.add_argument('--max_episode_steps', type=int, default=None, help ='Length of env trajectories') parser.add_argument('--env_episodes_file', type=str, default=None, help ='Load episodes from file instead of collecting') parser.add_argument('--test_env_episodes_file', type=str, default=None, help='Test trajectories (not used for training)') args = parser.parse_args() return args 
kaffe.graph.Graph.Graph def __init__(self, nodes=None, name=None): self.nodes = nodes or [] self.node_lut = {node.name: node for node in self.nodes} self.name = name 
evaluator.input|layer|set def set_input_layer(): config = configparser.ConfigParser() config.read('input.ini') input_id = int(config['INPUT']['input_layer']) print('input_id : ' + str(input_id)) input_image2 = cv2.imread('blood1.png') if input_id < num_classes - 1: input_image2 = X[input_id] else: input_image2 = X[0] input_image2 = np.expand_dims(input_image2, axis=0) return input_image2 
models.cae.Autoencoder.summary|init def init_summary(self, sess): """Init the summary folder with current time and date @param sess (tf.Session) the current session """ summary_id = strftime('%H%M%S_%d%m%Y', gmtime()) summary_folder = self.dir_header + '/log/' + summary_id + '_iter_' + str( self.start_iteration) self.tf_summary_writer = tf.summary.FileWriter(summary_folder, sess.graph) self.summaries = tf.summary.merge_all() 
avod.core.summary_utils.feature|from|maps|add|dict def add_feature_maps_from_dict(end_points, layer_name): """ Calls add_feature_maps for a specified layer in a dictionary of end points  Args: end_points: dictionary of network end points layer_name: dict key of the layer to add """ feature_maps = end_points.get(layer_name) add_feature_maps(feature_maps, layer_name) 
plato.agent.conversational_agent.conversational_single_agent.ConversationalSingleAgent.del def __del__(self): """ Do some house-keeping and save the models.  :return: nothing """ if self.recorder and self.SAVE_LOG: self.recorder.save() if self.nlu: self.nlu.save() if self.dialogue_manager: self.dialogue_manager.save() if self.nlg: self.nlg.save() self.curr_state = None self.prev_state = None self.curr_state = None self.prev_usr_utterance = None self.prev_sys_utterance = None self.prev_action = None self.prev_reward = None self.prev_success = None self.prev_task_success = None 
layers.MaxPoolingLayer.forward def forward(self, x): """Forward propagation  Arguments: x {tf tensor} -- x_{t}  Returns: x {tf tensor} -- x_{t+1} """ return tf.nn.max_pool(x, self.ksize, self.strides, self.padding) 
common.get_retrain_interpolation_formatter.SparsityFormatter.Formatter|Sparsity def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) 
main.evaluate|scws def evaluate_scws(sess, en_dh, batch_size, en_ss, en_srl): global global_maxSimC global global_avgSimC [senseVec, senseScore] = getEmbedding(sess, en_dh, batch_size, en_ss, en_srl.embedded_sense_input, en_srl.selected_sense_input_indices, scws=True) indices1 = range(0, len(senseVec), 2) indices2 = range(1, len(senseVec), 2) avgSimC = calAvgSimC(en_dh.scws_test_score, senseVec[indices1], senseScore[indices1], senseVec[indices2], senseScore[indices2]) maxSimC = calMaxSimC(en_dh.scws_test_score, senseVec[indices1], senseScore[indices1], senseVec[indices2], senseScore[indices2]) scores = [maxSimC, avgSimC] if maxSimC > ckpt_threshold and maxSimC > global_maxSimC: global_maxSimC = maxSimC eval_save_model(sess, '-bestMaxC-') if avgSimC > ckpt_threshold and avgSimC > global_avgSimC: global_avgSimC = avgSimC eval_save_model(sess, '-bestAvgC-') print('scws AvgSimC =', '{:.5f}'.format(avgSimC), 'MaxSimC =', '{:.5f}' .format(maxSimC)) ret_str = 'scws AvgSimC = {:.5f}'.format(avgSimC ) + ' MaxSimC = {:.5f}'.format(maxSimC) return ret_str 
neural_style.get|image|noise def get_noise_image(noise_ratio, content_img): np.random.seed(args.seed) noise_img = np.random.uniform(-20.0, 20.0, content_img.shape).astype(np .float32) img = noise_ratio * noise_img + (1.0 - noise_ratio) * content_img return img 
dataset.ART.load|data def load_data(self, annotation_file=art_annotation): count = 0 with open(annotation_file) as f: json_data = json.load(f) for filename in os.listdir(self.data_path): img_name = os.path.join(self.data_path, filename) anno_data = json_data[filename[:-4]][0] illegibility = anno_data['illegibility'] if illegibility: continue polygon = anno_data['points'] transcripts = anno_data['transcription'] languages = anno_data['language'] if len(transcripts) > self.max_len - 1: continue transcripts = preprocess(transcripts) skip = False for char in transcripts: if char not in self.label_dict.keys(): skip = True if skip: count = count + 1 continue seq_label = [] for char in transcripts: seq_label.append(self.label_dict[char]) seq_label.append(self.label_dict['EOS']) non_zero_count = len(seq_label) seq_label = seq_label + [self.label_dict['EOS']] * (self. max_len - non_zero_count) mask = [1] * non_zero_count + [0] * (self.max_len - non_zero_count) points_x = [point[0] for point in polygon] points_y = [point[1] for point in polygon] bbox = [np.amin(points_y), np.amin(points_x), np.amax(points_y), np.amax(points_x)] bbox = [int(item) for item in bbox] bbox_w, bbox_h = bbox[3] - bbox[1], bbox[2] - bbox[0] if bbox_w < 8 or bbox_h < 8: continue self.filenames.append(img_name) self.labels.append(seq_label) self.masks.append(mask) self.bboxes.append(bbox) self.points.append(polygon) 
cleverhans.devtools.tests.docscrape.NumpyFunctionDocString.parse def _parse(self): self._parsed_data = {'Signature': '', 'Summary': '', 'Extended Summary': [], 'Parameters': [], 'Other Parameters': [], 'Returns': [], 'Raises': [], 'Warns': [], 'See Also': [], 'Notes': [], 'References': '', 'Examples': '', 'index': {}} return NumpyDocString._parse(self) 
main.evaluate|bcws def evaluate_bcws(sess, en_dh, ch_dh, batch_size, en_ss, ch_ss, en_opt, ch_opt ): global global_bmaxSimC global global_bavgSimC [en_senseVec, en_senseScore] = getEmbedding(sess, en_dh, batch_size, en_ss, *bcws_dict[en_opt]) [ch_senseVec, ch_senseScore] = getEmbedding(sess, ch_dh, batch_size, ch_ss, *bcws_dict[ch_opt]) assert en_dh.test_score == ch_dh.test_score avgSimC = calAvgSimC(en_dh.test_score, en_senseVec, en_senseScore, ch_senseVec, ch_senseScore) maxSimC = calMaxSimC(en_dh.test_score, en_senseVec, en_senseScore, ch_senseVec, ch_senseScore) if maxSimC > bckpt_threshold and maxSimC > global_bmaxSimC: global_bmaxSimC = maxSimC eval_save_model(sess, '-bestbMaxC_{}_{}-'.format(en_opt, ch_opt)) if avgSimC > bckpt_threshold and avgSimC > global_bavgSimC: global_bavgSimC = avgSimC eval_save_model(sess, '-bestbAvgC_{}_{}-'.format(en_opt, ch_opt)) ret_str = 'bcws {} {} AvgSimC = '.format(en_opt, ch_opt) + '{:.5f}'.format( avgSimC) + ' MaxSimC = ' + '{:.5f}'.format(maxSimC) print(ret_str) return ret_str 
model.mrcnn|coord|graph|bins|loss def mrcnn_coord_bins_loss_graph(target_masks, target_coord, target_class_ids, pred_coord): """Mask L2 loss for the coordinates head.  target_masks: [batch, num_rois, height, width]. A float32 tensor of values 0 or 1. Uses zero padding to fill array. target_coord: [batch, num_rois, height, width]. Might be for x, y or z channel. A float32 tensor of values in the range of [0, 1]. Uses zero padding to fill array. target_class_ids: [batch, num_rois]. Integer class IDs. Zero padded. pred_coord: [batch, proposals, height, width, num_classes, num_bins] float32 tensor with values from 0 to 1. """ num_bins = tf.shape(pred_coord)[-1] target_class_ids = K.reshape(target_class_ids, (-1,)) mask_shape = tf.shape(target_masks) target_masks = K.reshape(target_masks, (-1, mask_shape[2], mask_shape[3])) target_coord = K.reshape(target_coord, (-1, mask_shape[2], mask_shape[3])) coord_shape = tf.shape(target_coord) target_coord_bins = target_coord * tf.cast(num_bins, tf.float32) - 1e-06 target_coord_bins = tf.floor(target_coord_bins) target_coord_bins = tf.cast(target_coord_bins, dtype=tf.int32) target_coord_bins_flatten = K.flatten(target_coord_bins) target_coord_one_hot = tf.one_hot(target_coord_bins_flatten, num_bins) target_coord_one_hot = K.reshape(target_coord_one_hot, (coord_shape[0], coord_shape[1], coord_shape[2], num_bins)) pred_shape = tf.shape(pred_coord) pred_coord = K.reshape(pred_coord, (-1, pred_shape[2], pred_shape[3], pred_shape[4], pred_shape[5])) pred_coord = tf.transpose(pred_coord, [0, 3, 1, 2, 4]) positive_ix = tf.where(target_class_ids > 0)[:, (0)] positive_class_ids = tf.cast(tf.gather(target_class_ids, positive_ix), tf.int64) indices = tf.stack([positive_ix, positive_class_ids], axis=1) y_true = tf.gather(target_coord_one_hot, positive_ix) mask = tf.gather(target_masks, positive_ix) mask = tf.cast(mask, dtype=tf.bool) y_true_in_mask = tf.boolean_mask(y_true, mask) y_pred = tf.gather_nd(pred_coord, indices) y_pred_in_mask = tf.boolean_mask(y_pred, mask) coord_loss_in_mask = K.categorical_crossentropy(y_true_in_mask, y_pred_in_mask) mean_loss = K.mean(coord_loss_in_mask) loss = K.switch(tf.size(y_true) > 0, mean_loss, tf.constant(0.0)) loss = K.reshape(loss, [1, 1]) return loss 
cpplint.Common|Suffixes|Drop def _DropCommonSuffixes(filename): """Drops common suffixes like _test.cc or -inl.h from filename.  For example: >>> _DropCommonSuffixes('foo/foo-inl.h') 'foo/foo' >>> _DropCommonSuffixes('foo/bar/foo.cc') 'foo/bar/foo' >>> _DropCommonSuffixes('foo/foo_internal.h') 'foo/foo' >>> _DropCommonSuffixes('foo/foo_unusualinternal.h') 'foo/foo_unusualinternal'  Args: filename: The input filename.  Returns: The filename with the common suffix removed. """ for suffix in ('test.cc', 'regtest.cc', 'unittest.cc', 'inl.h', 'impl.h', 'internal.h'): if filename.endswith(suffix) and len(filename) > len(suffix ) and filename[-len(suffix) - 1] in ('-', '_'): return filename[:-len(suffix) - 1] return os.path.splitext(filename)[0] 
pytorch_pretrained_bert.modeling_openai.OpenAIGPTModel.AIGPT|Open|Model def __init__(self, config): super(OpenAIGPTModel, self).__init__(config) num_tokens = config.vocab_size + config.n_special self.tokens_embed = nn.Embedding(num_tokens, config.n_embd) self.positions_embed = nn.Embedding(config.n_positions, config.n_embd) self.drop = nn.Dropout(config.embd_pdrop) block = Block(config.n_ctx, config, scale=True) self.h = nn.ModuleList([copy.deepcopy(block) for _ in range(config. n_layer)]) self.apply(self.init_weights) 
utils_classif.file|size|feature|get|per def get_feature_size_per_file(f_name): """ Return the dimensionality of the features in a given file. Typically, this will be the number of bins in a T-F representation """ shape = get_shape(os.path.join(f_name.replace('.data', '.shape'))) return shape[1] 
plato.agent.conversational_agent.conversational_generic_agent.ConversationalGenericAgent.dialogue|end def end_dialogue(self): """ Perform final dialogue turn. Save models if applicable.  :return: """ if self.dialogue_episode % self.train_interval == 0: for m in self.ConversationalModules: if isinstance(m, list): for sm in m: sm.train(self.recorder.dialogues) else: m.train(self.recorder.dialogues) if self.dialogue_episode % self.SAVE_INTERVAL == 0: for m in self.ConversationalModules: if isinstance(m, list): for sm in m: sm.save() else: m.save() self.dialogue_episode += 1 if self.dialogue_turn > 0: self.total_dialogue_turns += self.dialogue_turn _, _, obj_succ = self.reward_func.calculate(self.get_state(), [], goal= self.agent_goal, agent_role=self.agent_role) self.num_successful_dialogues += 1 if obj_succ else 0 
train.train def train(m, mt, data_sampler, min_classes, max_classes, train_shots, test_shots, meta_batch, meta_iters, test_iters, train_step, name): sess = tf.Session() sess.run(tf.global_variables_initializer()) losses = [] temp_yp = [] temp_ypn = [] nls = [] aps = [] buffer = [] lossesB = [] train_gen = data_sampler.sample_Task(meta_batch, min_classes, max_classes + 1, train_shots, test_shots, 'train') if mt is not None: test_gen = data_sampler.sample_Task(meta_batch, min_classes, max_classes + 1, train_shots, test_shots, 'test') m.loadWeights(sess, name, step=str(int(train_step)), model_name=name + '.ckpt') print('Starting meta training:') start = time.time() for i in range(meta_iters): xb1, yb1, xb2, yb2 = next(train_gen) num_l = [len(np.unique(np.argmax(yb1, axis=-1)))] if m.maml_n == 2: sess.run(m.init_assign, feed_dict={m.label_n: [5]}) l, _, vals, ps = sess.run([m.train_loss, m.meta_op, m.val_losses, m .val_predictions], feed_dict={m.train_xb: xb1, m.train_yb: yb1, m.val_xb: xb2, m.val_yb: yb2, m.label_n: num_l}) if m.maml_n == 2: sess.run(m.final_assign, feed_dict={m.label_n: num_l}) losses.append(vals) lossesB.append(vals) buffer.append(l) aux = [] tmp_pred = np.argmax(np.reshape(ps[-1], [-1, num_l[0]]), axis=-1) tmp_true = np.argmax(np.reshape(yb2, [-1, num_l[0]]), axis=-1) for ccci in range(num_l[0]): tmp_idx = np.where(tmp_true == ccci)[0] aux.append(np.mean(tmp_pred[tmp_idx] == tmp_true[tmp_idx])) temp_yp.append(np.mean(tmp_pred == tmp_true)) temp_ypn.append(aux) if i % 100 == 0: testString = '' if mt is not None and i % 1000 == 0: lossestest = [] buffertest = [] lossesBtest = [] temp_yptest = [] for z in range(100): if m.maml_n == 2: sess.run(mt.init_assign, feed_dict={mt.label_n: [5]}) xb1, yb1, xb2, yb2 = next(test_gen) num_l = [len(np.unique(np.argmax(yb1, axis=-1)))] l, vals, ps = sess.run([mt.test_train_loss, mt. test_val_losses, mt.val_predictions], feed_dict={mt .train_xb: xb1, mt.train_yb: yb1, mt.val_xb: xb2, mt.val_yb: yb2, mt.label_n: num_l}) lossestest.append(vals) lossesBtest.append(vals) buffertest.append(l) temp_yptest.append(np.mean(np.argmax(ps[-1], axis=-1) == np.argmax(yb2, axis=-1))) testString = f""" TEST: TLoss {np.mean(buffertest):.3f} VLoss {np.mean(lossesBtest, axis=0)[-1]:.3f}, ACCURACY {np.mean(temp_yptest):.4f}""" print( f'Epoch {i}: TLoss {np.mean(buffer):.4f}, VLoss {np.mean(lossesB, axis=0)[-1]:.4f},' , f'Accuracy {np.mean(temp_yp):.4}', f", Per label acc: {[float('%.4f' % elem) for elem in aux]}", f'Finished in {(time.time() - start)}s', testString) buffer = [] lossesB = [] temp_yp = [] start = time.time() if i % 5000 == 0: print('Saving...') m.saveWeights(sess, name, i, model_name=name + '.ckpt') m.saveWeights(sess, name, i, model_name=name + '.ckpt') 
bert-master.tokenization_test.TokenizationTest.no|basic|lower|test|tokenizer def test_basic_tokenizer_no_lower(self): tokenizer = tokenization.BasicTokenizer(do_lower_case=False) self.assertAllEqual(tokenizer.tokenize(' \tHeLLo!how  \n Are yoU?  '), ['HeLLo', '!', 'how', 'Are', 'yoU', '?']) 
texar.modules.networks.conv_networks.Conv1DNetwork.hparams|build|layer def _build_layer_hparams(self): pool_hparams = self._build_pool_hparams() conv_pool_hparams = self._build_conv1d_hparams(pool_hparams) dense_hparams = self._build_dense_hparams()  def _dropout_hparams(layer_id): return {'type': 'Dropout', 'kwargs': {'rate': self._hparams. dropout_rate, 'name': 'dropout_%d' % layer_id}} dropout_conv = _to_list(self._hparams.dropout_conv) dropout_dense = _to_list(self._hparams.dropout_dense) layers_hparams = [] nconv = self._hparams.num_conv_layers for conv_i in range(nconv): if conv_i in dropout_conv: layers_hparams.append(_dropout_hparams(conv_i)) if isinstance(conv_pool_hparams[conv_i], (list, tuple)): layers_hparams += conv_pool_hparams[conv_i] else: layers_hparams.append(conv_pool_hparams[conv_i]) if nconv in dropout_conv: layers_hparams.append(_dropout_hparams(nconv)) ndense = self._hparams.num_dense_layers if ndense > 0: layers_hparams.append({'type': 'Flatten'}) for dense_i in range(ndense): if dense_i in dropout_dense: layers_hparams.append(_dropout_hparams(dense_i + nconv)) layers_hparams.append(dense_hparams[dense_i]) if ndense in dropout_dense: layers_hparams.append(_dropout_hparams(ndense + nconv)) return layers_hparams 
utils.get|image def get_image(image_path, input_height, input_width, resize_height=64, resize_width=64, crop=True, grayscale=False): image = imread(image_path, grayscale) return transform(image, input_height, input_width, resize_height, resize_width, crop) 
ops.conv|cond|concat def conv_cond_concat(x, y): """Concatenate conditioning vector on feature map axis.""" x_shapes = x.get_shape() y_shapes = y.get_shape() return tf.concat([x, y * tf.ones([x_shapes[0], x_shapes[1], x_shapes[2], y_shapes[3]])], 3) 
xlnet-master.tpu_estimator._OutfeedHostCall.Host|Call|Outfeed def __init__(self, ctx): self._ctx = ctx self._names = [] self._host_fns = {} self._tensor_keys = collections.defaultdict(list) self._tensors = collections.defaultdict(list) self._tensor_dtypes = collections.defaultdict(list) self._tensor_shapes = collections.defaultdict(list) 
enas.cifar10.image_ops.global|pool|avg def global_avg_pool(x, data_format='NHWC'): if data_format == 'NHWC': x = tf.reduce_mean(x, [1, 2]) elif data_format == 'NCHW': x = tf.reduce_mean(x, [2, 3]) else: raise NotImplementedError('Unknown data_format {}'.format(data_format)) return x 
thumt.utils.inference.get|fn|inference def _get_inference_fn(model_fns, features):  def inference_fn(inputs, state): local_features = {'source': features['source'], 'source_length': features['source_length'], 'target': tf.pad(inputs[:, 1:], [[0, 0], [0, 1]]), 'target_length': tf.fill([tf.shape(inputs)[0]], tf.shape(inputs)[1])} outputs = [] next_state = [] for model_fn, model_state in zip(model_fns, state): if model_state: output, new_state = model_fn(local_features, model_state) outputs.append(output) next_state.append(new_state) else: output = model_fn(local_features) outputs.append(output) next_state.append({}) log_prob = tf.add_n(outputs) / float(len(outputs)) return log_prob, next_state return inference_fn 
nets.pix2pix_test.DiscriminatorTest.wrog|four|paddig|test|layers def test_four_layers_wrog_paddig(self): batch_size = 2 input_size = 256 images = tf.ones((batch_size, input_size, input_size, 3)) with tf.contrib.framework.arg_scope(pix2pix.pix2pix_arg_scope()): with self.assertRaises(TypeError): pix2pix.pix2pix_discriminator(images, num_filters=[64, 128, 256, 512], padding=1.5) 
deepctr.contrib.utils.QAAttGRUCell.size|state @property def state_size(self): return self._num_units 
snresnet_64.SNResNetProjectionDiscriminator.Projection|Res|Net|SN|Discriminator def __init__(self, ch=64, n_classes=0, activation=F.relu): super(SNResNetProjectionDiscriminator, self).__init__() self.activation = activation initializer = chainer.initializers.GlorotUniform() with self.init_scope(): self.block1 = OptimizedBlock(3, ch) self.block2 = Block(ch, ch * 2, activation=activation, downsample=True) self.block3 = Block(ch * 2, ch * 4, activation=activation, downsample=True) self.block4 = Block(ch * 4, ch * 8, activation=activation, downsample=True) self.block5 = Block(ch * 8, ch * 16, activation=activation, downsample=True) self.l6 = SNLinear(ch * 16, 1, initialW=initializer) if n_classes > 0: self.l_y = SNEmbedID(n_classes, ch * 16, initialW=initializer) 
GaussianProcess_bound.gaussian|process|fit def fit_gaussian_process(X_train, y_train): bound = 1e-12, 1000000.0 rbf_kernel = RBF(length_scale=1, length_scale_bounds=bound) matern_kernel = Matern(length_scale=1.0, length_scale_bounds=bound, nu=0.5) matern_kernel_1 = Matern(length_scale=1.0, length_scale_bounds=bound, nu=1.5) matern_kernel_2 = Matern(length_scale=1.0, length_scale_bounds=bound, nu=2.5) periodic_kernel = ExpSineSquared(length_scale=1.0, periodicity=1.0, length_scale_bounds=bound, periodicity_bounds=bound) rq_kernel = RationalQuadratic(length_scale=1.0, alpha=1.0, length_scale_bounds=bound, alpha_bounds=bound) if '_diff' in keyword: gp_kernel = matern_kernel_1 else: gp_kernel = matern_kernel_2 model = GaussianProcessRegressor(kernel=gp_kernel, n_restarts_optimizer =1500) model.fit(X_train, y_train) return model 
examples.datasets.get|dataset def get_dataset(name, n_train=None, n_test=None, permute_train=False): """Download, parse and process MNIST data to unit scale and one-hot labels.""" ds_train, ds_test = tfds.as_numpy(tfds.load(name, split=['train', 'test'], batch_size=-1, as_dataset_kwargs={'shuffle_files': False})) train_images, train_labels, test_images, test_labels = ds_train['image' ], ds_train['label'], ds_test['image'], ds_test['label'] train_images = _partial_flatten_and_normalize(train_images) test_images = _partial_flatten_and_normalize(test_images) train_labels = _one_hot(train_labels, 10) test_labels = _one_hot(test_labels, 10) if n_train is not None: train_images = train_images[:n_train] train_labels = train_labels[:n_train] if n_test is not None: test_images = test_images[:n_test] test_labels = test_labels[:n_test] if permute_train: perm = np.random.RandomState(0).permutation(train_images.shape[0]) train_images = train_images[perm] train_labels = train_labels[perm] return train_images, train_labels, test_images, test_labels 
gym_daisy_custom.envs.daisy_custom_env.DaisyCustomEnv.reset def reset(self): if self.visualize: input('Press Enter to continue env reset...') self.stepnum = 0 self.episode_reward = 0 self.badlen = 0 state_dict, _, _ = super(DaisyCustomEnv, self)._daisy_base_reset() return self.state_from_state_dict(state_dict) 
bert_classifier_main.main def main(_): """ Builds the model and runs. """ if FLAGS.distributed: import horovod.tensorflow as hvd hvd.init() tf.logging.set_verbosity(tf.logging.INFO) tx.utils.maybe_create_dir(FLAGS.output_dir) bert_pretrain_dir = ('bert_pretrained_models/%s' % FLAGS. config_bert_pretrain) if FLAGS.config_format_bert == 'json': bert_config = model_utils.transform_bert_to_texar_config(os.path. join(bert_pretrain_dir, 'bert_config.json')) elif FLAGS.config_format_bert == 'texar': bert_config = importlib.import_module( 'bert_config_lib.config_model_%s' % FLAGS.config_bert_pretrain) else: raise ValueError('Unknown config_format_bert.') num_classes = config_data.num_classes num_train_data = config_data.num_train_data if FLAGS.distributed: config_data.train_hparam['dataset']['num_shards'] = hvd.size() config_data.train_hparam['dataset']['shard_id'] = hvd.rank() config_data.train_hparam['batch_size'] //= hvd.size() train_dataset = tx.data.TFRecordData(hparams=config_data.train_hparam) eval_dataset = tx.data.TFRecordData(hparams=config_data.eval_hparam) test_dataset = tx.data.TFRecordData(hparams=config_data.test_hparam) iterator = tx.data.FeedableDataIterator({'train': train_dataset, 'eval': eval_dataset, 'test': test_dataset}) batch = iterator.get_next() input_ids = batch['input_ids'] segment_ids = batch['segment_ids'] batch_size = tf.shape(input_ids)[0] input_length = tf.reduce_sum(1 - tf.to_int32(tf.equal(input_ids, 0)), axis=1) with tf.variable_scope('bert'): embedder = tx.modules.WordEmbedder(vocab_size=bert_config. vocab_size, hparams=bert_config.embed) word_embeds = embedder(input_ids) segment_embedder = tx.modules.WordEmbedder(vocab_size=bert_config. type_vocab_size, hparams=bert_config.segment_embed) segment_embeds = segment_embedder(segment_ids) position_embedder = tx.modules.PositionEmbedder(position_size= bert_config.position_size, hparams=bert_config.position_embed) seq_length = tf.ones([batch_size], tf.int32) * tf.shape(input_ids)[1] pos_embeds = position_embedder(sequence_length=seq_length) input_embeds = word_embeds + segment_embeds + pos_embeds encoder = tx.modules.TransformerEncoder(hparams=bert_config.encoder) output = encoder(input_embeds, input_length) with tf.variable_scope('pooler'): bert_sent_hidden = tf.squeeze(output[:, 0:1, :], axis=1) bert_sent_output = tf.layers.dense(bert_sent_hidden, config_downstream.hidden_dim, activation=tf.tanh) output = tf.layers.dropout(bert_sent_output, rate=0.1, training =tx.global_mode_train()) logits = tf.layers.dense(output, num_classes, kernel_initializer=tf. truncated_normal_initializer(stddev=0.02)) preds = tf.argmax(logits, axis=-1, output_type=tf.int32) accu = tx.evals.accuracy(batch['label_ids'], preds) loss = tf.losses.sparse_softmax_cross_entropy(labels=batch['label_ids'], logits=logits) global_step = tf.Variable(0, trainable=False) static_lr = config_downstream.lr['static_lr'] num_train_steps = int(num_train_data / config_data.train_batch_size * config_data.max_train_epoch) num_warmup_steps = int(num_train_steps * config_data.warmup_proportion) lr = model_utils.get_lr(global_step, num_train_steps, num_warmup_steps, static_lr) opt = tx.core.get_optimizer(global_step=global_step, learning_rate=lr, hparams=config_downstream.opt) if FLAGS.distributed: opt = hvd.DistributedOptimizer(opt) train_op = tf.contrib.layers.optimize_loss(loss=loss, global_step= global_step, learning_rate=None, optimizer=opt)  def _is_head(): if not FLAGS.distributed: return True else: return hvd.rank() == 0  def _train_epoch(sess): """Trains on the training set, and evaluates on the dev set periodically. """ iterator.restart_dataset(sess, 'train') fetches = {'train_op': train_op, 'loss': loss, 'batch_size': batch_size, 'step': global_step} while True: try: feed_dict = {iterator.handle: iterator.get_handle(sess, 'train'), tx.global_mode(): tf.estimator.ModeKeys.TRAIN} rets = sess.run(fetches, feed_dict) step = rets['step'] dis_steps = config_data.display_steps if _is_head() and dis_steps > 0 and step % dis_steps == 0: tf.logging.info('step:%d; loss:%f' % (step, rets['loss'])) eval_steps = config_data.eval_steps if _is_head() and eval_steps > 0 and step % eval_steps == 0: _eval_epoch(sess) except tf.errors.OutOfRangeError: break  def _eval_epoch(sess): """Evaluates on the dev set. """ iterator.restart_dataset(sess, 'eval') cum_acc = 0.0 cum_loss = 0.0 nsamples = 0 fetches = {'accu': accu, 'loss': loss, 'batch_size': batch_size} while True: try: feed_dict = {iterator.handle: iterator.get_handle(sess, 'eval'), tx.context.global_mode(): tf.estimator. ModeKeys.EVAL} rets = sess.run(fetches, feed_dict) cum_acc += rets['accu'] * rets['batch_size'] cum_loss += rets['loss'] * rets['batch_size'] nsamples += rets['batch_size'] except tf.errors.OutOfRangeError: break tf.logging.info('eval accu: {}; loss: {}; nsamples: {}'.format( cum_acc / nsamples, cum_loss / nsamples, nsamples))  def _test_epoch(sess): """Does predictions on the test set. """ iterator.restart_dataset(sess, 'test') _all_preds = [] while True: try: feed_dict = {iterator.handle: iterator.get_handle(sess, 'test'), tx.context.global_mode(): tf.estimator. ModeKeys.PREDICT} _preds = sess.run(preds, feed_dict=feed_dict) _all_preds.extend(_preds.tolist()) except tf.errors.OutOfRangeError: break output_file = os.path.join(FLAGS.output_dir, 'test_results.tsv') with tf.gfile.GFile(output_file, 'w') as writer: writer.write('\n'.join(str(p) for p in _all_preds)) init_checkpoint = os.path.join(bert_pretrain_dir, 'bert_model.ckpt') model_utils.init_bert_checkpoint(init_checkpoint) if FLAGS.distributed: bcast = hvd.broadcast_global_variables(0) session_config = tf.ConfigProto() if FLAGS.distributed: session_config.gpu_options.visible_device_list = str(hvd.local_rank()) with tf.Session(config=session_config) as sess: sess.run(tf.global_variables_initializer()) sess.run(tf.local_variables_initializer()) sess.run(tf.tables_initializer()) if FLAGS.distributed: bcast.run() saver = tf.train.Saver() if FLAGS.checkpoint: saver.restore(sess, FLAGS.checkpoint) iterator.initialize_dataset(sess) if FLAGS.do_train: for i in range(config_data.max_train_epoch): _train_epoch(sess) saver.save(sess, FLAGS.output_dir + '/model.ckpt') if FLAGS.do_eval: _eval_epoch(sess) if FLAGS.do_test: _test_epoch(sess) 
modeling_gpt2_test.GPT2ModelTest.GPT2ModelTester.gpt|model|output|check def check_gpt2_model_output(self, result): self.parent.assertListEqual(list(result['hidden_states'].size()), [self .batch_size, self.n_choices, self.seq_length, self.n_embd]) 
tensorflow_translator.TFTranslator.matmul|resources def matmul_resources(self, op): """ checks which one of the direct ancestor tf.Operations is a constant and returns the underlying tensor as a numpy.ndarray inside a tuple. The matrix is manipulated in a way that it can be used as the left multiplier in the matrix multiplication.  Arguments --------- op : tf.Operation must have type "MatMul"  Return ------ output : tuple tuple with the matrix (of type numpy.ndarray) as its only item """ inputs = op.inputs left = inputs[0] right = inputs[1] if left.op.type == 'Const': matrix = self.sess.run(left) if not op.get_attr('transpose_a' ) else self.sess.run(left).transpose() else: matrix = self.sess.run(right).transpose() if not op.get_attr( 'transpose_b') else self.sess.run(right) return matrix, 
classification.ops.utils.params|to|d|mat|layer def mat2d_to_layer_params(vector_template, mat2d): """Converts a canonical 2D matrix representation back to a vector. Args: vector_template: A Tensor or pair of Tensors shaped like layer parameters. mat2d: A 2D Tensor with the same shape as the value of layer_params_to_mat2d(vector_template). Returns: A Tensor or pair of Tensors with the same coefficients as mat2d and the same shape as vector_template. """ if isinstance(vector_template, (tuple, list)): w_part, b_part = mat2d[:-1], mat2d[-1] return array_ops.reshape(w_part, vector_template[0].shape), b_part else: return array_ops.reshape(mat2d, vector_template.shape) 
data_loader.thread|gen|patches def gen_patches_thread(file_name, out_que): img = load_raw_image_packed(file_name) dim1, h, w, c = img.shape patches = None cam_iso = file_name[-41:-33] for i in range(0, h - patch_size + 1, stride): for j in range(0, w - patch_size + 1, stride): x = img[(0), i:i + patch_size, j:j + patch_size, :] for k in range(0, aug_times): if patches is None: patches = x[(np.newaxis), :, :, :] else: patches = np.concatenate((patches, x[(np.newaxis), :, :, :]), axis=0) cam_iso_patches = [cam_iso] * len(patches) out_que.put((patches, cam_iso_patches)) 
elpips.networks.squeezenet1_1_full_maxpool.forward def forward(self, X): o11, o12 = self.get_slice1(X) o21, o22 = self.get_slice2(o12) o31, o32 = self.get_slice3(o22) o41 = self.get_slice4(o32) o51 = self.get_slice5(o41) o61 = self.get_slice6(o51) o71 = self.get_slice7(o61) squeeze_outputs = namedtuple('SqueezeOutputs', ['o11', 'o12', 'o21', 'o22', 'o31', 'o32', 'o41', 'o51', 'o61', 'o71']) return squeeze_outputs(o11, o12, o21, o22, o31, o32, o41, o51, o61, o71) 
layers.BatchNormLayer.lr|op|setter def _lr_setter_op(self): """Op to set lr """ self.lr_ph = utils.get_ph(self.learning_rate) self.set_lr_op = tf.assign(self.learning_rate, self.lr_ph) 
mix_dataset.avg def avg(l): return sum(l) / len(l) 
plato.agent.component.dialogue_manager.dialogue_manager.DialogueManager.get|state def get_state(self): """ Get the current dialogue state  :return: the dialogue state """ return self.DSTracker.get_state() 
hbaselines.envs.hac.envs.UR5.get|goal|next def get_next_goal(self): """See parent class.""" end_goal = np.zeros(shape=(len(self.context_range),)) goal_possible = False while not goal_possible: end_goal = np.zeros(shape=(len(self.context_range),)) end_goal[0] = np.random.uniform(self.context_range[0][0], self. context_range[0][1]) end_goal[1] = np.random.uniform(self.context_range[1][0], self. context_range[1][1]) end_goal[2] = np.random.uniform(self.context_range[2][0], self. context_range[2][1]) theta_1 = end_goal[0] theta_2 = end_goal[1] theta_3 = end_goal[2] forearm_pos_3 = np.array([0.425, 0, 0, 1]) wrist_1_pos_4 = np.array([0.39225, -0.1197, 0, 1]) t_1_0 = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0.089159], [0, 0, 0, 1]]) t_2_1 = np.array([[np.cos(theta_1), -np.sin(theta_1), 0, 0], [np. sin(theta_1), np.cos(theta_1), 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]) t_3_2 = np.array([[np.cos(theta_2), 0, np.sin(theta_2), 0], [0, 1, 0, 0.13585], [-np.sin(theta_2), 0, np.cos(theta_2), 0], [0, 0, 0, 1]]) t_4_3 = np.array([[np.cos(theta_3), 0, np.sin(theta_3), 0.425], [0, 1, 0, 0], [-np.sin(theta_3), 0, np.cos(theta_3), 0], [0, 0, 0, 1]]) forearm_pos = t_1_0.dot(t_2_1).dot(t_3_2).dot(forearm_pos_3)[:3] wrist_1_pos = t_1_0.dot(t_2_1).dot(t_3_2).dot(t_4_3).dot(wrist_1_pos_4 )[:3] if np.absolute(end_goal[0]) > np.pi / 4 and forearm_pos[2 ] > 0.05 and wrist_1_pos[2] > 0.15: goal_possible = True self.display_end_goal(end_goal) return end_goal 
deepctr.layers.interaction.AFMLayer.call def call(self, inputs, training=None, **kwargs): if K.ndim(inputs[0]) != 3: raise ValueError( 'Unexpected inputs dimensions %d, expect to be 3 dimensions' % K.ndim(inputs)) embeds_vec_list = inputs row = [] col = [] for r, c in itertools.combinations(embeds_vec_list, 2): row.append(r) col.append(c) p = tf.concat(row, axis=1) q = tf.concat(col, axis=1) inner_product = p * q bi_interaction = inner_product attention_temp = tf.nn.relu(tf.nn.bias_add(tf.tensordot(bi_interaction, self.attention_W, axes=(-1, 0)), self.attention_b)) self.normalized_att_score = tf.nn.softmax(tf.tensordot(attention_temp, self.projection_h, axes=(-1, 0)), dim=1) attention_output = tf.reduce_sum(self.normalized_att_score * bi_interaction, axis=1) attention_output = self.dropout(attention_output) afm_out = self.tensordot([attention_output, self.projection_p]) return afm_out 
common.get|density|formatter def get_density_formatter():   class SparsityFormatter(matplotlib.ticker.PercentFormatter):  def __init__(self, *args, flip=True, **kwargs): super().__init__(*args, **kwargs) self.flip = flip  def __call__(self, x, i=None): return _format_times_density(x) if self.flip: x = 1 - x return super().__call__(x, i) return SparsityFormatter(xmax=1, decimals=None) 
setup_cifar.CIFARModel.CIFAR|Model def __init__(self, restore=None, session=None, use_softmax=False, use_brelu =False, activation='relu'):  def bounded_relu(x): return K.relu(x, max_value=1) if use_brelu: activation = bounded_relu else: activation = activation print('inside CIFARModel: activation = {}'.format(activation)) self.num_channels = 3 self.image_size = 32 self.num_labels = 10 model = Sequential() model.add(Conv2D(64, (3, 3), input_shape=(32, 32, 3))) model.add(Activation(activation)) model.add(Conv2D(64, (3, 3))) model.add(Activation(activation)) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(128, (3, 3))) model.add(Activation(activation)) model.add(Conv2D(128, (3, 3))) model.add(Activation(activation)) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Flatten()) model.add(Dense(256)) model.add(Activation(activation)) model.add(Dense(256)) model.add(Activation(activation)) model.add(Dense(10)) if use_softmax: model.add(Activation('softmax')) if restore: model.load_weights(restore) layer_outputs = [] for layer in model.layers: if isinstance(layer, Conv2D) or isinstance(layer, Dense): layer_outputs.append(K.function([model.layers[0].input], [layer .output])) self.layer_outputs = layer_outputs self.model = model 
extensions.u_hemis.application.U_HeMISApplication.output|interpret def interpret_output(self, batch_output): if self.is_inference: return self.output_decoder.decode_batch(batch_output['window'], batch_output['location']) return True 
tf_train.size|and|image|get|validate def validate_and_get_image_size(images): assert images.shape[1] == 3 image_dimensions = images.shape[2], images.shape[3] assert image_dimensions[0] % 2 == 0 and image_dimensions[1] % 2 == 0 return image_dimensions 
xlnet-master.run_squad.FeatureWriter.process_feature.feature|float|create def create_float_feature(values): f = tf.train.Feature(float_list=tf.train.FloatList(value=list(values))) return f 
nmt.model_helper.from|pretrained|txt|emb|create def _create_pretrained_emb_from_txt(vocab_file, embed_file, num_trainable_tokens=3, dtype=tf.float32, scope=None): """Load pretrain embeding from embed_file, and return an embedding matrix.  Args: vocab_file: Path to vocab file. embed_file: Path to a Glove formmated embedding txt file. num_trainable_tokens: Make the first n tokens in the vocab file as trainable variables. Default is 3, which is "<unk>", "<s>" and "</s>". dtype: data type. scope: tf scope name.  Returns: pretrained embedding table variable. """ vocab, _ = vocab_utils.load_vocab(vocab_file) trainable_tokens = vocab[:num_trainable_tokens] utils.print_out('# Using pretrained embedding: %s.' % embed_file) utils.print_out('  with trainable tokens: ') emb_dict, emb_size = vocab_utils.load_embed_txt(embed_file) for token in trainable_tokens: utils.print_out('    %s' % token) if token not in emb_dict: emb_dict[token] = [0.0] * emb_size emb_mat = np.array([emb_dict[token] for token in vocab], dtype=dtype. as_numpy_dtype()) emb_mat = tf.constant(emb_mat) emb_mat_const = tf.slice(emb_mat, [num_trainable_tokens, 0], [-1, -1]) with tf.variable_scope(scope or 'pretrain_embeddings', dtype=dtype ) as scope: emb_mat_var = tf.get_variable('emb_mat_var', [num_trainable_tokens, emb_size]) return tf.concat([emb_mat_var, emb_mat_const], 0) 
data_prep_util.get|sampling|command def get_sampling_command(obj_filename, ply_filename): cmd = SAMPLING_BIN + ' ' + obj_filename cmd += ' ' + ply_filename cmd += ' -n_samples %d ' % SAMPLING_POINT_NUM cmd += ' -leaf_size %f ' % SAMPLING_LEAF_SIZE return cmd 
facenet-master.src.calculate_filtering_metrics.parse|arguments def parse_arguments(argv): parser = argparse.ArgumentParser() parser.add_argument('dataset_dir', type=str, help= 'Path to the directory containing aligned dataset.') parser.add_argument('model_file', type=str, help= 'File containing the frozen model in protobuf (.pb) format to use for feature extraction.' ) parser.add_argument('data_file_name', type=str, help= 'The name of the file to store filtering data in.') parser.add_argument('--image_size', type=int, help='Image size.', default=160) parser.add_argument('--batch_size', type=int, help= 'Number of images to process in a batch.', default=90) return parser.parse_args(argv) 
CRPM_Net.CRPM_Net.conv|down def down_conv(self): dw_h_convs = OrderedDict() pools = OrderedDict() x = [self.input_real, self.input_imag] for layer in range(self.layers): features = 2 ** layer * self.features_root if layer == 0: conv1_real, conv1_imag = complex_cross_conv(input_real=x[0], input_imag=x[1], scope_name='down_conv_' + str(layer) + '/' + 'conv1', input_shape=[self.filter_size, self.filter_size, self.channels, features], keep_prob=self.keep_prob, padding ='VALID', regularizer=self.regularizer) print([self.filter_size, self.filter_size, self.channels, features] ) elif layer < self.layers - 1: conv1_real, conv1_imag = complex_cross_conv(input_real=x[0], input_imag=x[1], scope_name='down_conv_' + str(layer) + '/' + 'conv1', input_shape=[self.filter_size, self.filter_size, features // 2, features], keep_prob=self.keep_prob, padding ='VALID', regularizer=self.regularizer) print([self.filter_size, self.filter_size, features // 2, features] ) else: conv1_real, conv1_imag = complex_cross_conv(input_real=x[0], input_imag=x[1], scope_name='down_conv_' + str(layer) + '/' + 'conv1', input_shape=[1, 1, features // 2, features], keep_prob=self.keep_prob, padding='VALID', regularizer=self .regularizer) dw_h_convs[layer] = [conv1_real, conv1_imag] if layer < self.layers - 1: pools[layer] = [max_pool(dw_h_convs[layer][0], 2), max_pool( dw_h_convs[layer][1], 2)] x = pools[layer] return dw_h_convs 
neural_tangents.utils.monte_carlo._sample_once_kernel_fn.once|kernel|fn|sample @partial(batch.batch, batch_size=batch_size, device_count=device_count, store_on_device=store_on_device) def kernel_fn_sample_once(x1, x2, key, get): _, params = init_fn(key, x1.shape) return kernel_fn(x1, x2, params, get) 
thumt.utils.hooks.EvaluationHook.begin def begin(self): if self._timer.last_triggered_step() is None: self._timer.update_last_triggered_step(0) global_step = tf.train.get_global_step() if not tf.gfile.Exists(self._save_path): tf.logging.info('Making dir: %s' % self._save_path) tf.gfile.MakeDirs(self._save_path) params_pattern = os.path.join(self._base_dir, '*.json') params_files = tf.gfile.Glob(params_pattern) for name in params_files: new_name = name.replace(self._base_dir, self._save_path) tf.gfile.Copy(name, new_name, overwrite=True) if global_step is None: raise RuntimeError('Global step should be created first') self._global_step = global_step 
nets.inception_resnet_v2.block def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None): """Builds the 17x17 resnet block.""" with tf.variable_scope(scope, 'Block17', [net], reuse=reuse): with tf.variable_scope('Branch_0'): tower_conv = slim.conv2d(net, 192, 1, scope='Conv2d_1x1') with tf.variable_scope('Branch_1'): tower_conv1_0 = slim.conv2d(net, 128, 1, scope='Conv2d_0a_1x1') tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7], scope= 'Conv2d_0b_1x7') tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1], scope= 'Conv2d_0c_7x1') mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2]) up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None, activation_fn=None, scope='Conv2d_1x1') scaled_up = up * scale if activation_fn == tf.nn.relu6: scaled_up = tf.clip_by_value(scaled_up, -6.0, 6.0) net += scaled_up if activation_fn: net = activation_fn(net) return net 
monte_carlo_test.MonteCarloTest.analytic|sample|test|nngp|vs @jtu.parameterized.named_parameters(jtu.cases_from_list({'testcase_name': '[batch_size={}, device_count={} store_on_device={} ]'.format( batch_size, device_count, store_on_device), 'batch_size': batch_size, 'device_count': device_count, 'store_on_device': store_on_device} for batch_size in BATCH_SIZES for device_count in DEVICE_COUNTS for store_on_device in STORE_ON_DEVICE)) def test_sample_vs_analytic_nngp(self, batch_size, device_count, store_on_device): utils.stub_out_pmap(batch, device_count) x1, x2, init_fn, apply_fn, stax_kernel_fn, key = _get_inputs_and_model( 1024, 256, xla_bridge.get_backend().platform == 'tpu') sample = monte_carlo.monte_carlo_kernel_fn(init_fn, apply_fn, key, 200, batch_size, device_count, store_on_device) ker_empirical = sample(x1, x2, 'nngp') ker_analytic = stax_kernel_fn(x1, x2, 'nngp') utils.assert_close_matrices(self, ker_analytic, ker_empirical, 0.02) 
pvae.pixelvae_bbans_two_layer.get|theta def get_theta1(eps1_vals, z2_vals): z1 = np.zeros(latent1_shape, dtype='float32') for idx in post1_elem_idxs: mu, sig = gen_net1(z1, z2_vals) z1_temp = mu + sig * eps1_vals z1[idx] = z1_temp[idx] return np.stack((np.zeros_like(mu), np.zeros_like(sig), mu, sig), axis=-1) 
gpt.Attention.attn def _attn(self, q, k, v, sequence_mask): w = torch.matmul(q, k) if self.scale: w = w / math.sqrt(v.size(-1)) b_subset = self.b[:, :, :w.size(-2), :w.size(-1)] if sequence_mask is not None: b_subset = b_subset * sequence_mask.view(sequence_mask.size(0), 1, -1) b_subset = b_subset.permute(1, 0, 2, 3) w = w * b_subset + -1000000000.0 * (1 - b_subset) w = nn.Softmax(dim=-1)(w) w = self.attn_dropout(w) return torch.matmul(w, v) 
cleverhans.devtools.tests.docscrape.NumpyDocString.at|section|is def _is_at_section(self): self._doc.seek_next_non_empty_line() if self._doc.eof(): return False l1 = self._doc.peek().strip() if l1.startswith('.. index::'): return True l2 = self._doc.peek(1).strip() return len(l1) == len(l2) and l2 == '-' * len(l1) 
Attentive_autoencoder.model|save def save_model(model, model_path, isoverwrite=True): model.save_weights(model_path, isoverwrite) 
gen_saliency_voc.pass|forward|saliency def forward_pass_saliency(imgS, newSize): saliency_net.blobs['img'].reshape(1, 3, newSize[0], newSize[1]) transformer = caffe.io.Transformer({'img': saliency_net.blobs['img']. data.shape}) transformer.set_mean('img', np.array([103.939, 116.779, 123.68])) transformer.set_transpose('img', (2, 0, 1)) transformer.set_channel_swap('img', (2, 1, 0)) transformer.set_raw_scale('img', 255.0) saliency_net.blobs['img'].data[...] = transformer.preprocess('img', imgS) out = saliency_net.forward(end=saliencyTopLayer) msk = saliency_net.blobs[saliencyTopLayer].data[...] msk = msk.reshape((imgScale, imgScale)) return msk 
graphsage.layers.Layer.log|vars def _log_vars(self): for var in self.vars: tf.summary.histogram(self.name + '/vars/' + var, self.vars[var]) 
embeddings.OpenKE.config.Config.Config.nbatches|set def set_nbatches(self, nbatches): self.nbatches = nbatches 
utils.vecmap|for|dictionary|print def print_dictionary_for_vecmap(filename, dict): with open(filename, 'w') as file: for w in dict: if w != '' and w != '\n': print(w + ' ' + w, file=file) 
extract_scalars_from_logs.tabulate|events def tabulate_events(dpath): summary_iterators = [EventAccumulator(os.path.join(dpath, dname)). Reload() for dname in os.listdir(dpath)] tags = summary_iterators[0].Tags()['scalars'] out = defaultdict(list) for tag in tags: out[tag].extend([[e.value for e in acc.Scalars(tag)] for acc in summary_iterators]) return out, tags 
data_handling.usps|read|data def read_usps_data(args, file_path=usps_data_path): """ loads usps dataset from matlab file :param args: run arguments :param file_path: path to usps data .mat file :return: train and test data tuples and the effective scale a if data has been scaled to [0,a] """ assert args.train_set_size + args.test_set_size <= 11000 source_mat = loadmat(file_path)['data'] train_per_label = args.train_set_size // 10 test_per_label = args.test_set_size // 10 train_data, test_data = [], [] for label in range(10): label_data = source_mat[:, :, (label)].T subset = np.random.choice(np.arange(1100), train_per_label + test_per_label, replace=False) data_select = label_data[(subset), :] train_data.append(data_select[:train_per_label, :]) test_data.append(data_select[train_per_label:, :]) train_data = np.concatenate(train_data, axis=0) test_data = np.concatenate(test_data, axis=0) perm = np.random.permutation(args.train_set_size) train_data = train_data[(perm), :] perm = np.random.permutation(args.test_set_size) test_data = test_data[(perm), :] train_data = train_data.astype(np.float32) / 255.0 test_data = test_data.astype(np.float32) / 255.0 train_data = train_data - train_data.mean() test_data = test_data - test_data.mean() train_xy_data = xy_data(train_data, None) test_xy_data = xy_data(test_data, None) return train_xy_data, test_xy_data 
alig.test.ModelTh.get|weights def get_weights(self): w1 = self.linear1.weight.data.t().numpy() b1 = self.linear1.bias.data.numpy() w2 = self.linear2.weight.data.t().numpy() b2 = self.linear2.bias.data.numpy() return w1, b1, w2, b2 
gan.latent.Latent.error|reg def reg_error(self, out_tf_var, ground_truth): raise NotImplementedError 
graphsage.models.SampleAndAggregate.aggregate def aggregate(self, samples, input_features, dims, num_samples, support_sizes, batch_size=None, aggregators=None, name=None, concat= False, model_size='small'): """ At each layer, aggregate hidden representations of neighbors to compute the hidden representations at next layer. Args: samples: a list of samples of variable hops away for convolving at each layer of the network. Length is the number of layers + 1. Each is a vector of node indices. input_features: the input features for each sample of various hops away. dims: a list of dimensions of the hidden representations from the input layer to the final layer. Length is the number of layers + 1. num_samples: list of number of samples for each layer. support_sizes: the number of nodes to gather information from for each layer. batch_size: the number of inputs (different for batch inputs and negative samples). Returns: The hidden representation at the final layer for all nodes in batch """ if batch_size is None: batch_size = self.batch_size hidden = [tf.nn.embedding_lookup(input_features, node_samples) for node_samples in samples] new_agg = aggregators is None if new_agg: aggregators = [] for layer in range(len(num_samples)): if new_agg: dim_mult = 2 if concat and layer != 0 else 1 if layer == len(num_samples) - 1: aggregator = self.aggregator_cls(dim_mult * dims[layer], dims[layer + 1], act=lambda x: x, dropout=self. placeholders['dropout'], name=name, concat=concat, model_size=model_size) else: aggregator = self.aggregator_cls(dim_mult * dims[layer], dims[layer + 1], dropout=self.placeholders['dropout'], name=name, concat=concat, model_size=model_size) aggregators.append(aggregator) else: aggregator = aggregators[layer] next_hidden = [] for hop in range(len(num_samples) - layer): dim_mult = 2 if concat and layer != 0 else 1 neigh_dims = [batch_size * support_sizes[hop], num_samples[len( num_samples) - hop - 1], dim_mult * dims[layer]] h = aggregator((hidden[hop], tf.reshape(hidden[hop + 1], neigh_dims))) next_hidden.append(h) hidden = next_hidden return hidden[0], aggregators 
gym_pycolab.plot.Plot.directives|clear|engine def _clear_engine_directives(self): """Reset this `Plot`'s set of directives to the `Engine`.  The reset directives essentially tell the `Engine` to make no changes to its internal state. The `Engine` will typically call this method at the end of every game iteration, once all of the existing directives have been consumed.  Only `Engine` and `Plot` methods may call this method. """ self._engine_directives = self._EngineDirectives() 
texar.data.data.dataset_utils.random_shard_dataset.shard|fn def _shard_fn(dataset): sharded_dataset = tf.data.Dataset.from_tensor_slices(boundaries).shuffle( num_shards, seed=seed).flat_map(lambda lb: dataset.skip(lb).take( shard_size)) return sharded_dataset 
infer_utils.vocab|tables|create def create_vocab_tables(vocab_file):  def load_vocab(vocab_file): vocab = [] for _, word in enumerate(open(vocab_file, 'r', encoding='utf-8')): vocab.append(word.strip()) return vocab vocab = load_vocab(vocab_file) vocab_table = {k: v for v, k in enumerate(vocab)} reverse_vocab_table = {k: v for k, v in enumerate(vocab)} return vocab_table, reverse_vocab_table 
normalizer.StandardScaler.backward|process def backward_process(self, x): return x * self.std + self.mu 
texar.modules.networks.network_base.layers|build def _build_layers(network, layers=None, layer_hparams=None): """Builds layers.  Either :attr:`layer_hparams` or :attr:`layers` must be provided. If both are given, :attr:`layers` will be used.  Args: network: An instance of a subclass of :class:`~texar.modules.networks.network_base.FeedForwardNetworkBase` layers (optional): A list of layer instances. layer_hparams (optional): A list of layer hparams, each to which is fed to :func:`~texar.core.layers.get_layer` to create the layer instance. """ with tf.variable_scope(network.variable_scope): if layers is not None: network._layers = layers else: if layer_hparams is None: raise ValueError( 'Either `layer` or `layer_hparams` is required.') network._layers = [] for _, hparams in enumerate(layer_hparams): network._layers.append(get_layer(hparams=hparams)) for layer in network._layers: layer_name = uniquify_str(layer.name, network._layer_names) network._layer_names.append(layer_name) network._layers_by_name[layer_name] = layer 
normalizer.StandardScaler.Scaler|Standard def __init__(self, mu, std): self.mu = mu self.std = std 
kaffe.tensorflow.network.Network.feed def feed(self, *args): """Set the input(s) for the next operation by replacing the terminal nodes. The arguments can be either layer names or the actual layers. """ assert len(args) != 0 self.terminals = [] for fed_layer in args: if isinstance(fed_layer, basestring): try: fed_layer = self.layers[fed_layer] except KeyError: raise KeyError('Unknown layer name fed: %s' % fed_layer) self.terminals.append(fed_layer) return self 
fasterai.visualize.get|stable|image|colorizer def get_stable_image_colorizer(root_folder: Path=Path('./'), weights_name: str='ColorizeStable_gen', results_dir='result_images', render_factor: int=35) ->ModelImageVisualizer: learn = gen_inference_wide(root_folder=root_folder, weights_name= weights_name) filtr = MasterFilter([ColorizerFilter(learn=learn)], render_factor= render_factor) vis = ModelImageVisualizer(filtr, results_dir=results_dir) return vis 
sidd_utils.kl|data|forward|div def kl_div_forward_data(p_data, q_data, left_edge=0.0, right_edge=1.0, n_bins=1000): """ Forward KL divergence between two sets of data points p and q""" p, _ = get_histogram(p_data, left_edge, right_edge, n_bins) q, _ = get_histogram(q_data, left_edge, right_edge, n_bins) return kl_div_forward(p, q) 
transcribe_audio_file.main.model|fn def model_fn(features, labels, mode, config, params): return las_model_fn(features, labels, mode, config, params, binf2phone= binf2phone_np) 
tf_utils.layers.to|NCHW def to_NCHW(x): return tf.transpose(x, perm=([0] if len(x.get_shape()) else []) + [3, 1, 2] ) 
NCC.Centroid|Nearest|Classification def NearestCentroidClassification(X_train, X_test, y_train_n, y_test_n, dataset_name): """  :param X_train: if using DTAN, should already be aligned :param X_test: if using DTAN, should already be aligned :param y_train_n: numerical labels (not one-hot) :param y_test_n: numerical labels (not one-hot) :param dataset_name: :return: test set NCC accuracy """ input_shape = X_train.shape[1:] n_classes = len(np.unique(y_train_n)) class_names = np.unique(y_train_n, axis=0) aligned_means = np.zeros((n_classes, input_shape[0], input_shape[1])) ncc_labels = [] for class_num in class_names: train_class_idx = y_train_n == class_num X_train_aligned_within_class = X_train[train_class_idx] aligned_means[(int(class_num)), :] = np.mean( X_train_aligned_within_class, axis=0) ncc_labels.append(class_num) ncc_labels = np.asarray(ncc_labels) knn_clf = KNeighborsTimeSeriesClassifier(n_neighbors=1, metric='euclidean') knn_clf.fit(aligned_means, ncc_labels) predicted_labels = knn_clf.predict(X_test) acc = accuracy_score(y_test_n, predicted_labels) print(f'{dataset_name} - NCC results: {acc}') 
instruction_encoder.InstructionEncoder.call def call(self, input_tensor): """Function call of instruction encoder.  Args: input_tensor: tf.int64 tensor with shape [batch_size, max_seq_length] padded with some pad token id.  Returns: A tuple<output, states>. Output: a tf.float32 tensor with shape [batch_size, max_seq_length, output_dim]. State: A list which has num_hidden_layer elements. Every elements is a (state_c, state_h) tuple. This is concatenated last forward and backward states of each LSTM layer. """ embedding = self._word_embeddings(input_tensor) bilstm_result = self._bi_lstm(embedding) output = bilstm_result[0] states = bilstm_result[1:] num_states = len(states) encoder_states = [] for idx in range(int(num_states / 2)): state_h = tf.concat([states[idx][0], states[idx + int(num_states / 2)][0]], axis=1) state_c = tf.concat([states[idx][1], states[idx + int(num_states / 2)][1]], axis=1) encoder_states.append((state_h, state_c)) return output, encoder_states 
agents.ImpalaDeep.head def _head(self, core_output): policy_logits = self._policy_logits(core_output) baseline = tf.squeeze(self._baseline(core_output), axis=-1) new_action = tf.random.categorical(policy_logits, 1, dtype=tf.int32) new_action = tf.squeeze(new_action, 1, name='action') return AgentOutput(new_action, policy_logits, baseline) 
src.adapter.adapter.adapter|nn def adapter_nn(self, input, target_size, name, method): """ build adapter nn :param input: :param target_size: output size :param name: :param method: forward_method see README.md :return: """ linear = method.split('-')[0] layer_num = int(method.split('-')[1]) layer_norm = method.split('-')[2] print('{} adapter forward method: {} layer {} with {}'.format(self.name, layer_num, linear, layer_norm)) if linear in ['relu', 'tanh']: if linear == 'relu': adapter_activate = tf.nn.relu elif linear == 'tanh': adapter_activate = tf.nn.tanh else: print('config error in model.{}.activate'.format(self.name)) exit(-1) print('add {} non linear'.format(self.name)) adapter_output = NN.fflayer(input, target_size, bias=False, activation=adapter_activate, name='trains_0' + name) if layer_norm == 'layer_norm': adapter_output = tf.contrib.layers.layer_norm(adapter_output) if layer_num >= 2: for i in range(layer_num - 1): adapter_output = NN.fflayer(adapter_output, target_size, activation=adapter_activate, name='trans_{}'.format(i + 1) + name) if layer_norm == 'layer_norm': adapter_output = tf.contrib.layers.layer_norm( adapter_output) elif linear == 'linear': adapter_output = NN.fflayer(input, target_size, name='trans_0' + name) else: print('the first parameter in forward method error!') exit(1) return adapter_output 
train_bnn_torchsso.test def test(args, epoch, num_batch, net, optimizer, testLoader, biOptimizer, alphas, betas): net.eval() test_loss = 0 incorrect = 0 logits = [] labels = [] for data, target in testLoader: data, target = data.cuda(), target.cuda() with torch.no_grad(): if args.ps: output = optimizer.prediction(data, alphas, biOptimizer.tau, betas, F.softmax) else: output = optimizer.prediction(data, alphas, biOptimizer.tau, betas, _ada_gumbel_softmax) logits.append(output) labels.append(target) test_loss += F.nll_loss(output, target).item() pred = output.data.max(1)[1] incorrect += pred.ne(target.data).cpu().sum() test_loss = test_loss test_loss /= len(testLoader) nTotal = len(testLoader.dataset) err = 100.0 * incorrect / nTotal ece = _ECELoss()(torch.cat(logits, 0), torch.cat(labels, 0), args.save) logging.info( 'Test set: Average loss: {:.4f}, Error: {}/{} ({:.0f}%), ECE: {}'. format(test_loss, incorrect, nTotal, err, ece)) 
func_model_81.Fatt|Net|define|L def define_LFattNet(sz_input, sz_input2, view_n, learning_rate): """ 81 inputs""" input_list = [] for i in range(len(view_n) * len(view_n)): print('input ' + str(i)) input_list.append(Input(shape=(sz_input, sz_input2, 1))) """ 81 features""" feature_extraction_layer = feature_extraction(sz_input, sz_input2) feature_list = [] for i in range(len(view_n) * len(view_n)): print('feature ' + str(i)) feature_list.append(feature_extraction_layer(input_list[i])) """ cost volume """ cv = Lambda(_getCostVolume_)(feature_list) """ channel attention """ cv, attention = channel_attention(cv) """ cost volume regression """ cost = basic(cv) cost = Lambda(lambda x: K.permute_dimensions(K.squeeze(x, -1), (0, 2, 3, 1)))(cost) pred = Activation('softmax')(cost) pred = Lambda(disparityregression)(pred) model = Model(inputs=input_list, outputs=[pred, attention]) model.summary() opt = Adam(lr=learning_rate) model.compile(optimizer=opt, loss='mae') return model 
facenet-master.src.generative.modify_attribute.main def main(args): img_mean = np.array([134.10714722, 102.52040863, 87.15436554]) img_stddev = np.sqrt(np.array([3941.30175781, 2856.94287109, 2519.35791016])) vae_def = importlib.import_module(args.vae_def) vae = vae_def.Vae(args.latent_var_size) gen_image_size = vae.get_image_size() with tf.Graph().as_default(): tf.set_random_seed(args.seed) images = tf.placeholder(tf.float32, shape=(None, gen_image_size, gen_image_size, 3), name='input') images_norm = (images - img_mean) / img_stddev images_norm_resize = tf.image.resize_images(images_norm, ( gen_image_size, gen_image_size)) mean, log_variance = vae.encoder(images_norm_resize, True) epsilon = tf.random_normal((tf.shape(mean)[0], args.latent_var_size)) std = tf.exp(log_variance / 2) latent_var = mean + epsilon * std reconstructed_norm = vae.decoder(latent_var, False) reconstructed = reconstructed_norm * img_stddev + img_mean saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3) gpu_memory_fraction = 1.0 gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction= gpu_memory_fraction) sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False)) sess.run(tf.global_variables_initializer()) sess.run(tf.local_variables_initializer()) coord = tf.train.Coordinator() tf.train.start_queue_runners(coord=coord, sess=sess) with sess.as_default(): vae_checkpoint = os.path.expanduser(args.vae_checkpoint) print('Restoring VAE checkpoint: %s' % vae_checkpoint) saver.restore(sess, vae_checkpoint) filename = os.path.expanduser(args.attributes_filename) with h5py.File(filename, 'r') as f: latent_vars = np.array(f.get('latent_vars')) attributes = np.array(f.get('attributes')) attribute_vectors = np.array(f.get('attribute_vectors')) attribute_index = 31 image_indices = [8, 11, 13, 18, 19, 26, 31, 39, 47, 54, 56, 57, 58, 59, 60, 73] nrof_images = len(image_indices) nrof_interp_steps = 10 sweep_latent_var = np.zeros((nrof_interp_steps * nrof_images, args.latent_var_size), np.float32) for j in range(nrof_images): image_index = image_indices[j] idx = np.argwhere(attributes[:, (attribute_index)] == -1)[ image_index, 0] for i in range(nrof_interp_steps): sweep_latent_var[(i + nrof_interp_steps * j), : ] = latent_vars[(idx), : ] + 5.0 * i / nrof_interp_steps * attribute_vectors[( attribute_index), :] recon = sess.run(reconstructed, feed_dict={latent_var: sweep_latent_var}) img = facenet.put_images_on_grid(recon, shape=( nrof_interp_steps * 2, int(math.ceil(nrof_images / 2)))) image_filename = os.path.expanduser(args.output_image_filename) print('Writing generated image to %s' % image_filename) misc.imsave(image_filename, img) 
query_methods.EGLSampling.Sampling|EGL def __init__(self, model, input_shape, num_labels, gpu): super().__init__(model, input_shape, num_labels, gpu) 
texar.core.layers.get|initializer def get_initializer(hparams=None): """Returns an initializer instance.  .. role:: python(code) :language: python  Args: hparams (dict or HParams, optional): Hyperparameters with the structure  .. code-block:: python  { "type": "initializer_class_or_function", "kwargs": { #... } }  The "type" field can be a initializer class, its name or module path, or class instance. If class name is provided, the class must be from one the following modules: :tf_main:`tf.initializers <initializers>`, :tf_main:`tf.keras.initializers <keras/initializers>`, :tf_main:`tf < >`, and :mod:`texar.custom`. The class is created by :python:`initializer_class(**kwargs)`. If a class instance is given, "kwargs" is ignored and can be omitted.  Besides, the "type" field can also be an initialization function called with :python:`initialization_fn(**kwargs)`. In this case "type" can be the function, or its name or module path. If function name is provided, the function must be from one of the above modules or module `tf.contrib.layers`. If no keyword argument is required, "kwargs" can be omitted.  Returns: An initializer instance. `None` if :attr:`hparams` is `None`. """ if hparams is None: return None kwargs = hparams.get('kwargs', {}) if isinstance(kwargs, HParams): kwargs = kwargs.todict() modules = ['tensorflow.initializers', 'tensorflow.keras.initializers', 'tensorflow', 'texar.custom'] try: initializer = utils.check_or_get_instance(hparams['type'], kwargs, modules) except TypeError: modules += ['tensorflow.contrib.layers'] initializer_fn = utils.get_function(hparams['type'], modules) initializer = initializer_fn(**kwargs) return initializer 
cnn_helpers.x|make|no|conv|bias def make_conv_1x1_no_bias(op_name, in_tensor, filters, strides=(1, 1, 1, 1), padding='VALID', weight_decay=0.0005, stddev=0.1): return make_conv_no_bias(op_name, in_tensor, 1, 1, filters, strides, padding, weight_decay, stddev) 
plato.domain.create_domain_sqlite_db.run def run(config): """ This script will create an SQL database and the corresponding ontology, given a .csv or .tsv file containing the data.  It will produce a .db file (the database) and a .json file (the ontology), reflecting the settings in the configuration file.  Warning: The first column of the .csv / .tsv will be treated as the primary key.  """ db_creator = CreateSQLiteDB() plato_path = '' if not os.path.isfile(config): import plato plato_path = '/'.join(plato.__file__.split('/')[:-1]) + '/' config = plato_path + 'example/config/domain/' + config args = db_creator.arg_parse(config) if not args: raise ValueError('Terminating') csv_filename = plato_path + args['GENERAL']['csv_file_name'] table_name = args['GENERAL']['db_table_name'] db_name = plato_path + args['GENERAL']['db_file_path'] ontology_name = plato_path + args['GENERAL']['ontology_file_path'] informable_slots = [] requestable_slots = [] system_requestable_slots = [] if 'ONTOLOGY' in args: if 'informable_slots' in args['ONTOLOGY']: informable_slots = args['ONTOLOGY']['informable_slots'] if 'requestable_slots' in args['ONTOLOGY']: requestable_slots = args['ONTOLOGY']['requestable_slots'] if 'system_requestable_slots' in args['ONTOLOGY']: system_requestable_slots = args['ONTOLOGY'][ 'system_requestable_slots'] column_names = [] MAX_DB_ENTRIES = -1 delim = '\t' if csv_filename.split('.')[1] == 'tsv' else ',' with open(csv_filename) as csv_input: csv_reader = csv.reader(csv_input, delimiter=delim) for entry in csv_reader: column_names = entry if not informable_slots: informable_slots = column_names[1:] if not requestable_slots: requestable_slots = column_names[1:] if not system_requestable_slots: system_requestable_slots = column_names[1:] break sqlcmd_create_table = ('CREATE TABLE IF NOT EXISTS ' + table_name + '(' + column_names[0] + ' text PRIMARY KEY,' + ' text,'.join([ column_names[c] for c in range(1, len(column_names))]) + ');') conn = db_creator.create_sql_connection(db_name) with conn: if conn is not None: db_creator.create_sql_table(conn, sqlcmd_create_table) else: print('Error! cannot create the database connection.') with open(csv_filename) as csv_input: csv_reader = csv.reader(csv_input, delimiter=delim) first_entry = True punctuation = string.punctuation.replace('$', '') punctuation = punctuation.replace('-', '') punctuation = punctuation.replace('_', '') punctuation = punctuation.replace('.', '') punctuation = punctuation.replace('&', '') punctuation_remover = str.maketrans('', '', punctuation) print('Populating database ') entries_count = 1 for entry in csv_reader: if first_entry: first_entry = False else: sql_cmd = 'INSERT INTO ' + table_name + '(' + ','.join([ c for c in column_names] ) + ')' + ' VALUES(' + ','.join(['?' for c in column_names]) + ')' entry = [(str(round(float(e), 1)) if db_creator. check_float(e) else e) for e in entry] entry = [str(''.join(i for i in e if ord(i) < 128)). replace('"', '') for e in entry] entry = [e.replace("'", '') for e in entry] entry = [e.rstrip().lower().translate( punctuation_remover) for e in entry] entry = [(e if e else 'None') for e in entry] cur = conn.cursor() cur.execute(sql_cmd, tuple(entry)) entries_count += 1 if entries_count % 10000 == 0: print(f'(added {entries_count} entries)') if 0 < MAX_DB_ENTRIES <= entries_count: break print( f'{table_name} database created with {entries_count} items!\nCreating ontology...' ) db_creator.create_ontology(conn, table_name, ontology_name, informable_slots, requestable_slots, system_requestable_slots) print(f'{table_name} ontology created!') 
classification.ops.utils.posdef|inv def posdef_inv(tensor, damping): """Computes the inverse of tensor + damping * identity.""" identity = linalg_ops.eye(tensor.shape.as_list()[0], dtype=tensor.dtype) damping = math_ops.cast(damping, dtype=tensor.dtype) return posdef_inv_functions[POSDEF_INV_METHOD](tensor, identity, damping) 
avod.core.box_4c_encoder_test.Box4cEncoderTest.to|c|d|test|tf|box def test_tf_box_4c_to_box_3d(self): np_boxes_4c = np.asarray([[1.0, 0.0, -1.0, 0.5, 0.5, -1.0, 0.0, 1.0, 1.0, 3.0], [1.0, 0.0, -1.0, -0.5, 0.0, -1.0, 0.5, 1.0, 1.0, 3.0], [ 1.0, 0.0, -1.0, -0.5, 0.0, -1.0, 0.5, 1.0, 1.0, 3.0], [1.0, 0.0, - 1.0, -0.5, 0.0, -1.0, 0.5, 1.0, 1.0, 3.0], [1.0, 0.0, -1.0, -0.5, 0.0, -1.0, 0.5, 1.0, 1.0, 3.0]]) np_ground_plane = np.asarray([0, -1, 0, -1]) np_boxes_3d = [box_4c_encoder.np_box_4c_to_box_3d(box_4c, np_ground_plane) for box_4c in np_boxes_4c] tf_boxes_4c = tf.convert_to_tensor(np_boxes_4c, dtype=tf.float32) tf_ground_plane = tf.convert_to_tensor(np_ground_plane, dtype=tf.float32) tf_boxes_3d = box_4c_encoder.tf_box_4c_to_box_3d(tf_boxes_4c, tf_ground_plane) sess = tf.Session() with sess.as_default(): tf_boxes_3d_out = tf_boxes_3d.eval() for box_idx in range(len(np_boxes_3d)): np.testing.assert_almost_equal(np_boxes_3d[box_idx], tf_boxes_3d_out[box_idx], decimal=3) 
pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.on|punc|split|run def _run_split_on_punc(self, text): """Splits punctuation on a piece of text.""" if text in self.never_split: return [text] chars = list(text) i = 0 start_new_word = True output = [] while i < len(chars): char = chars[i] if _is_punctuation(char): output.append([char]) start_new_word = True else: if start_new_word: output.append([]) start_new_word = False output[-1].append(char) i += 1 return [''.join(x) for x in output] 
plato.agent.conversational_agent.conversational_multi_agent.ConversationalMultiAgent.get|state def get_state(self): """ Get this agent's state  :return: a DialogueState """ return self.dialogue_manager.get_state() 
rnns.ModRNNCell.size|output @property def output_size(self): return self._num_units 
classification.ops.fisher_factors.ConvInputKroneckerFactor.var|scope @property def _var_scope(self): return 'ff_convinkron/' + scope_string_from_params([self._inputs, self. _filter_shape, self._strides, self._padding, self._has_bias]) 
t_sgan_sn_upsample.SpectralNormalization.spectral|norm def spectral_norm(self, w, r=5): w_shape = K.int_shape(w) in_dim = np.prod(w_shape[:-1]).astype(int) out_dim = w_shape[-1] w = K.reshape(w, (in_dim, out_dim)) u = K.ones((1, in_dim)) for i in range(r): v = K.l2_normalize(K.dot(u, w)) u = K.l2_normalize(K.dot(v, K.transpose(w))) return K.sum(K.dot(K.dot(u, w), K.transpose(v))) 
cdvae-cls-gan-mcc.CDVAECLSGAN.encode def encode(self, x, feat_type): if not feat_type in ['sp', 'mcc']: print('feature type does not match!') raise NotImplementedError x_in_minmax = self.normalizers[feat_type]['minmax'].forward_process(x) x_in = tf.expand_dims(tf.expand_dims(x_in_minmax, 0), 0) if feat_type == 'sp': return self.sp_enc(x_in) elif feat_type == 'mcc': return self.mcc_enc(x_in) 
utils.images|save def save_images(images, size, image_path): return imsave(inverse_transform(images), size, image_path) 
texar.modules.encoders.rnn_encoders.output|layer|forward|single def _forward_single_output_layer(inputs, input_size, output_layer): """Forwards the input through a single output layer.  Args: inputs: A Tensor of shape `[batch_size, max_time] + input_size` if :attr:`time_major=False`, or shape `[max_time, batch_size] + input_size` if :attr:`time_major=True`. input_size: An `int` or 1D `int` array. """ dim = np.prod(input_size) inputs_flat = inputs inputs_flat = tf.reshape(inputs_flat, [-1, dim]) output_flat = output_layer(inputs_flat) output_size = output_layer.compute_output_shape([1, dim]).as_list()[1:] output_size = np.array(output_size) output_shape = tf.concat([tf.shape(inputs)[:2], output_size], axis=0) output = tf.reshape(output_flat, output_shape) return output, output_size 
bleu_tool.get|ngrams def _get_ngrams(segment, max_order): """Extracts all n-grams upto a given maximum order from an input segment.  Args: segment: text segment from which n-grams will be extracted. max_order: maximum length in tokens of the n-grams returned by this methods.  Returns: The Counter containing all n-grams upto max_order in segment with a count of how many times each n-gram occurred. """ ngram_counts = collections.Counter() for order in xrange(1, max_order + 1): for i in xrange(0, len(segment) - order + 1): ngram = tuple(segment[i:i + order]) ngram_counts[ngram] += 1 return ngram_counts 
fasterai.visualize.VideoColorizer.extract|raw|frames def _extract_raw_frames(self, source_path: Path): bwframes_folder = self.bwframes_root / source_path.stem bwframe_path_template = str(bwframes_folder / '%5d.jpg') bwframes_folder.mkdir(parents=True, exist_ok=True) self._purge_images(bwframes_folder) ffmpeg.input(str(source_path)).output(str(bwframe_path_template), format='image2', vcodec='mjpeg', qscale=0).run(capture_stdout=True) 
evaluate_ruemonge2014.fn|input def input_fn(filelist, batch_size=16, buffer_size=10000): dataset = tf.data.TFRecordDataset(filelist) dataset = dataset.map(parse_fn, num_parallel_calls=4) dataset = dataset.padded_batch(batch_size, padded_shapes=(None, INPUT_DIM + 1), padding_values=-1.0, drop_remainder=False) return dataset 
texar.utils.beam_search.beam_search.grow|alive def grow_alive(curr_seq, curr_scores, curr_log_probs, curr_finished, states): """Given sequences and scores, will gather the top k=beam size sequences.  Args: curr_seq: current topk sequence that has been grown by one position. [batch_size, beam_size, i+1] curr_scores: scores for each of these sequences. [batch_size, beam_size] curr_log_probs: log probs for each of these sequences. [batch_size, beam_size] curr_finished: Finished flags for each of these sequences. [batch_size, beam_size] states: dict (possibly nested) of decoding states.  Returns: Tuple of (Topk sequences based on scores, log probs of these sequences, Finished flags of these sequences) """ curr_scores += tf.to_float(curr_finished) * -INF return compute_topk_scores_and_seq(curr_seq, curr_scores, curr_log_probs, curr_finished, beam_size, batch_size, 'grow_alive', states) 
texar.data.embedding.Embedding.vector|size @property def vector_size(self): """The embedding dimention size. """ return self._hparams.dim 
preprocess.prepare.get|query|length def get_query_length(topic_file): l = 0 with open(topic_file, 'rb') as f: for line in f: tokens = line.strip().split() print(tokens) l = max(l, len(tokens) - 1) print('Max length of query is {0}'.format(l)) 
thumt.models.rnnsearch.encoder|gru def _gru_encoder(cell, inputs, sequence_length, initial_state, dtype=None): output_size = cell.output_size dtype = dtype or inputs.dtype batch = tf.shape(inputs)[0] time_steps = tf.shape(inputs)[1] zero_output = tf.zeros([batch, output_size], dtype) if initial_state is None: initial_state = cell.zero_state(batch, dtype) input_ta = tf.TensorArray(dtype, time_steps, tensor_array_name= 'input_array') output_ta = tf.TensorArray(dtype, time_steps, tensor_array_name= 'output_array') input_ta = input_ta.unstack(tf.transpose(inputs, [1, 0, 2]))  def loop_func(t, out_ta, state): inp_t = input_ta.read(t) cell_output, new_state = cell(inp_t, state) cell_output = _copy_through(t, sequence_length, zero_output, cell_output) new_state = _copy_through(t, sequence_length, state, new_state) out_ta = out_ta.write(t, cell_output) return t + 1, out_ta, new_state time = tf.constant(0, dtype=tf.int32, name='time') loop_vars = time, output_ta, initial_state outputs = tf.while_loop(lambda t, *_: t < time_steps, loop_func, loop_vars, parallel_iterations=32, swap_memory=True) output_final_ta = outputs[1] final_state = outputs[2] all_output = output_final_ta.stack() all_output.set_shape([None, None, output_size]) all_output = tf.transpose(all_output, [1, 0, 2]) return all_output, final_state 
main.train|epoch|netowrk|ker def train_epoch_ker_netowrk(train_set, batch_size, optimizer, device, model, num_choice, loss_func): model.train() dataset_loader = data.DataLoader(train_set, batch_size=batch_size, num_workers=0, shuffle=True, collate_fn=collate_csqa_paths) bce_loss_func = nn.BCELoss() for k, (statements, correct_labels, cpt_paths, rel_paths, qa_pairs ) in enumerate(tqdm(dataset_loader, desc='Train Batch')): optimizer.zero_grad() statements = statements.to(device) correct_labels = correct_labels.to(device) flat_statements = [] flat_qa_pairs = [] flat_cpt_paths = [] flat_rel_paths = [] assert len(statements) == len(cpt_paths) == len(rel_paths) == len( qa_pairs) for i in range(len(statements)): cur_statement = statements[i][0] cur_qa_pairs = qa_pairs[i] cur_cpt_paths = cpt_paths[i] cur_rel_paths = rel_paths[i] flat_statements.extend(cur_statement) flat_qa_pairs.extend(cur_qa_pairs) flat_cpt_paths.extend(cur_cpt_paths) flat_rel_paths.extend(cur_rel_paths) flat_statements = torch.stack(flat_statements).to(device) flat_logits = model(flat_statements, flat_qa_pairs, flat_cpt_paths, flat_rel_paths) if len(flat_logits) != len(flat_statements): ys = [] for j, correct in enumerate(correct_labels): for i in range(num_choice): if i != correct[0]: ys.append(torch.FloatTensor([0])) else: ys.append(torch.FloatTensor([1])) start = 0 for cur_flat_logits in flat_logits: end = start + len(cur_flat_logits) y = torch.cat(ys[start:end]).to(cur_flat_logits.device) loss = bce_loss_func(cur_flat_logits, y) loss.backward() optimizer.step() start = end else: y = torch.Tensor([1] * len(statements) * (num_choice - 1)).to( device) assert len(flat_logits) == len(flat_statements) assert len(flat_statements) == len(statements) * num_choice x1 = [] x2 = [] for j, correct in enumerate(correct_labels): for i in range(num_choice): cur_logit = flat_logits[j * num_choice + i] if i != correct[0]: x2.append(cur_logit) else: for _ in range(num_choice - 1): x1.append(cur_logit) mrloss = loss_func(torch.cat(x1), torch.cat(x2), y) mrloss.backward() optimizer.step() 
avod.core.box_list_ops_test.BoxListOpsTest.intersection|test|matched def test_matched_intersection(self): corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]]) corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0]]) exp_output = [2.0, 0.0] boxes1 = box_list.BoxList(corners1) boxes2 = box_list.BoxList(corners2) intersect = box_list_ops.matched_intersection(boxes1, boxes2) with self.test_session() as sess: intersect_output = sess.run(intersect) self.assertAllClose(intersect_output, exp_output) 
facenet-master.src.classifier.split|dataset def split_dataset(dataset, min_nrof_images_per_class, nrof_train_images_per_class): train_set = [] test_set = [] for cls in dataset: paths = cls.image_paths if len(paths) >= min_nrof_images_per_class: np.random.shuffle(paths) train_set.append(facenet.ImageClass(cls.name, paths[: nrof_train_images_per_class])) test_set.append(facenet.ImageClass(cls.name, paths[ nrof_train_images_per_class:])) return train_set, test_set 
fasterai.unet.DynamicUnetWide.Wide|Dynamic|Unet def __init__(self, encoder: nn.Module, n_classes: int, blur: bool=False, blur_final=True, self_attention: bool=False, y_range: Optional[Tuple[ float, float]]=None, last_cross: bool=True, bottle: bool=False, norm_type: Optional[NormType]=NormType.Batch, nf_factor: int=1, **kwargs): nf = 512 * nf_factor extra_bn = norm_type == NormType.Spectral imsize = 256, 256 sfs_szs = model_sizes(encoder, size=imsize) sfs_idxs = list(reversed(_get_sfs_idxs(sfs_szs))) self.sfs = hook_outputs([encoder[i] for i in sfs_idxs]) x = dummy_eval(encoder, imsize).detach() ni = sfs_szs[-1][1] middle_conv = nn.Sequential(custom_conv_layer(ni, ni * 2, norm_type= norm_type, extra_bn=extra_bn, **kwargs), custom_conv_layer(ni * 2, ni, norm_type=norm_type, extra_bn=extra_bn, **kwargs)).eval() x = middle_conv(x) layers = [encoder, batchnorm_2d(ni), nn.ReLU(), middle_conv] for i, idx in enumerate(sfs_idxs): not_final = i != len(sfs_idxs) - 1 up_in_c, x_in_c = int(x.shape[1]), int(sfs_szs[idx][1]) do_blur = blur and (not_final or blur_final) sa = self_attention and i == len(sfs_idxs) - 3 n_out = nf if not_final else nf // 2 unet_block = UnetBlockWide(up_in_c, x_in_c, n_out, self.sfs[i], final_div=not_final, blur=blur, self_attention=sa, norm_type= norm_type, extra_bn=extra_bn, **kwargs).eval() layers.append(unet_block) x = unet_block(x) ni = x.shape[1] if imsize != sfs_szs[0][-2:]: layers.append(PixelShuffle_ICNR(ni, **kwargs)) if last_cross: layers.append(MergeLayer(dense=True)) ni += in_channels(encoder) layers.append(res_block(ni, bottle=bottle, norm_type=norm_type, ** kwargs)) layers += [custom_conv_layer(ni, n_classes, ks=1, use_activ=False, norm_type=norm_type)] if y_range is not None: layers.append(SigmoidRange(*y_range)) super().__init__(*layers) 
estimate_gradient_norm.EstimateLipschitz.max|abseig|compute def _compute_max_abseig(self, pt_hvs, batch_inputs, true_label, target_label, max_eig_iters, print_flag): i = 0 cond = False pt_eigs = [] print('pt_hvs[0] shape = {}'.format(pt_hvs[0].shape)) while i < max_eig_iters and cond == False: tmp_hv, tmp_hv_norm, tmp_vhv, tmp_vnorm, tmp_est_eig = self.sess.run([ self.hv_op, self.hv_norm_op, self.vhv_op, self.randv_norm_op, self.eig_est], feed_dict={self.img: batch_inputs, self.randv: pt_hvs[i], self.true_label: true_label, self.target_label: target_label}) tmp_vhv = np.squeeze(tmp_vhv) tmp_vnorm = np.squeeze(tmp_vnorm) tmp_est_eig = np.squeeze(tmp_est_eig) if print_flag: print('current step = {}, est_eig = {}'.format(i, tmp_est_eig - 0)) pt_hvs.append(tmp_hv + 0 * pt_hvs[i]) pt_eigs.append(tmp_est_eig) if i > 0: cond_element = abs(tmp_est_eig - pt_eigs[i - 1]) < 0.001 if print_flag: print('cond = {}'.format(cond_element)) cond = cond_element.all() i += 1 if i == max_eig_iters: print('==== Reach max iterations!!! ====') return pt_eigs[-1] 
parallel.DataParallelCriterion.forward def forward(self, inputs, *targets, **kwargs): if not self.device_ids: return self.module(inputs, *targets, **kwargs) targets, kwargs = self.scatter(targets, kwargs, self.device_ids) if len(self.device_ids) == 1: return self.module(inputs, *targets[0], **kwargs[0]) replicas = self.replicate(self.module, self.device_ids[:len(inputs)]) outputs = _criterion_parallel_apply(replicas, inputs, targets, kwargs) return self.gather(outputs, self.output_device) 
lsr_loss.lsr|loss def lsr_loss(logits, one_hot_labels, label_smoothing=0, weight=1.0, scope=None ): """Define a Cross Entropy loss using softmax_cross_entropy_with_logits.  It can scale the loss by weight factor, and smooth the labels.  Args: logits: [batch_size, num_classes] logits outputs of the network . one_hot_labels: [batch_size, num_classes] target one_hot_encoded labels. label_smoothing: if greater than 0 then smooth the labels. weight: scale the loss by this factor. scope: Optional scope for name_scope.  Returns: A tensor with the softmax_cross_entropy loss. """ logits.get_shape().assert_is_compatible_with(one_hot_labels.get_shape()) with tf.name_scope(scope, 'lsr_loss', [logits, one_hot_labels]): num_classes = one_hot_labels.get_shape()[-1].value one_hot_labels = tf.cast(one_hot_labels, logits.dtype) if label_smoothing > 0: smooth_positives = 1.0 - label_smoothing smooth_negatives = label_smoothing / num_classes one_hot_labels = (one_hot_labels * smooth_positives + smooth_negatives) cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels= one_hot_labels, logits=logits, name='xentropy') weight = tf.convert_to_tensor(weight, dtype=logits.dtype.base_dtype, name='loss_weight') return tf.multiply(weight, tf.reduce_mean(cross_entropy), name='value') 
SRGANs-Spectral-Regularization-GANs--master.train_mn.load|models def load_models(config): gen_conf = config.models['generator'] gen = yaml_utils.load_model(gen_conf['fn'], gen_conf['name'], gen_conf[ 'args']) dis_conf = config.models['discriminator'] dis = yaml_utils.load_model(dis_conf['fn'], dis_conf['name'], dis_conf[ 'args']) return gen, dis 
facenet-master.tmp.visualize.T def T(layer): """Helper for getting layer output tensor""" return tf.get_default_graph().get_tensor_by_name('%s:0' % layer) 
dnc.jobs|gradient|add def add_gradient_jobs(jobs): optimizers = 'adam', 'adagrad', 'nag', 'sgd', 'alig', 'rmsprop' lr_list = 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0 for optimizer in optimizers: for lr in lr_list: jobs.append( 'python train.py --optimizer {optimizer} --learning_rate {lr}' .format(optimizer=optimizer, lr=lr)) jobs.append('python train.py --optimizer alig') 
thumt.utils.weight_ratio.weight|mean|ratio def weight_ratio_mean(input, output, stab=0): """ inputs: (..., dim) output: (..., 1) weight ratios: [(..., dim)] """ dim = tf.cast(tf.shape(input)[-1], tf.float32) output_shape = tf.shape(input) inputs = tf.reshape(input, [-1, input.shape[-1].value]) output = tf.reshape(output, [-1, output.shape[-1].value]) w = inputs / dim / stabilize(output, stab) return tf.reshape(w, output_shape) 
cpplint.Method|Out|Of|Definition|Is|Line def IsOutOfLineMethodDefinition(clean_lines, linenum): """Check if current line contains an out-of-line method definition.  Args: clean_lines: A CleansedLines instance containing the file. linenum: The number of the line to check. Returns: True if current line contains an out-of-line method definition. """ for i in xrange(linenum, max(-1, linenum - 10), -1): if Match('^([^()]*\\w+)\\(', clean_lines.elided[i]): return Match('^[^()]*\\w+::\\w+\\(', clean_lines.elided[i] ) is not None return False 
facenet-master.src.train_tripletloss.evaluate def evaluate(sess, image_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, batch_size, nrof_folds, log_dir, step, summary_writer, embedding_size): start_time = time.time() print('Running forward pass on LFW images: ', end='') nrof_images = len(actual_issame) * 2 assert len(image_paths) == nrof_images labels_array = np.reshape(np.arange(nrof_images), (-1, 3)) image_paths_array = np.reshape(np.expand_dims(np.array(image_paths), 1), (-1, 3)) sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array}) emb_array = np.zeros((nrof_images, embedding_size)) nrof_batches = int(np.ceil(nrof_images / batch_size)) label_check_array = np.zeros((nrof_images,)) for i in xrange(nrof_batches): batch_size = min(nrof_images - i * batch_size, batch_size) emb, lab = sess.run([embeddings, labels_batch], feed_dict={ batch_size_placeholder: batch_size, learning_rate_placeholder: 0.0, phase_train_placeholder: False}) emb_array[(lab), :] = emb label_check_array[lab] = 1 print('%.3f' % (time.time() - start_time)) assert np.all(label_check_array == 1) _, _, accuracy, val, val_std, far = lfw.evaluate(emb_array, actual_issame, nrof_folds=nrof_folds) print('Accuracy: %1.3f+-%1.3f' % (np.mean(accuracy), np.std(accuracy))) print('Validation rate: %2.5f+-%2.5f @ FAR=%2.5f' % (val, val_std, far)) lfw_time = time.time() - start_time summary = tf.Summary() summary.value.add(tag='lfw/accuracy', simple_value=np.mean(accuracy)) summary.value.add(tag='lfw/val_rate', simple_value=val) summary.value.add(tag='time/lfw', simple_value=lfw_time) summary_writer.add_summary(summary, step) with open(os.path.join(log_dir, 'lfw_result.txt'), 'at') as f: f.write('%d\t%.5f\t%.5f\n' % (step, np.mean(accuracy), val)) 
data_utils.batch|build|seq def build_batch_seq2seq(data_batch, len_batch, use_mscoco14=False): """Build a batch of data for the sequence to sequence model""" enc_inputs = [] dec_inputs = [] targets = [] inp_lens = [] out_lens = [] for st, slen in zip(data_batch, len_batch): num_para = len(st) if use_mscoco14: iter_range = [0, 2] else: iter_range = range(num_para) for i in iter_range: j = (i + 1) % num_para inp = st[i][:-1] d_in = st[j][:-1] d_out = st[j][1:] len_inp = slen[i] len_out = slen[j] enc_inputs.append(inp) dec_inputs.append(d_in) targets.append(d_out) inp_lens.append(len_inp) out_lens.append(len_out) batch_dict = {'enc_inputs': np.array(enc_inputs), 'dec_inputs': np. array(dec_inputs), 'targets': np.array(targets), 'inp_lens': np. array(inp_lens), 'out_lens': np.array(out_lens)} return batch_dict 
SMILESX_utils.get|tokentoint def get_tokentoint(tokens): return dict((c, i) for i, c in enumerate(tokens)) 
xlnet-master.tpu_estimator._predict_on_tpu_system.on|single|shard|multi|tpu|steps|predict def multi_tpu_predict_steps_on_single_shard():  def cond(scalar_stopping_signal): return math_ops.logical_not(_StopSignals.should_stop( scalar_stopping_signal)) inputs = [_StopSignals.NON_STOPPING_SIGNAL] outputs = training_loop.while_loop(cond, single_tpu_predict_step, inputs=inputs, name=b'loop') return outputs 
classification.ops.fisher_blocks.NaiveDiagonalFB.minibatch|register|additional def register_additional_minibatch(self, batch_size): """Register an additional minibatch. Args: batch_size: The batch size, used in the covariance estimator. """ self._batch_sizes.append(batch_size) 
layers.rlngru_layer.slice def _slice(_x, n, dim): if _x.ndim == 3: return _x[:, :, n * dim:(n + 1) * dim] return _x[:, n * dim:(n + 1) * dim] 
neural_tangents.predict.gp|inference def gp_inference(kernel_fn, x_train, y_train, x_test, get, diag_reg=0.0, compute_cov=False): """Compute the mean and variance of the `posterior` of NNGP and NTK.  Note that this method is equivalent to `gradient_descent_mse_gp` at infinite time. Example: ```python >>> predict = gradient_descent_mse_gp(kernel_fn, x_train, y_train, x_test, >>>                                   get, diag_reg, compute_cov) >>> predict(np.inf) == predict(None) == gp_inference(kernel_fn, x_train, >>>     y_train, x_test, get, diag_reg, compute_cov) ```  Args: kernel_fn: A kernel function that computes NNGP and NTK. x_train: A `np.ndarray`, representing the training data. y_train: A `np.ndarray`, representing the labels of the training data. x_test: A `np.ndarray`, representing the test data. get: string, the mode of the Gaussian process, either "nngp" or "ntk", or a tuple, or None. If `None` then both `nngp` and `ntk` predictions are returned. diag_reg: A float, representing the strength of the regularization. compute_cov: A boolean. If `True` computing both `mean` and `variance` and only `mean` otherwise.  Returns: Either a Gaussian(`mean`, `variance`) namedtuple or `mean` of the GP posterior. """ if get is None: get = 'nngp', 'ntk' kdd, ktd, ktt = _get_matrices(kernel_fn, x_train, x_test, get, compute_cov) gp_inference_mat = _gp_inference_mat_jit_cpu if _is_on_cpu(kdd ) else _gp_inference_mat_jit return gp_inference_mat(kdd, ktd, ktt, y_train, get, diag_reg) 
archs.hydra.Model.Model def __init__(self, train_lr, meta_lr, image_shape, isMIN, label_size=2): super().__init__(train_lr, meta_lr, image_shape, isMIN, label_size) self.finals = 64 if isMIN: self.finals = 800 
texar.data.data.data_iterators.DataIterator.Data|Iterator def __init__(self, datasets): DataIteratorBase.__init__(self, datasets) self._variable_scope = get_unique_named_variable_scope('data_iterator') with tf.variable_scope(self._variable_scope): first_dataset = self._datasets[sorted(self.dataset_names)[0]] self._iterator = tf.data.Iterator.from_structure(first_dataset. output_types, first_dataset.output_shapes) self._iterator_init_ops = {name: self._iterator.make_initializer(d) for name, d in self._datasets.items()} 
utils.HyperParamSetterWithCosine.With|Hyper|Cosine|Setter|Param def __init__(self, param, base_lr, start_step, n_step, step_based=True): """ Cosine learning rate """ super(HyperParamSetterWithCosine, self).__init__(param) self._base_lr = base_lr self._start_step = start_step self._n_step = n_step self._step = step_based 
utils.training.load|weights def load_weights(model, fpath): print("loading weights '{}'".format(fpath)) weights = torch.load(fpath) startEpoch = weights['startEpoch'] model.load_state_dict(weights['state_dict']) print('loaded weights (lastEpoch {}, loss {}, error {})'.format( startEpoch - 1, weights['loss'], weights['error'])) return startEpoch 
pvae.pixelvae_bbans.codec|obs def obs_codec(theta): append, pop = codecs.AutoRegressive(partial(pixelcnn, theta), np.shape( images[0]), np.shape(images[0]) + (256,), obs_elem_idxs, obs_elem_codec )  def pop_(msg): msg, (data, _) = pop(msg) return msg, data return append, pop_ 
data_preprocessing.DataPreprocess.Data|Preprocess def __init__(self, n_timestamps, D, step_size, ntx_max, ntx, nrx_max, nrx, nsubcarrier_max, nsubcarrier, output_shape, file_prefix, label): self.file_prefix = file_prefix self.data_shape = n_timestamps, nrx_max, ntx_max, nsubcarrier_max self.step_size = step_size self.n_timestamps = n_timestamps self.ntx = ntx self.nrx = nrx self.nsubcarrier = nsubcarrier self.subcarrier_spacing = int(nsubcarrier_max / nsubcarrier) self.x_train, self.y_train, self.x_test, self.y_test = np.array([] ), np.array([]), np.array([]), np.array([]) self.no_label_test = None self.x_evaluate = {} self.classes_num = {} self.label = label self.output_shape = output_shape 
deepctr.models.deepfm.FM|Deep def DeepFM(feature_dim_dict, embedding_size=8, use_fm=True, dnn_hidden_units=(128, 128), l2_reg_linear=1e-05, l2_reg_embedding= 1e-05, l2_reg_dnn=0, init_std=0.0001, seed=1024, dnn_dropout=0, dnn_activation='relu', dnn_use_bn=False, task='binary'): """Instantiates the DeepFM Network architecture.  :param feature_dim_dict: dict,to indicate sparse field and dense field like {'sparse':{'field_1':4,'field_2':3,'field_3':2},'dense':['field_4','field_5']} :param embedding_size: positive integer,sparse feature embedding_size :param use_fm: bool,use FM part or not :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN :param l2_reg_linear: float. L2 regularizer strength applied to linear part :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector :param l2_reg_dnn: float. L2 regularizer strength applied to DNN :param init_std: float,to use as the initialize std of embedding vector :param seed: integer ,to use as random seed. :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate. :param dnn_activation: Activation function to use in DNN :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN :param task: str, ``"binary"`` for  binary logloss or  ``"regression"`` for regression loss :return: A Keras model instance. """ check_feature_config_dict(feature_dim_dict) deep_emb_list, linear_emb_list, dense_input_dict, inputs_list = ( preprocess_input_embedding(feature_dim_dict, embedding_size, l2_reg_embedding, l2_reg_linear, init_std, seed, create_linear_weight=True)) linear_logit = get_linear_logit(linear_emb_list, dense_input_dict, l2_reg_linear) fm_input = concat_fun(deep_emb_list, axis=1) deep_input = tf.keras.layers.Flatten()(fm_input) fm_out = FM()(fm_input) deep_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed)(deep_input) deep_logit = tf.keras.layers.Dense(1, use_bias=False, activation=None)( deep_out) if len(dnn_hidden_units) == 0 and use_fm == False: final_logit = linear_logit elif len(dnn_hidden_units) == 0 and use_fm == True: final_logit = tf.keras.layers.add([linear_logit, fm_out]) elif len(dnn_hidden_units) > 0 and use_fm == False: final_logit = tf.keras.layers.add([linear_logit, deep_logit]) elif len(dnn_hidden_units) > 0 and use_fm == True: final_logit = tf.keras.layers.add([linear_logit, fm_out, deep_logit]) else: raise NotImplementedError output = PredictionLayer(task)(final_logit) model = tf.keras.models.Model(inputs=inputs_list, outputs=output) return model 
model_helper.sequence|loss|sigmoid def sequence_loss_sigmoid(logits, targets, weights): with tf.name_scope(name='sequence_loss', values=[logits, targets, weights] ): num_classes = tf.shape(logits)[2] logits_flat = tf.reshape(logits, [-1, num_classes]) targets = tf.cast(tf.reshape(targets, [-1, num_classes]), tf.float32) crossent = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits( labels=targets, logits=logits_flat), axis=1) crossent *= tf.reshape(weights, [-1]) crossent = tf.reduce_sum(crossent) total_size = tf.reduce_sum(weights) total_size += 1e-12 crossent /= total_size return crossent 
plato.agent.component.nlg.slot_filling_nlg.SlotFillingNLG.NLG|Slot|Filling def __init__(self, args=None): """ Nothing to initialize. We need the args to support use by the Generic Agent. """ super(SlotFillingNLG, self).__init__() 
gym_pycolab.tests.test_things.get|post|update def get_post_update(entity, the_plot): """Retrieve post-update callable for `entity` for the next game iteration.  Once retrieved, the post-update callable is cleared, so the callable will only be called for the current game iteration.  This function is intended mainly as a helper for the `Sprite`s and `Drape`s defined in this module. Most user code will not need to use it.  Args: entity: the pycolab game entity for which we wish to retrieve any post-update callable. the_plot: the `Plot` object for the pycolab game passed to `post_update` when registering a callable for `entity`.  Returns: the callable registered for this entity via `post_update`, or a null callable if none was registered. """ return the_plot.setdefault('test_post_update', {}).pop(entity.character, lambda *args, **kwargs: None) 
data.val|data|generator def val_data_generator(patient_indexes, h5_file_path, batch_size=1): i = 0 file = h5py.File(h5_file_path, 'r') imgs = file['data'] labels = file['label'] slice_indexes = [] for patient_index in patient_indexes: for slice_index in range(189): slice_indexes.append(patient_index * 189 + slice_index) num_of_slices = len(slice_indexes) while True: batch_img = [] batch_label = [] for b in range(batch_size): current_img = imgs[slice_indexes[i]][5:229, 2:194] current_label = labels[slice_indexes[i]][5:229, 2:194] batch_img.append(current_img) batch_label.append(current_label) i = (i + 1) % num_of_slices yield np.expand_dims(np.array(batch_img), 3), np.expand_dims(np. array(batch_label), 3) 
alig.test.Test.setup|th def setup_th(self): torch.set_default_dtype(torch.double) self.model_th = ModelTh(self.w1, self.b1, self.w2, self.b2) self.feed_dict_th = {'x': torch.from_numpy(self.x), 'y': torch. from_numpy(self.y)} 
plato.utilities.parser.parse_dstc2.Parser.action|encode|dstc def encode_action_dstc(self, actions, system=True): """ Endoce the dialogue actions - specific for DSTC2  :param actions: :param system: :return: """ if not actions: print( 'WARNING: Parse DSTC2 action encoding called with empty actions list (returning 0).' ) return 0 action = actions[0] if self.dstc2_acts and action.intent in self.dstc2_acts: return self.dstc2_acts.index(action.intent) if action.intent == 'request': if system and action.params[0].slot in self.system_requestable_slots: return len(self.dstc2_acts) + self.system_requestable_slots.index( action.params[0].slot) elif action.params[0].slot in self.requestable_slots: return len(self.dstc2_acts) + self.requestable_slots.index(action .params[0].slot) if action.intent == 'inform' and action.params[0 ].slot in self.requestable_slots: if system: return len(self.dstc2_acts) + len(self.system_requestable_slots ) + self.requestable_slots.index(action.params[0].slot) else: return len(self.dstc2_acts) + len(self.requestable_slots ) + self.requestable_slots.index(action.params[0].slot) print( 'Parse DSTC2 action encoder warning: Selecting default action (unable to encode: {0})!' .format(action)) return 0 
task.Dyck.dyck|is def _is_dyck(self, word): stack = [] pairs = self.inverse_bracket_pairs for br in word: if stack and br in pairs and stack[-1] == pairs.get(br): stack.pop() else: stack.append(br) return not stack 
cifar10.load|training|data def load_training_data(): """ Load all the training-data for the CIFAR-10 data-set.  The data-set is split into 5 data-files which are merged here.  Returns the images, class-numbers and one-hot encoded class-labels. """ images = np.zeros(shape=[_num_images_train, img_size, img_size, num_channels], dtype=float) cls = np.zeros(shape=[_num_images_train], dtype=int) begin = 0 for i in range(_num_files_train): images_batch, cls_batch = _load_data(filename='data_batch_' + str(i + 1)) num_images = len(images_batch) end = begin + num_images images[begin:end, :] = images_batch cls[begin:end] = cls_batch begin = end return images, cls, one_hot_encoded(class_numbers=cls, num_classes= num_classes) 
thumt.models.seq2seq.Seq2Seq.get|training|func def get_training_func(self, initializer, regularizer=None, dtype=None):  def training_fn(features, params=None, reuse=None): if params is None: params = self.parameters custom_getter = utils.custom_getter if dtype else None with tf.variable_scope(self._scope, initializer=initializer, regularizer=regularizer, reuse=reuse, custom_getter= custom_getter, dtype=dtype): loss = model_graph(features, 'train', params) return loss return training_fn 
sidd_utils.get|klds|avg def get_avg_klds(temp_ids=[1, 2, 3, 5, 7]): root = ( '/home/abdo/skynet/_Code/fourier_flows/experiments/sidd/190130/UncSdnUncGainUnc_s/' ) epcs = [1000] n = 4 kld_avgs = np.ndarray([len(temp_ids), n]) for i, id in enumerate(temp_ids): for epc in epcs: fn = os.path.join(root, 'samples%d' % id, 'epoch_%04d' % epc, 'kld_avgs.txt') klds = np.loadtxt(fn) kld_avgs[(i), :] = klds return kld_avgs 
eval.precision|avg def avg_precision(actual=None, predicted=None): """Computes average label precision. function my Manoj Plakal, but here is done with strings""" for i, p in enumerate(predicted): if actual == p: return 1.0 / (i + 1.0) return 0.0 
nets.resnet_utils.subsample def subsample(inputs, factor, scope=None): """Subsamples the input along the spatial dimensions.  Args: inputs: A `Tensor` of size [batch, height_in, width_in, channels]. factor: The subsampling factor. scope: Optional variable_scope.  Returns: output: A `Tensor` of size [batch, height_out, width_out, channels] with the input, either intact (if factor == 1) or subsampled (if factor > 1). """ if factor == 1: return inputs else: return slim.max_pool2d(inputs, [1, 1], stride=factor, scope=scope) 
hdrcnn_train.calc|loss|and|print def calc_loss_and_print(x_data, y_data, print_dir, step, N): val_loss, orig_loss, n_batch = 0, 0, 0 for b in range(int(x_data.shape[0] / FLAGS.batch_size)): x_batch = x_data[b * FLAGS.batch_size:(b + 1) * FLAGS.batch_size, :, :, :] y_batch = y_data[b * FLAGS.batch_size:(b + 1) * FLAGS.batch_size, :, :, :] feed_dict = {x: x_batch, y_: y_batch} err1, err2, y_predict, y_gt, M = sess.run([cost, cost_input_output, y, y_, msk], feed_dict=feed_dict) val_loss += err1 orig_loss += err2 n_batch += 1 batch_dir = print_dir if x_data.shape[0] > x_batch.shape[0]: batch_dir = '%s/batch_%03d' % (print_dir, n_batch) if n_batch <= N or N < 0: if not os.path.exists(batch_dir): os.makedirs(batch_dir) for i in range(0, x_batch.shape[0]): yy_p = np.squeeze(y_predict[i]) xx = np.squeeze(x_batch[i]) yy = np.squeeze(y_gt[i]) mm = np.squeeze(M[i]) x_lin = np.power(np.divide(0.6 * xx, np.maximum(1.6 - xx, 1e-10)), 1.0 / 0.9) yy_p = np.exp(yy_p) - eps y_final = (1 - mm) * x_lin + mm * yy_p yy_p = np.power(np.maximum(yy_p, 0.0), 0.5) y_final = np.power(np.maximum(y_final, 0.0), 0.5) yy = np.power(np.maximum(yy, 0.0), 0.5) xx = np.power(np.maximum(x_lin, 0.0), 0.5) if FLAGS.print_im: img_io.writeLDR(xx, '%s/%06d_%03d_in.png' % (batch_dir, step, i + 1), -3) img_io.writeLDR(yy, '%s/%06d_%03d_gt.png' % (batch_dir, step, i + 1), -3) img_io.writeLDR(y_final, '%s/%06d_%03d_out.png' % ( batch_dir, step, i + 1), -3) if FLAGS.print_hdr: img_io.writeEXR(xx, '%s/%06d_%03d_in.exr' % (batch_dir, step, i + 1)) img_io.writeEXR(yy, '%s/%06d_%03d_gt.exr' % (batch_dir, step, i + 1)) img_io.writeEXR(y_final, '%s/%06d_%03d_out.exr' % ( batch_dir, step, i + 1)) return val_loss / n_batch, orig_loss / n_batch 
texar.modules.decoders.rnn_decoder_base.RNNDecoderBase.finalize def finalize(self, outputs, final_state, sequence_lengths): raise NotImplementedError 
nets.resnet_v2_test.ResnetUtilsTest.Conv|Same|Odd|test|D def testConv2DSameOdd(self): n, n2 = 5, 3 x = create_test_input(1, n, n, 1) w = create_test_input(1, 3, 3, 1) w = tf.reshape(w, [3, 3, 1, 1]) tf.get_variable('Conv/weights', initializer=w) tf.get_variable('Conv/biases', initializer=tf.zeros([1])) tf.get_variable_scope().reuse_variables() y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope='Conv') y1_expected = tf.to_float([[14, 28, 43, 58, 34], [28, 48, 66, 84, 46], [43, 66, 84, 102, 55], [58, 84, 102, 120, 64], [34, 46, 55, 64, 30]]) y1_expected = tf.reshape(y1_expected, [1, n, n, 1]) y2 = resnet_utils.subsample(y1, 2) y2_expected = tf.to_float([[14, 43, 34], [43, 84, 55], [34, 55, 30]]) y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1]) y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope='Conv') y3_expected = y2_expected y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope='Conv') y4_expected = y2_expected with self.test_session() as sess: sess.run(tf.global_variables_initializer()) self.assertAllClose(y1.eval(), y1_expected.eval()) self.assertAllClose(y2.eval(), y2_expected.eval()) self.assertAllClose(y3.eval(), y3_expected.eval()) self.assertAllClose(y4.eval(), y4_expected.eval()) 
mylogger.add_logging_level.to|root|log def log_to_root(message, *args, **kwargs): logging.log(levelNum, message, *args, **kwargs) 
pytorch_pretrained_bert.tokenization.BasicTokenizer.text|clean def _clean_text(self, text): """Performs invalid character removal and whitespace cleanup on text.""" output = [] for char in text: cp = ord(char) if cp == 0 or cp == 65533 or _is_control(char): continue if _is_whitespace(char): output.append(' ') else: output.append(char) return ''.join(output) 
deepctr.models.din.DIN def DIN(feature_dim_dict, seq_feature_list, embedding_size=8, hist_len_max= 16, dnn_use_bn=False, dnn_hidden_units=(200, 80), dnn_activation='relu', att_hidden_size=(80, 40), att_activation='dice', att_weight_normalization=False, l2_reg_dnn=0, l2_reg_embedding=1e-06, dnn_dropout=0, init_std=0.0001, seed=1024, task='binary'): """Instantiates the Deep Interest Network architecture.  :param feature_dim_dict: dict,to indicate sparse field (**now only support sparse feature**)like {'sparse':{'field_1':4,'field_2':3,'field_3':2},'dense':[]} :param seq_feature_list: list,to indicate  sequence sparse field (**now only support sparse feature**),must be a subset of ``feature_dim_dict["sparse"]`` :param embedding_size: positive integer,sparse feature embedding_size. :param hist_len_max: positive int, to indicate the max length of seq input :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in deep net :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of deep net :param dnn_activation: Activation function to use in deep net :param att_hidden_size: list,list of positive integer , the layer number and units in each layer of attention net :param att_activation: Activation function to use in attention net :param att_weight_normalization: bool.Whether normalize the attention score of local activation unit. :param l2_reg_dnn: float. L2 regularizer strength applied to DNN :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate. :param init_std: float,to use as the initialize std of embedding vector :param seed: integer ,to use as random seed. :param task: str, ``"binary"`` for  binary logloss or  ``"regression"`` for regression loss :return: A Keras model instance.  """ check_feature_config_dict(feature_dim_dict) sparse_input, dense_input, user_behavior_input = get_input(feature_dim_dict , seq_feature_list, hist_len_max) sparse_embedding_dict = {feat.name: Embedding(feat.dimension, embedding_size, embeddings_initializer=RandomNormal(mean=0.0, stddev=init_std, seed=seed), embeddings_regularizer=l2( l2_reg_embedding), name='sparse_emb_' + str(i) + '-' + feat.name, mask_zero=feat.name in seq_feature_list) for i, feat in enumerate( feature_dim_dict['sparse'])} query_emb_list = get_embedding_vec_list(sparse_embedding_dict, sparse_input, feature_dim_dict['sparse'], seq_feature_list, seq_feature_list) keys_emb_list = get_embedding_vec_list(sparse_embedding_dict, user_behavior_input, feature_dim_dict['sparse'], seq_feature_list, seq_feature_list) deep_input_emb_list = get_embedding_vec_list(sparse_embedding_dict, sparse_input, feature_dim_dict['sparse'], mask_feat_list= seq_feature_list) keys_emb = concat_fun(keys_emb_list) deep_input_emb = concat_fun(deep_input_emb_list) query_emb = concat_fun(query_emb_list) hist = AttentionSequencePoolingLayer(att_hidden_size, att_activation, weight_normalization=att_weight_normalization, supports_masking=True)([ query_emb, keys_emb]) deep_input_emb = Concatenate()([NoMask()(deep_input_emb), hist]) deep_input_emb = Flatten()(deep_input_emb) if len(dense_input) > 0: deep_input_emb = Concatenate()([deep_input_emb] + list(dense_input. values())) output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed)(deep_input_emb) final_logit = Dense(1, use_bias=False)(output) output = PredictionLayer(task)(final_logit) model_input_list = get_inputs_list([sparse_input, dense_input, user_behavior_input]) model = Model(inputs=model_input_list, outputs=output) return model 
texar.modules.embedders.embedders_test.EmbedderTest.embedder|test|position def _test_position_embedder(self, hparams): """Tests :class:`texar.modules.PositionEmbedder`. """ pos_size = 100 embedder = PositionEmbedder(position_size=pos_size, hparams=hparams) inputs = tf.ones([64, 16], dtype=tf.int32) outputs = embedder(inputs) emb_dim = embedder.dim if not isinstance(emb_dim, (list, tuple)): emb_dim = [emb_dim] hparams_dim = hparams['dim'] if not isinstance(hparams['dim'], (list, tuple)): hparams_dim = [hparams['dim']] self.assertEqual(outputs.shape, [64, 16] + emb_dim) self.assertEqual(emb_dim, hparams_dim) self.assertEqual(embedder.position_size, 100) self.assertEqual(len(embedder.trainable_variables), 1) seq_length = tf.random_uniform([64], maxval=pos_size, dtype=tf.int32) outputs = embedder(sequence_length=seq_length) with self.test_session() as sess: sess.run(tf.global_variables_initializer()) outputs_, max_seq_length = sess.run([outputs, tf.reduce_max( seq_length)], feed_dict={global_mode(): tf.estimator.ModeKeys. TRAIN}) self.assertEqual(outputs_.shape, (64, max_seq_length) + tuple(emb_dim)) 
tica.wolf|ledoit|blackwell|rao def rao_blackwell_ledoit_wolf(S, n): """Rao-Blackwellized Ledoit-Wolf shrinkaged estimator of the covariance matrix.  Parameters ---------- S : array, shape=(n, n) Sample covariance matrix (e.g. estimated with np.cov(X.T)) n : int Number of data points.  Returns ------- sigma : array, shape=(n, n) shrinkage : float  References ---------- .. [1] Chen, Yilun, Ami Wiesel, and Alfred O. Hero III. "Shrinkage estimation of high dimensional covariance matrices" ICASSP (2009) """ p = len(S) assert S.shape == (p, p) alpha = (n - 2) / (n * (n + 2)) beta = ((p + 1) * n - 2) / (n * (n + 2)) trace_S2 = np.sum(S * S) U = p * trace_S2 / np.trace(S) ** 2 - 1 rho = min(alpha + beta / U, 1) F = np.trace(S) / p * np.eye(p) return (1 - rho) * S + rho * F, rho 
train.Trainer.evaluation|cycle|run def run_evaluation_cycle(self): for split in ['dev', 'test']: self.evaluator.validate(self.opt.train.dynamic.epoch, split, self. losses[split]) if self.do_gen: gen.do_gen_run(self.opt, self.generator, self.opt.train.dynamic .epoch, split, self.losses[split]) iter_num = self.opt.train.dynamic.epoch for loss_name in self.losses[split]: self.logger.add_scalar('{}/{}'.format(split, loss_name), self. losses[split][loss_name][iter_num], iter_num) 
plato.agent.component.dialogue_manager.dialogue_manager_generic.DialogueManagerGeneric.training|is def is_training(self): """ Assess whether there are any trainable components in this dialogue Manager.  :return: True or False """ return self.TRAIN_DST or self.TRAIN_POLICY 
bert.modeling.BertModel.Model|Bert def __init__(self, config, is_training, input_ids, input_mask=None, token_type_ids=None, use_one_hot_embeddings=True, scope=None): """Constructor for BertModel.  Args: config: `BertConfig` instance. is_training: bool. true for training model, false for eval model. Controls whether dropout will be applied. input_ids: int32 Tensor of shape [batch_size, seq_length]. input_mask: (optional) int32 Tensor of shape [batch_size, seq_length]. token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length]. use_one_hot_embeddings: (optional) bool. Whether to use one-hot word embeddings or tf.embedding_lookup() for the word embeddings. On the TPU, it is much faster if this is True, on the CPU or GPU, it is faster if this is False. scope: (optional) variable scope. Defaults to "bert".  Raises: ValueError: The config is invalid or one of the input tensor shapes is invalid. """ config = copy.deepcopy(config) if not is_training: config.hidden_dropout_prob = 0.0 config.attention_probs_dropout_prob = 0.0 input_shape = get_shape_list(input_ids, expected_rank=2) batch_size = input_shape[0] seq_length = input_shape[1] if input_mask is None: input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32) if token_type_ids is None: token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf. int32) with tf.variable_scope(scope, default_name='bert'): with tf.variable_scope('embeddings'): self.embedding_output, self.embedding_table = embedding_lookup( input_ids=input_ids, vocab_size=config.vocab_size, embedding_size=config.hidden_size, initializer_range=config .initializer_range, word_embedding_name='word_embeddings', use_one_hot_embeddings=use_one_hot_embeddings) self.embedding_output = embedding_postprocessor(input_tensor= self.embedding_output, use_token_type=True, token_type_ids= token_type_ids, token_type_vocab_size=config. type_vocab_size, token_type_embedding_name= 'token_type_embeddings', use_position_embeddings=True, position_embedding_name='position_embeddings', initializer_range=config.initializer_range, max_position_embeddings=config.max_position_embeddings, dropout_prob=config.hidden_dropout_prob) with tf.variable_scope('encoder'): attention_mask = create_attention_mask_from_input_mask(input_ids, input_mask) self.all_encoder_layers, self.attn_maps = transformer_model( input_tensor=self.embedding_output, attention_mask= attention_mask, hidden_size=config.hidden_size, num_hidden_layers=config.num_hidden_layers, num_attention_heads=config.num_attention_heads, intermediate_size=config.intermediate_size, intermediate_act_fn=get_activation(config.hidden_act), hidden_dropout_prob=config.hidden_dropout_prob, attention_probs_dropout_prob=config. attention_probs_dropout_prob, initializer_range=config. initializer_range, do_return_all_layers=True) self.sequence_output = self.all_encoder_layers[-1] with tf.variable_scope('pooler'): first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1) self.pooled_output = tf.layers.dense(first_token_tensor, config .hidden_size, activation=tf.tanh, kernel_initializer= create_initializer(config.initializer_range)) 
gym_pycolab.envs.pycolab_grid_worlds_env.PycolabGridWorldsLevel18Env.Grid|Worlds|Pycolab|Level|Env def __init__(self): super(PycolabGridWorldsEnv, self).__init__() 
AffineCouplingFitSdnGain2.AffineCouplingFitSdnGain2.log|and|inverse|det|jacobian def _inverse_and_log_det_jacobian(self, y, yy, nlf0=None, nlf1=None, iso= None, cam=None): beta1, beta2 = sdn_iso_model_params_2(iso) scale = tf.sqrt(beta1 * yy + beta2) x = y if scale is not None: x /= scale if scale is None: log_abs_det_J_inv = tf.constant(0.0, dtype=y.dtype, name='ildj') else: log_abs_det_J_inv = -tf.reduce_sum(tf.log(scale), axis=[1, 2, 3]) if self._last_layer: return tf.layers.flatten(x), log_abs_det_J_inv return x, log_abs_det_J_inv 
base.Trainer.get|hooks def get_hooks(self, saver): merged_summary_op = tf.summary.merge_all() saver_hook = tf.train.CheckpointSaverHook(checkpoint_dir=self.dirs, save_steps=self.arch['training']['save_freq'], saver=saver) summary_hook = tf.train.SummarySaverHook(save_steps=self.arch[ 'training']['summary_freq'], summary_op=merged_summary_op, output_dir=self.dirs) stop_hook = tf.train.StopAtStepHook(last_step=self.arch['training'][ 'max_iter']) return [saver_hook, summary_hook, stop_hook] 
loss.dice def dice(y_true, y_pred): TP = K.sum(y_true * y_pred) union = K.sum(y_true) + K.sum(y_pred) return 2.0 * TP / union if union != 0 else 0 
commons.ops.batch_norm.batch|norm def __init__(self, epsilon=1e-05, momentum=0.9, name='batch_norm'): with tf.variable_scope(name): self.epsilon = epsilon self.momentum = momentum self.name = name 
nmt.beam_search_decoder.batch|tile def _tile_batch(t, multiplier): """Core single-tensor implementation of tile_batch.""" t = tf.convert_to_tensor(t, name='t') shape_t = tf.shape(t) if t.shape.ndims is None or t.shape.ndims < 1: raise ValueError('t must have statically known rank') tiling = [1] * (t.shape.ndims + 1) tiling[1] = multiplier tiled_static_batch_size = t.shape[0].value * multiplier if t.shape[0 ].value is not None else None tiled = tf.tile(tf.expand_dims(t, 1), tiling) tiled = tf.reshape(tiled, tf.concat(([shape_t[0] * multiplier], shape_t [1:]), 0)) tiled.set_shape(tf.TensorShape([tiled_static_batch_size]).concatenate(t .shape[1:])) return tiled 
evaluate.get|arguments def get_arguments(): """Parse all the arguments provided from the CLI.  Returns: A list of parsed arguments. """ parser = argparse.ArgumentParser(description= 'Discovering Class Specific Pixels (DCSP) Evaluation Script.') parser.add_argument('--data-dir', type=str, default=DATA_DIRECTORY, help='Path to the directory containing the PASCAL VOC dataset.') parser.add_argument('--data-list', type=str, default=DATA_LIST_PATH, help='Path to the file listing the images in the dataset.') parser.add_argument('--num-steps', type=int, default=NUM_STEPS, help= 'Number of images in the validation set.') parser.add_argument('--restore-from', type=str, default=RESTORE_FROM, help='Where restore model parameters from.') return parser.parse_args() 
pytorch_pretrained_bert.modeling_transfo_xl.MultiHeadAttn.forward def forward(self, h, attn_mask=None, mems=None): if mems is not None: c = torch.cat([mems, h], 0) else: c = h if self.pre_lnorm: c = self.layer_norm(c) head_q = self.q_net(h) head_k, head_v = torch.chunk(self.kv_net(c), 2, -1) head_q = head_q.view(h.size(0), h.size(1), self.n_head, self.d_head) head_k = head_k.view(c.size(0), c.size(1), self.n_head, self.d_head) head_v = head_v.view(c.size(0), c.size(1), self.n_head, self.d_head) attn_score = torch.einsum('ibnd,jbnd->ijbn', (head_q, head_k)) attn_score.mul_(self.scale) if attn_mask is not None and attn_mask.any().item(): if attn_mask.dim() == 2: attn_score.masked_fill_(attn_mask[(None), :, :, (None)], -float ('inf')) elif attn_mask.dim() == 3: attn_score.masked_fill_(attn_mask[:, :, :, (None)], -float('inf')) attn_prob = F.softmax(attn_score, dim=1) attn_prob = self.dropatt(attn_prob) attn_vec = torch.einsum('ijbn,jbnd->ibnd', (attn_prob, head_v)) attn_vec = attn_vec.contiguous().view(attn_vec.size(0), attn_vec.size(1 ), self.n_head * self.d_head) attn_out = self.o_net(attn_vec) attn_out = self.drop(attn_out) if self.pre_lnorm: output = h + attn_out else: output = self.layer_norm(h + attn_out) return output 
SRGANs-Spectral-Regularization-GANs--master.updater.dis|dcgan|loss def loss_dcgan_dis(dis_fake, dis_real): L1 = F.mean(F.softplus(-dis_real)) L2 = F.mean(F.softplus(dis_fake)) loss = L1 + L2 return loss 
darkflow.dark.darknet.Darknet.Darknet def __init__(self, FLAGS): self.get_weight_src(FLAGS) self.modify = False print('Parsing {}'.format(self.src_cfg)) src_parsed = self.parse_cfg(self.src_cfg, FLAGS) self.src_meta, self.src_layers = src_parsed if self.src_cfg == FLAGS.model: self.meta, self.layers = src_parsed else: print('Parsing {}'.format(FLAGS.model)) des_parsed = self.parse_cfg(FLAGS.model, FLAGS) self.meta, self.layers = des_parsed self.load_weights() 
map_embeddings.main def main(): parser = argparse.ArgumentParser(description= 'Map word embeddings in two languages into a shared space') parser.add_argument('src_input', help='the input source embeddings') parser.add_argument('trg_input', help='the input target embeddings') parser.add_argument('src_output', help='the output source embeddings') parser.add_argument('trg_output', help='the output target embeddings') parser.add_argument('--encoding', default='utf-8', help= 'the character encoding for input/output (defaults to utf-8)') parser.add_argument('--precision', choices=['fp16', 'fp32', 'fp64'], default='fp32', help='the floating-point precision (defaults to fp32)') parser.add_argument('--cuda', action='store_true', help= 'use cuda (requires cupy)') parser.add_argument('--batch_size', default=1024, type=int, help= 'batch size (defaults to 10000); does not affect results, larger is usually faster but uses more memory' ) parser.add_argument('--seed', type=int, default=0, help= 'the random seed (defaults to 0)') recommended_group = parser.add_argument_group('recommended settings', 'Recommended settings for different scenarios') recommended_type = recommended_group.add_mutually_exclusive_group() recommended_type.add_argument('--supervised', metavar='DICTIONARY', help='recommended if you have a large training dictionary') recommended_type.add_argument('--semi_supervised', metavar='DICTIONARY', help='recommended if you have a small seed dictionary') recommended_type.add_argument('--identical', action='store_true', help= 'recommended if you have no seed dictionary but can rely on identical words' ) recommended_type.add_argument('--unsupervised', action='store_true', help= 'recommended if you have no seed dictionary and do not want to rely on identical words' ) recommended_type.add_argument('--acl2018', action='store_true', help= 'reproduce our ACL 2018 system') recommended_type.add_argument('--aaai2018', metavar='DICTIONARY', help= 'reproduce our AAAI 2018 system') recommended_type.add_argument('--acl2017', action='store_true', help= 'reproduce our ACL 2017 system with numeral initialization') recommended_type.add_argument('--acl2017_seed', metavar='DICTIONARY', help='reproduce our ACL 2017 system with a seed dictionary') recommended_type.add_argument('--emnlp2016', metavar='DICTIONARY', help ='reproduce our EMNLP 2016 system') init_group = parser.add_argument_group('advanced initialization arguments', 'Advanced initialization arguments') init_type = init_group.add_mutually_exclusive_group() init_type.add_argument('-d', '--init_dictionary', default=sys.stdin. fileno(), metavar='DICTIONARY', help= 'the training dictionary file (defaults to stdin)') init_type.add_argument('--init_identical', action='store_true', help= 'use identical words as the seed dictionary') init_type.add_argument('--init_numerals', action='store_true', help= 'use latin numerals (i.e. words matching [0-9]+) as the seed dictionary' ) init_type.add_argument('--init_unsupervised', action='store_true', help ='use unsupervised initialization') init_group.add_argument('--unsupervised_vocab', type=int, default=0, help= 'restrict the vocabulary to the top k entries for unsupervised initialization' ) mapping_group = parser.add_argument_group('advanced mapping arguments', 'Advanced embedding mapping arguments') mapping_group.add_argument('--normalize', choices=['unit', 'center', 'unitdim', 'centeremb', 'none'], nargs='*', default=[], help= 'the normalization actions to perform in order') mapping_group.add_argument('--whiten', action='store_true', help= 'whiten the embeddings') mapping_group.add_argument('--src_reweight', type=float, default=0, nargs='?', const=1, help='re-weight the source language embeddings') mapping_group.add_argument('--trg_reweight', type=float, default=0, nargs='?', const=1, help='re-weight the target language embeddings') mapping_group.add_argument('--src_dewhiten', choices=['src', 'trg'], help='de-whiten the source language embeddings') mapping_group.add_argument('--trg_dewhiten', choices=['src', 'trg'], help='de-whiten the target language embeddings') mapping_group.add_argument('--dim_reduction', type=int, default=0, help ='apply dimensionality reduction') mapping_type = mapping_group.add_mutually_exclusive_group() mapping_type.add_argument('-c', '--orthogonal', action='store_true', help='use orthogonal constrained mapping') mapping_type.add_argument('-u', '--unconstrained', action='store_true', help='use unconstrained mapping') self_learning_group = parser.add_argument_group( 'advanced self-learning arguments', 'Advanced arguments for self-learning') self_learning_group.add_argument('--self_learning', action='store_true', help='enable self-learning') self_learning_group.add_argument('--vocabulary_cutoff', type=int, default=0, help='restrict the vocabulary to the top k entries') self_learning_group.add_argument('--direction', choices=['forward', 'backward', 'union'], default='union', help= 'the direction for dictionary induction (defaults to union)') self_learning_group.add_argument('--csls', type=int, nargs='?', default =0, const=10, metavar='NEIGHBORHOOD_SIZE', dest='csls_neighborhood', help='use CSLS for dictionary induction') self_learning_group.add_argument('--threshold', default=1e-06, type= float, help='the convergence threshold (defaults to 0.000001)') self_learning_group.add_argument('--validation', default=None, metavar= 'DICTIONARY', help='a dictionary file for validation at each iteration' ) self_learning_group.add_argument('--stochastic_initial', default=0.1, type=float, help= 'initial keep probability stochastic dictionary induction (defaults to 0.1)' ) self_learning_group.add_argument('--stochastic_multiplier', default=2.0, type=float, help= 'stochastic dictionary induction multiplier (defaults to 2.0)') self_learning_group.add_argument('--stochastic_interval', default=50, type=int, help= 'stochastic dictionary induction interval (defaults to 50)') self_learning_group.add_argument('--log', help= 'write to a log file in tsv format at each iteration') self_learning_group.add_argument('-v', '--verbose', action='store_true', help='write log information to stderr at each iteration') args = parser.parse_args() if args.supervised is not None: parser.set_defaults(init_dictionary=args.supervised, normalize=[ 'unit', 'center', 'unit'], whiten=True, src_reweight=0.5, trg_reweight=0.5, src_dewhiten='src', trg_dewhiten='trg', batch_size=1000) if args.semi_supervised is not None: parser.set_defaults(init_dictionary=args.semi_supervised, normalize =['unit', 'center', 'unit'], whiten=True, src_reweight=0.5, trg_reweight=0.5, src_dewhiten='src', trg_dewhiten='trg', self_learning=True, vocabulary_cutoff=20000, csls_neighborhood=10) if args.identical: parser.set_defaults(init_identical=True, normalize=['unit', 'center', 'unit'], whiten=True, src_reweight=0.5, trg_reweight= 0.5, src_dewhiten='src', trg_dewhiten='trg', self_learning=True, vocabulary_cutoff=20000, csls_neighborhood=10) if args.unsupervised or args.acl2018: parser.set_defaults(init_unsupervised=True, unsupervised_vocab=4000, normalize=['unit', 'center', 'unit'], whiten=True, src_reweight =0.5, trg_reweight=0.5, src_dewhiten='src', trg_dewhiten='trg', self_learning=True, vocabulary_cutoff=20000, csls_neighborhood=10) if args.aaai2018: parser.set_defaults(init_dictionary=args.aaai2018, normalize=[ 'unit', 'center'], whiten=True, trg_reweight=1, src_dewhiten= 'src', trg_dewhiten='trg', batch_size=1000) if args.acl2017: parser.set_defaults(init_numerals=True, orthogonal=True, normalize= ['unit', 'center'], self_learning=True, direction='forward', stochastic_initial=1.0, stochastic_interval=1, batch_size=1000) if args.acl2017_seed: parser.set_defaults(init_dictionary=args.acl2017_seed, orthogonal= True, normalize=['unit', 'center'], self_learning=True, direction='forward', stochastic_initial=1.0, stochastic_interval=1, batch_size=1000) if args.emnlp2016: parser.set_defaults(init_dictionary=args.emnlp2016, orthogonal=True, normalize=['unit', 'center'], batch_size=1000) args = parser.parse_args() if (args.src_dewhiten is not None or args.trg_dewhiten is not None ) and not args.whiten: print('ERROR: De-whitening requires whitening first', file=sys.stderr) sys.exit(-1) if args.precision == 'fp16': dtype = 'float16' elif args.precision == 'fp32': dtype = 'float32' elif args.precision == 'fp64': dtype = 'float64' srcfile = open(args.src_input, encoding=args.encoding, errors= 'surrogateescape') trgfile = open(args.trg_input, encoding=args.encoding, errors= 'surrogateescape') src_words, x = embeddings.read(srcfile, dtype=dtype) trg_words, z = embeddings.read(trgfile, dtype=dtype) if args.cuda: if not supports_cupy(): print('ERROR: Install CuPy for CUDA support', file=sys.stderr) sys.exit(-1) xp = get_cupy() x = xp.asarray(x) z = xp.asarray(z) else: xp = np xp.random.seed(args.seed) src_word2ind = {word: i for i, word in enumerate(src_words)} trg_word2ind = {word: i for i, word in enumerate(trg_words)} embeddings.normalize(x, args.normalize) embeddings.normalize(z, args.normalize) src_indices = [] trg_indices = [] if args.init_unsupervised: sim_size = min(x.shape[0], z.shape[0] ) if args.unsupervised_vocab <= 0 else min(x.shape[0], z.shape[ 0], args.unsupervised_vocab) u, s, vt = xp.linalg.svd(x[:sim_size], full_matrices=False) xsim = (u * s).dot(u.T) u, s, vt = xp.linalg.svd(z[:sim_size], full_matrices=False) zsim = (u * s).dot(u.T) del u, s, vt xsim.sort(axis=1) zsim.sort(axis=1) embeddings.normalize(xsim, args.normalize) embeddings.normalize(zsim, args.normalize) sim = xsim.dot(zsim.T) if args.csls_neighborhood > 0: knn_sim_fwd = topk_mean(sim, k=args.csls_neighborhood) knn_sim_bwd = topk_mean(sim.T, k=args.csls_neighborhood) sim -= knn_sim_fwd[:, (xp.newaxis)] / 2 + knn_sim_bwd / 2 if args.direction == 'forward': src_indices = xp.arange(sim_size) trg_indices = sim.argmax(axis=1) elif args.direction == 'backward': src_indices = sim.argmax(axis=0) trg_indices = xp.arange(sim_size) elif args.direction == 'union': src_indices = xp.concatenate((xp.arange(sim_size), sim.argmax( axis=0))) trg_indices = xp.concatenate((sim.argmax(axis=1), xp.arange( sim_size))) del xsim, zsim, sim elif args.init_numerals: numeral_regex = re.compile('^[0-9]+$') src_numerals = {word for word in src_words if numeral_regex.match( word) is not None} trg_numerals = {word for word in trg_words if numeral_regex.match( word) is not None} numerals = src_numerals.intersection(trg_numerals) for word in numerals: src_indices.append(src_word2ind[word]) trg_indices.append(trg_word2ind[word]) elif args.init_identical: identical = set(src_words).intersection(set(trg_words)) for word in identical: src_indices.append(src_word2ind[word]) trg_indices.append(trg_word2ind[word]) else: f = open(args.init_dictionary, encoding=args.encoding, errors= 'surrogateescape') for line in f: src, trg = line.split() try: src_ind = src_word2ind[src] trg_ind = trg_word2ind[trg] src_indices.append(src_ind) trg_indices.append(trg_ind) except KeyError: print('WARNING: OOV dictionary entry ({0} - {1})'.format( src, trg), file=sys.stderr) if args.validation is not None: f = open(args.validation, encoding=args.encoding, errors= 'surrogateescape') validation = collections.defaultdict(set) oov = set() vocab = set() for line in f: src, trg = line.split() try: src_ind = src_word2ind[src] trg_ind = trg_word2ind[trg] validation[src_ind].add(trg_ind) vocab.add(src) except KeyError: oov.add(src) oov -= vocab validation_coverage = len(validation) / (len(validation) + len(oov)) if args.log: log = open(args.log, mode='w', encoding=args.encoding, errors= 'surrogateescape') xw = xp.empty_like(x) zw = xp.empty_like(z) src_size = x.shape[0] if args.vocabulary_cutoff <= 0 else min(x.shape[0 ], args.vocabulary_cutoff) trg_size = z.shape[0] if args.vocabulary_cutoff <= 0 else min(z.shape[0 ], args.vocabulary_cutoff) simfwd = xp.empty((args.batch_size, trg_size), dtype=dtype) simbwd = xp.empty((args.batch_size, src_size), dtype=dtype) if args.validation is not None: simval = xp.empty((len(validation.keys()), z.shape[0]), dtype=dtype) best_sim_forward = xp.full(src_size, -100, dtype=dtype) src_indices_forward = xp.arange(src_size) trg_indices_forward = xp.zeros(src_size, dtype=int) best_sim_backward = xp.full(trg_size, -100, dtype=dtype) src_indices_backward = xp.zeros(trg_size, dtype=int) trg_indices_backward = xp.arange(trg_size) knn_sim_fwd = xp.zeros(src_size, dtype=dtype) knn_sim_bwd = xp.zeros(trg_size, dtype=dtype) best_objective = objective = -100.0 it = 1 last_improvement = 0 keep_prob = args.stochastic_initial t = time.time() end = not args.self_learning while True: if it - last_improvement > args.stochastic_interval: if keep_prob >= 1.0: end = True keep_prob = min(1.0, args.stochastic_multiplier * keep_prob) last_improvement = it if args.orthogonal or not end: u, s, vt = xp.linalg.svd(z[trg_indices].T.dot(x[src_indices])) w = vt.T.dot(u.T) x.dot(w, out=xw) zw[:] = z elif args.unconstrained: x_pseudoinv = xp.linalg.inv(x[src_indices].T.dot(x[src_indices]) ).dot(x[src_indices].T) w = x_pseudoinv.dot(z[trg_indices]) x.dot(w, out=xw) zw[:] = z else: xw[:] = x zw[:] = z  def whitening_transformation(m): u, s, vt = xp.linalg.svd(m, full_matrices=False) return vt.T.dot(xp.diag(1 / s)).dot(vt) if args.whiten: wx1 = whitening_transformation(xw[src_indices]) wz1 = whitening_transformation(zw[trg_indices]) xw = xw.dot(wx1) zw = zw.dot(wz1) wx2, s, wz2_t = xp.linalg.svd(xw[src_indices].T.dot(zw[ trg_indices])) wz2 = wz2_t.T xw = xw.dot(wx2) zw = zw.dot(wz2) xw *= s ** args.src_reweight zw *= s ** args.trg_reweight if args.src_dewhiten == 'src': xw = xw.dot(wx2.T.dot(xp.linalg.inv(wx1)).dot(wx2)) elif args.src_dewhiten == 'trg': xw = xw.dot(wz2.T.dot(xp.linalg.inv(wz1)).dot(wz2)) if args.trg_dewhiten == 'src': zw = zw.dot(wx2.T.dot(xp.linalg.inv(wx1)).dot(wx2)) elif args.trg_dewhiten == 'trg': zw = zw.dot(wz2.T.dot(xp.linalg.inv(wz1)).dot(wz2)) if args.dim_reduction > 0: xw = xw[:, :args.dim_reduction] zw = zw[:, :args.dim_reduction] if end: break else: if args.direction in ('forward', 'union'): if args.csls_neighborhood > 0: for i in range(0, trg_size, simbwd.shape[0]): j = min(i + simbwd.shape[0], trg_size) zw[i:j].dot(xw[:src_size].T, out=simbwd[:j - i]) knn_sim_bwd[i:j] = topk_mean(simbwd[:j - i], k=args .csls_neighborhood, inplace=True) for i in range(0, src_size, simfwd.shape[0]): j = min(i + simfwd.shape[0], src_size) xw[i:j].dot(zw[:trg_size].T, out=simfwd[:j - i]) simfwd[:j - i].max(axis=1, out=best_sim_forward[i:j]) simfwd[:j - i] -= knn_sim_bwd / 2 dropout(simfwd[:j - i], 1 - keep_prob).argmax(axis=1, out=trg_indices_forward[i:j]) if args.direction in ('backward', 'union'): if args.csls_neighborhood > 0: for i in range(0, src_size, simfwd.shape[0]): j = min(i + simfwd.shape[0], src_size) xw[i:j].dot(zw[:trg_size].T, out=simfwd[:j - i]) knn_sim_fwd[i:j] = topk_mean(simfwd[:j - i], k=args .csls_neighborhood, inplace=True) for i in range(0, trg_size, simbwd.shape[0]): j = min(i + simbwd.shape[0], trg_size) zw[i:j].dot(xw[:src_size].T, out=simbwd[:j - i]) simbwd[:j - i].max(axis=1, out=best_sim_backward[i:j]) simbwd[:j - i] -= knn_sim_fwd / 2 dropout(simbwd[:j - i], 1 - keep_prob).argmax(axis=1, out=src_indices_backward[i:j]) if args.direction == 'forward': src_indices = src_indices_forward trg_indices = trg_indices_forward elif args.direction == 'backward': src_indices = src_indices_backward trg_indices = trg_indices_backward elif args.direction == 'union': src_indices = xp.concatenate((src_indices_forward, src_indices_backward)) trg_indices = xp.concatenate((trg_indices_forward, trg_indices_backward)) if args.direction == 'forward': objective = xp.mean(best_sim_forward).tolist() elif args.direction == 'backward': objective = xp.mean(best_sim_backward).tolist() elif args.direction == 'union': objective = (xp.mean(best_sim_forward) + xp.mean( best_sim_backward)).tolist() / 2 if objective - best_objective >= args.threshold: last_improvement = it best_objective = objective if args.validation is not None: src = list(validation.keys()) xw[src].dot(zw.T, out=simval) nn = asnumpy(simval.argmax(axis=1)) accuracy = np.mean([(1 if nn[i] in validation[src[i]] else 0) for i in range(len(src))]) similarity = np.mean([max([simval[i, j].tolist() for j in validation[src[i]]]) for i in range(len(src))]) duration = time.time() - t if args.verbose: print(file=sys.stderr) print('ITERATION {0} ({1:.2f}s)'.format(it, duration), file =sys.stderr) print('\t- Objective:        {0:9.4f}%'.format(100 * objective), file=sys.stderr) print('\t- Drop probability: {0:9.4f}%'.format(100 - 100 * keep_prob), file=sys.stderr) if args.validation is not None: print('\t- Val. similarity:  {0:9.4f}%'.format(100 * similarity), file=sys.stderr) print('\t- Val. accuracy:    {0:9.4f}%'.format(100 * accuracy), file=sys.stderr) print('\t- Val. coverage:    {0:9.4f}%'.format(100 * validation_coverage), file=sys.stderr) sys.stderr.flush() if args.log is not None: val = '{0:.6f}\t{1:.6f}\t{2:.6f}'.format(100 * similarity, 100 * accuracy, 100 * validation_coverage ) if args.validation is not None else '' print('{0}\t{1:.6f}\t{2}\t{3:.6f}'.format(it, 100 * objective, val, duration), file=log) log.flush() t = time.time() it += 1 srcfile = open(args.src_output, mode='w', encoding=args.encoding, errors='surrogateescape') trgfile = open(args.trg_output, mode='w', encoding=args.encoding, errors='surrogateescape') embeddings.write(src_words, xw, srcfile) embeddings.write(trg_words, zw, trgfile) srcfile.close() trgfile.close() 
generate-argmaxpool-test.ukernel|split|name def split_ukernel_name(name): match = re.match( '^xnn_(f16|f32)_argmaxpool_ukernel_((\\d+)p)?(\\d+)x__(.+)_c(\\d+)$', name) if match is None: raise ValueError('Unexpected microkernel name: ' + name) if match.group(2): primary_tile = int(match.group(3)) incremental_tile = int(match.group(4)) else: primary_tile = int(match.group(4)) incremental_tile = 0 channel_tile = int(match.group(6)) arch, isa = xnncommon.parse_target_name(target_name=match.group(5)) return primary_tile, incremental_tile, channel_tile, arch, isa 
model.PyramidROIAlign.Align|ROI|Pyramid def __init__(self, pool_shape, image_shape, **kwargs): super(PyramidROIAlign, self).__init__(**kwargs) self.pool_shape = tuple(pool_shape) self.image_shape = tuple(image_shape) 
seq2seq_data2text.Seq2seqData2text.train|step def train_step(self, sess, batch_dict, ei): """One step training""" if self.learning_rate_decay: lr_decay = float((ei + 3) // 3) lr = self.init_learning_rate / lr_decay else: lr = self.init_learning_rate lambda_kl = self.lambda_kl_config feed_dict = {self.enc_keys: batch_dict['enc_keys'], self.enc_vals: batch_dict['enc_vals'], self.enc_locs: batch_dict['enc_locs'], self .enc_lens: batch_dict['enc_lens'], self.dec_inputs: batch_dict[ 'dec_inputs'], self.dec_targets: batch_dict['dec_targets'], self. dec_lens: batch_dict['dec_lens'], self.drop_out: batch_dict[ 'drop_out'], self.learning_rate: lr, self.lambda_kl: lambda_kl} output_dict = sess.run(self.train_output, feed_dict=feed_dict) return output_dict 
tfprocess.TFProcess.conv|block def conv_block(self, inputs, filter_size, input_channels, output_channels, name ): W_conv = weight_variable(name, [filter_size, filter_size, input_channels, output_channels], self.model_dtype) self.add_weights(W_conv) net = inputs net = conv2d(net, W_conv) net = self.batch_norm(net) net = tf.nn.relu(net) return net 
commons.utils.train def train(hparams, phs, d_update_op, g_update_op, d_loss, g_loss, x_sample, x_lossy, real_val_iterator, theta_ph, theta_gen_ph, mdevice, iter_ph, inf_def): sess = tf.Session() scalar_summaries = [] scalar_summaries.append(tf.summary.scalar('d_loss', d_loss)) scalar_summaries.append(tf.summary.scalar('g_loss', g_loss)) scalar_summary = tf.summary.merge(scalar_summaries) summary_writer = tf.summary.FileWriter(hparams.summary_dir) model_saver = tf.train.Saver(max_to_keep=hparams.max_checkpoints) init_op = tf.global_variables_initializer() sess.run(init_op) init_train_iter = basic_utils.try_restore(hparams, sess, model_saver) num_batches = hparams.train_size // hparams.batch_size if hparams.dataset == 'mnist': inf_net = inf_def.InferenceNetwork() for train_iter in range(init_train_iter + 1, hparams.max_train_iter): if train_iter % 500 == 0: save_path = os.path.join(hparams.ckpt_dir, 'snapshot') model_saver.save(sess, save_path, global_step=train_iter) print('Saved model at iteration {}'.format(train_iter)) real_vals = real_val_iterator.next(hparams) theta_val = mdevice.sample_theta(hparams) theta_gen_val = mdevice.sample_theta(hparams) z_val = sample_z_val(hparams) x_measured_val = None if hparams.train_mode == 'unmeasure': x_real_val = real_vals[0] x_measured_val = mdevice.measure_np(hparams, x_real_val, theta_val) x_lossy_val = mdevice.unmeasure_np(hparams, x_measured_val, theta_val) real_vals[0] = x_lossy_val vals = [z_val] + real_vals feed_dict = {ph: val for ph, val in zip(phs, vals)} feed_dict[theta_ph] = theta_val feed_dict[theta_gen_ph] = theta_gen_val feed_dict[iter_ph] = train_iter feed_dict.pop(None, None) for _ in range(hparams.d_iters): sess.run(d_update_op, feed_dict=feed_dict) for _ in range(hparams.g_iters): sess.run(g_update_op, feed_dict=feed_dict) epoch = train_iter // num_batches scalar_summary_str = sess.run(scalar_summary, feed_dict=feed_dict) batch_num = train_iter % num_batches print('Epoch: [{}] [{}/{}], [{}/{}]'.format(epoch, batch_num, num_batches, train_iter, hparams.max_train_iter)) summary_writer.add_summary(scalar_summary_str, train_iter) if batch_num % 100 == 1: x_sample_val, x_lossy_val = get_samples(hparams, phs, theta_ph, x_sample, x_lossy, mdevice, sess, real_vals) save_samples(hparams, phs, epoch, batch_num, x_sample_val, x_lossy_val, x_measured_val) if hparams.dataset == 'mnist': save_inception_data(hparams, phs, theta_ph, x_sample, x_lossy, mdevice, sess, real_vals, inf_net, x_sample_val) save_path = os.path.join(hparams.ckpt_dir, 'snapshot') model_saver.save(sess, save_path, global_step=hparams.max_train_iter - 1) print('Saved model at iteration {}'.format(hparams.max_train_iter - 1)) train_iter = hparams.max_train_iter - 1 epoch = train_iter // num_batches batch_num = train_iter % num_batches real_vals = real_val_iterator.next(hparams) theta_val = mdevice.sample_theta(hparams) x_measured_val = None if hparams.train_mode == 'unmeasure': x_real_val = real_vals[0] x_measured_val = mdevice.measure_np(hparams, x_real_val, theta_val) x_lossy_val = mdevice.unmeasure_np(hparams, x_measured_val, theta_val) real_vals[0] = x_lossy_val x_sample_val, x_lossy_val = get_samples(hparams, phs, theta_ph, x_sample, x_lossy, mdevice, sess, real_vals) save_samples(hparams, phs, epoch, batch_num, x_sample_val, x_lossy_val, x_measured_val) if hparams.dataset == 'mnist': save_inception_data(hparams, phs, theta_ph, x_sample, x_lossy, mdevice, sess, real_vals, inf_net, x_sample_val) return sess 
svae_dc.svae_dc.SVAE_DC.get|distr|latent def get_latent_distr(self, x): return self.p_tau_gvn_x(x) 
ML_MonteCarloUQ.make|equal|length def make_equal_length(a, new_length): old_indices = np.arange(0, len(a)) new_indices = np.linspace(0, len(a) - 1, new_length) spl = UnivariateSpline(old_indices, a, k=1, s=0) new_array = spl(new_indices) return new_array 
model.PCGN_model.PCGNModel.embedding|init def init_embedding(self, vocab_size, embed_size, dtype=tf.float32, initializer=None, initial_values=None): """ embeddings: initialize trainable embeddings or load pretrained from files """ if initial_values: embedding = tf.Variable(initial_value=initial_values, name= 'embedding', dtype=dtype) else: if initializer is None: initializer = tf.contrib.layers.xavier_initializer() embedding = tf.Variable(initializer(shape=(vocab_size, embed_size)), name='embedding', dtype=dtype) return embedding 
deepctr.models.fnn.FNN def FNN(feature_dim_dict, embedding_size=8, dnn_hidden_units=(128, 128), l2_reg_embedding=1e-05, l2_reg_linear=1e-05, l2_reg_dnn=0, init_std= 0.0001, seed=1024, dnn_dropout=0, dnn_activation='relu', task='binary'): """Instantiates the Factorization-supported Neural Network architecture.  :param feature_dim_dict: dict,to indicate sparse field and dense field like {'sparse':{'field_1':4,'field_2':3,'field_3':2},'dense':['field_4','field_5']} :param embedding_size: positive integer,sparse feature embedding_size :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of deep net :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector :param l2_reg_linear: float. L2 regularizer strength applied to linear weight :param l2_reg_dnn: float . L2 regularizer strength applied to DNN :param init_std: float,to use as the initialize std of embedding vector :param seed: integer ,to use as random seed. :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate. :param dnn_activation: Activation function to use in DNN :param task: str, ``"binary"`` for  binary logloss or  ``"regression"`` for regression loss :return: A Keras model instance. """ check_feature_config_dict(feature_dim_dict) deep_emb_list, linear_emb_list, dense_input_dict, inputs_list = ( preprocess_input_embedding(feature_dim_dict, embedding_size, l2_reg_embedding, l2_reg_linear, init_std, seed, create_linear_weight=True)) linear_logit = get_linear_logit(linear_emb_list, dense_input_dict, l2_reg_linear) deep_input = tf.keras.layers.Flatten()(concat_fun(deep_emb_list)) deep_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, False, seed)(deep_input) deep_logit = tf.keras.layers.Dense(1, use_bias=False, activation=None)( deep_out) final_logit = tf.keras.layers.add([deep_logit, linear_logit]) output = PredictionLayer(task)(final_logit) model = tf.keras.models.Model(inputs=inputs_list, outputs=output) return model 
utils.ElapsedTimer.elapsed def elapsed(self, sec): if sec < 0: sec = '%0.2f' % (sec * 100) return sec + ' msec' elif sec < 60: sec = '%0.4f' % sec return sec + ' sec' elif sec < 60 * 60: sec = '%0.4f' % (sec / 60) return sec + ' min' else: sec = '%0.4f' % (sec / (60 * 60)) return sec + ' hr' 
average_weights.swa def swa(inputs, output, weights=None): """ Average weights of the weight files.  inputs : List of filenames to use as inputs output : String of output filename weights : List of numbers to use for weighting the inputs """ out_weights = [] if weights == None: weights = [1.0] * len(inputs) if len(weights) != len(inputs): raise ValueError( "Number of weights doesn't match number of input files") weights = [(float(w) / sum(weights)) for w in weights] for count, filename in enumerate(inputs): with open(filename, 'r') as f: weights_in = [] for line in f: weights_in.append(weights[count] * np.array(list(map(float, line.split(' '))))) if count == 0: out_weights = weights_in else: if len(out_weights) != len(weights_in): raise ValueError('Nets have different sizes') for e, w in enumerate(weights_in): if len(w) != len(out_weights[e]): raise ValueError('Nets have different sizes') out_weights[e] += w with open(output, 'w') as f: for e, w in enumerate(out_weights): if e == 0: f.write('1\n') else: f.write(' '.join(map(str, w)) + '\n') 
neural_tangents.predict.momentum.fn|init def init_fn(fx_train=0.0, fx_test=0.0): train_size = fx_train.shape[0] fx_train, fx_test = fl(fx_train), fl(fx_test) qx_train = np.zeros_like(fx_train) qx_test = np.zeros_like(fx_test) return 2 * train_size * output_dimension, np.concatenate((fx_train, qx_train, fx_test, qx_test), axis=0) 
evaluate_shapenet_onehot.augment|fn def augment_fn2(batch_xyz): augment_xyz = batch_xyz augment_xyz = data_util.rotate_perturbation_point_cloud(augment_xyz) augment_xyz = data_util.random_scale_point_cloud(augment_xyz) augment_xyz = data_util.shift_point_cloud(augment_xyz) augment_xyz = data_util.jitter_point_cloud(augment_xyz) batch_xyz = augment_xyz return batch_xyz 
layers.rlngru|layer def rlngru_layer(tparams, state_below, init_state, options, prefix='rlngru', mask=None, one_step=False, **kwargs): """ Feedforward pass through GRU with RMS-LN """ nsteps = state_below.shape[0] if state_below.ndim == 3: n_samples = state_below.shape[1] else: n_samples = 1 dim = tparams[_p(prefix, 'Ux')].shape[1] if init_state == None: init_state = tensor.alloc(0.0, n_samples, dim) if mask == None: mask = tensor.alloc(1.0, state_below.shape[0], 1)  def _slice(_x, n, dim): if _x.ndim == 3: return _x[:, :, n * dim:(n + 1) * dim] return _x[:, n * dim:(n + 1) * dim] state_below_ = tensor.dot(state_below, tparams[_p(prefix, 'W')]) + tparams[ _p(prefix, 'b')] state_belowx = tensor.dot(state_below, tparams[_p(prefix, 'Wx')] ) + tparams[_p(prefix, 'bx')] U = tparams[_p(prefix, 'U')] Ux = tparams[_p(prefix, 'Ux')]  def _step_slice(m_, x_, xx_, h_, U, Ux, s1, s2, s3, s4): x_ = rln(x_, s1) xx_ = rln(xx_, s2) preact = tensor.dot(h_, U) preact = rln(preact, s3) preact += x_ r = tensor.nnet.sigmoid(_slice(preact, 0, dim)) u = tensor.nnet.sigmoid(_slice(preact, 1, dim)) preactx = tensor.dot(h_, Ux) preactx = rln(preactx, s4) preactx = preactx * r preactx = preactx + xx_ h = tensor.tanh(preactx) h = u * h_ + (1.0 - u) * h h = m_[:, (None)] * h + (1.0 - m_)[:, (None)] * h_ return h seqs = [mask, state_below_, state_belowx] _step = _step_slice non_seqs = [tparams[_p(prefix, 'U')], tparams[_p(prefix, 'Ux')]] non_seqs += [tparams[_p(prefix, 's1')], tparams[_p(prefix, 's2')], tparams[_p(prefix, 's3')], tparams[_p(prefix, 's4')]] if one_step: rval = _step(*(seqs + [init_state, tparams[_p(prefix, 'U')], tparams[_p(prefix, 'Ux')]])) else: rval, updates = theano.scan(_step, sequences=seqs, outputs_info=[ init_state], non_sequences=non_seqs, name=_p(prefix, '_layers'), n_steps=nsteps, profile=False, strict=True) rval = [rval] return rval 
official.resnet.resnet_run_loop.resnet|define|flags def define_resnet_flags(resnet_size_choices=None): """Add flags and validators for ResNet.""" flags_core.define_base() flags_core.define_performance(num_parallel_calls=False, tf_gpu_thread_mode=True, datasets_num_private_threads=True, datasets_num_parallel_batches=True) flags_core.define_image() flags_core.define_benchmark() flags.adopt_module_key_flags(flags_core) flags.DEFINE_enum(name='resnet_version', short_name='rv', default='1', enum_values=['1', '2'], help=flags_core.help_wrap( 'Version of ResNet. (1 or 2) See README.md for details.')) flags.DEFINE_bool(name='fine_tune', short_name='ft', default=False, help=flags_core.help_wrap( 'If True do not train any parameters except for the final layer.')) flags.DEFINE_string(name='pretrained_model_checkpoint_path', short_name ='pmcp', default=None, help=flags_core.help_wrap( 'If not None initialize all the network except the final layer with these values' )) flags.DEFINE_boolean(name='eval_only', default=False, help=flags_core. help_wrap( 'Skip training and only perform evaluation on the latest checkpoint.')) flags.DEFINE_boolean(name='image_bytes_as_serving_input', default=False, help=flags_core.help_wrap( 'If True exports savedmodel with serving signature that accepts JPEG image bytes instead of a fixed size [HxWxC] tensor that represents the image. The former is easier to use for serving at the expense of image resize/cropping being done as part of model inference. Note, this flag only applies to ImageNet and cannot be used for CIFAR.' )) flags.DEFINE_boolean(name='turn_off_distribution_strategy', default= False, help=flags_core.help_wrap( 'Set to True to not use distribution strategies.')) lottery.add_flags(flags) choice_kwargs = dict(name='resnet_size', short_name='rs', default='50', help=flags_core.help_wrap('The size of the ResNet model to use.')) if resnet_size_choices is None: flags.DEFINE_string(**choice_kwargs) else: flags.DEFINE_enum(enum_values=resnet_size_choices, **choice_kwargs) flags.DEFINE_string('lth_prediction_result_dir', '', 'Prediction result dir.') flags.DEFINE_boolean('lth_generate_predictions', False, 'generate_predictions') flags.DEFINE_boolean('lth_no_pruning', False, 'no pruning') 
cleverhans.devtools.tests.docscrape.NumpyClassDocString.Class|Numpy|String|Doc def __init__(self, docstring, class_name, class_object): super(NumpyClassDocString, self).__init__(docstring) self.class_name = class_name methods = dict((name, func) for name, func in inspect.getmembers( class_object)) self.has_parameters = False if '__init__' in methods: if not inspect.ismethod(methods['__init__']): return args, varargs, keywords, defaults = inspect.getargspec(methods[ '__init__']) if args and args != ['self'] or varargs or keywords or defaults: self.has_parameters = True 
scheduled_sampling_main.model|build def build_model(batch, train_data, self_sampling_proba): """ Assembles the seq2seq model. It is the same as build_model() in baseline_seq2seq_attn.py except using ScheduledEmbeddingTrainingHelper. """ source_embedder = tx.modules.WordEmbedder(vocab_size=train_data. source_vocab.size, hparams=config_model.embedder) encoder = tx.modules.BidirectionalRNNEncoder(hparams=config_model.encoder) enc_outputs, _ = encoder(source_embedder(batch['source_text_ids'])) target_embedder = tx.modules.WordEmbedder(vocab_size=train_data. target_vocab.size, hparams=config_model.embedder) decoder = tx.modules.AttentionRNNDecoder(memory=tf.concat(enc_outputs, axis=2), memory_sequence_length=batch['source_length'], vocab_size= train_data.target_vocab.size, hparams=config_model.decoder) helper = tx.modules.get_helper(helper_type= 'ScheduledEmbeddingTrainingHelper', inputs=target_embedder(batch[ 'target_text_ids'][:, :-1]), sequence_length=batch['target_length'] - 1, embedding=target_embedder, sampling_probability=self_sampling_proba) training_outputs, _, _ = decoder(helper=helper, initial_state=decoder. zero_state(batch_size=tf.shape(batch['target_length'])[0], dtype=tf .float32)) train_op = tx.core.get_train_op(tx.losses. sequence_sparse_softmax_cross_entropy(labels=batch[ 'target_text_ids'][:, 1:], logits=training_outputs.logits, sequence_length=batch['target_length'] - 1), hparams=config_model.opt) start_tokens = tf.ones_like(batch['target_length'] ) * train_data.target_vocab.bos_token_id beam_search_outputs, _, _ = tx.modules.beam_search_decode(decoder_or_cell =decoder, embedding=target_embedder, start_tokens=start_tokens, end_token=train_data.target_vocab.eos_token_id, beam_width= config_model.beam_width, max_decoding_length=60) return train_op, beam_search_outputs 
regression.misc.layers.FeedForward.forward def forward(self, inputs): inputs = tf.concat([inputs, tf.ones(tf.concat([tf.shape(inputs)[:-1], [ 1]], axis=0))], axis=-1) pws = self.pws(tf.shape(inputs)[0]) return tf.matmul(inputs, pws) 
base.Trainer.restore def restore(self): if self.ckpt: load(self.saver, self.sess, self.dirs, self.ckpt) elif self.args.logdir: load(self.saver, self.sess, self.dirs) 
svae_dc.svae_dc_nets.ConvGauss.Gauss|Conv def __init__(self, input_size, in_seq_len, output_size, out_seq_len, hidden_size, pr, n_conv_layers=3): assert n_conv_layers >= 3 and n_conv_layers <= 4 super(ConvGauss, self).__init__() self.out_seq_len = out_seq_len chnls = [hidden_size, int(hidden_size / 2), int(hidden_size / 4)] if n_conv_layers > 3: chnls.append(int(hidden_size / 4)) if in_seq_len > out_seq_len: chnls.reverse() logging.info('Constructed ConvGauss {:d}x{:d}->{:d}x{:d} chnls:'.format (input_size, in_seq_len, output_size, out_seq_len)) logging.info(chnls) self.nn = torch.nn.Sequential() l_in_out = in_seq_len for l in range(n_conv_layers): knl = 4 strd = 2 if out_seq_len > in_seq_len: l_in_out, knl, strd = ConvGauss.l_out(l_in_out, knl, strd, ConvGauss.l_out_deconv) layer = nn.ConvTranspose1d(chnls[l - 1] if l > 0 else input_size, chnls[l], knl, strd) else: l_in_out, knl, strd = ConvGauss.l_out(l_in_out, knl, strd, ConvGauss.l_out_conv) layer = nn.Conv1d(chnls[l - 1] if l > 0 else input_size, chnls[ l], knl, strd) logging.info('l_in_out {:d} knl {:d} strd {:d}'.format(l_in_out, knl, strd)) self.nn.add_module('conv' + str(l), layer) self.nn.add_module('conv_nl' + str(l), pr.nl) if l_in_out > 4: self.nn.add_module('bn' + str(l), nn.BatchNorm1d(chnls[l])) self.mu = nn.Sequential(nn.Linear(l_in_out * chnls[-1], output_size * out_seq_len), pr.mu_nl) self.logvar = nn.Sequential(nn.Linear(l_in_out * chnls[-1], output_size * out_seq_len)) self.logvar_scheduler = pr.logvar_scheduler 
parse_data_from_log.get|input|arguments def get_input_arguments(): parser = argparse.ArgumentParser() parser.add_argument('-m', '--mode', help= 'if 1, run under training mode, if 0 run under test mode', type=int, default=1) args = parser.parse_args() return args 
seq2seq_attn.main.eval|epoch def _eval_epoch(sess, mode): if mode == 'val': data_iterator.switch_to_val_data(sess) else: data_iterator.switch_to_test_data(sess) refs, hypos = [], [] while True: try: fetches = [batch['target_text'][:, 1:], infer_outputs. predicted_ids[:, :, (0)]] feed_dict = {tx.global_mode(): tf.estimator.ModeKeys.EVAL} target_texts_ori, output_ids = sess.run(fetches, feed_dict= feed_dict) target_texts = tx.utils.strip_special_tokens(target_texts_ori, is_token_list=True) output_texts = tx.utils.map_ids_to_strs(ids=output_ids, vocab= val_data.target_vocab) for hypo, ref in zip(output_texts, target_texts): hypos.append(hypo) refs.append([ref]) except tf.errors.OutOfRangeError: break return tx.evals.corpus_bleu_moses(list_of_references=refs, hypotheses=hypos ) 
avod.core.minibatch_samplers.balanced_positive_negative_sampler_test.BalancedPositiveNegativeSamplerTest.incorrect|raises|shape|with|indicator|test|error def test_raises_error_with_incorrect_indicator_shape(self): labels = tf.constant([True, False, False]) indicator = tf.constant([[True, False, True]]) sampler = (balanced_positive_negative_sampler. BalancedPositiveNegativeSampler()) with self.assertRaises(ValueError): sampler.subsample(indicator, 64, labels) 
texar.utils.utils.default|str def default_str(str_, default_str): """Returns :attr:`str_` if it is not `None` or empty, otherwise returns :attr:`default_str`.  Args: str_: A string. default_str: A string.  Returns: Either :attr:`str_` or :attr:`default_str`. """ if str_ is not None and str_ != '': return str_ else: return default_str 
utils_test.PrioritizedReplayTest.test|priorities|initial def test_initial_priorities(self): tf.random.set_seed(5) rb = utils.PrioritizedReplay(size=2, specs=tf.TensorSpec([], tf.int32), importance_sampling_exponent=0.5) rb.insert(tf.constant([1, 2]), tf.constant([0.1, 0.9])) num_sampled = 1000 _, _, sampled_values = rb.sample(num_sampled, 1) counted_values = collections.Counter(sampled_values.numpy()) self.assertGreater(counted_values[1], num_sampled * 0.1 * 0.7) self.assertLess(counted_values[1], num_sampled * 0.1 * 1.3) 
models.densenet.BottleneckBlock.forward def forward(self, x): out = self.conv1(self.relu(self.bn1(x))) if self.droprate > 0: out = F.dropout(out, p=self.droprate, inplace=False, training=self. training) out = self.conv2(self.relu(self.bn2(out))) if self.droprate > 0: out = F.dropout(out, p=self.droprate, inplace=False, training=self. training) return torch.cat([x, out], 1) 
model.PCGN_beamsearch.PCGNBeamSearchDecoder.personality|external|express def external_personality_express(self, decoder_logits_train, keys, blog_desc_inetract, feat_emb=None, use_external_feat_express=False, user_map=None): """ Args: query: Tensor, shape `[batch_size, num_units]` to compare to keys. keys: Processed memory, shape `[batch_size, max_time, num_units]`. scale: Whether to apply a scale to the score function. Returns: A `[batch_size, max_time]` tensor of unnormalized score values.  """ batch_size, num_units = decoder_logits_train.get_shape() t_decoder_logits = decoder_logits_train context = self.get_context(t_decoder_logits, keys, blog_desc_inetract, batch_size, num_units) if use_external_feat_express: context = tf.concat([context, feat_emb], axis=-1) if user_map is not None: context = tf.matmul(context, user_map) cnt = tf.concat([t_decoder_logits, context], axis=-1) return cnt 
common.flop_plot.ax|fmt def fmt_ax(ax, ax2=None): ax.set_xlabel('Compression Ratio') ax.set_ylabel('Speedup') ax.set_xscale('log') if min_max_y: ax.set_ylim((min_max_y[0], min_max_y[1])) if ax2: ax2.set_ylim((0.95, 1.3)) ax.set_xlim((ax.get_xlim()[1], ax.get_xlim()[0])) if dont_plot_x: densities = [all_densities[i] for i in range(len(all_densities)) if i not in dont_plot_x] ax.xaxis.set_major_locator(get_major_locator(all_densities, nbins=nbins)) ax.xaxis.set_minor_locator(matplotlib.ticker.NullLocator()) ax.xaxis.set_major_formatter(get_density_formatter()) ax.yaxis.set_major_locator(matplotlib.ticker.MaxNLocator(nbins=nybins, steps=[1, 2, 2.5, 5, 10])) ax.grid(True) ax.set_ylim(0.1, ax.get_ylim()[1]) ax.yaxis.set_major_formatter(matplotlib.ticker.StrMethodFormatter( '{x:.2f}$\\times$')) fig.set_tight_layout(True) format_axes(ax) 
randsphere.samples|l def l1_samples(m, n): U = np.random.uniform(0, 1.0, (m, n - 1)) V = np.empty(shape=(m, n + 1)) V[:, (0)] = 0.0 V[:, (-1)] = 1.0 V[:, 1:-1] = np.sort(U) X = V[:, 1:] - V[:, :-1] s = randsign(m * n).reshape(m, n) return X * s 
prune_common.get|product|inner def get_inner_product(weights): """ weights: of shape [h, w, input_channel, output_channel] or [input_nodes, output_nodes] return: of shape [input_channle, input_channel] """ if len(weights.shape) == 2: weights = np.reshape(weights, [1, 1] + list(weights.shape)) shape = weights.shape weights = np.reshape(weights, [shape[0] * shape[1], shape[2], shape[3]]) feature = weights feature_t = np.transpose(feature, [0, 2, 1]) sim = np.matmul(feature, feature_t) sim = np.abs(sim) mean_sim = np.mean(sim, axis=0, keepdims=False) return mean_sim 
xlnet-master.tpu_estimator._InputPipeline.InputsStructureRecorder.has|labels def has_labels(self): return 'labels' in self._feature_structure 
nets.inception_v1_test.InceptionV1Test.Unknown|test|Image|Shape def testUnknownImageShape(self): tf.reset_default_graph() batch_size = 2 height, width = 224, 224 num_classes = 1000 input_np = np.random.uniform(0, 1, (batch_size, height, width, 3)) with self.test_session() as sess: inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3)) logits, end_points = inception.inception_v1(inputs, num_classes) self.assertTrue(logits.op.name.startswith('InceptionV1/Logits')) self.assertListEqual(logits.get_shape().as_list(), [batch_size, num_classes]) pre_pool = end_points['Mixed_5c'] feed_dict = {inputs: input_np} tf.global_variables_initializer().run() pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict) self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024] ) 
nmt.attention_utils.BahdanauMonotonicAttention.Bahdanau|Monotonic|Attention def __init__(self, num_units, memory, memory_sequence_length=None, normalize=False, score_mask_value=None, sigmoid_noise=0.0, sigmoid_noise_seed=None, score_bias_init=0.0, mode='parallel', dtype= None, name='BahdanauMonotonicAttention'): """Construct the Attention mechanism.  Args: num_units: The depth of the query mechanism. memory: The memory to query; usually the output of an RNN encoder.  This tensor should be shaped `[batch_size, max_time, ...]`. memory_sequence_length (optional): Sequence lengths for the batch entries in memory.  If provided, the memory tensor rows are masked with zeros for values past the respective sequence lengths. normalize: Python boolean.  Whether to normalize the energy term. score_mask_value: (optional): The mask value for score before passing into `probability_fn`. The default is -inf. Only used if `memory_sequence_length` is not None. sigmoid_noise: Standard deviation of pre-sigmoid noise.  See the docstring for `_monotonic_probability_fn` for more information. sigmoid_noise_seed: (optional) Random seed for pre-sigmoid noise. score_bias_init: Initial value for score bias scalar.  It's recommended to initialize this to a negative value when the length of the memory is large. mode: How to compute the attention distribution.  Must be one of 'recursive', 'parallel', or 'hard'.  See the docstring for `tf.contrib.seq2seq.monotonic_attention` for more information. dtype: The data type for the query and memory layers of the attention mechanism. name: Name to use when creating ops. """ if dtype is None: dtype = dtypes.float32 wrapped_probability_fn = functools.partial(_monotonic_probability_fn, sigmoid_noise=sigmoid_noise, mode=mode, seed=sigmoid_noise_seed) super(BahdanauMonotonicAttention, self).__init__(query_layer= model_helper.Dense(num_units, name='query_layer', use_bias=False, dtype=dtype), memory_layer=model_helper.Dense(num_units, name= 'memory_layer', use_bias=False, dtype=dtype), memory=memory, probability_fn=wrapped_probability_fn, memory_sequence_length= memory_sequence_length, score_mask_value=score_mask_value, name=name) self._num_units = num_units self._normalize = normalize self._name = name self._score_bias_init = score_bias_init 
amb_measure.BlurAddNoise.measure def measure(self, hparams, x, theta_ph): x_blurred = amb_measure_utils.blur(hparams, x) x_measured = tf.add(x_blurred, theta_ph, name='x_measured') return x_measured 
official.resnet.imagenet_preprocessing.image|subtraction|mean def _mean_image_subtraction(image, means, num_channels): """Subtracts the given means from each image channel.  For example: means = [123.68, 116.779, 103.939] image = _mean_image_subtraction(image, means)  Note that the rank of `image` must be known.  Args: image: a tensor of size [height, width, C]. means: a C-vector of values to subtract from each channel. num_channels: number of color channels in the image that will be distorted.  Returns: the centered image.  Raises: ValueError: If the rank of `image` is unknown, if `image` has a rank other than three or if the number of channels in `image` doesn't match the number of values in `means`. """ if image.get_shape().ndims != 3: raise ValueError('Input must be of size [height, width, C>0]') if len(means) != num_channels: raise ValueError('len(means) must match the number of channels') means = tf.expand_dims(tf.expand_dims(means, 0), 0) return image - means 
texar.hyperparams.HParams.str def __str__(self): """Return a string of the hparams. """ hparams_dict = self.todict() return json.dumps(hparams_dict, sort_keys=True, indent=2) 
commons.arch.model|fn|auxcond def model_fn_auxcond(hparams, z, x, y, generator, discriminator, mdevice): theta_ph = mdevice.get_theta_ph(hparams) theta_gen_ph = mdevice.get_theta_ph(hparams) x_gen = generator(hparams, y, z, 'gen', train=True, reuse=False) x_sample = generator(hparams, y, z, 'gen', train=False, reuse=True) x_lossy, x_gen_lossy = get_lossy(hparams, mdevice, x, theta_ph, x_gen, theta_gen_ph) _, d_logit, ac_logits = discriminator(hparams, x_lossy, y, 'discrim', train=True, reuse=False) _, d_gen_logit, ac_logits_gen = discriminator(hparams, x_gen_lossy, y, 'discrim', train=True, reuse=True) d_loss, g_loss = get_loss(hparams, d_logit, d_gen_logit, x_lossy, x_gen_lossy, discriminator, ac_logits, ac_logits_gen, y) d_update_op, g_update_op, iter_ph = utils.get_train_ops(hparams, d_loss, g_loss) return (x_lossy, x_sample, theta_ph, theta_gen_ph, d_loss, g_loss, d_update_op, g_update_op, iter_ph) 
nmt.decoder.dynamic_decode.shape def _shape(batch_size, from_shape): if not isinstance(from_shape, tf.TensorShape) or from_shape.ndims == 0: return tf.TensorShape(None) else: batch_size = tf.contrib.util.constant_value(tf.convert_to_tensor( batch_size, name='batch_size')) return tf.TensorShape([batch_size]).concatenate(from_shape) 
dp_sgd_autoencoder.logs|save def save_logs(train_log, test_log, name): save_dir = 'results/{}'.format(name) if not os.path.exists(save_dir): os.makedirs(save_dir) np.save(os.path.join(save_dir, 'train_log.npy'), train_log) np.save(os.path.join(save_dir, 'test_log.npy'), test_log) print('saved logs for run: {}'.format(name)) 
deepctr.contrib.utils.QAAttGRUCell.call def call(self, inputs, state, att_score=None): """Gated recurrent unit (GRU) with nunits cells.""" if self._gate_linear is None: bias_ones = self._bias_initializer if self._bias_initializer is None: bias_ones = init_ops.constant_initializer(1.0, dtype=inputs.dtype) with vs.variable_scope('gates'): self._gate_linear = _Linear([inputs, state], 2 * self. _num_units, True, bias_initializer=bias_ones, kernel_initializer=self._kernel_initializer) value = math_ops.sigmoid(self._gate_linear([inputs, state])) r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1) r_state = r * state if self._candidate_linear is None: with vs.variable_scope('candidate'): self._candidate_linear = _Linear([inputs, r_state], self. _num_units, True, bias_initializer=self._bias_initializer, kernel_initializer=self._kernel_initializer) c = self._activation(self._candidate_linear([inputs, r_state])) new_h = (1.0 - att_score) * state + att_score * c return new_h, new_h 
avod.core.box_8c_encoder.np|to|d|co|box def np_box_3d_to_box_8co(box_3d): """Computes the 3D bounding box corner positions from Box3D format.  The order of corners are preserved during this conversion.  Args: box_3d: 1 x 7 ndarray of box_3d in the format [x, y, z, l, w, h, ry] Returns: corners_3d: An ndarray or a tensor of shape (3 x 8) representing the box as corners in the following format -> [[x1,...,x8], [y1...,y8], [z1,...,z8]]. """ format_checker.check_box_3d_format(box_3d) ry = box_3d[6] rot = np.array([[np.cos(ry), 0, np.sin(ry), box_3d[0]], [0, 1, 0, box_3d[1]], [-np.sin(ry), 0, np.cos(ry), box_3d[2]]]) length = box_3d[3] width = box_3d[4] height = box_3d[5] x_corners = np.array([length / 2, length / 2, -length / 2, -length / 2, length / 2, length / 2, -length / 2, -length / 2]) y_corners = np.array([0.0, 0.0, 0.0, 0.0, -height, -height, -height, - height]) z_corners = np.array([width / 2, -width / 2, -width / 2, width / 2, width / 2, -width / 2, -width / 2, width / 2]) ones_col = np.ones(x_corners.shape) box_8c = np.dot(rot, np.array([x_corners, y_corners, z_corners, ones_col])) box_8c = box_8c[0:3] return box_8c 
vocab.VocabEntry.setitem def __setitem__(self, key, value): raise ValueError('vocabulary is readonly') 
utils.center|crop def center_crop(x, crop_h, crop_w, resize_h=64, resize_w=64): cx = 89 cy = 121 return scipy.misc.imresize(x[cy - 64:cy + 64, cx - 64:cx + 64], [ resize_h, resize_w]) 
gym_pycolab.envs.pycolab_grid_worlds_env.PycolabGridWorldsLevel19Env.Grid|Worlds|Pycolab|Level|Env def __init__(self): super(PycolabGridWorldsEnv, self).__init__() 
avod.core.anchor_encoder_test.AnchorEncoderTest.offset|to|anchor|tensor|test def test_offset_tensor_to_anchor(self): anchors = np.asarray([[1, 2, 3, 4, 6, 5], [0, 0, 0, 2, 3, 1]], dtype=np .float32) anchor_tensor = tf.convert_to_tensor(anchors, dtype=tf.float32) anchor_offsets = np.array([[0.5, 0.02, 0.01, 0.1, 0.4, 0.03], [0.04, 0.1, 0.03, 0.001, 0.3, 0.03]], dtype=np.float32) anchor_offset_tensor = tf.convert_to_tensor(anchor_offsets, dtype=tf. float32) expected_anchors = np.array([[3.0, 2.12, 3.05, 4.42, 8.9509, 5.152], [ 0.08, 0.3, 0.03, 2.002, 4.05, 1.03]], dtype=np.float32) anchors_tensor = anchor_encoder.offset_to_anchor(anchor_tensor, anchor_offset_tensor) sess = tf.Session() with sess.as_default(): anchors = anchors_tensor.eval() np.testing.assert_almost_equal(anchors, expected_anchors, decimal=3) 
model.GAN.validate def validate(self): total_ssim = 0 total_psnr = 0 psnr_weight = 1 / 20 ssim_weight = 1 val_image_count = len(self.A_val) for i in range(val_image_count): x = np.expand_dims(self.A_val[i], axis=0) feed_dict = {self.RealA: x, self.isTrain: False} generated_B = self.FakeB.eval(feed_dict=feed_dict) print('Validation Image', i, end='\r') generated_B = ((generated_B[0] + 1) / 2 * 255).astype(np.uint8) real_B = ((self.B_val[i] + 1) / 2 * 255).astype(np.uint8) psnr = compare_psnr(real_B, generated_B) ssim = compare_ssim(real_B, generated_B, multichannel=True) total_psnr = total_psnr + psnr total_ssim = total_ssim + ssim average_psnr = total_psnr / val_image_count average_ssim = total_ssim / val_image_count score = average_psnr * psnr_weight + average_ssim * ssim_weight if score > self.score_best: self.score_best = score self.saver.save(self.sess, os.path.join(self.ckpt_dir, 'gan'), global_step=self.step.eval()) line = 'Better Score: %.6f, PSNR: %.6f, SSIM: %.6f' % (score, average_psnr, average_ssim) print(line) with open(os.path.join(self.ckpt_dir, 'logs.txt'), 'a') as f: line += '\n' f.write(line) if self.save_samples: try: image_list = os.listdir(self.sample_image_dir) except: print('Sample images not found. Terminating program') exit(0) for i, file in enumerate(image_list, 1): print('Sample Image', i, end='\r') x = cv2.imread(os.path.join(self.sample_image_dir, file), 1) x = x / 255 * 2 - 1 x = np.reshape(x, (1, 256, 256, 3)) feed_dict = {self.RealA: x, self.isTrain: False} img = self.FakeB.eval(feed_dict=feed_dict) img = img[(0), :, :, :] img = ((img + 1) / 2 * 255).astype(np.uint8) cv2.imwrite(os.path.join(self.ckpt_dir, file), img) 
neural_style.normalize def normalize(weights): denom = sum(weights) if denom > 0.0: return [(float(i) / denom) for i in weights] else: return [0.0] * len(weights) 
attributionpriors.mnist.mnist_reader.decode def decode(serialized_example): """Parses an image and label from the given `serialized_example`.""" features = tf.parse_single_example(serialized_example, features={ 'image_raw': tf.FixedLenFeature([], tf.string), 'label': tf. FixedLenFeature([], tf.int64)}) image = tf.decode_raw(features['image_raw'], tf.uint8) image.set_shape(mnist.IMAGE_PIXELS) image = tf.reshape(image, [28, 28, 1]) label = tf.cast(features['label'], tf.int32) return image, label 
graphsage.supervised_train.calc|f def calc_f1(y_true, y_pred): if not FLAGS.sigmoid: y_true = np.argmax(y_true, axis=1) y_pred = np.argmax(y_pred, axis=1) else: y_pred[y_pred > 0.5] = 1 y_pred[y_pred <= 0.5] = 0 return metrics.f1_score(y_true, y_pred, average='micro'), metrics.f1_score( y_true, y_pred, average='macro') 
feature_reader.Whole|reader|feature def Whole_feature_reader(filename, feat_param, dtype=np.float32): """FUNCTION TO READ whole utterance of features """ SP_DIM = feat_param['fftl'] // 2 + 1 MCC_DIM = feat_param['mcep_dim'] FEAT_DIM = feat_param['feat_dim'] values = np.fromfile(filename, dtype).astype(np.float64).reshape([-1, FEAT_DIM]) sp = values[:, :SP_DIM].copy(order='C') mcc = values[:, SP_DIM:SP_DIM + MCC_DIM].copy(order='C') ap = values[:, SP_DIM + MCC_DIM:SP_DIM * 2 + MCC_DIM].copy(order='C') f0 = values[:, (SP_DIM * 2 + MCC_DIM)].copy(order='C') en_sp = values[:, (SP_DIM * 2 + MCC_DIM + 1)].copy(order='C') en_mcc = values[:, (SP_DIM * 2 + MCC_DIM + 2)].copy(order='C') speaker = values[:, (-1)].astype(np.int64) dictionary = {'sp': sp, 'mcc': mcc, 'ap': ap, 'f0': f0, 'en_sp': en_sp, 'en_mcc': en_mcc, 'speaker': speaker} return dictionary 
eranlayers.eran|reshape def eran_reshape(inputs, new_shape, name=None): """ adds a tf.Reshape to the graph. Inputs will be reshaped to (-1,)+new_shape  Arguments --------- inputs : tf.Tensor the preceding layer new_shape : list or tuple the new shape, has 1 to 3 entries name : str optional name for the Reshape operation   Return ------ output : tf.Tensor tensor associated with the Reshape operation """ assert len(new_shape ) < 4, 'shape should have less than 4 entries (batch size is taken care of)' batch_shape = [-1] for s in new_shape: batch_shape.append(s) return tf.reshape(inputs, batch_shape, name=name) 
facenet-master.tmp.mnist_noise_labels.type|data def data_type(): """Return the type of the activations, weights, and placeholder variables.""" if FLAGS.use_fp16: return tf.float16 else: return tf.float32 
clever.get|weibull|fit|best def get_best_weibull_fit(sample, use_reg=False, shape_reg=0.01): fitted_paras = {'c': [], 'loc': [], 'scale': [], 'ks': [], 'pVal': []} loc_shift = np.amax(sample) dist_range = np.amax(sample) - np.amin(sample) shape_rescale = dist_range print('shape rescale = {}'.format(shape_rescale)) rescaled_sample = np.copy(sample) rescaled_sample -= loc_shift rescaled_sample /= shape_rescale print('loc_shift = {}'.format(loc_shift)) if use_reg: results = pool.map(partial(fit_and_test, rescaled_sample, sample, loc_shift, shape_rescale, partial(fmin_with_reg, shape_reg= shape_reg)), c_init) else: results = pool.map(partial(fit_and_test, rescaled_sample, sample, loc_shift, shape_rescale, scipy.optimize.fmin), c_init) for res, c_i in zip(results, c_init): c = res[0] loc = res[1] scale = res[2] ks = res[3] pVal = res[4] print( '[DEBUG][L2] c_init = {:5.5g}, fitted c = {:6.2f}, loc = {:7.2f}, scale = {:7.2f}, ks = {:4.2f}, pVal = {:4.2f}, max = {:7.2f}' .format(c_i, c, loc, scale, ks, pVal, loc_shift)) fitted_paras['c'].append(c) fitted_paras['loc'].append(loc) fitted_paras['scale'].append(scale) fitted_paras['ks'].append(ks) fitted_paras['pVal'].append(pVal) max_pVal = np.nanmax(fitted_paras['pVal']) if np.isnan(max_pVal) or max_pVal < 0.001: print('ill-conditioned samples. Using maximum sample value.') return -1, -1, -max(sample), -1, -1, -1 max_pVal_idx = fitted_paras['pVal'].index(max_pVal) c_init_best = c_init[max_pVal_idx] c_best = fitted_paras['c'][max_pVal_idx] loc_best = fitted_paras['loc'][max_pVal_idx] scale_best = fitted_paras['scale'][max_pVal_idx] ks_best = fitted_paras['ks'][max_pVal_idx] pVal_best = fitted_paras['pVal'][max_pVal_idx] return c_init_best, c_best, loc_best, scale_best, ks_best, pVal_best 
tflib.ops.conv2d.Conv|D def Conv2D(name, input_dim, output_dim, filter_size, inputs, he_init=True, mask_type=None, stride=1, weightnorm=None, biases=True, gain=1.0): """ inputs: tensor of shape (batch size, num channels, height, width) mask_type: one of None, 'a', 'b'  returns: tensor of shape (batch size, num channels, height, width) """ with tf.name_scope(name) as scope: if mask_type is not None: mask_type, mask_n_channels = mask_type mask = np.ones((filter_size, filter_size, input_dim, output_dim ), dtype='float32') center = filter_size // 2 mask[center + 1:, :, :, :] = 0.0 mask[(center), center + 1:, :, :] = 0.0 for i in range(mask_n_channels): for j in range(mask_n_channels): if (mask_type == 'a' and i >= j or mask_type == 'b' and i > j): mask[(center), (center), i::mask_n_channels, j:: mask_n_channels] = 0.0  def uniform(stdev, size): return np.random.uniform(low=-stdev * np.sqrt(3), high=stdev * np.sqrt(3), size=size).astype('float32') fan_in = input_dim * filter_size ** 2 fan_out = output_dim * filter_size ** 2 / stride ** 2 if mask_type is not None: fan_in /= 2.0 fan_out /= 2.0 if he_init: filters_stdev = np.sqrt(4.0 / (fan_in + fan_out)) else: filters_stdev = np.sqrt(2.0 / (fan_in + fan_out)) if _weights_stdev is not None: filter_values = uniform(_weights_stdev, (filter_size, filter_size, input_dim, output_dim)) else: filter_values = uniform(filters_stdev, (filter_size, filter_size, input_dim, output_dim)) filter_values *= gain filters = lib.param(name + '.Filters', filter_values) if weightnorm == None: weightnorm = _default_weightnorm if weightnorm: norm_values = np.sqrt(np.sum(np.square(filter_values), axis=(0, 1, 2))) target_norms = lib.param(name + '.g', norm_values) with tf.name_scope('weightnorm') as scope: norms = tf.sqrt(tf.reduce_sum(tf.square(filters), reduction_indices=[0, 1, 2])) filters = filters * (target_norms / norms) if mask_type is not None: with tf.name_scope('filter_mask'): filters = filters * mask result = tf.nn.conv2d(input=inputs, filter=filters, strides=[1, 1, stride, stride], padding='SAME', data_format='NCHW') if biases: _biases = lib.param(name + '.Biases', np.zeros(output_dim, dtype='float32')) result = tf.nn.bias_add(result, _biases, data_format='NCHW') return result 
texar.data.data.dataset_utils._make_smaller_batch_filter_fn.filter|fn def _filter_fn(data): if isinstance(data, (list, tuple)): return _filter_fn(data[0]) elif isinstance(data, dict): return _filter_fn(data[next(iter(data))]) else: return tf.equal(tf.shape(data)[0], batch_size) 
word_chatbot.WordChatbot.generate|data def generate_data(self, data_dir, tmp_dir, task_id=-1): self.data_dir = data_dir self.mode = {problem.DatasetSplit.TRAIN: 'train', problem.DatasetSplit. EVAL: 'dev', problem.DatasetSplit.TEST: 'test'} filepath_fns = {problem.DatasetSplit.TRAIN: self.training_filepaths, problem.DatasetSplit.EVAL: self.dev_filepaths, problem.DatasetSplit .TEST: self.test_filepaths} split_paths = [(split['split'], filepath_fns[split['split']](data_dir, split['shards'], shuffled=self.already_shuffled)) for split in self .dataset_splits] all_paths = [] for _, paths in split_paths: all_paths.extend(paths) if self.is_generate_per_split: for split, paths in split_paths: self.preprocess_data(self.mode[split]) generator_utils.generate_files(self.generate_encoded_samples( data_dir, tmp_dir, split), paths) else: self.preprocess_data(self.mode[problem.DatasetSplit.TRAIN]) generator_utils.generate_files(self.generate_encoded_samples( data_dir, tmp_dir, problem.DatasetSplit.TRAIN), all_paths) generator_utils.shuffle_dataset(all_paths, extra_fn=self._pack_fn()) 
data_loader.load|data def load_data(data_dir='/shared-data/SIDD_Medium_Raw/Data/', verbose=False): file_list = glob.glob(os.path.join(data_dir, '*/*GT_RAW_010.MAT')) data1 = None cam_iso_info = [] tt = time.time() for i in range(len(file_list)): cam_iso = file_list[i][-41:-33] patch = gen_patches(file_list[i]) cam_iso_info.append([cam_iso] * len(patch)) if data1 is None: data1 = patch else: data1 = np.concatenate((data1, patch), axis=0) if verbose: print(str(i + 1) + '/' + str(len(file_list)) + ' is done ^_^') assert len(cam_iso_info) == len(data1) tt = time.time() - tt discard_n = len(data1) - len(data1) // batch_size * batch_size data1 = np.delete(data1, range(discard_n), axis=0) cam_iso_info = cam_iso_info[discard_n:] assert len(cam_iso_info) == len(data1) print('^_^-training data finished-^_^ time = %s sec' % str(tt)) return data1, cam_iso_info 
deepctr.input_embedding.preprocess|input|embedding def preprocess_input_embedding(feature_dim_dict, embedding_size, l2_reg_embedding, l2_reg_linear, init_std, seed, create_linear_weight=True ): sparse_input_dict, dense_input_dict = create_singlefeat_inputdict( feature_dim_dict) sequence_input_dict, sequence_input_len_dict, sequence_max_len_dict = ( create_varlenfeat_inputdict(feature_dim_dict)) inputs_list, deep_emb_list, linear_emb_list = get_inputs_embedding( feature_dim_dict, embedding_size, l2_reg_embedding, l2_reg_linear, init_std, seed, sparse_input_dict, dense_input_dict, sequence_input_dict, sequence_input_len_dict, sequence_max_len_dict, create_linear_weight) return deep_emb_list, linear_emb_list, dense_input_dict, inputs_list 
train_app.train|loop def train_loop(preprocess_fn, network_factory, train_x, train_y, num_images_per_id, batch_size, log_dir, image_shape=None, restore_path= None, exclude_from_restore=None, run_id=None, number_of_steps=None, loss_mode='cosine-softmax', learning_rate=0.001, trainable_scopes=None, save_summaries_secs=60, save_interval_secs=300): """Start training.  Parameters ---------- preprocess_fn : Callable[tf.Tensor] -> tf.Tensor A callable that applies preprocessing to a given input image tensor of dtype tf.uint8 and returns a floating point representation (tf.float32). network_factory : Callable[tf.Tensor] -> (tf.Tensor, tf.Tensor) A callable that takes as argument a preprocessed input image of dtype tf.float32 and returns the feature representation as well as a logits tensors. The logits may be set to None if not required by the loss. train_x : List[str] | np.ndarray A list of image filenames or a tensor of images. train_y : List[int] | np.ndarray A list or one-dimensional array of labels for the images in `train_x`. num_images_per_id : int Sample `num_images_per_id` images for each label at each training iteration. The number of identities sampled at each iteration is computed as `batch_size / num_images_per_id`. The `batch_size` must be divisible by this number. batch_size : int The number of images at each training iteration. log_dir : str Used to construct the log and checkpoint directory. They are stored in `log_dir/run_id`. image_shape : Tuple[int, int, int] | NoneType Image shape (height, width, channels) or None. If None, `train_x` must be an array of images such that the shape can be queries from this variable. restore_path : Optional[str] If not None, resumes training from the given checkpoint file. exclude_from_restore : Optional[List[str]] An optional list of variable scopes to be used in conjunction with `restore_path`. If not None, variables in the given scopes are not restored from the checkpoint file. run_id : Optional[str] A string that identifies the training run; used to construct the log and checkpoint directory `log_dir/run_id`. If None, a random string is created. number_of_steps : Optional[int] The total number of training iterations. If None, training runs indefenitely. loss_mode : Optional[str] A string that identifies the loss function used for training; must be one of 'cosine-softmax', 'magnet', 'triplet'. This value defaults to 'cosine-softmax'. learning_rate : Optional[float] Adam learning rate; defaults to 1e-3. trainable_scopes : Optional[List[str]] Optional list of variable scopes. If not None, only variables within the given scopes are trained. Otherwise all variables are trained. save_summaries_secs : Optional[int] Save training summaries every `save_summaries_secs` seconds to the log directory. save_interval_secs : Optional[int] Save checkpoints every `save_interval_secs` seconds to the log directory.  """ if image_shape is None: assert type(train_x) == np.ndarray image_shape = train_x.shape[1:] elif type(train_x) == np.ndarray: assert train_x.shape[1:] == image_shape read_from_file = type(train_x) != np.ndarray trainer, train_op = create_trainer(preprocess_fn, network_factory, read_from_file, image_shape, batch_size, loss_mode, learning_rate= learning_rate, trainable_scopes=trainable_scopes) feed_generator = queued_trainer.random_sample_identities_forever(batch_size , num_images_per_id, train_x, train_y) variables_to_restore = slim.get_variables_to_restore(exclude= exclude_from_restore) trainer.run(feed_generator, train_op, log_dir, restore_path= restore_path, variables_to_restore=variables_to_restore, run_id= run_id, save_summaries_secs=save_summaries_secs, save_interval_secs =save_interval_secs, number_of_steps=number_of_steps) 
operations.int|shape def int_shape(x): return list(map(int, x.get_shape())) 
conceptnet.sequence|compile|final def compile_final_sequence(final_event1, final_event2, final_relation, text_encoder): final = [] final.append(final_event1) final.append(final_relation) final.append(final_event2) final[-1].append(text_encoder.encoder['<END>']) return final 
facenet-master.contributed.cluster.parse|arguments def parse_arguments(argv): parser = argparse.ArgumentParser() parser.add_argument('model', type=str, help= 'Either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file' ) parser.add_argument('data_dir', type=str, help= 'The directory containing the images to cluster into folders.') parser.add_argument('out_dir', type=str, help= 'The output directory where the image clusters will be saved.') parser.add_argument('--image_size', type=int, help= 'Image size (height, width) in pixels.', default=160) parser.add_argument('--margin', type=int, help= 'Margin for the crop around the bounding box (height, width) in pixels.' , default=44) parser.add_argument('--min_cluster_size', type=int, help= 'The minimum amount of pictures required for a cluster.', default=1) parser.add_argument('--cluster_threshold', type=float, help= 'The minimum distance for faces to be in the same cluster', default=1.0 ) parser.add_argument('--largest_cluster_only', action='store_true', help ='This argument will make that only the biggest cluster is saved.') parser.add_argument('--gpu_memory_fraction', type=float, help= 'Upper bound on the amount of GPU memory that will be used by the process.' , default=1.0) return parser.parse_args(argv) 
thumt.utils.lrp.v|n|linear def linear_v2n(inputs, output_size, bias, w_x_inp, params, concat=False, dtype=None, scope=None, d2=False): """ Linear layer :param inputs: A Tensor or a list of Tensors with shape [batch, input_size] :param output_size: An integer specify the output size :param bias: a boolean value indicate whether to use bias term :param concat: a boolean value indicate whether to concatenate all inputs :param dtype: an instance of tf.DType, the default value is ``tf.float32'' :param scope: the scope of this layer, the default value is ``linear'' :returns: a Tensor with shape [batch, output_size] :raises RuntimeError: raises ``RuntimeError'' when input sizes do not compatible with each other """ with tf.variable_scope(scope, default_name='linear', values=[inputs]): if not isinstance(inputs, (list, tuple)): inputs = [inputs] batch_shape = tf.shape(inputs[0])[:-1] input_size = [item.get_shape()[-1].value for item in inputs] if len(inputs) != len(input_size): raise RuntimeError('inputs and input_size unmatched!') output_shape = tf.concat([tf.shape(inputs[0])[:-1], [output_size]], axis=0) inputs = [tf.reshape(inp, [-1, inp.shape[-1].value]) for inp in inputs] results = [] weight_ratios = [] weight_shapes = [] matrixs = [] if concat: input_size = sum(input_size) inputs = tf.concat(inputs, 1) shape = [input_size, output_size] weight_shape = tf.concat([batch_shape, shape], -1) matrix = tf.get_variable('matrix', shape, dtype=dtype) results.append(tf.matmul(inputs, matrix)) else: for i in range(len(input_size)): shape = [input_size[i], output_size] weight_shapes.append(tf.concat([batch_shape, shape], -1)) name = 'matrix_%d' % i matrix = tf.get_variable(name, shape, dtype=dtype) matrixs.append(matrix) results.append(tf.matmul(inputs[i], matrix)) output = tf.add_n(results) if bias: shape = [output_size] bias = tf.get_variable('bias', shape, dtype=dtype) output = tf.nn.bias_add(output, bias) operator = weight_ratio_linear_v2n if d2: operator = weight_ratio_linear_v2n_2d if concat: weight_ratios = operator([inputs], [matrix], output, w_x_inp, bias=bias, stab=params.stab) else: weight_ratios = operator(inputs, matrixs, output, w_x_inp, bias =bias, stab=params.stab) output = tf.reshape(output, output_shape) return {'output': output, 'weight_ratios': weight_ratios} 
m_phate.m_phate.M_PHATE.params|set def set_params(self, **params): """Set the parameters on this estimator.  Any parameters not given as named arguments will be left at their current value.  Parameters ----------  n_components : int, optional, default: 2 number of dimensions in which the data will be embedded  intraslice_knn : int, optional, default: 2 number of nearest neighbors on which to build intraslice kernels  interslice_knn : int, optional, default: 25 number of nearest neighbors on which to build interslice kernels  decay : int, optional, default: 5 sets decay rate of kernel tails. If None, alpha decaying kernel is not used  n_landmark : int, optional, default: 4000 number of landmarks to use in fast PHATE  t : int, optional, default: 'auto' power to which the diffusion operator is powered. This sets the level of diffusion. If 'auto', t is selected according to the knee point in the Von Neumann Entropy of the diffusion operator  gamma : float, optional, default: 0 Informational distance constant between -1 and 1. `gamma=1` gives the PHATE log potential, `gamma=0` gives a square root potential.  n_pca : int, optional, default: 100 Number of principal components to use for calculating neighborhoods. For extremely large datasets, using n_pca < 20 allows neighborhoods to be calculated in roughly log(n_samples) time.  n_svd : int, optional, default: 100 Number of singular vectors to use for calculating landmarks.  n_jobs : integer, optional, default: 1 The number of jobs to use for the computation. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used  random_state : integer or numpy.RandomState, optional, default: None The generator used to initialize SMACOF (metric, nonmetric) MDS If an integer is given, it fixes the seed Defaults to the global `numpy` random number generator  verbose : `int` or `boolean`, optional (default: 1) If `True` or `> 0`, print status messages """ reset_kernel = False if 'interslice_knn' in params and params['interslice_knn' ] != self.interslice_knn: self.interslice_knn = params['interslice_knn'] reset_kernel = True del params['interslice_knn'] if 'intraslice_knn' in params: params['knn'] = params['intraslice_knn'] del params['intraslice_knn'] if 'n_svd' in params and params['n_svd'] != self.n_svd: self._set_graph_params(n_svd=params['n_svd']) self.n_svd = params['n_svd'] del params['n_svd'] if reset_kernel: self._reset_graph() return super().set_params(**params) 
deepzono_milp.maxpool|handle def handle_maxpool(model, var_list, layerno, src_counter, pool_size, input_shape, lbi, ubi, lbi_prev, ubi_prev, use_milp): start = len(var_list) num_neurons = input_shape[0] * input_shape[1] * input_shape[2] binary_counter = start maxpool_counter = start if use_milp == 1: maxpool_counter = start + num_neurons for j in range(num_neurons): var_name = 'x' + str(start + j) var = model.addVar(vtype=GRB.BINARY, name=var_name) var_list.append(var) o1 = int(input_shape[0] / pool_size[0]) o2 = int(input_shape[1] / pool_size[1]) o3 = int(input_shape[2] / pool_size[2]) output_size = o1 * o2 * o3 for j in range(output_size): var_name = 'x' + str(maxpool_counter + j) var = model.addVar(vtype=GRB.CONTINUOUS, lb=lbi[j], ub=ubi[j], name =var_name) var_list.append(var) output_offset = 0 for out_x in range(o1): for out_y in range(o2): for out_z in range(o3): sum_u = 0.0 sum_l = 0.0 max_u = float('-inf') max_l = float('-inf') pool_map = [] inf = [] sup = [] l = 0 for x_shift in range(pool_size[0]): for y_shift in range(pool_size[1]): x_val = out_x * 2 + x_shift y_val = out_y * 2 + y_shift mat_offset = x_val * input_shape[1] * input_shape[2 ] + y_val * input_shape[2] + out_z pool_cur_dim = src_counter + mat_offset pool_map.append(mat_offset) inf.append(lbi_prev[mat_offset]) sup.append(ubi_prev[mat_offset]) sum_u = sum_u + sup[l] sum_l = sum_l + inf[l] if sup[l] > max_u: max_u = sup[l] if inf[l] > max_l: max_l = inf[l] l = l + 1 dst_index = maxpool_counter + output_offset p01 = pool_size[0] * pool_size[1] if use_milp == 1: binary_expr = LinExpr() for l in range(p01): src_index = pool_map[l] src_var = src_index + src_counter binary_var = src_index + binary_counter if ubi_prev[src_index] < max_l: continue expr = var_list[dst_index] - var_list[src_var] model.addConstr(expr, GRB.GREATER_EQUAL, 0) max_u_rest = float('-inf') for j in range(p01): if j == l: continue if sup[j] > max_u_rest: max_u_rest = sup[j] cst = max_u_rest - inf[l] expr = var_list[dst_index] - var_list[src_var ] + cst * var_list[binary_var] model.addConstr(expr, GRB.LESS_EQUAL, cst) model.addGenConstrIndicator(var_list[binary_var], True, var_list[dst_index] - var_list[src_var], GRB.EQUAL, 0.0) binary_expr += var_list[binary_var] model.addConstr(binary_expr, GRB.EQUAL, 1) else: add_expr = LinExpr() add_expr += -1 * var_list[dst_index] for l in range(p01): src_index = pool_map[l] src_var = src_index + src_counter expr = var_list[dst_index] - var_list[src_var] model.addConstr(expr, GRB.GREATER_EQUAL, 0) add_expr += var_list[src_var] model.addConstr(add_expr, GRB.GREATER_EQUAL, sum_l - max_l) output_offset += 1 return maxpool_counter 
metrics.get|accuracies def get_accuracies(logits, labels): with tf.device('/device:GPU:0'), tf.name_scope('metrics'): labels = tf.argmax(labels, 1) in_top_1 = tf1.nn.in_top_k(logits, labels, 1) acc_top_1 = tf.reduce_mean(tf.cast(in_top_1, tf.float32)) return acc_top_1 
utils.calculate|d|projections def calculate_2d_projections(coordinates_3d, intrinsics): """ Input: coordinates: [3, N] intrinsics: [3, 3] Return projected_coordinates: [N, 2] """ projected_coordinates = intrinsics @ coordinates_3d projected_coordinates = projected_coordinates[:2, : ] / projected_coordinates[(2), :] projected_coordinates = projected_coordinates.transpose() projected_coordinates = np.array(projected_coordinates, dtype=np.int32) return projected_coordinates 
UtilsNetwork.get|data def get_data(keyword, samples, var_name, level, n_input, model_path_folder= None, normalize=True, scaler='m', point='sobol', rs=None): if point != 'sobol' and point != 'random': raise ValueError('check point argument') if keyword == 'parab': dataset = pd.read_csv('./CaseStudies/Parabolic/Data/solution_' + point + '_deltaT_' + str(level) + '.csv', header=0, sep=',') elif keyword == 'shock': dataset = pd.read_csv('./CaseStudies/ShockTube/Data/shock_tube_' + str(level) + '.csv', header=0, sep=',') elif keyword == 'airf': dataset = pd.read_csv('./CaseStudies/Airfoil/Data/airfoil_data_' + str(level) + '.csv', header=0, sep=',') else: raise ValueError('Chose one option between parab, shock and airf') if point == 'random' and keyword != 'parab': raise ValueError('Random Point available only for Projectile Motion') if samples == 'all' or samples == 'All': samples = len(dataset) - 1 if scaler == 'm': min_val = min(dataset[var_name]) max_val = max(dataset[var_name]) elif scaler == 's': min_val = dataset[var_name].mean() max_val = dataset[var_name].std() else: raise ValueError( 'Select one scaler between MinMax (m) and Standard (s)') if normalize: if scaler == 'm': dataset[var_name] = (dataset[var_name] - min_val) / (max_val - min_val) elif scaler == 's': dataset[var_name] = (dataset[var_name] - min_val) / max_val else: min_val = 0 max_val = 1 print(dataset.head()) loc_var_name = dataset.columns.get_loc(var_name) if rs is not None: X, X_test, y, y_test = train_test_split(dataset.iloc[:, :n_input]. values, dataset.iloc[:, (loc_var_name)].values, train_size= samples, shuffle=True, random_state=rs) else: X = dataset.iloc[:samples, :n_input].values y = dataset.iloc[:samples, (loc_var_name)].values X_test = dataset.iloc[samples:, :n_input].values y_test = dataset.iloc[samples:, (loc_var_name)].values print(X) if model_path_folder is not None: with open(model_path_folder + '/InfoData.txt', 'w') as file: file.write( """dev_norm_train,dev_norm_test,dev_train,dev_test,mean_norm_train,mean_norm_test,mean_train,mean_test, """ ) file.write(str(np.std(y)) + ',' + str(np.std(y_test)) + ',' + str(np.std(y * (max_val - min_val) + min_val)) + ',' + str( np.std(y_test * (max_val - min_val) + min_val)) + ',' + str (np.mean(y)) + ',' + str(np.mean(y_test)) + ',' + str(np. mean(y * (max_val - min_val) + min_val)) + ',' + str(np. mean(y_test * (max_val - min_val) + min_val))) return X, y, X_test, y_test, min_val, max_val 
nets.cyclegan.cyclegan|upsample def cyclegan_upsample(net, num_outputs, stride, method='conv2d_transpose'): """Upsamples the given inputs.  Args: net: A Tensor of size [batch_size, height, width, filters]. num_outputs: The number of output filters. stride: A list of 2 scalars or a 1x2 Tensor indicating the scale, relative to the inputs, of the output dimensions. For example, if kernel size is [2, 3], then the output height and width will be twice and three times the input size. method: The upsampling method: 'nn_upsample_conv', 'bilinear_upsample_conv', or 'conv2d_transpose'.  Returns: A Tensor which was upsampled using the specified method.  Raises: ValueError: if `method` is not recognized. """ with tf.variable_scope('upconv'): net_shape = tf.shape(net) height = net_shape[1] width = net_shape[2] spatial_pad_1 = np.array([[0, 0], [1, 1], [1, 1], [0, 0]]) if method == 'nn_upsample_conv': net = tf.image.resize_nearest_neighbor(net, [stride[0] * height, stride[1] * width]) net = tf.pad(net, spatial_pad_1, 'REFLECT') net = layers.conv2d(net, num_outputs, kernel_size=[3, 3], padding='valid') elif method == 'bilinear_upsample_conv': net = tf.image.resize_bilinear(net, [stride[0] * height, stride [1] * width]) net = tf.pad(net, spatial_pad_1, 'REFLECT') net = layers.conv2d(net, num_outputs, kernel_size=[3, 3], padding='valid') elif method == 'conv2d_transpose': net = layers.conv2d_transpose(net, num_outputs, kernel_size=[3, 3], stride=stride, padding='valid') net = net[:, 1:, 1:, :] else: raise ValueError('Unknown method: [%s]', method) return net 
commons.measure.MeasurementDevice.Device|Measurement def __init__(self, hparams): self.batch_dims = [hparams.batch_size] + hparams.image_dims self.output_type = None 
mnist.inf.basic_utils.get|path|ckpt def get_ckpt_path(ckpt_dir): ckpt_dir = os.path.abspath(ckpt_dir) ckpt = tf.train.get_checkpoint_state(ckpt_dir) if ckpt and ckpt.model_checkpoint_path: ckpt_path = os.path.join(ckpt_dir, ckpt.model_checkpoint_path) else: ckpt_path = None return ckpt_path 
borealisflows.layers.BatchNorm.log|and|inverse|det|jacobian def _inverse_and_log_det_jacobian(self, y): x = self._inverse(y) log_abs_det_J_inv = self._inverse_log_det_jacobian(y) return x, log_abs_det_J_inv 
neural_tangents.stax.FanInSum.kernel|fn def kernel_fn(kernels): is_gaussian = all(ker.is_gaussian for ker in kernels) if not is_gaussian: raise NotImplementedError( '`FanInSum` layer is only implemented for the case if all input layers guaranteed to be mean-zero gaussian, i.e. having all `is_gaussianset to `True`.' ) marginal, cross = kernels[0].marginal, kernels[0].cross shape1, shape2 = kernels[0].shape1, kernels[0].shape2 if not all(k.marginal == marginal and k.cross == cross for k in kernels): raise NotImplementedError( '`FanInSum` layer is only implemented for the case if all input layers output the same typeof covariance matrices, i.e. having all matching `marginal` and `cross` attributes' ) n_kernels = len(kernels) n_height_width = sum(ker.is_height_width for ker in kernels) if n_height_width == n_kernels: is_height_width = True elif n_height_width >= n_kernels / 2: is_height_width = True for i in range(n_kernels): if not kernels[i].is_height_width: kernels[i] = _flip_height_width(kernels[i]) else: is_height_width = False for i in range(n_kernels): if kernels[i].is_height_width: kernels[i] = _flip_height_width(kernels[i]) if not all([(k.shape1 == shape1 and k.shape2 == shape2) for k in kernels]): raise ValueError('All shapes should be equal in FanInSum.') kers = tuple(None if all(ker[i] is None for ker in kernels) else sum( ker[i] for ker in kernels) for i in range(4)) return Kernel(*(kers + (is_gaussian, is_height_width, marginal, cross, None, None))) 
dump_embedding.load|arguments def load_arguments(): argparser = argparse.ArgumentParser(sys.argv[0]) argparser.add_argument('--lr_rate', type=float, default=0.025, help= 'learning rate') argparser.add_argument('--batch_size', type=int, default=512, help= 'batch size') argparser.add_argument('--sample_size', type=int, default=25, help= 'number of negative samples for skip-gram') argparser.add_argument('--context_window', type=int, default=5, help= 'context window size') argparser.add_argument('--sense_number', type=int, default=3, help= 'sense number per word') argparser.add_argument('--selection_dim', type=int, default=300, help= 'embedding dimension for the selection module') argparser.add_argument('--representation_dim', type=int, default=300, help='embedding dimension for the representation module') argparser.add_argument('--dataset_dir', type=str, required=True, help= 'directory containing the dataset') argparser.add_argument('--memory', type=float, default=0.1, help= 'GPU memory fraction used for model training') argparser.add_argument('--ckpt_path', type=str, required=True, help= 'path to checkpoint files') args = argparser.parse_args() return args 
translate.rnn.DropoutGRUCell.__init__.batch|noise def batch_noise(s): s = tf.concat(([1], tf.TensorShape(s).as_list()), 0) return tf.random_uniform(s) 
v2_write_training.main def main(args): procs = [] q_games = mp.SimpleQueue() if args: prefix = args.pop(0) print('Reading from chunkfiles {}'.format(prefix)) procs.append(mp.Process(target=disk_fetch_games, args=(q_games, prefix))) else: print('Reading from MongoDB') procs.append(mp.Process(target=mongo_fetch_games, args=(q_games, 275000))) q_test = mp.SimpleQueue() q_train = mp.SimpleQueue() procs.append(mp.Process(target=split_train_test, args=(q_games, q_train, q_test))) q_write_train = mp.SimpleQueue() q_write_test = mp.SimpleQueue() procs.append(mp.Process(target=chunk_parser, args=(q_train, q_write_train, 1 << 20, 8192))) procs.append(mp.Process(target=chunk_parser, args=(q_test, q_write_test, 1 << 16, 8192))) procs.append(mp.Process(target=chunk_writer, args=(q_write_train, NameSrc('train_')))) procs.append(mp.Process(target=chunk_writer, args=(q_write_test, NameSrc('test_')))) for p in procs: p.start() for p in procs: p.join() 
texar.data.data.scalar_data_test.ScalarDataTest.Up|set def setUp(self): tf.test.TestCase.setUp(self) int_data = np.linspace(0, 100, num=101, dtype=np.int32).tolist() int_data = [str(i) for i in int_data] int_file = tempfile.NamedTemporaryFile() int_file.write('\n'.join(int_data).encode('utf-8')) int_file.flush() self._int_file = int_file self._int_hparams = {'num_epochs': 1, 'batch_size': 1, 'shuffle': False, 'dataset': {'files': self._int_file.name, 'data_type': 'int', 'data_name': 'label'}} self._float_hparams = {'num_epochs': 1, 'batch_size': 1, 'shuffle': False, 'dataset': {'files': self._int_file.name, 'data_type': 'float', 'data_name': 'feat'}} 
facenet-master.tmp.deepdream.main.deepdream|render def render_deepdream(t_obj, img0=img_noise, iter_n=10, step=1.5, octave_n=4, octave_scale=1.4): t_score = tf.reduce_mean(t_obj) t_grad = tf.gradients(t_score, t_input)[0] img = img0 octaves = [] for _ in range(octave_n - 1): hw = img.shape[:2] lo = resize(img, np.int32(np.float32(hw) / octave_scale)) hi = img - resize(lo, hw) img = lo octaves.append(hi) for octave in range(octave_n): if octave > 0: hi = octaves[-octave] img = resize(img, hi.shape[:2]) + hi for _ in range(iter_n): g = calc_grad_tiled(img, t_grad) img += g * (step / (np.abs(g).mean() + 1e-07)) showarray(img / 255.0) 
deepctr.utils.check_version.check def check(version): try: url_pattern = 'https://pypi.python.org/pypi/deepctr/json' req = requests.get(url_pattern) latest_version = parse('0') version = parse(version) if req.status_code == requests.codes.ok: j = json.loads(req.text.encode('utf-8')) releases = j.get('releases', []) for release in releases: ver = parse(release) if ver.is_prerelease or ver.is_postrelease: continue latest_version = max(latest_version, ver) if latest_version > version: logging.warning( """ DeepCTR version {0} detected. Your version is {1}. Use `pip install -U deepctr` to upgrade.Changelog: https://github.com/shenweichen/DeepCTR/releases/tag/v{0}""" .format(latest_version, version)) except Exception as e: return 
data_gen.omni_gen.unison|copies|shuffled def unison_shuffled_copies(a, b): assert len(a) == len(b) p = np.random.permutation(len(a)) return a[p], b[p] 
config_utils.get|model|config|pcgn def get_pcgn_model_config(config): encode_num_layers = config['encoder']['num_layers'] encode_num_units = config['encoder']['num_units'] encode_cell_type = config['encoder']['cell_type'] encode_bidir = config['encoder']['bidirectional'] attn_num_units = config['decoder']['attn_num_units'] decode_num_layers = config['decoder']['num_layers'] decode_num_units = config['decoder']['num_units'] decode_cell_type = config['decoder']['cell_type'] use_user_feat = config['user_profile']['use_user_feat'] use_gate_memory = config['user_profile']['use_gate_memory'] use_user_desc = config['user_profile']['use_user_desc'] use_blog_user_coattn = config['user_profile']['use_blog_user_coattn'] use_external_desc_express = config['user_profile'][ 'use_external_desc_express'] use_external_feat_express = config['user_profile'][ 'use_external_feat_express'] user_feat_dim = config['user_profile']['user_feat_dim'] user_feat_unit = config['user_profile']['user_feat_unit'] user_feat_mem_unit = config['user_profile']['user_feat_mem_unit'] desc_rnn_unit = config['user_profile']['desc_rnn_unit'] desc_attn_num_units = config['user_profile']['desc_attn_num_units'] user_map_unit = config['user_profile']['user_map_unit'] return (encode_num_layers, encode_num_units, encode_cell_type, encode_bidir, attn_num_units, decode_num_layers, decode_num_units, decode_cell_type, use_user_feat, use_gate_memory, use_user_desc, use_blog_user_coattn, use_external_desc_express, use_external_feat_express, user_feat_dim, user_feat_unit, user_feat_mem_unit, desc_rnn_unit, desc_attn_num_units, user_map_unit) 
vae.VAE.decode def decode(self, z, y, feat_type): if not feat_type == self.feat_type: print('feature type does not match!') raise NotImplementedError xh = self.dec(z, y) xh = tf.squeeze(xh, 0) return self.normalizers[self.feat_type]['minmax'].backward_process(xh) 
evaluate_similarity.to|csv|results def results_to_csv(res, correlation='spearman', printRes=True, returnRes=False ): assert correlation in ['spearman', 'pearson' ], 'Unrecognized Correlation method' txtRest = '' txtCov = '' for y in res: txtRest = txtRest + str(y[correlation]) + ',' txtCov = txtCov + str(y['coverage']) + ',' if printRes: print(txtRest) print(txtCov) if returnRes: return txtRest, txtCov 
deepctr.layers.interaction.CrossNet.build def build(self, input_shape): if len(input_shape) != 2: raise ValueError( 'Unexpected inputs dimensions %d, expect to be 2 dimensions' % (len(input_shape),)) dim = input_shape[-1].value self.kernels = [self.add_weight(name='kernel' + str(i), shape=(dim, 1), initializer=glorot_normal(seed=self.seed), regularizer=l2(self. l2_reg), trainable=True) for i in range(self.layer_num)] self.bias = [self.add_weight(name='bias' + str(i), shape=(dim, 1), initializer=Zeros(), trainable=True) for i in range(self.layer_num)] super(CrossNet, self).build(input_shape) 
cleverhans.model.Model.get|logits def get_logits(self, x): """ :param x: A symbolic representation of the network input :return: A symbolic representation of the output logits (i.e., the values fed as inputs to the softmax layer). """ return self.get_layer(x, 'logits') 
embeddings.OpenKE.config.Config.Config.alpha|set def set_alpha(self, alpha): self.alpha = alpha 
deepMOT-master.utils.loss.Soft|row|Max def rowSoftMax(MunkresOut, scale=100.0, threshold=0.7): """ row wise softmax function :param MunkresOut: MunkresNet Output Variable Matrix of shape [batch, h, w] :return: row wise Softmax matrix """ clutter = torch.ones(MunkresOut.size(0), MunkresOut.size(1), 1).cuda( ) * threshold return F.softmax(torch.cat([MunkresOut, clutter], dim=2) * scale, dim=2) 
thumt.utils.lrp.combine|v|heads|n def combine_heads_v2n(inputs, name=None): with tf.name_scope(name, default_name='combine_heads', values=[inputs]): x = inputs x = tf.transpose(x, [0, 2, 3, 1, 4]) old_shape = x.get_shape().dims a, b = old_shape[-2:] new_shape = old_shape[:-2] + [a * b if a and b else None] x = tf.reshape(x, tf.concat([tf.shape(x)[:-2], [-1]], 0)) x.set_shape(new_shape) return x 
plato.agent.conversational_agent.conversational_single_agent.ConversationalSingleAgent.dialogue|end def end_dialogue(self): """ Perform final dialogue turn. Train and save models if applicable.  :return: nothing """ self.recorder.record(self.curr_state, self.curr_state, self.prev_action, self.prev_reward, self.prev_success, input_utterance=self. prev_usr_utterance, output_utterance=self.prev_sys_utterance, task_success=self.prev_task_success, force_terminate=True) self.dialogue_episode += 1 if self.IS_TRAINING: if self.dialogue_episode % self.train_interval == 0 and len(self. recorder.dialogues) >= self.minibatch_length: for epoch in range(self.train_epochs): print('Training epoch {0} of {1}'.format(epoch + 1, self. train_epochs)) minibatch = random.sample(self.recorder.dialogues, self. minibatch_length) if self.nlu and self.nlu.training: self.nlu.train(minibatch) if self.dialogue_manager.is_training(): self.dialogue_manager.train(minibatch) if self.nlg and self.nlg.training: self.nlg.train(minibatch) self.cumulative_rewards += self.recorder.dialogues[-1][-1][ 'cumulative_reward'] print('CUMULATIVE REWARD: {0}'.format(self.recorder.dialogues[-1][-1][ 'cumulative_reward'])) if self.dialogue_turn > 0: self.total_dialogue_turns += self.dialogue_turn if self.dialogue_episode % self.SAVE_INTERVAL == 0: if self.nlu: self.nlu.save() if self.dialogue_manager: self.dialogue_manager.save() if self.nlg: self.nlg.save() if self.recorder.dialogues[-1][-1]['success']: print('SUCCESS (Subjective)!') self.num_successful_dialogues += int(self.recorder.dialogues[-1][-1 ]['success']) else: print('FAILURE (Subjective).') if self.recorder.dialogues[-1][-1]['task_success']: self.num_task_success += int(self.recorder.dialogues[-1][-1][ 'task_success']) print('OBJECTIVE TASK SUCCESS: {0}'.format(self.recorder.dialogues[-1][ -1]['task_success'])) 
official.resnet.layer_test.BaseTest.test|block def test_block_1(self): self._resnet_block_ops(test=True, batch_size=BATCH_SIZE, **BLOCK_TESTS[1]) 
neural_tangents.utils.batch.parallel def _parallel(kernel_fn, device_count=-1): """Returns a function that computes a kernel in batches in parallel.  When batching in parallel, the data is split over a set number of devices. The number of devices must be less than or equal to the number of physical devices. Moreover, the dataset size needs to divide the device count.  Given two datasets x1 and x2, parallel splits the kernel calculation over devices such that each device computes a batch of rows of shape [|x1| / device_count, |x2|].  Args: kernel_fn: A function that computes a kernel between two datasets, `kernel_fn(x1, x2)` or the compositional kernel for an input kernel `kernel_fn(kernel_in)`. Here x1 and x2 are `np.ndarray`s of floats of shape [n1] + input_shape and [n2] + input_shape; `kernel_in` is a Kernel object. The kernel function should return a PyTree. device_count: Integer specifying the number of devices over which to split the data. If device_count = 0, the computation is parallelized over all available devices.  Returns: A new function with the same signature as kernel_fn that computes the kernel by batching over the dataset in parallel over a specified number of cores. """ kernel_fn = _jit_or_pmap_broadcast(kernel_fn, device_count) if device_count == -1: device_count = xla_bridge.device_count()  def parallel_fn_x1(x1, x2=None, *args, **kwargs): x2_is_none = x2 is None if x2_is_none: x2 = x1 n1 = x1.shape[0] assert x1.shape[1:] == x2.shape[1:] input_shape = x1.shape[1:] _device_count = device_count n1_per_device, ragged = divmod(n1, device_count) if n1_per_device and ragged: raise ValueError( 'Dataset size ({}) must divide number of physical devices ({}).' .format(n1, device_count)) elif not n1_per_device: _device_count = ragged n1_per_device = 1 x1 = np.reshape(x1, (_device_count, n1_per_device) + input_shape) kernel = kernel_fn(x1, x2, *args, **kwargs) return _flatten_kernel(kernel, x2_is_none, True)  def parallel_fn_kernel(kernel, *args, **kwargs): n1 = kernel.var1.shape[0] _device_count = device_count n1_per_device, ragged = divmod(n1, device_count) if n1_per_device and ragged: raise ValueError( 'Dataset size ({}) must divide number of physical devices ({}).' .format(n1, device_count)) elif not n1_per_device: _device_count = ragged n1_per_device = 1 kernel_dict = kernel._asdict() var2 = kernel_dict['var2'] var2_is_none = var2 is None if var2 is None: var2 = kernel_dict['var1'] kernel_dict['var2'] = np.broadcast_to(var2, (_device_count,) + var2 .shape) for k, v in kernel_dict.items(): if k in ('nngp', 'ntk', 'var1'): kernel_dict[k] = np.reshape(v, (_device_count, n1_per_device) + v.shape[1:]) if k in ('shape1',): kernel_dict[k] = (n1_per_device,) + v[1:] kernel = kernel_fn(Kernel(**kernel_dict), *args, **kwargs) if var2_is_none: kernel = kernel._replace(var2=None) return _flatten_kernel(kernel, var2_is_none, True)  def parallel_fn(x1_or_kernel, x2=None, *args, **kwargs): if isinstance(x1_or_kernel, np.ndarray): return parallel_fn_x1(x1_or_kernel, x2, *args, **kwargs) elif isinstance(x1_or_kernel, Kernel): assert not x2 return parallel_fn_kernel(x1_or_kernel, *args, **kwargs) raise NotImplementedError() parallel_fn.is_parallel = True parallel_fn.device_count = device_count return parallel_fn 
ant.AntEnv.physics @property def physics(self): """Return the MuJoCo physics model.""" if mujoco_py.get_version() >= '1.50': return self.sim else: return self.model 
texar.modules.decoders.beam_search_decode_test.BeamSearchDecodeTest.basic|rnn|given|test|decoder|state|initial def test_basic_rnn_decoder_given_initial_state(self): """Tests beam search with BasicRNNDecoder given initial state. """ hparams = {'rnn_cell': {'kwargs': {'num_units': self._cell_dim}}} decoder = tx.modules.BasicRNNDecoder(vocab_size=self._vocab_size, hparams=hparams) cell_state = decoder.cell.zero_state(self._batch_size, tf.float32) self._test_beam_search(decoder, initial_state=cell_state) tiled_cell_state = tile_batch(cell_state, multiplier=self._beam_width) self._test_beam_search(decoder, tiled_initial_state=tiled_cell_state, initiated=True) 
europilot.controllerstate.ControllerState.State|Controller def __init__(self): self.state = self.__init_dict() 
dp_sgd_autoencoder.grads|clip def clip_grads(w_ps_grads, max_bound, layerwise_clip): quad_sums = [tf.reduce_sum(w ** 2, axis=[1, 2]) for w in w_ps_grads] if layerwise_clip: grad_norms = [tf.sqrt(s) for s in quad_sums] norm_factors = [tf.minimum(max_bound / n, tf.ones(n.get_shape())) for n in grad_norms] w_ps_grads = [(w * f[:, (None), (None)]) for w, f in zip(w_ps_grads, norm_factors)] else: grad_norms = tf.sqrt(tf.add_n(quad_sums)) norm_factors = tf.minimum(max_bound / grad_norms, tf.ones( grad_norms.get_shape())) w_ps_grads = [(w * norm_factors[:, (None), (None)]) for w in w_ps_grads ] return w_ps_grads 
plato.agent.component.dialogue_policy.reinforcement_learning.q_policy.QPolicy.encode|state def encode_state(self, state): """ Encodes the dialogue state into an index used to address the Q matrix.  :param state: the state to encode :return: int - a unique state ID """ temp = [] temp += [int(b) for b in format(state.turn, '06b')] for value in state.slots_filled.values(): temp.append(1) if value else temp.append(0) for slot in self.ontology.ontology['requestable']: temp.append(1) if slot == state.requested_slot else temp.append(0) temp.append(int(state.is_terminal_state)) if state.item_in_focus: for slot in self.ontology.ontology['requestable']: if slot in state.item_in_focus and state.item_in_focus[slot]: temp.append(1) else: temp.append(0) else: temp += [0] * len(self.ontology.ontology['requestable']) if state.db_matches_ratio >= 0: temp += [int(b) for b in format(int(round(state.db_matches_ratio, 2 ) * 100), '07b')] else: temp += [int(b) for b in format(int(round(state.db_matches_ratio, 2 ) * 100), '07b')[1:]] temp.append(1) if state.system_made_offer else temp.append(0) if state.user_acts: temp += [int(b) for b in format(self.encode_action(state.user_acts, False), '05b')] else: temp += [0, 0, 0, 0, 0] if state.last_sys_acts: temp += [int(b) for b in format(self.encode_action([state. last_sys_acts[0]]), '04b')] else: temp += [0, 0, 0, 0] if state.user_goal: for c in self.ontology.ontology['informable']: if (c in state.user_goal.constraints and state.user_goal. constraints[c].value): temp.append(1) else: temp.append(0) for r in self.ontology.ontology['requestable']: if r in state.user_goal.requests and state.user_goal.requests[r ].value: temp.append(1) else: temp.append(0) else: temp += [0] * (len(self.ontology.ontology['informable']) + len(self .ontology.ontology['requestable'])) state_enc = 0 for t in temp: state_enc = state_enc << 1 | t return state_enc 
bert.tokenization.FullTokenizer.tokenize def tokenize(self, text): split_tokens = [] for token in self.basic_tokenizer.tokenize(text): for sub_token in self.wordpiece_tokenizer.tokenize(token): split_tokens.append(sub_token) return split_tokens 
svae_dc.svae_dc.SVAE_DC.SVAE|DC def __init__(self, pr, coder_type, latent_type, good_th, dyn_comp, device): super(SVAE_DC, self).__init__() self.latent_type = latent_type self.latent_seq_len = pr.K_tau self.y_size = pr.y_size self.register_buffer('xi_dim_weights', pr.xi_dim_weights) self.register_buffer('tau_dim_weights', torch.ones(pr.tau_size)) self.register_buffer('zero', torch.zeros(1)) self.register_buffer('one', torch.ones(1)) self.tau_dim_weights[-pr.y_size:] = 100.0 self.tau_dim_weights /= torch.sum(self.tau_dim_weights) self.good_th = good_th self.dyn_comp = dyn_comp logging.info('SVAE_DC using tau_dim_weights') logging.info(self.tau_dim_weights) self.register_buffer('small_logvars_tau', nets.LogvarLimitScheduler. MAX_LOGVAR * torch.ones(1, pr.tau_size)) if coder_type == 'conv': self.p_xi_gvn_taum = nets.ConvGauss(pr.tau_size - pr.y_size, pr. K_tau, pr.xi_size, pr.T_xi, pr.hidden_size, pr) self.p_y_gvn_psi = nets.LearnableGaussianDiagCondDistr(pr.y_size * pr.K_tau, pr.y_size, int(pr.hidden_size / 2), pr) else: print('Invalid/unimplemented encoder/decoder option', coder_type) assert False if latent_type == 'conv': self.p_tau_gvn_x = nets.ConvGauss(pr.x_size, 1, pr.tau_size, pr. K_tau, pr.hidden_size * 4, pr, n_conv_layers=4) elif latent_type == 'mlp': self.p_tau_gvn_x = nets.MlpGauss(pr.x_size, 1, pr.tau_size, pr. K_tau, pr.hidden_size * 8, pr) else: print('Invalid/unimplemented latent type option', latent_type) assert False if coder_type == 'conv': self.q_tau_gvn_xi = nets.ConvGauss(pr.xi_size + pr.y_size, pr.T_xi, pr.tau_size, pr.K_tau, pr.hidden_size, pr) else: print('Invalid/unimplemented encoder/decoder option', coder_type) assert False self.to(device) 
gym_pycolab.worlds.grid_worlds.empty|num def num_empty(level, shape): my_game_art = level empty = 0 row = 0 for i in range(shape[0]): row += 1 col = 0 for j in range(shape[1]): col += 1 if my_game_art[i][j] == INVERSE_REPAINT_MAPPING[C_BACKGROUND]: empty += 1 my_game_art[i] = list(my_game_art[i]) return empty 
print_and_read.printer|tab def tab_printer(log): """ Function to print the logs in a nice tabular format. """ t = Texttable() t.add_rows([['Epoch', log['losses'][-1][0]]]) print(t.draw()) t = Texttable() t.add_rows([['Loss', round(log['losses'][-1][1], 3)]]) print(t.draw()) t = Texttable() t.add_rows([['Modularity', round(log['cluster_quality'][-1][1], 3)]]) print(t.draw()) 
modeling_gpt2_test.GPT2ModelTest.GPT2ModelTester.double|gpt|check|heads|output def check_gpt2_double_heads_output(self, result): total_voc = self.vocab_size self.parent.assertListEqual(list(result['lm_logits'].size()), [self. batch_size, self.n_choices, self.seq_length, total_voc]) self.parent.assertListEqual(list(result['mc_logits'].size()), [self. batch_size, self.n_choices]) 
xlnet-master.tpu_estimator.TPUInfeedOutfeedSessionHook.end def end(self, session): self._finished = True logging.info('Stop infeed thread controller') self._infeed_controller.join() self._rendezvous.record_done('infeed') logging.info('Stop output thread controller') self._outfeed_controller.join() self._rendezvous.record_done('outfeed') logging.info('Shutdown TPU system.') session.run(self._finalize_ops) 
layers.BinaryConvolutionLayer.Convolution|Binary|Layer def __init__(self, out_dim, filter_size, strides=[1, 1, 1, 1], padding= 'SAME', ema_decay_rate=0.5, rho=0.1, dtype=tf.float32): """Initializer  Arguments: out_dim {int} -- Output dimension filter_size {int} -- filter size dimension (only square for now)  Keyword Arguments: strides {list (4)} -- Strides (default: {[1, 1, 1, 1]}) padding {'SAME', 'VALID', 'FULL'} -- padding (default: {'SAME'}) ema_decay_rate {float} -- decay rate (default: {0.5}) rho {float} -- regularization parameter (default: {1e-1}) dtype {tf dtype} -- data type (default: {tf.float32}) """ super(BinaryConvolutionLayer, self).__init__(out_dim, ema_decay_rate, rho, dtype) self.filter_size = filter_size self.strides = strides self.padding = padding self.name = 'Binary-Conv2D' 
PlotHeuristic.func def func(x, eps, c): return x + np.random.normal(c, eps, x.size) 
translate.translation_model.TranslationModel.training|init def init_training(self, sgd_after_n_epoch=None, **kwargs): self.read_data(**kwargs) self.epoch = self.batch_size * self.global_step // self.train_size global_step = self.global_step.eval() epoch = self.epoch.eval() if sgd_after_n_epoch is not None and epoch >= sgd_after_n_epoch: self.training.use_sgd = True else: self.training.use_sgd = False if kwargs.get('batch_mode') != 'random' and not kwargs.get('shuffle'): for _ in range(global_step): next(self.batch_iterator) self.training.time = 0 self.training.steps = 0 self.training.loss = 0 self.training.baseline_loss = 0 self.training.losses = [] self.training.last_decay = global_step self.training.scores = [] 
pvae.pixelvae_bbans_two_layer.net|partial|gen def gen_net2_partial(z1): return session.run(theta, feed_dict={latents1: z1, bn_is_training: False, bn_stats_iter: 0, total_iters: 99999}) 
craystack.codecs.parallel.push def push(message, symbols): assert len(symbols) == len(codecs) for (push, _), symbol in reversed(list(zip(codecs, symbols))): message = push(message, symbol) return message 
modeling_openai_test.OpenAIGPTModelTest.string|to|config|test|json def test_config_to_json_string(self): config = OpenAIGPTConfig(vocab_size_or_config_json_file=99, n_embd=37) obj = json.loads(config.to_json_string()) self.assertEqual(obj['vocab_size'], 99) self.assertEqual(obj['n_embd'], 37) 
grounding_concepts.test def test(): nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser', 'textcat']) nlp.add_pipe(nlp.create_pipe('sentencizer')) res = match_mentioned_concepts(nlp, sents=[ 'Sometimes people say that someone stupid has no swimming pool.'], answers=['swimming pool']) print(res) 
base_model.Base.calc|dloss def calc_dloss(self, x, y, tx, ty, ux, vy, config): raise NotImplementedError('Please Implement this method') 
commons.measure_utils.get|func|tv|inpaint def get_inpaint_func_tv():  def inpaint_func(image, mask): """Total variation inpainting""" inpainted = np.zeros_like(image) for c in range(image.shape[2]): image_c = image[:, :, (c)] mask_c = mask[:, :, (c)] if np.min(mask_c) > 0: inpainted[:, :, (c)] = image_c else: h, w = image_c.shape inpainted_c_var = cvxpy.Variable(h, w) obj = cvxpy.Minimize(cvxpy.tv(inpainted_c_var)) constraints = [cvxpy.mul_elemwise(mask_c, inpainted_c_var) == cvxpy.mul_elemwise(mask_c, image_c)] prob = cvxpy.Problem(obj, constraints) prob.solve() inpainted[:, :, (c)] = inpainted_c_var.value return inpainted return inpaint_func 
neural_tangents.stax.randn def _randn(stddev=0.01): """`jax.experimental.stax.randn` for implicitly-typed results."""  def init(rng, shape): return stddev * random.normal(rng, shape) return init 
classifier_mnist_task_switch_train.crossentropy|masked def masked_crossentropy(y_true, y_pred): y_pred = mask_logits_to_softmax(y_true, y_pred) return keras.losses.categorical_crossentropy(y_true, y_pred) 
operations.get|var|maybe|avg def get_var_maybe_avg(var_name, ema, **kwargs): """ utility for retrieving polyak averaged params """ v = tf.get_variable(var_name, **kwargs) if ema is not None: v = ema.average(v) return v 
avod.datasets.kitti.kitti_dataset.KittiDataset.image|bev|dir @property def bev_image_dir(self): raise NotImplementedError('BEV images not saved to disk yet!') 
house_parser.Pano.get|available|headings def get_available_headings(self): return [image.heading for image in self.images if image.camera_index == 0] 
neural_tangents.utils.batch.scan def _scan(f, init, xs, store_on_device): """Implements an unrolled version of scan.  Based on `jax.lax.scan` and has an identical API.  TODO: We introduce this function because lax.scan currently has a higher peak memory usage than the unrolled version. We will aim to swap this out for lax.scan when issue #1273 and related have been resolved. """ stack = np.stack if store_on_device else jit(np.stack, backend='cpu') carry = init ys = [] for x in xs: carry, y = f(carry, x) ys += [y] return carry, tree_multimap(lambda *y: stack(y), *ys) 
train_app.encoder|create def _create_encoder(preprocess_fn, network_factory, image_shape, batch_size =32, session=None, checkpoint_path=None, read_from_file=False): if read_from_file: num_channels = image_shape[-1] if len(image_shape) == 3 else 1 input_var = tf.placeholder(tf.string, (None,)) image_var = tf.map_fn(lambda x: tf.image.decode_jpeg(tf.read_file(x ), channels=num_channels), input_var, back_prop=False, dtype=tf .uint8) image_var = tf.image.resize_images(image_var, image_shape[:2]) else: input_var = tf.placeholder(tf.uint8, (None,) + image_shape) image_var = input_var preprocessed_image_var = tf.map_fn(lambda x: preprocess_fn(x, is_training=False), image_var, back_prop=False, dtype=tf.float32) feature_var, _ = network_factory(preprocessed_image_var) feature_dim = feature_var.get_shape().as_list()[-1] if session is None: session = tf.Session() if checkpoint_path is not None: tf.train.get_or_create_global_step() init_assign_op, init_feed_dict = slim.assign_from_checkpoint( checkpoint_path, slim.get_model_variables()) session.run(init_assign_op, feed_dict=init_feed_dict)  def encoder(data_x): out = np.zeros((len(data_x), feature_dim), np.float32) queued_trainer.run_in_batches(lambda x: session.run(feature_var, feed_dict=x), {input_var: data_x}, out, batch_size) return out return encoder 
gym_pycolab.tests.engine_test.EngineTest.With|Default|Reward|Discount|test|And|Episode|End def testRewardAndEpisodeEndWithDefaultDiscount(self): """Game entities can assign reward, terminate game with default discount.""" self._do_test_reward_and_episode_end(expected_discount=0.0, q_pre_update=lambda actions, board, layers, backdrop, things, the_plot: the_plot.terminate_episode()) 
avod.core.feature_extractors.img_vgg.ImgVgg.build def build(self, inputs, input_pixel_size, is_training, scope='img_vgg'): """ Modified VGG for image feature extraction.  Note: All the fully_connected layers have been transformed to conv2d layers and are implemented in the main model.  Args: inputs: a tensor of size [batch_size, height, width, channels]. input_pixel_size: size of the input (H x W) is_training: True for training, False fo validation/testing. scope: Optional scope for the variables.  Returns: The last op containing the log predictions and end_points dict. """ vgg_config = self.config with slim.arg_scope(self.vgg_arg_scope(weight_decay=vgg_config. l2_weight_decay)): with tf.variable_scope(scope, 'img_vgg', [inputs]) as sc: end_points_collection = sc.name + '_end_points' with slim.arg_scope([slim.conv2d, slim.fully_connected, slim. max_pool2d], outputs_collections=end_points_collection): net = slim.repeat(inputs, vgg_config.vgg_conv1[0], slim. conv2d, vgg_config.vgg_conv1[1], [3, 3], normalizer_fn= slim.batch_norm, normalizer_params={'is_training': is_training}, scope='conv1') net = slim.max_pool2d(net, [2, 2], scope='pool1') net = slim.repeat(net, vgg_config.vgg_conv2[0], slim.conv2d, vgg_config.vgg_conv2[1], [3, 3], normalizer_fn=slim. batch_norm, normalizer_params={'is_training': is_training}, scope='conv2') net = slim.max_pool2d(net, [2, 2], scope='pool2') net = slim.repeat(net, vgg_config.vgg_conv3[0], slim.conv2d, vgg_config.vgg_conv3[1], [3, 3], normalizer_fn=slim. batch_norm, normalizer_params={'is_training': is_training}, scope='conv3') net = slim.max_pool2d(net, [2, 2], scope='pool3') net = slim.repeat(net, vgg_config.vgg_conv4[0], slim.conv2d, vgg_config.vgg_conv4[1], [3, 3], normalizer_fn=slim. batch_norm, normalizer_params={'is_training': is_training}, scope='conv4') with tf.variable_scope('upsampling'): downsampling_factor = 8 downsampled_shape = input_pixel_size / downsampling_factor upsampled_shape = (downsampled_shape * vgg_config. upsampling_multiplier) feature_maps_out = tf.image.resize_bilinear(net, upsampled_shape) end_points = slim.utils.convert_collection_to_dict( end_points_collection) return feature_maps_out, end_points 
cpplint.Check|Value|R|Reference def CheckRValueReference(filename, clean_lines, linenum, nesting_state, error): """Check for rvalue references.  Args: filename: The name of the current file. clean_lines: A CleansedLines instance containing the file. linenum: The number of the line to check. nesting_state: A NestingState instance which maintains information about the current stack of nested blocks being parsed. error: The function to call with any errors found. """ line = clean_lines.elided[linenum] match = Match('^(.*\\S)&&', line) if not match: match = Match('(.*)&&\\S', line) if not match or '(&&)' in line or Search('\\boperator\\s*$', match.group(1) ): return typenames = GetTemplateArgs(clean_lines, linenum) and_pos = len(match.group(1)) if IsRValueType(typenames, clean_lines, nesting_state, linenum, and_pos): if not IsRValueAllowed(clean_lines, linenum, typenames): error(filename, linenum, 'build/c++11', 3, 'RValue references are an unapproved C++ feature.') else: error(filename, linenum, 'whitespace/operators', 3, 'Missing spaces around &&') 
opt.warmup|cosine def warmup_cosine(x, warmup=0.002): s = 1 if x <= warmup else 0 return s * (x / warmup) + (1 - s) * (0.5 * (1 + torch.cos(math.pi * x))) 
sensor_correction.utils.outliers|mask def mask_outliers(array, spread=1.5): """Identifiy outliers using interquatile range statistics.""" a = array.ravel() q = np.percentile(a.ravel(), [25, 50, 75]) qrange = q[-1] - q[0] outliers = (a > q[1] + spread * qrange) | (a < q[1] - spread * qrange) return outliers.reshape(array.shape) 
regression.misc.layers.NormalPrior.pws def pws(self, n_particles): if not hasattr(self, '_pws'): self._p_samples(n_particles) assert_same_num = tf.assert_equal(n_particles, tf.shape(self._pws)[0], message='n_particles must be the same with the previous one') with tf.control_dependencies([assert_same_num]): return self._pws 
cleverhans.devtools.tests.docscrape.NumpyDocString.str def __str__(self): out = [] out += self._str_signature() out += self._str_summary() out += self._str_extended_summary() for param_list in ('Parameters', 'Other Parameters', 'Returns', 'Raises', 'Warns'): out += self._str_param_list(param_list) out += self._str_see_also() for s in ('Notes', 'References', 'Examples'): out += self._str_section(s) out += self._str_index() return '\n'.join(out) 
models.attacks.ElasticNetMethod.generate.ead|wrap def ead_wrap(x_val, y_val): return np.array(attack.attack(x_val, y_val), dtype=np.float32) 
sidd_utils.divide|parts def divide_parts(n, n_parts): """divide a number into a list of parts""" div, rem = divmod(n, n_parts) divs = [div] * n_parts if rem != 0: for r in range(rem): divs[r] += 1 return divs 
texar.modules.decoders.tf_helpers.GreedyEmbeddingHelper.inputs|next def next_inputs(self, time, outputs, state, sample_ids, name=None, reach_max_time=None): """Gets the inputs for next step.""" finished = math_ops.equal(sample_ids, self._end_token) all_finished = math_ops.reduce_all(finished) if reach_max_time is not None: all_finished = tf.logical_or(all_finished, reach_max_time) if self._embedding_args_cnt == 1: del time, outputs next_inputs = control_flow_ops.cond(all_finished, lambda : self. _start_inputs, lambda : self._embedding_fn(sample_ids)) elif self._embedding_args_cnt == 2: del outputs times = tf.ones(self._batch_size, dtype=tf.int32) * (time + 1) next_inputs = control_flow_ops.cond(all_finished, lambda : self. _start_inputs, lambda : self._embedding_fn(sample_ids, times)) return finished, next_inputs, state 
pvae.tflib.ops.batchnorm.Batchnorm.force|updates def _force_updates(): """Internal function forces updates moving_vars if is_training.""" float_stats_iter = tf.cast(stats_iter, tf.float32) update_moving_mean = tf.assign(moving_mean, float_stats_iter / ( float_stats_iter + 1) * moving_mean + 1 / (float_stats_iter + 1) * batch_mean) update_moving_variance = tf.assign(moving_variance, float_stats_iter / (float_stats_iter + 1) * moving_variance + 1 / (float_stats_iter + 1) * batch_var) with tf.control_dependencies([update_moving_mean, update_moving_variance]): return tf.identity(outputs) 
src.model.cnn.random|var def var_random(name, shape, regularizable=False): """ Initialize a random variable using xavier initialization. Add regularization if regularizable=True :param name: :param shape: :param regularizable: :return: """ v = tf.get_variable(name, shape=shape, initializer=tf.contrib.layers. xavier_initializer()) if regularizable: with tf.name_scope(name + '/Regularizer/'): tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, tf.nn. l2_loss(v)) return v 
utils.get|data|train def get_train_data(data_path, label_path, label_path_local, rate): train_data, train_label = [], [] test_data, test_label = [], [] label = sio.loadmat(label_path)['clas1'] label_local = sio.loadmat(label_path_local)['clas1'] row, col = label.shape get_random_sample(label, label_local) 
gym_bullet_extensions.test_reach_targets.reach|eepos def reach_eepos(robot, tgt_ee_pos, tgt_ee_quat, fing_dist, tgt_viz_id, num_steps=100): tgt_ee_pos = np.array(tgt_ee_pos) tgt_ee_quat = np.array(tgt_ee_quat) print('tgt_ee_pos', tgt_ee_pos, 'fing_dist', fing_dist) robot.sim.resetBasePositionAndOrientation(tgt_viz_id, tgt_ee_pos, tgt_ee_quat) qpos = robot._ee_pos_to_qpos_raw(tgt_ee_pos, tgt_ee_quat, fing_dist) print('target qpos', qpos) qpos_ok = robot.ee_pos_to_qpos(tgt_ee_pos, tgt_ee_quat, fing_dist) print('qpos_ok', qpos_ok) robot.refresh_viz() input('----------------------- Press Enter to reset_to_qpos()') robot.reset_to_qpos(qpos) robot.refresh_viz() input('-------------------------------------------- Press Enter to reset()' ) robot.reset() robot.refresh_viz() input('---------------- Press Enter to do pybullet PD control') tgt_qpos = qpos tgt_qvel = np.zeros_like(tgt_qpos) for i in range(num_steps): robot.move_to_qposvel(tgt_qpos, tgt_qvel, mode=pybullet.PD_CONTROL, kp=100.0, kd=10.0) robot.refresh_viz() print('Final qpos', robot.get_qpos()) ee_pos, ee_quat, ee_vel, _ = robot.get_ee_pos_ori_vel() ee_euler = robot.sim.getEulerFromQuaternion(ee_quat) tgt_ee_euler = robot.sim.getEulerFromQuaternion(tgt_ee_quat) print('Final ee pos quat vel', ee_pos, ee_quat, ee_vel) print('tgt_ee_euler', tgt_ee_euler, '\nvs final    ', ee_euler) input('---------------- Done. Press Enter to go on') robot.reset() robot.refresh_viz() 
AffineCouplingSdnEx4.AffineCouplingSdnEx4.log|and|inverse|det|jacobian def _inverse_and_log_det_jacobian(self, y, yy, nlf0=None, nlf1=None, iso= None, cam=None): scale = sdn_model_params_ex4(yy, iso, self.gain_init) x = y if scale is not None: x /= scale if scale is None: log_abs_det_J_inv = tf.constant(0.0, dtype=y.dtype, name='ildj') else: log_abs_det_J_inv = -tf.reduce_sum(tf.log(scale), axis=[1, 2, 3]) if self._last_layer: return tf.layers.flatten(x), log_abs_det_J_inv return x, log_abs_det_J_inv 
avod.core.evaluator.Evaluator.get|accuracy|cls def get_cls_accuracy(self, predictions, eval_avod_losses, eval_rpn_losses, global_step): """Updates the calculated accuracies for rpn and avod losses.  Args: predictions: A dictionary containing the model outputs. eval_avod_losses: A dictionary containing all the avod averaged losses. eval_rpn_losses: A dictionary containing all the rpn averaged losses. global_step: Current global step that is being evaluated. """ objectness_pred = predictions[RpnModel.PRED_MB_OBJECTNESS] objectness_gt = predictions[RpnModel.PRED_MB_OBJECTNESS_GT] objectness_accuracy = self.calculate_cls_accuracy(objectness_pred, objectness_gt) sum_rpn_obj_accuracy = eval_rpn_losses[KEY_SUM_RPN_OBJ_ACC] sum_rpn_obj_accuracy += objectness_accuracy eval_rpn_losses.update({KEY_SUM_RPN_OBJ_ACC: sum_rpn_obj_accuracy}) print('Step {}: RPN Objectness Accuracy: {}'.format(global_step, objectness_accuracy)) if self.full_model: classification_pred = predictions[AvodModel. PRED_MB_CLASSIFICATION_SOFTMAX] classification_gt = predictions[AvodModel.PRED_MB_CLASSIFICATIONS_GT] classification_accuracy = self.calculate_cls_accuracy( classification_pred, classification_gt) sum_avod_cls_accuracy = eval_avod_losses[KEY_SUM_AVOD_CLS_ACC] sum_avod_cls_accuracy += classification_accuracy eval_avod_losses.update({KEY_SUM_AVOD_CLS_ACC: sum_avod_cls_accuracy}) print('Step {}: AVOD Classification Accuracy: {}'.format( global_step, classification_accuracy)) 
model.graph|rpn def rpn_graph(feature_map, anchors_per_location, anchor_stride): """Builds the computation graph of Region Proposal Network.  feature_map: backbone features [batch, height, width, depth] anchors_per_location: number of anchors per pixel in the feature map anchor_stride: Controls the density of anchors. Typically 1 (anchors for every pixel in the feature map), or 2 (every other pixel).  Returns: rpn_logits: [batch, H, W, 2] Anchor classifier logits (before softmax) rpn_probs: [batch, W, W, 2] Anchor classifier probabilities. rpn_bbox: [batch, H, W, (dy, dx, log(dh), log(dw))] Deltas to be applied to anchors. """ shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu', strides=anchor_stride, name='rpn_conv_shared')(feature_map) x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid', activation='linear', name='rpn_class_raw')(shared) rpn_class_logits = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], - 1, 2]))(x) rpn_probs = KL.Activation('softmax', name='rpn_class_xxx')(rpn_class_logits ) x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding='valid', activation='linear', name='rpn_bbox_pred')(shared) rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x) return [rpn_class_logits, rpn_probs, rpn_bbox] 
classification.network.vgg.vgg @register_model('vgg16') def vgg16(inputs, sampler, is_training, batch_norm, layer_collection, particles ): return vgg(inputs, sampler, is_training, batch_norm, layer_collection, particles, [2, 2, 3, 3, 3]) 
kaffe.graph.NodeMapper.node|map def map_node(self, node): map_func = self.get_handler(node.kind, 'map') mapped_node = map_func(node) assert mapped_node is not None mapped_node.node = node return mapped_node 
snresnet.SNResNetConcatDiscriminator.call def __call__(self, x, y=None): h = x h = self.block1(h) h = self.block2(h) h = self.block3(h) if y is not None: emb = self.l_y(y) H, W = h.shape[2], h.shape[3] emb = F.broadcast_to(F.reshape(emb, (emb.shape[0], emb.shape[1], 1, 1)), (emb.shape[0], emb.shape[1], H, W)) h = F.concat([h, emb], axis=1) h = self.block4(h) h = self.block5(h) h = self.block6(h) h = self.activation(h) h = F.sum(h, axis=(2, 3)) output = self.l7(h) return output 
acwgangp.ACWGANGP.load def load(self, checkpoint_dir): import re print(' [*] Reading checkpoints...') checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir, self. model_name) ckpt = tf.train.get_checkpoint_state(checkpoint_dir) if ckpt and ckpt.model_checkpoint_path: ckpt_name = os.path.basename(ckpt.model_checkpoint_path) self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name)) counter = int(next(re.finditer('(\\d+)(?!.*\\d)', ckpt_name)).group(0)) print(' [*] Success to read {}'.format(ckpt_name)) return True, counter else: print(' [*] Failed to find a checkpoint') return False, 0 
AffineCouplingSdnEx5.AffineCouplingSdnEx5.log|and|inverse|det|jacobian def _inverse_and_log_det_jacobian(self, y, yy, nlf0=None, nlf1=None, iso= None, cam=None): scale = sdn_model_params_ex5(yy, iso, self.gain_init, cam, self.param_inits ) x = y if scale is not None: x /= scale if scale is None: log_abs_det_J_inv = tf.constant(0.0, dtype=y.dtype, name='ildj') else: log_abs_det_J_inv = -tf.reduce_sum(tf.log(scale), axis=[1, 2, 3]) if self._last_layer: return tf.layers.flatten(x), log_abs_det_J_inv return x, log_abs_det_J_inv 
tf_util.d|pool|avg def avg_pool2d(inputs, kernel_size, scope, stride=[2, 2], padding='VALID'): """ 2D avg pooling.  Args: inputs: 4-D tensor BxHxWxC kernel_size: a list of 2 ints stride: a list of 2 ints  Returns: Variable tensor """ with tf.variable_scope(scope) as sc: kernel_h, kernel_w = kernel_size stride_h, stride_w = stride outputs = tf.nn.avg_pool(inputs, ksize=[1, kernel_h, kernel_w, 1], strides=[1, stride_h, stride_w, 1], padding=padding, name=sc.name) return outputs 
pytorch_pretrained_bert.modeling.BertConfig.repr def __repr__(self): return str(self.to_json_string()) 
avod.core.summary_utils.maps|feature|add def add_feature_maps(feature_maps, layer_name): """ Adds an image summary showing tiled feature maps  Args: feature_maps: a tensor of feature maps to show, dimensions should be (1, ?, ?, ?) (batch_size, height, width, depth) layer_name: name of the layer which will show up in tensorboard """ with tf.name_scope(layer_name): batch, maps_height, maps_width, num_maps = np.array(feature_maps.shape ).astype(np.int32) map_width_out = 300 ratio = map_width_out / maps_width map_height_out = int(maps_height * ratio) map_size_out = tf.convert_to_tensor([map_height_out, map_width_out], tf.int32) resized_maps = tf.image.resize_bilinear(feature_maps, map_size_out) output = tf.slice(resized_maps, (0, 0, 0, 0), (1, -1, -1, -1)) output = tf.reshape(output, (map_height_out, map_width_out, num_maps)) map_width_out += 5 map_height_out += 5 output = tf.image.resize_image_with_crop_or_pad(output, map_height_out, map_width_out) map_sizes = [1, 32, 64, 128, 256, 512] image_sizes = [(1, 1), (4, 8), (8, 8), (8, 16), (8, 32), (16, 32)] size_idx = map_sizes.index(num_maps) desired_image_size = image_sizes[size_idx] image_width = desired_image_size[0] image_height = desired_image_size[1] output = tf.reshape(output, (map_height_out, map_width_out, image_height, image_width)) output = tf.transpose(output, (2, 0, 3, 1)) output = tf.reshape(output, (1, image_height * map_height_out, image_width * map_width_out, 1)) layer_name = layer_name.split('/')[-1] tf.summary.image(layer_name, output, max_outputs=16) 
vkge.training.util.get|latent|distributions def get_latent_distributions(distribution, mu_s, mu_p, mu_o, log_sigma_sq_s, log_sigma_sq_p, log_sigma_sq_o): """ Returns tf distributions for the generative network """ if distribution == 'normal': q_s = tfd.MultivariateNormalDiag(mu_s, distribution_scale( log_sigma_sq_s)) q_p = tfd.MultivariateNormalDiag(mu_p, distribution_scale( log_sigma_sq_p)) q_o = tfd.MultivariateNormalDiag(mu_o, distribution_scale( log_sigma_sq_o)) elif distribution == 'vmf': q_s = VonMisesFisher(mu_s, distribution_scale(log_sigma_sq_s) + 1) q_p = VonMisesFisher(mu_p, distribution_scale(log_sigma_sq_p) + 1) q_o = VonMisesFisher(mu_o, distribution_scale(log_sigma_sq_o) + 1) else: raise NotImplemented return q_s, q_p, q_o 
infer.to|text def to_text(vocab_list, sample_ids): sym_list = [vocab_list[x] for x in sample_ids] + [utils.EOS] return args.delimiter.join(sym_list[:sym_list.index(utils.EOS)]) 
utils.HybridKmeans_implementation.implementation|Hybrid|Kmeans def __init__(self, k=2, delta=1e-06, max_iter=100): self.k_ = k self.delta_ = delta self.max_iter_ = max_iter self.cluster_centers_ = None self.closest_centers_ = None self.inertia_ = self.delta_ self.iter_ = 0 
translate.import_graph.ImportGraph.Graph|Import def __init__(self, loc, model_path=None): self.graph = tf.Graph() self.sess = tf.Session(graph=self.graph) with self.graph.as_default(): ckpt = tf.train.get_checkpoint_state(loc) if ckpt and ckpt.model_checkpoint_path: if model_path is None: ckpt_name = ckpt.model_checkpoint_path else: ckpt_name = loc + '/' + model_path self.saver = tf.train.import_meta_graph(ckpt_name + '.meta') self.saver.restore(self.sess, ckpt_name) 
data_loader.SyntheticDataGenerator.get|batch def get_batch(self): raise NotImplementedError() 
texar.data.data.dataset_utils_test.TransformationTest.test_make_chained_transformation.c|tran def _tran_c(data): return data + 10000 
PlotFunc.f def f3(c, L): return r * 2 ** (c * L) / (1 - 2 ** -c) 
borealisflows.layers.BatchNorm.denormalize def _denormalize(self, x): return x * tf.sqrt(self.train_v + self.eps) + self.train_m 
train_inception_v3_baseline_1.go def go(start_epoch, end_epoch, run_name, weights_file, profile_compute_time_every_n_steps, save_summary_info_every_n_steps, log_annotated_images, image_size, batch_size, num_gpus, data_dir, black_list_file, log_dir, do_validate_all, do_validate_nbl): tf.reset_default_graph() out = Output(log_dir, run_name, profile_compute_time_every_n_steps, save_summary_info_every_n_steps) out.log_msg('Setting up data feeds...') training_dataset = DataSet('train', image_size, batch_size, num_gpus, data_dir, None) validation_dataset = DataSet('validation', image_size, batch_size, num_gpus, data_dir, black_list_file) training_data = train_inputs(training_dataset, log_annotated_images) validation_data = eval_inputs(validation_dataset, log_annotated_images) nbl_val_data = non_blacklisted_eval_inputs(validation_dataset, log_annotated_images) training_steps = training_dataset.training_batches_per_epoch() validation_steps = validation_dataset.validation_batches_per_epoch() nbl_val_steps = validation_dataset.nbl_validation_batches_per_epoch() with tf.device('/device:CPU:0'): with tf.name_scope('input/placeholders'): is_training_ph = tf.placeholder(tf.bool) is_validating_nbl_ph = tf.placeholder(tf.bool) global_step = tf.train.get_or_create_global_step() decay_steps = int(training_steps * 2.0) learning_rate_op = tf.train.exponential_decay(0.045, global_step, decay_steps, 0.94, staircase=True) opt = tf.train.RMSPropOptimizer(learning_rate_op, decay=0.9, momentum=0.9, epsilon=1.0) train_op, loss_op, acc_top_1_op, acc_top_5_op = run_towers(opt, global_step, is_training_ph, is_validating_nbl_ph, training_data, validation_data, nbl_val_data, DataSet. num_classes(), num_gpus) out.log_msg('Starting Session...') with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess: out.set_session_graph(sess.graph) if weights_file is not None: out.log_msg('Restoring weights file: {}'.format(weights_file)) tf.train.Saver().restore(sess, weights_file) else: tf.global_variables_initializer().run() coord = tf.train.Coordinator() threads = tf.train.start_queue_runners(sess=sess, coord=coord) try: for e in range(start_epoch, end_epoch + 1): train(out, sess, e, training_steps, train_op, loss_op, global_step, learning_rate_op, is_training_ph, is_validating_nbl_ph) if do_validate_all: validate(out, sess, e, validation_steps, loss_op, acc_top_1_op, acc_top_5_op, global_step, learning_rate_op, is_training_ph, is_validating_nbl_ph) if do_validate_nbl: validate_nbl(out, sess, e, nbl_val_steps, loss_op, acc_top_1_op, acc_top_5_op, global_step, learning_rate_op, is_training_ph, is_validating_nbl_ph) except tf.errors.OutOfRangeError: out.log_msg('Finished.') finally: coord.request_stop() coord.join(threads) sess.close() out.close_files() 
input_data_act_polar.InputData.batch|scan|next def next_batch_scan(self, batch_size): if self.__cur_test_id >= self.valNum: self.__cur_test_id = 0 return None, None, None elif self.__cur_test_id + batch_size >= self.valNum: batch_size = self.valNum - self.__cur_test_id if self.polar: batch_sat = np.zeros([batch_size, 112, 616, 3], dtype=np.float32) else: batch_sat = np.zeros([batch_size, 256, 256, 3], dtype=np.float32) batch_grd = np.zeros([batch_size, 112, 616, 3], dtype=np.float32) batch_utm = np.zeros([batch_size, 2], dtype=np.float32) batch_dis_utm = np.zeros([batch_size, batch_size, 1], dtype=np.float32) for i in range(batch_size): img_idx = self.__cur_test_id + i img = cv2.imread(self.valList[img_idx][4]) if self.polar: if img is None or img.shape[0] != self.panoRows or img.shape[1 ] != self.panoCols: print('InputData::next_pair_batch: read fail: %s, %d, ' % ( self.valList[img_idx][4], i)) continue elif img is None or img.shape[0] != img.shape[1]: print('InputData::next_pair_batch: read fail: %s, %d, ' % (self .valList[img_idx][4], i)) continue img = img.astype(np.float32) img[:, :, (0)] -= 103.939 img[:, :, (1)] -= 116.779 img[:, :, (2)] -= 123.6 batch_sat[(i), :, :, :] = img img = cv2.imread(self.valList[img_idx][1]) if img is None: print('InputData::next_pair_batch: read fail: %s, %d, ' % (self .valList[img_idx][2], i)) continue img = cv2.resize(img, (616, 112), interpolation=cv2.INTER_AREA) img = img.astype(np.float32) img[:, :, (0)] -= 103.939 img[:, :, (1)] -= 116.779 img[:, :, (2)] -= 123.6 batch_grd[(i), :, :, :] = img batch_utm[i, 0] = self.valUTM[0, img_idx] batch_utm[i, 1] = self.valUTM[1, img_idx] self.__cur_test_id += batch_size for ih in range(batch_size): for jh in range(batch_size): batch_dis_utm[ih, jh, 0] = (batch_utm[ih, 0] - batch_utm[jh, 0] ) * (batch_utm[ih, 0] - batch_utm[jh, 0]) + (batch_utm[ih, 1] - batch_utm[jh, 1]) * (batch_utm[ih, 1] - batch_utm[jh, 1]) return batch_sat, batch_grd, batch_dis_utm 
official.resnet.imagenet_test.BaseTest.shape|v|test|imagenetmodel def test_imagenetmodel_shape_v1(self): self._test_imagenetmodel_shape(resnet_version=1) 
nets.vgg_test.VGG19Test.test|Global|Pool def testGlobalPool(self): batch_size = 1 height, width = 256, 256 num_classes = 1000 with self.test_session(): inputs = tf.random_uniform((batch_size, height, width, 3)) logits, _ = vgg.vgg_19(inputs, num_classes, spatial_squeeze=False, global_pool=True) self.assertEquals(logits.op.name, 'vgg_19/fc8/BiasAdd') self.assertListEqual(logits.get_shape().as_list(), [batch_size, 1, 1, num_classes]) 
texar.modules.classifiers.bert_classifier_test.BertClassifierTest.test|trainable|variables def test_trainable_variables(self): """Tests the functionality of automatically collecting trainable variables. """ inputs = tf.placeholder(dtype=tf.int32, shape=[None, None]) clas = BertClassifier() _, _ = clas(inputs) self.assertEqual(len(clas.trainable_variables), 199 + 2) hparams = {'clas_strategy': 'all_time', 'max_seq_length': 8} clas = BertClassifier(hparams=hparams) _, _ = clas(inputs) self.assertEqual(len(clas.trainable_variables), 199 + 2) hparams = {'clas_strategy': 'time_wise'} clas = BertClassifier(hparams=hparams) _, _ = clas(inputs) self.assertEqual(len(clas.trainable_variables), 199 + 2) 
embeddings.OpenKE.config.Config.Config.get|by|parameters|name def get_parameters_by_name(self, var_name): with self.graph.as_default(): with self.sess.as_default(): if var_name in self.trainModel.parameter_lists: return self.sess.run(self.trainModel.parameter_lists[var_name]) else: return None 
bert-master.create_pretraining_data.document|from|instances|create def create_instances_from_document(all_documents, document_index, max_seq_length, short_seq_prob, masked_lm_prob, max_predictions_per_seq, vocab_words, rng): """Creates `TrainingInstance`s for a single document.""" document = all_documents[document_index] max_num_tokens = max_seq_length - 3 target_seq_length = max_num_tokens if rng.random() < short_seq_prob: target_seq_length = rng.randint(2, max_num_tokens) instances = [] current_chunk = [] current_length = 0 i = 0 while i < len(document): segment = document[i] current_chunk.append(segment) current_length += len(segment) if i == len(document) - 1 or current_length >= target_seq_length: if current_chunk: a_end = 1 if len(current_chunk) >= 2: a_end = rng.randint(1, len(current_chunk) - 1) tokens_a = [] for j in range(a_end): tokens_a.extend(current_chunk[j]) tokens_b = [] is_random_next = False if len(current_chunk) == 1 or rng.random() < 0.5: is_random_next = True target_b_length = target_seq_length - len(tokens_a) for _ in range(10): random_document_index = rng.randint(0, len( all_documents) - 1) if random_document_index != document_index: break random_document = all_documents[random_document_index] random_start = rng.randint(0, len(random_document) - 1) for j in range(random_start, len(random_document)): tokens_b.extend(random_document[j]) if len(tokens_b) >= target_b_length: break num_unused_segments = len(current_chunk) - a_end i -= num_unused_segments else: is_random_next = False for j in range(a_end, len(current_chunk)): tokens_b.extend(current_chunk[j]) truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng) assert len(tokens_a) >= 1 assert len(tokens_b) >= 1 tokens = [] segment_ids = [] tokens.append('[CLS]') segment_ids.append(0) for token in tokens_a: tokens.append(token) segment_ids.append(0) tokens.append('[SEP]') segment_ids.append(0) for token in tokens_b: tokens.append(token) segment_ids.append(1) tokens.append('[SEP]') segment_ids.append(1) tokens, masked_lm_positions, masked_lm_labels = ( create_masked_lm_predictions(tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)) instance = TrainingInstance(tokens=tokens, segment_ids= segment_ids, is_random_next=is_random_next, masked_lm_positions=masked_lm_positions, masked_lm_labels=masked_lm_labels) instances.append(instance) current_chunk = [] current_length = 0 i += 1 return instances 
process_data.clean|str|sst def clean_str_sst(string): string = re.sub("[^A-Za-z0-9(),!?\\'\\`]", ' ', string) string = re.sub('\\s{2,}', ' ', string) return string.strip().lower() 
commons.ops.linear def linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False): shape = input_.get_shape().as_list() with tf.variable_scope(scope or 'Linear'): matrix = tf.get_variable('Matrix', [shape[1], output_size], tf. float32, tf.random_normal_initializer(stddev=stddev)) bias = tf.get_variable('bias', [output_size], initializer=tf. constant_initializer(bias_start)) if with_w: return tf.matmul(input_, matrix) + bias, matrix, bias else: return tf.matmul(input_, matrix) + bias 
neural_tangents.utils.batch.scan def _scan(f, init, xs, store_on_device): """Implements an unrolled version of scan.  Based on `jax.lax.scan` and has an identical API.  TODO(schsam): We introduce this function because lax.scan currently has a higher peak memory usage than the unrolled version. We will aim to swap this out for lax.scan when issue #1273 and related have been resolved. """ stack = np.stack if store_on_device else jit(np.stack, backend='cpu') carry = init ys = [] for x in xs: carry, y = f(carry, x) ys += [y] return carry, tree_multimap(lambda *y: stack(y), *ys) 
celebA.gen.wgan_utils.grid|imshow|save def save_imshow_grid(images, logs_dir, filename, shape): """ Plot images in a grid of a given shape. """ fig = plt.figure(1) grid = ImageGrid(fig, 111, nrows_ncols=shape, axes_pad=0.05) size = shape[0] * shape[1] for i in trange(size, desc='Saving images'): grid[i].axis('off') grid[i].imshow(images[i]) plt.savefig(os.path.join(logs_dir, filename)) 
examples.datasets.normalize|flatten|partial|and def _partial_flatten_and_normalize(x): """Flatten all but the first dimension of an `np.ndarray`.""" x = np.reshape(x, (x.shape[0], -1)) return (x - np.mean(x)) / np.std(x) 
utilFunctions.LorentzVector.Delta|R def DeltaR(self, other): deta = self.Eta() - other.Eta() dphi = self.Phi() - other.Phi() pi = math.pi while dphi > pi: dphi -= 2 * pi while dphi < -pi: dphi += 2 * pi return math.sqrt(deta * deta + dphi * dphi) 
nets.ESRGAN.ESRGAN_model.model|ESRGAN def __init__(self, lr_shape, hr_shape, SCALE=4): self.SCALE = SCALE self.lr_shape, self.hr_shape = lr_shape, hr_shape self.lr_height, self.lr_width, self.channels = lr_shape self.hr_height, self.hr_width, _ = hr_shape self.n_residual_in_residual_dense_block = 16 optimizer = Adam(0.0002, 0.5) self.vgg = self.build_vgg() self.vgg.trainable = False self.vgg.compile(loss='mse', optimizer=optimizer, metrics=['accuracy']) self.disc_patch = 30, 40, 1 self.gf = 64 self.df = 64 self.discriminator = self.build_discriminator() self.discriminator.compile(loss='mse', optimizer=optimizer, metrics=[ 'accuracy']) self.generator = self.build_generator() img_hr = Input(shape=self.hr_shape) img_lr = Input(shape=self.lr_shape) fake_hr = self.generator(img_lr) fake_features = self.vgg(fake_hr) self.discriminator.trainable = False validity = self.discriminator(fake_hr) self.combined = Model([img_lr, img_hr], [validity, fake_features]) self.combined.compile(loss=['binary_crossentropy', 'mse'], loss_weights =[0.001, 1], optimizer=optimizer) 
AffineCouplingSdnEx1.AffineCouplingSdnEx1.det|jacobian|log|forward def _forward_log_det_jacobian(self, x, yy, nlf0=None, nlf1=None, iso=None, cam=None): if self._last_layer: x = tf.reshape(x, (-1, self.i0, self.i1, self.ic)) yy = tf.reshape(yy, (-1, self.i0, self.i1, self.ic)) scale = sdn_model_params_ex1(yy, iso) if scale is None: return tf.constant(0.0, dtype=x.dtype, name='fldj') return tf.reduce_sum(tf.log(scale), axis=[1, 2, 3]) 
utils.data_utils.pair|truncate|seq def _truncate_seq_pair(tokens_a, tokens_b, max_length): """Truncates a sequence pair in place to the maximum length.""" while True: total_length = len(tokens_a) + len(tokens_b) if total_length <= max_length: break if len(tokens_a) > len(tokens_b): tokens_a.pop() else: tokens_b.pop() 
seed_rl-master.grpc.python.ops_test.OpsTest.test|multiple|functions|bind @parameterized.parameters(([], False), ([1], True)) def test_bind_multiple_functions(self, dim, batched): address = self.get_unix_address() server = ops.Server([address])  @tf.function(input_signature=[tf.TensorSpec(dim, tf.int32)]) def foo(x): return x + 1  @tf.function(input_signature=[tf.TensorSpec(dim, tf.int32), tf. TensorSpec(dim, tf.int32)]) def bar(x, y): return x * y server.bind(foo, batched=batched) server.bind(bar, batched=batched) server.start() client = ops.Client(address) self.assertAllEqual(43, client.foo(42)) self.assertAllEqual(100, client.bar(10, 10)) server.shutdown() 
profiling.Aggregator.add def add(self, v): self.sum += v self.count += 1 
gan.op.layer_norm.norm|layer def __init__(self, name='layer_norm'): self.name = name 
avod.datasets.kitti.kitti_dataset.KittiDataset.get|path|velodyne def get_velodyne_path(self, sample_name): return self.velo_dir + '/' + sample_name + '.bin' 
gym_pycolab.tests.maze_walker_test.MazeWalkerTest.test|To|Board|Confined def testConfinedToBoard(self): """A confined-to-board MazeWalker can't walk off the board.""" art = ['     ', '  P  ', '     '] engine = ascii_art.ascii_art_to_game(art=art, what_lies_beneath=' ', sprites=dict(P=ascii_art.Partial(tt.TestMazeWalker, impassable='', confined_to_board=True))) engine.its_showtime() EDGE = prefab_sprites.MazeWalker.EDGE self.assertMachinima(engine=engine, post_updates=dict(P=lambda actions, board, layers, backdrop, things, the_plot: self.assertEqual( the_plot['walk_result_P'], the_plot['machinima_args'][0])), frames= [('n', ['  P  ', '     ', '     '], None), ('n', ['  P  ', '     ', '     '], EDGE), ('nw', ['  P  ', '     ', '     '], (' ', EDGE, EDGE)), ('ne', ['  P  ', '     ', '     '], (EDGE, EDGE, ' ')), ( 'sw', ['     ', ' P   ', '     '], None), ('sw', ['     ', '     ', 'P    '], None), ('sw', ['     ', '     ', 'P    '], (EDGE, EDGE, EDGE)), ('e', ['     ', '     ', ' P   '], None), ('e', ['     ', '     ', '  P  '], None), ('e', ['     ', '     ', '   P '], None), ('e', ['     ', '     ', '    P'], None), ('se', ['     ', '     ', '    P'], (EDGE, EDGE, EDGE))]) 
ml.crossvalid def crossvalid(train_df, test_df, clfs, run_index, fold_index): features_cols = train_df.columns.difference(['Drug1', 'Drug2', 'Class', 'Drug_x', 'Drug_y']) X = train_df[features_cols].values print(type(X)) y = train_df['Class'].values.ravel() X_new = test_df[features_cols].values y_new = test_df['Class'].values.ravel() results = pd.DataFrame() for name, clf in clfs: clf.fit(X, y) scores = get_scores(clf, X_new, y_new) scores['method'] = name scores['fold'] = fold_index scores['run'] = run_index results = results.append(scores, ignore_index=True) return results, X, y, X_new, y_new 
darkflow.cli.cli|Handler def cliHandler(args): FLAGS = argHandler() FLAGS.setDefaults() FLAGS.parseArgs(args)  def _get_dir(dirs): for d in dirs: this = os.path.abspath(os.path.join(os.path.curdir, d)) if not os.path.exists(this): os.makedirs(this) requiredDirectories = [FLAGS.imgdir, FLAGS.binary, FLAGS.backup, os. path.join(FLAGS.imgdir, 'out')] if FLAGS.summary: requiredDirectories.append(FLAGS.summary) _get_dir(requiredDirectories) try: FLAGS.load = int(FLAGS.load) except: pass tfnet = TFNet(FLAGS) if FLAGS.demo: tfnet.camera() exit('Demo stopped, exit.') if FLAGS.train: print('Enter training ...') tfnet.train() if not FLAGS.savepb: exit('Training finished, exit.') if FLAGS.savepb: print('Rebuild a constant version ...') tfnet.savepb() exit('Done') tfnet.predict() 
cifar10.load|class|names def load_class_names(): """ Load the names for the classes in the CIFAR-10 data-set.  Returns a list with the names. Example: names[3] is the name associated with class-number 3. """ raw = _unpickle(filename='batches.meta')[b'label_names'] names = [x.decode('utf-8') for x in raw] return names 
translator.session|config def session_config(params): optimizer_options = tf.OptimizerOptions(opt_level=tf.OptimizerOptions. L1, do_function_inlining=False) graph_options = tf.GraphOptions(optimizer_options=optimizer_options) config = tf.ConfigProto(allow_soft_placement=True, graph_options= graph_options) if params.device_list: device_str = ','.join([str(i) for i in params.device_list]) config.gpu_options.visible_device_list = device_str return config 
svae_dc.utils.bo_utils.UpperConfidenceBound.Confidence|Bound|Upper def __init__(self, model, beta, maximize=True) ->None: super().__init__(model=model) self.maximize = maximize if not torch.is_tensor(beta): beta = torch.tensor(beta) self.register_buffer('beta', beta) 
model.PCGN_model.PCGNModel.model|build def build_model(self): print('building model... ...') with tf.variable_scope('seq2seq_placeholder'): self.encoder_inputs = tf.placeholder(tf.int32, [None, None], name= 'encoder_inputs') self.decoder_inputs = tf.placeholder(tf.int32, [None, None], name= 'decoder_inputs') self.decoder_targets = tf.placeholder(tf.int32, [None, None], name= 'decoder_targets') self.decoder_targets_masks = tf.placeholder(tf.bool, [None, None], name='mask') self.encoder_length = tf.placeholder(tf.int32, [None], name= 'encoder_length') self.decoder_length = tf.placeholder(tf.int32, [None], name= 'decoder_length') self.user_feat = tf.placeholder(tf.float32, [None, self.feat_dim], name='user_feat') self.user_desc = tf.placeholder(tf.int32, [None, None], name= 'user_desc') self.desc_length = tf.placeholder(tf.int32, [None], name= 'user_desc_length') self.max_target_sequence_length = tf.constant(value=self. target_max_length, name='max_target_len') with tf.variable_scope('seq2seq_embedding'): self.embedding = self.init_embedding(self.vocab_size, self. embedding_size) with tf.variable_scope('seq2seq_encoder'): encoder_outputs, encoder_states = build_encoder(self.embedding, self.encoder_inputs, self.encoder_length, self. encode_num_layers, self.encode_num_units, self.encode_cell_type, bidir=self.encode_bidir) if self.use_user_desc or self.use_user_feat: with tf.variable_scope('user_profile_encoder'): desc_initializer = tf.contrib.layers.xavier_initializer() self.user_feat_mem_embedding = tf.layers.Dense(self. user_feat_mem_unit, use_bias=False, activation=tf.nn.relu, kernel_initializer=desc_initializer, name='user_feat_mem_layer' ) self.user_feats, self.user_embs, self.user_desc_encode = (self. build_user_embedding(self.user_feat, self.user_desc, self. desc_length, self.user_feat_unit, self.desc_rnn_unit, self. embedding, self.use_user_desc, self.use_user_feat)) if self.use_external_desc_express: dim2 = self.desc_rnn_unit dim1 = self.decode_num_units if self.use_blog_user_coattn: dim1 = dim1 * 2 self.blog_desc_inetract = tf.Variable(desc_initializer( shape=(dim1, dim2)), name='blog_desc_inetraction_layer', dtype=tf.float32) if self.use_external_feat_express: dim2 = dim2 + self.user_feat_unit self.user_map_layer = tf.Variable(desc_initializer(shape=( dim2, self.user_map_unit)), name='user_map_layer', dtype=tf.float32) with tf.variable_scope('seq2seq_decoder'): encoder_length = self.encoder_length if self.use_user_desc or self.use_user_feat: user_feats = self.user_feats user_embs = self.user_embs if self.use_user_desc: desc_length = self.desc_length user_desc_encode = self.user_desc_encode if self.beam_search: print('use beamsearch decoding..') encoder_outputs = tile_batch(encoder_outputs, multiplier=self. beam_size) encoder_states = tile_batch(encoder_states, multiplier=self. beam_size) encoder_length = tile_batch(encoder_length, multiplier=self. beam_size) if self.use_user_desc or self.use_user_feat: user_feats = tile_batch(user_feats, multiplier=self.beam_size) user_embs = tile_batch(user_embs, multiplier=self.beam_size) if self.use_user_desc: desc_length = tile_batch(desc_length, multiplier=self. beam_size) user_desc_encode = tile_batch(user_desc_encode, multiplier=self.beam_size) attention_mechanism = BahdanauAttention(num_units=self. attn_num_units, memory=encoder_outputs, memory_sequence_length= encoder_length) if self.use_blog_user_coattn: attention_mechanism_desc = BahdanauAttention(num_units=self. desc_attn_num_units, memory=user_desc_encode, memory_sequence_length=desc_length) decoder_cell = create_rnn_cell(self.decode_num_layers, self. decode_num_units, self.decode_cell_type) if self.use_blog_user_coattn: _attention_mechanism = (attention_mechanism, attention_mechanism_desc) _attention_layer_size = [self.decode_num_units, self. decode_num_units] else: _attention_mechanism = attention_mechanism _attention_layer_size = self.decode_num_units if self.use_user_feat: if self.use_gate_memory: _read_g = tf.layers.Dense(self.user_feat_mem_unit, use_bias =False, name='internal_read_gate') _write_g = tf.layers.Dense(self.user_feat_mem_unit, use_bias=False, name='internal_write_gate') if self.use_blog_user_coattn: _read_atten_gate = tf.layers.Dense(2 * self. desc_attn_num_units, use_bias=False, name= 'internal_read_attn_gate') else: _read_atten_gate = None else: _read_g = None _write_g = None _read_atten_gate = None decoder_cell = PCGNWrapper(cell=decoder_cell, attention_mechanism=_attention_mechanism, user_feats= user_feats, user_embs=user_embs, user_feat_mem_units=self. user_feat_mem_unit, user_feat_mem_embedding=self. user_feat_mem_embedding, read_gate=_read_g, write_gate= _write_g, use_gate_memory=self.use_gate_memory, attention_layer_size=_attention_layer_size, read_atten_gate =_read_atten_gate, name='PCGNWrapper') else: decoder_cell = AttentionWrapper(cell=decoder_cell, attention_mechanism=_attention_mechanism, attention_layer_size=_attention_layer_size, name= 'Attention_Wrapper') batch_size = (self.batch_size if not self.beam_search else self. batch_size * self.beam_size) decoder_initial_state = decoder_cell.zero_state(batch_size= batch_size, dtype=tf.float32).clone(cell_state=encoder_states) output_layer = tf.layers.Dense(self.vocab_size, use_bias=False, name='output_projection') if self.mode == 'train': decoder_inputs_embedded = tf.nn.embedding_lookup(self.embedding, self.decoder_inputs) training_helper = TrainingHelper(inputs=decoder_inputs_embedded, sequence_length=self.decoder_length, name='training_helper') training_decoder = BasicDecoder(cell=decoder_cell, helper= training_helper, initial_state=decoder_initial_state) (self.decoder_outputs, self.final_state, self.final_sequence_length ) = (dynamic_decode(decoder=training_decoder, impute_finished=True, maximum_iterations=self. max_target_sequence_length)) self.decoder_logits_train = tf.identity(self.decoder_outputs. rnn_output) if self.use_external_desc_express: if self.use_external_feat_express: _user_feats = user_embs else: _user_feats = None self.decoder_logits_train = self.external_personality_express( self.decoder_logits_train, user_desc_encode, self. blog_desc_inetract, user_feats=_user_feats, use_external_feat_express=self. use_external_feat_express, user_map=self.user_map_layer) with tf.variable_scope('decoder'): self.generic_logits = output_layer(self.decoder_logits_train) if self.use_gate_memory: self.feat_mem = self.final_state.user_feat_mem with tf.variable_scope('loss'): g_probs = tf.nn.softmax(self.generic_logits) train_log_probs = tf.log(g_probs) self.g_losses = tf.nn.sparse_softmax_cross_entropy_with_logits( logits=self.generic_logits, labels=self.decoder_targets) losses = tf.boolean_mask(self.g_losses, self. decoder_targets_masks) self.loss = tf.reduce_mean(losses) if self.use_gate_memory: self.int_mem_reg = tf.reduce_mean(tf.norm(self.feat_mem + 1e-07, axis=1)) self.loss += self.int_mem_reg CE = tf.nn.sparse_softmax_cross_entropy_with_logits(logits= train_log_probs, labels=self.decoder_targets) CE = tf.boolean_mask(CE, tf.cast(self.decoder_targets_masks, tf .bool)) self.CE = tf.reduce_mean(CE) optimizer = tf.train.AdamOptimizer(self.learning_rate) trainable_params = tf.trainable_variables() gradients = tf.gradients(self.loss, trainable_params) clip_gradients, _ = tf.clip_by_global_norm(gradients, self. max_gradient_norm) self.train_op = optimizer.apply_gradients(zip(clip_gradients, trainable_params)) elif self.mode == 'infer': start_tokens = tf.ones([self.batch_size], tf.int32) * SOS_ID end_token = EOS_ID if self.use_user_feat or self.use_user_desc: if self.use_external_desc_express: _embed_desc = user_desc_encode _blog_desc_inetract = self.blog_desc_inetract _user_map = self.user_map_layer if self.use_external_feat_express: _feat_embed = user_embs else: _feat_embed = None else: _embed_desc = None _blog_desc_inetract = None _user_map = None _feat_embed = None inference_decoder = PCGNBeamSearchDecoder(cell=decoder_cell, embedding=self.embedding, start_tokens=start_tokens, end_token=end_token, initial_state= decoder_initial_state, beam_width=self.beam_size, output_layer=output_layer, use_external_desc_express= self.use_external_desc_express, embed_desc=_embed_desc, blog_desc_inetract=_blog_desc_inetract, feat_embed= _feat_embed, use_external_feat_express=self. use_external_feat_express, user_map=_user_map) else: inference_decoder = BeamSearchDecoder(cell=decoder_cell, embedding=self.embedding, start_tokens=start_tokens, end_token=end_token, initial_state= decoder_initial_state, beam_width=self.beam_size, output_layer=output_layer) decoder_outputs, _, _ = dynamic_decode(decoder= inference_decoder, maximum_iterations=self.infer_max_iter) infer_outputs = decoder_outputs.predicted_ids self.infer_outputs = tf.transpose(infer_outputs, [0, 2, 1], name='infer_outputs') self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=self. max_to_keep) 
generate_dataset.generate|sample def generate_sample(targets_path, img_name, templates, probabilities_vector, positions_list, images_out_path, nb_imgs_generated, binary_annotation_lines_queue, multiclass_annotation_lines_queue, data_out_path, load_path): np.random.seed(None) img_path = os.path.join(targets_path, img_name) try: target = cv2.imread(img_path).astype(np.float32) / 255.0 except: print('Error with:', img_path) return if load_path is None: image_data = {'bbox_data': []} nb_signs_in_img = np.random.randint(1, 5 + 1) total = 0 target, add_value, multiply_value = augment_target(target) image_data['add_value'] = add_value image_data['multiply_value'] = multiply_value bboxes = [] template_ids_selected = np.random.choice(list(range(len(templates)) ), size=nb_signs_in_img, replace=False) image_data['template_ids'] = template_ids_selected scale = None while total < nb_signs_in_img: template = templates[template_ids_selected[total]][0] template_mask = templates[template_ids_selected[total]][1] template_category = templates[template_ids_selected[total]][2] target, bbox, scale, data = process_img(target.copy(), template .copy(), template_mask, probabilities_vector, positions_list, multiply_value, bboxes=bboxes, scale=scale) if bbox: bbox['category'] = template_category bboxes.append(bbox) image_data['bbox_data'].append({'bbox': bbox, 'data': data}) total += 1 probs = [0.4, 0.5] while len(probs) > 0: do_place_below = np.random.choice([True, False], p=[probs[0 ], 1 - probs[0]]) and total < nb_signs_in_img if not (do_place_below and total < nb_signs_in_img and bbox): break probs = probs[1:] position = bbox['xmin'], bbox['ymax'] template = templates[template_ids_selected[total]][0] template_mask = templates[template_ids_selected[total]][1] template_category = templates[template_ids_selected[total]][2] target, bbox, scale, data = process_img(target.copy(), template, template_mask, probabilities_vector, positions_list, multiply_value, position, bboxes, scale =scale) if bbox: bbox['category'] = template_category bboxes.append(bbox) image_data['bbox_data'].append({'bbox': bbox, 'data': data} ) total += 1 blur_value = float(np.random.uniform(0, 7)) * scale image_data['blur_value'] = blur_value else: with open(os.path.join(load_path, '{:05d}.pkl'.format( nb_imgs_generated)), 'rb') as data_in_f: image_data = pickle.load(data_in_f) target, _, _ = augment_target(target, multiply_value=image_data[ 'multiply_value'], add_value=image_data['add_value']) bboxes = [] scale = None template_ids_selected = image_data['template_ids'] for total, bbox_data in enumerate(image_data['bbox_data']): template = templates[template_ids_selected[total]][0] template_mask = templates[template_ids_selected[total]][1] data = bbox_data['data'] target, _, _, _ = process_img(target.copy(), template.copy(), template_mask, probabilities_vector, positions_list, image_data['multiply_value'], bboxes=bboxes, scale=scale, prelodaded_data=data) bboxes.append(bbox_data['bbox']) blur_value = image_data['blur_value'] blur_effect = iaa.Sequential([iaa.GaussianBlur(blur_value, deterministic=True)]) target = blur_effect.augment_image(target) img_out_path = os.path.join(images_out_path, '{:05d}_{}.jpg'.format( nb_imgs_generated, os.path.splitext(img_name)[0])) cv2.imwrite(img_out_path, (target * 255).astype(np.uint8)) binary_annotation_lines = [] multiclass_annotation_lines = [] for bbox in bboxes: binary_line = '{},{},{},{},{},{}'.format(img_out_path, bbox['xmin'], bbox['ymin'], bbox['xmax'], bbox['ymax'], 'traffic_sign') multiclass_line = '{},{},{},{},{},{}'.format(img_out_path, bbox[ 'xmin'], bbox['ymin'], bbox['xmax'], bbox['ymax'], bbox['category'] ) binary_annotation_lines.append(binary_line) multiclass_annotation_lines.append(multiclass_line) multiclass_annotation_lines_queue.put(multiclass_annotation_lines) binary_annotation_lines_queue.put(binary_annotation_lines) with open(os.path.join(data_out_path, '{:05d}.pkl'.format( nb_imgs_generated)), 'wb') as data_out_f: pickle.dump(image_data, data_out_f) 
train.Trainer.train|step def train_step(self, data_batch, n_train): """Train step  Arguments: data_batch {tuple of np float} -- (x_batch, y_batch) n_train {None or list of float} -- Layers to train None: train all layers Scalar: train randomly selected sublist of this length List: train layers whose id are found in the list """ x_batch, y_batch = data_batch feed_batch = {self.network.x: x_batch, self.network.y: y_batch} feed_dropout = dict([(obj.keep_prob, obj.keep_prob_train) for obj in self.network.layers if hasattr(obj, 'keep_prob')]) feed_batch.update(feed_dropout) xs_val, ps_val = self.sess.run([self.network.xs, self.network.ps], feed_batch) trainable_layers = [obj for obj in self.network.layers if obj.is_trainable] if n_train is not None: if isinstance(n_train, list): trainable_layers = [obj for obj in trainable_layers if obj in n_train] else: n_train = np.minimum(n_train, len(trainable_layers)) trainable_layers = random.sample(trainable_layers, n_train) for obj in trainable_layers: l = obj.layer_id obj.train(session=self.sess, feeds=(xs_val[l], ps_val[l + 1])) 
texar.modules.networks.conv_networks.Conv1DNetwork.hparams|build|pool def _build_pool_hparams(self): pool_type = self._hparams.pooling if pool_type == 'MaxPooling': pool_type = 'MaxPooling1D' elif pool_type == 'AveragePooling': pool_type = 'AveragePooling1D' npool = self._hparams.num_conv_layers pool_size = _to_list(self._hparams.pool_size, 'pool_size', npool) strides = _to_list(self._hparams.pool_strides, 'pool_strides', npool) other_kwargs = self._hparams.other_pool_kwargs or {} if isinstance(other_kwargs, HParams): other_kwargs = other_kwargs.todict() if not isinstance(other_kwargs, dict): raise ValueError("hparams['other_pool_kwargs'] must be a dict.") pool_hparams = [] for i in range(npool): kwargs_i = {'pool_size': pool_size[i], 'strides': strides[i], 'name': 'pool_%d' % (i + 1)} kwargs_i.update(other_kwargs) pool_hparams_ = get_pooling_layer_hparams({'type': pool_type, 'kwargs': kwargs_i}) pool_hparams.append(pool_hparams_) return pool_hparams 
archs.maml.MAML.load|Weights def loadWeights(self, sess, name, step=0, modeldir='./model_checkpoint/', model_name='model.ckpt'): if self.saver == None: z = self.saving_weights self.saver = tf.train.Saver(var_list=z, max_to_keep=12) saver = self.saver checkpoint_path = modeldir + f'{name}/' + model_name + '-' + step if os.path.isfile(checkpoint_path + '.marker'): saver.restore(sess, checkpoint_path) print('The checkpoint has been loaded.') else: print(checkpoint_path + '.marker not found. Starting from scratch.') 
pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLLMHeadModel.reset|length def reset_length(self, tgt_len, ext_len, mem_len): self.transformer.reset_length(tgt_len, ext_len, mem_len) 
misc.load def load(saver, sess, logdir, ckpt=None): """ Try to load model form a dir (search for the newest checkpoint) """ if ckpt: ckpt = os.path.join(logdir, ckpt) global_step = int(ckpt.split('/')[-1].split('-')[-1]) logging.info('  Global step: {}'.format(global_step)) saver.restore(sess, ckpt) return global_step else: ckpt = tf.train.latest_checkpoint(logdir) if ckpt: logging.info('  Checkpoint found: {}'.format(ckpt)) global_step = int(ckpt.split('/')[-1].split('-')[-1]) logging.info('  Global step: {}'.format(global_step)) saver.restore(sess, ckpt) return global_step else: print('No checkpoint found') return None 
tests.test_screen.TestScreenGrab.local|performance|test|mss def test_local_mss_performance(self): fps = 0 screen_grab = LocalScreenGrab(self._box) now = time.time() cur = now while cur - now < 1: screen_grab.grab() fps += 1 cur = time.time() MIN_FPS = 30 assert fps > MIN_FPS 
tf_utils.common.img|stretch def img_stretch(img): img = img.astype(np.float32) img -= np.min(img) img /= np.max(img) + 1e-12 return img 
texar.data.vocabulary_test.VocabularyTest.test|construction|vocab def test_vocab_construction(self): """Test vocabulary construction. """ vocab_list = ['word', ''] vocab_file = tempfile.NamedTemporaryFile() vocab_file.write('\n'.join(vocab_list).encode('utf-8')) vocab_file.flush() vocab = vocabulary.Vocab(vocab_file.name) self.assertEqual(vocab.size, len(vocab_list) + 4) self.assertEqual(set(vocab.token_to_id_map_py.keys()), set(['word', '' ] + vocab.special_tokens)) unk_token_id = vocab.token_to_id_map_py['new'] unk_token_text = vocab.id_to_token_map_py[unk_token_id] self.assertEqual(unk_token_text, vocab.unk_token) 
train_inception_v3_baseline_w_Adam.validate def validate(out, sess, epoch, validation_steps, loss_op, acc_top_1_op, acc_top_5_op, global_step, is_training_ph, is_validating_nbl_ph): g_step, acc_top1, acc_top5, test_loss = 0, 0, 0, 0 for i in range(validation_steps): out.validation_step_begin(i, validation_steps) g_step, l, acc1, acc5 = sess.run([global_step, loss_op, acc_top_1_op, acc_top_5_op], feed_dict={is_training_ph: False, is_validating_nbl_ph: False}) acc_top1 = (acc1 + i * acc_top1) / (i + 1) acc_top5 = (acc5 + i * acc_top5) / (i + 1) test_loss = (l + i * test_loss) / (i + 1) out.validation_end(sess, epoch, g_step, False, test_loss, 'DEFAULT', acc_top1, acc_top5) 
embeddings.TransE.run def run(): opt_method = args.opt_method int_pretrain = args.pretrain if int_pretrain == 1: pretrain = True elif int_pretrain == 0: pretrain = False else: raise ValueError('arg "pretrain" must be 0 or 1') config = Config() config.set_in_path('./openke_data/') config.set_log_on(1) config.set_work_threads(30) config.set_train_times(1000) config.set_nbatches(512) config.set_alpha(0.001) config.set_bern(0) config.set_dimension(100) config.set_margin(1.0) config.set_ent_neg_rate(1) config.set_rel_neg_rate(0) config.set_opt_method(opt_method) """revision starts""" config.set_pretrain(pretrain) if pretrain: OUTPUT_PATH = './openke_data/embs/glove_initialized/glove.' else: OUTPUT_PATH = './openke_data/embs/xavier_initialized/' """revision ends""" config.set_export_files(OUTPUT_PATH + 'transe.' + opt_method + '.tf', steps=500) config.set_out_files(OUTPUT_PATH + 'transe.' + opt_method + '.vec.json') print('Opt-method: %s' % opt_method) print('Pretrain: %d' % pretrain) config.init() config.set_model(models.TransE) print('Begin training TransE') config.run() 
test.main def main(unused_args): assert len(unused_args) == 1, unused_args setup_experiment() mnist_ds = mnist.read_data_sets(FLAGS.data_dir, dtype=tf.float32, reshape=False, validation_size=FLAGS.validation_size) test_ds = getattr(mnist_ds, FLAGS.dataset) test_images, test_labels = test_ds.images, test_ds.labels if FLAGS.sort_labels: ys_indices = np.argsort(test_labels) test_images = test_images[ys_indices] test_labels = test_labels[ys_indices] img_shape = [None, 1, 28, 28] X = tf.placeholder(tf.float32, shape=img_shape, name='X') y = tf.placeholder(tf.int32, shape=[None]) y_onehot = tf.one_hot(y, FLAGS.num_classes) model = create_model(FLAGS, name=FLAGS.model_name)  def test_model(x, **kwargs): return model(x, train=False, **kwargs) out_x = test_model(X) attack_clip = FLAGS.attack_clip if FLAGS.attack_clip > 0 else None if FLAGS.attack_box_clip: boxmin, boxmax = 0.0, 1.0 else: boxmin, boxmax = None, None X_df = deepfool(lambda x: test_model(x)['logits'], X, labels=y, max_iter=FLAGS.attack_iter, clip_dist=attack_clip, over_shoot=FLAGS .attack_overshoot, boxmin=boxmin, boxmax=boxmax) X_df_all = deepfool(lambda x: test_model(x)['logits'], X, max_iter= FLAGS.attack_iter, clip_dist=attack_clip, over_shoot=FLAGS. attack_overshoot, boxmin=boxmin, boxmax=boxmax) if FLAGS.hc_confidence == 'same': confidence = out_x['conf'] else: confidence = float(FLAGS.hc_confidence) X_hc = high_confidence_attack(lambda x: test_model(x)['logits'], X, labels=y, random=FLAGS.hc_random, max_iter=FLAGS.attack_iter, clip_dist=attack_clip, confidence=confidence, boxmin=boxmin, boxmax =boxmax) X_hcd = tf.stop_gradient(X_hc) X_rec = high_confidence_attack(lambda x: model(x)['logits'], X_hcd, targets=out_x['pred'], attack_topk=None, max_iter=FLAGS.attack_iter, clip_dist=attack_clip, confidence=out_x['conf'], boxmin=boxmin, boxmax=boxmax) out_x_df = test_model(X_df) out_x_hc = test_model(X_hc) reduce_ind = 1, 2, 3 X_norm = tf.sqrt(tf.reduce_sum(X ** 2, axis=reduce_ind)) l2_df = tf.sqrt(tf.reduce_sum((X_df - X) ** 2, axis=reduce_ind)) l2_df_norm = l2_df / X_norm smoothness_df = tf.reduce_mean(tf.image.total_variation(X_df)) l2_df_all = tf.sqrt(tf.reduce_sum((X_df_all - X) ** 2, axis=reduce_ind)) l2_df_all_norm = l2_df_all / X_norm l2_hc = tf.sqrt(tf.reduce_sum((X_hc - X) ** 2, axis=reduce_ind)) l2_hc_norm = l2_hc / X_norm smoothness_hc = tf.reduce_mean(tf.image.total_variation(X_hc)) l1_rec = tf.reduce_sum(tf.abs(X - X_rec), axis=reduce_ind) l2_rec = tf.sqrt(tf.reduce_sum((X - X_rec) ** 2, axis=reduce_ind)) psnr = tf.py_func(batch_compute_psnr, [X, X_df], tf.float32) ssim = tf.py_func(batch_compute_ssim, [X, X_df], tf.float32) nll = tf.reduce_mean(tf.losses.softmax_cross_entropy(y_onehot, out_x[ 'logits'])) err = 1 - slim.metrics.accuracy(out_x['pred'], y) conf = tf.reduce_mean(out_x['conf']) err_df = 1 - slim.metrics.accuracy(out_x_df['pred'], y) conf_df = tf.reduce_mean(out_x_df['conf']) err_hc = 1 - slim.metrics.accuracy(out_x_hc['pred'], y) conf_hc = tf.reduce_mean(out_x_hc['conf']) metrics = OrderedDict([('nll', nll), ('err', err), ('conf', conf), ( 'err_df', err_df), ('err_hc', err_hc), ('l2_df', tf.reduce_mean( l2_df)), ('l2_df_norm', tf.reduce_mean(l2_df_norm)), ('l2_df_all', tf.reduce_mean(l2_df_all)), ('l2_df_all_norm', tf.reduce_mean( l2_df_all_norm)), ('conf_df', conf_df), ('smoothness_df', smoothness_df), ('l2_hc', tf.reduce_mean(l2_hc)), ('l2_hc_norm', tf .reduce_mean(l2_hc_norm)), ('conf_hc', conf_hc), ('smoothness_hc', smoothness_hc), ('l1_rec', tf.reduce_mean(l1_rec)), ('l2_rec', tf. reduce_mean(l2_rec)), ('psnr', tf.reduce_mean(psnr)), ('ssim', tf. reduce_mean(ssim))]) metrics_mean, metrics_upd = register_metrics(metrics) gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.45) with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess: tf.local_variables_initializer().run() model_loader = tf.train.Saver(tf.model_variables()) model_filename = ('model' if FLAGS.restore_epoch_index is None else 'model-%d' % FLAGS.restore_epoch_index) model_path = os.path.join(FLAGS.load_dir, 'chks', model_filename) model_loader.restore(sess, model_path) summary_writer = tf.summary.FileWriter(FLAGS.working_dir, sess.graph) summaries = tf.summary.merge_all() test_iterator = batch_iterator(test_images, test_labels, FLAGS. batch_size, shuffle=False) start_time = time.time() for batch_index, (images, labels) in enumerate(test_iterator, 1): if batch_index % FLAGS.summary_frequency == 0: hc_images, df_images, rec_images, summary = sess.run([X_hc, X_df, X_rec, summaries, metrics_upd], feed_dict={X: images, y: labels})[:-1] save_path = os.path.join(FLAGS.samples_dir, 'epoch_orig-%d.png' % batch_index) save_images(images, save_path) save_path = os.path.join(FLAGS.samples_dir, 'epoch_hc-%d.png' % batch_index) save_images(hc_images, save_path) save_path = os.path.join(FLAGS.samples_dir, 'epoch_df-%d.png' % batch_index) save_images(df_images, save_path) save_path = os.path.join(FLAGS.samples_dir, 'epoch_rec-%d.png' % batch_index) save_images(rec_images, save_path) else: summary = sess.run([metrics_upd, summaries], feed_dict={X: images, y: labels})[-1] summary_writer.add_summary(summary, batch_index) str_bfr = six.StringIO() str_bfr.write('Test results [{:.2f}s]:'.format(time.time() - start_time)) print_results_str(str_bfr, metrics.keys(), sess.run(metrics_mean), throw_on_nan=False) logging.info(str_bfr.getvalue()[:-1]) 
kaffe.tensorflow.transformer.TensorFlowTransformer.Flow|Tensor|Transformer def __init__(self, def_path, data_path, verbose=True, phase='test'): self.verbose = verbose self.phase = phase self.load(def_path, data_path, phase) self.params = None self.source = None 
thumt.utils.inference.beam_search.finished|is def _is_finished(t, s): log_probs = s.inputs[1] finished_flags = s.finish[0] finished_scores = s.finish[2] max_lp = tf.pow((5.0 + tf.to_float(max_step)) / 6.0, alpha) best_alive_score = log_probs[:, (0)] / max_lp worst_finished_score = tf.reduce_min(finished_scores * tf.to_float( finished_flags), axis=1) add_mask = 1.0 - tf.to_float(tf.reduce_any(finished_flags, 1)) worst_finished_score += tf.float32.min * add_mask bound_is_met = tf.reduce_all(tf.greater(worst_finished_score, best_alive_score)) cond = tf.logical_and(tf.less(t, max_step), tf.logical_not(bound_is_met)) return cond 
hbaselines.envs.mixed_autonomy.envs.FlowEnv.Flow|Env def __init__(self, env_name, env_params=None, render=False, version=0): """Create the environment.  Parameters ---------- env_name : str the name of the environment to create env_params : dict environment-specific parameters render : bool whether to render the environment version : int environment version number, needed for testing purposes  Returns ------- gym.Env the environment  Raises ------ AssertionError if the `env_name` parameter is not valid """ assert env_name in ['ring', 'merge', 'figure_eight'] env_params = env_params or {} flow_params = dict() if env_name == 'merge': flow_params = merge(**env_params) elif env_name == 'ring': flow_params = ring(**env_params) elif env_name == 'figure_eight': flow_params = figure_eight(**env_params) create_env, _ = make_create_env(flow_params, version, render) self.wrapped_env = create_env() self.step_number = 0 self.horizon = self.wrapped_env.env_params.horizon 
t2t_csaky.main.main def main(): parser = argparse.ArgumentParser() parser.add_argument('--mode', type=str, help= 'Can be one of the following: {                                                train,                                                decode,                                                generate_data,                                                experiment}' ) args = parser.parse_args() run_mode = {'train': run.training, 'decode': run.decoding, 'generate_data': run.data_generating, 'experiment': run.experiment} if args.mode in run_mode: run_mode[args.mode]() else: print( 'Program exited, because no suitable mode was defined.            The mode flag has to be set to one of the following:' ) print('  train') print('  decode') print('  generate_data') print('  experiment') 
models.util.get|matrix def get_matrix(n, tf='dct'): if tf == 'dft': F = la.dft(n, scale='sqrtn') elif tf == 'dct': I = np.identity(n) F = dct(I, norm='ortho') return F 
regression.ops.mvg_optimizer.MVGOptimizer.v|damp @property def v_damp(self): pi = tf.sqrt((tf.trace(self.fisher_v) / self.shape[1] + 1e-08) / (tf. trace(self.fisher_u) / self.shape[0] + 1e-08)) coeff = self.lam / (self.N * self.ita) v_tmp = tf.matrix_inverse(self.fisher_v + pi * (coeff + self.damp) ** 0.5 * tf.eye(self.shape[1])) return (self.lam / self.N) ** 0.5 * v_tmp 
embeddings.OpenKE.config.Config.Config.triple|test|classification|set def set_test_triple_classification(self, flag): self.test_triple_classification = flag 
commons.measure.PadRotateProjectWithTheta.measure def measure(self, hparams, x, theta_ph): x_padded = measure_utils.pad(hparams, x) x_measured_list = [] for i in range(hparams.num_angles): angles = theta_ph[:, (i)] x_rotated = measure_utils.rotate(x_padded, angles) x_projected = measure_utils.project(hparams, x_rotated) x_measured = measure_utils.concat(x_projected, angles) x_measured_list.append(x_measured) x_measured = tf.concat(x_measured_list, axis=1, name='x_measured') return x_measured 
amb_measure.BlurAddNoise.np|measure def measure_np(self, hparams, x_val, theta_val): x_blurred = amb_measure_utils.blur_np(hparams, x_val) x_measured = x_blurred + theta_val return x_measured 
transcribe_audio_file.to|text def to_text(vocab_list, sample_ids): sym_list = [vocab_list[x] for x in sample_ids] + [utils.EOS] return args.delimiter.join(sym_list[:sym_list.index(utils.EOS)]) 
deepctr.models.autoint.Int|Auto def AutoInt(feature_dim_dict, embedding_size=8, att_layer_num=3, att_embedding_size=8, att_head_num=2, att_res=True, dnn_hidden_units=( 256, 256), dnn_activation='relu', l2_reg_dnn=0, l2_reg_embedding=1e-05, dnn_use_bn=False, dnn_dropout=0, init_std=0.0001, seed=1024, task='binary' ): """Instantiates the AutoInt Network architecture.  :param feature_dim_dict: dict,to indicate sparse field and dense field like {'sparse':{'field_1':4,'field_2':3,'field_3':2},'dense':['field_4','field_5']} :param embedding_size: positive integer,sparse feature embedding_size :param att_layer_num: int.The InteractingLayer number to be used. :param att_embedding_size: int.The embedding size in multi-head self-attention network. :param att_head_num: int.The head number in multi-head  self-attention network. :param att_res: bool.Whether or not use standard residual connections before output. :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN :param dnn_activation: Activation function to use in DNN :param l2_reg_dnn: float. L2 regularizer strength applied to DNN :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector :param dnn_use_bn:  bool. Whether use BatchNormalization before activation or not in DNN :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate. :param init_std: float,to use as the initialize std of embedding vector :param seed: integer ,to use as random seed. :param task: str, ``"binary"`` for  binary logloss or  ``"regression"`` for regression loss :return: A Keras model instance. """ if len(dnn_hidden_units) <= 0 and att_layer_num <= 0: raise ValueError('Either hidden_layer or att_layer_num must > 0') check_feature_config_dict(feature_dim_dict) deep_emb_list, _, _, inputs_list = preprocess_input_embedding( feature_dim_dict, embedding_size, l2_reg_embedding, 0, init_std, seed, create_linear_weight=False) att_input = concat_fun(deep_emb_list, axis=1) for _ in range(att_layer_num): att_input = InteractingLayer(att_embedding_size, att_head_num, att_res )(att_input) att_output = tf.keras.layers.Flatten()(att_input) deep_input = tf.keras.layers.Flatten()(concat_fun(deep_emb_list)) if len(dnn_hidden_units) > 0 and att_layer_num > 0: deep_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed)(deep_input) stack_out = tf.keras.layers.Concatenate()([att_output, deep_out]) final_logit = tf.keras.layers.Dense(1, use_bias=False, activation=None )(stack_out) elif len(dnn_hidden_units) > 0: deep_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed)(deep_input) final_logit = tf.keras.layers.Dense(1, use_bias=False, activation=None )(deep_out) elif att_layer_num > 0: final_logit = tf.keras.layers.Dense(1, use_bias=False, activation=None )(att_output) else: raise NotImplementedError output = PredictionLayer(task)(final_logit) model = tf.keras.models.Model(inputs=inputs_list, outputs=output) return model 
tica.tICA.fit def _fit(self, X): X = np.asarray(array2d(X), dtype=np.float64) if X.shape[1] > X.shape[0]: warnings.warn( 'The number of features (%d) is greater than the length of the data (%d). The covariance matrix is not guaranteed to be positive definite.' % (X.shape[1], X.shape[0])) self._initialize(X.shape[1]) if not len(X) > self.lag_time: warnings.warn( 'length of data (%d) is too short for the lag time (%d)' % (len (X), self.lag_time)) return self.n_observations_ += X.shape[0] self.n_sequences_ += 1 self._outer_0_to_T_lagged += np.dot(X[:-self.lag_time].T, X[self.lag_time:] ) self._sum_0_to_TminusTau += X[:-self.lag_time].sum(axis=0) self._sum_tau_to_T += X[self.lag_time:].sum(axis=0) self._sum_0_to_T += X.sum(axis=0) self._outer_0_to_TminusTau += np.dot(X[:-self.lag_time].T, X[:-self. lag_time]) self._outer_offset_to_T += np.dot(X[self.lag_time:].T, X[self.lag_time:]) self._is_dirty = True 
empirical_test.EmpiricalTest.testTaylorExpansion.exact|f def f_2_exact(x0, x, params): w1, w2, b = params dx = x - x0 return f_lin_exact(x0, x, params) + 0.5 * np.dot(np.dot(dx.T, w1), dx) 
deepctr.models.afm.AFM def AFM(feature_dim_dict, embedding_size=8, use_attention=True, attention_factor=8, l2_reg_linear=1e-05, l2_reg_embedding=1e-05, l2_reg_att=1e-05, afm_dropout=0, init_std=0.0001, seed=1024, task='binary' ): """Instantiates the Attentonal Factorization Machine architecture.  :param feature_dim_dict: dict,to indicate sparse field and dense field like {'sparse':{'field_1':4,'field_2':3,'field_3':2},'dense':['field_4','field_5']} :param embedding_size: positive integer,sparse feature embedding_size :param use_attention: bool,whether use attention or not,if set to ``False``.it is the same as **standard Factorization Machine** :param attention_factor: positive integer,units in attention net :param l2_reg_linear: float. L2 regularizer strength applied to linear part :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector :param l2_reg_att: float. L2 regularizer strength applied to attention net :param afm_dropout: float in [0,1), Fraction of the attention net output units to dropout. :param init_std: float,to use as the initialize std of embedding vector :param seed: integer ,to use as random seed. :param task: str, ``"binary"`` for  binary logloss or  ``"regression"`` for regression loss :return: A Keras model instance. """ check_feature_config_dict(feature_dim_dict) deep_emb_list, linear_emb_list, dense_input_dict, inputs_list = ( preprocess_input_embedding(feature_dim_dict, embedding_size, l2_reg_embedding, l2_reg_linear, init_std, seed, create_linear_weight=True)) linear_logit = get_linear_logit(linear_emb_list, dense_input_dict, l2_reg_linear) fm_input = concat_fun(deep_emb_list, axis=1) if use_attention: fm_logit = AFMLayer(attention_factor, l2_reg_att, afm_dropout, seed)( deep_emb_list) else: fm_logit = FM()(fm_input) final_logit = tf.keras.layers.add([linear_logit, fm_logit]) output = PredictionLayer(task)(final_logit) model = tf.keras.models.Model(inputs=inputs_list, outputs=output) return model 
parallel.apply|criterion|parallel def _criterion_parallel_apply(modules, inputs, targets, kwargs_tup=None, devices=None): assert len(modules) == len(inputs) assert len(targets) == len(inputs) if kwargs_tup: assert len(modules) == len(kwargs_tup) else: kwargs_tup = ({},) * len(modules) if devices is not None: assert len(modules) == len(devices) else: devices = [None] * len(modules) lock = threading.Lock() results = {} if torch_ver != '0.3': grad_enabled = torch.is_grad_enabled()  def _worker(i, module, input, target, kwargs, device=None): if torch_ver != '0.3': torch.set_grad_enabled(grad_enabled) if device is None: device = get_a_var(input).get_device() try: with torch.cuda.device(device): if not isinstance(input, (list, tuple)): input = input, if not isinstance(target, (list, tuple)): target = target, output = module(*(input + target), **kwargs) with lock: results[i] = output except Exception as e: with lock: results[i] = e if len(modules) > 1: threads = [threading.Thread(target=_worker, args=(i, module, input, target, kwargs, device)) for i, (module, input, target, kwargs, device) in enumerate(zip(modules, inputs, targets, kwargs_tup, devices))] for thread in threads: thread.start() for thread in threads: thread.join() else: _worker(0, modules[0], inputs[0], kwargs_tup[0], devices[0]) outputs = [] for i in range(len(inputs)): output = results[i] if isinstance(output, Exception): raise output outputs.append(output) return outputs 
generate.l|handler|rgb def _handler_rgb_l1(ir_path, vis_path, model_path, model_pre_path, ssim_weight, index, output_path=None): ir_img = get_test_image_rgb(ir_path, flag=False) vis_img = get_test_image_rgb(vis_path, flag=False) dimension = ir_img.shape ir_img = ir_img.reshape([1, dimension[0], dimension[1], dimension[2]]) vis_img = vis_img.reshape([1, dimension[0], dimension[1], dimension[2]]) ir_img1 = ir_img[:, :, :, (0)] ir_img1 = ir_img1.reshape([1, dimension[0], dimension[1], 1]) ir_img2 = ir_img[:, :, :, (1)] ir_img2 = ir_img2.reshape([1, dimension[0], dimension[1], 1]) ir_img3 = ir_img[:, :, :, (2)] ir_img3 = ir_img3.reshape([1, dimension[0], dimension[1], 1]) vis_img1 = vis_img[:, :, :, (0)] vis_img1 = vis_img1.reshape([1, dimension[0], dimension[1], 1]) vis_img2 = vis_img[:, :, :, (1)] vis_img2 = vis_img2.reshape([1, dimension[0], dimension[1], 1]) vis_img3 = vis_img[:, :, :, (2)] vis_img3 = vis_img3.reshape([1, dimension[0], dimension[1], 1]) print('img shape final:', ir_img1.shape) with tf.Graph().as_default(), tf.Session() as sess: infrared_field = tf.placeholder(tf.float32, shape=ir_img1.shape, name='content') visible_field = tf.placeholder(tf.float32, shape=ir_img1.shape, name='style') dfn = DenseFuseNet(model_pre_path) enc_ir = dfn.transform_encoder(infrared_field) enc_vis = dfn.transform_encoder(visible_field) target = tf.placeholder(tf.float32, shape=enc_ir.shape, name='target') output_image = dfn.transform_decoder(target) saver = tf.train.Saver() saver.restore(sess, model_path) enc_ir_temp, enc_vis_temp = sess.run([enc_ir, enc_vis], feed_dict={ infrared_field: ir_img1, visible_field: vis_img1}) feature = L1_norm(enc_ir_temp, enc_vis_temp) output1 = sess.run(output_image, feed_dict={target: feature}) enc_ir_temp, enc_vis_temp = sess.run([enc_ir, enc_vis], feed_dict={ infrared_field: ir_img2, visible_field: vis_img2}) feature = L1_norm(enc_ir_temp, enc_vis_temp) output2 = sess.run(output_image, feed_dict={target: feature}) enc_ir_temp, enc_vis_temp = sess.run([enc_ir, enc_vis], feed_dict={ infrared_field: ir_img3, visible_field: vis_img3}) feature = L1_norm(enc_ir_temp, enc_vis_temp) output3 = sess.run(output_image, feed_dict={target: feature}) output1 = output1.reshape([1, dimension[0], dimension[1]]) output2 = output2.reshape([1, dimension[0], dimension[1]]) output3 = output3.reshape([1, dimension[0], dimension[1]]) output = np.stack((output1, output2, output3), axis=-1) save_images(ir_path, output, output_path, prefix='fused' + str( index), suffix='_densefuse_l1norm_' + str(ssim_weight)) 
texar.data.data.dataset_utils.make_chained_transformation.fn|chained def _chained_fn(data): for tran_fns_i in tran_fns: data = tran_fns_i(data, *args, **kwargs) return data 
elpips.networks.squeezenet1_1_full_maxpool.fire|module def fire_module(self, input, index, ch_in, ch_out_squeeze, ch_out_expand): net = self._squeeze(input, index, ch_in, ch_out_squeeze) return self._expand(net, index, ch_out_squeeze, ch_out_expand) 
actor.main def main(_): if are_summaries_enabled(): summary_writer = tf.summary.create_file_writer(os.path.join(FLAGS. logdir, 'actor_{}'.format(FLAGS.task)), flush_millis=20000, max_queue=1000) timer_cls = profiling.ExportingTimer else: summary_writer = tf.summary.create_noop_writer() timer_cls = utils.nullcontext actor_step = 0 with summary_writer.as_default(): while True: try: client = grpc.Client(FLAGS.server_address) env = config.create_environment(FLAGS.task) run_id = np.random.randint(np.iinfo(np.int64).max) observation = env.reset() reward = 0.0 raw_reward = 0.0 done = False while True: tf.summary.experimental.set_step(actor_step) env_output = utils.EnvOutput(reward, done, np.array( observation)) with timer_cls('actor/elapsed_inference_s', 1000): action = client.inference((FLAGS.task, run_id, env_output, raw_reward)) with timer_cls('actor/elapsed_env_step_s', 1000): observation, reward, done, info = env.step(action. numpy()) raw_reward = float(info.get('score_reward', reward)) if done: with timer_cls('actor/elapsed_env_reset_s', 10): observation = env.reset() actor_step += 1 except (tf.errors.UnavailableError, tf.errors.CancelledError) as e: logging.exception(e) env.close() 
output.Output.train|begin|step def train_step_begin(self, step): if self.pctens is not None and step % self.pctens == 0: self.run_options = tf.RunOptions(report_tensor_allocations_upon_oom =True, trace_level=tf.RunOptions.FULL_TRACE) self.run_metadata = tf.RunMetadata() 
xlnet-master.tpu_estimator._OutfeedHostCallHook.begin def begin(self): self._init_ops = contrib_summary.summary_writer_initializer_op() self._finalize_ops = [] for op in self._init_ops: self._finalize_ops.append(contrib_summary.flush(writer=op.inputs[0])) 
gym_pycolab.things.Drape.Drape def __init__(self, curtain, character): """Construct a `Drape`.  Whatever arguments your `Drape` subclass takes, its first two should be the same `curtain` and `character` arguments taken here.  Unless you really know what you're doing, your `Drape` subclass should be certain to call this `__init__` method before doing anything else in its own constructor. Here's example code to copy and paste:  super(MyCoolDrape, self).__init__(curtain, character)  A `Drape` object that does not wish to be visible after construction should have its constructor assign `self._visible = False` after calling its superclass constructor.  Args: curtain: A 2-D numpy array with dtype `bool_`, which will be "owned" by this `Drape` and made accessible at `self.curtain`. Values in this array will be used as a mask for painting `character` onto the game board, at whatever time is dictated by this game's `Engine`'s z-ordering. Subclasses of `Drape` will update the mask by changing the data inside `self.curtain`. character: The character that this `Drape` paints onto the board. Subclasses will not paint this character directly; it's here for the object's reference when examining the arguments to `update`. """ self._c_u_r_t_a_i_n = curtain self._c_h_a_r_a_c_t_e_r = character 
v2_write_training.QueueChunkSrc.next def next(self): print('Queue next') if self.gen is None: self.gen = queue_gen(self.q, []) try: return next(self.gen) except: return None 
setup.download|data def download_data(url, zipped_path): data_stream = requests.get(url, stream=True) with open(zipped_path, 'wb') as file: total_length = int(data_stream.headers.get('content-length')) for chunk in progress.bar(data_stream.iter_content(chunk_size=1024), expected_size=total_length / 1024 + 1): if chunk: file.write(chunk) file.flush() zip_file = zipfile.ZipFile(zipped_path, 'r') zip_file.extractall('') zip_file.close() 
embeddings.OpenKE.config.Config.Config.parameters|set def set_parameters(self, lists): for i in lists: self.set_parameters_by_name(i, lists[i]) 
commons.measure.DropDevice.np|unmeasure def unmeasure_np(self, hparams, x_measured_val, theta_val): if hparams.unmeasure_type == 'medfilt': unmeasure_func = lambda image, mask: signal.medfilt(image) elif hparams.unmeasure_type == 'inpaint-telea': inpaint_type = cv2.INPAINT_TELEA unmeasure_func = measure_utils.get_inpaint_func_opencv(hparams, inpaint_type) elif hparams.unmeasure_type == 'inpaint-ns': inpaint_type = cv2.INPAINT_NS unmeasure_func = measure_utils.get_inpaint_func_opencv(hparams, inpaint_type) elif hparams.unmeasure_type == 'inpaint-tv': unmeasure_func = measure_utils.get_inpaint_func_tv() elif hparams.unmeasure_type == 'blur': unmeasure_func = measure_utils.get_blur_func() else: raise NotImplementedError x_unmeasured_val = np.zeros_like(x_measured_val) for i in range(x_measured_val.shape[0]): x_unmeasured_val[i] = unmeasure_func(x_measured_val[i], theta_val[i]) return x_unmeasured_val 
nets.nasnet.nasnet.stem|imagenet def _imagenet_stem(inputs, hparams, stem_cell, current_step=None, reuse=None): """Stem used for models trained on ImageNet.""" num_stem_cells = 2 with tf.variable_scope('', reuse=reuse): num_stem_filters = int(32 * hparams.stem_multiplier) net = slim.conv2d(inputs, num_stem_filters, [3, 3], stride=2, scope ='conv0', padding='VALID') net = slim.batch_norm(net, scope='conv0_bn') cell_outputs = [None, net] filter_scaling = 1.0 / hparams.filter_scaling_rate ** num_stem_cells for cell_num in range(num_stem_cells): net = stem_cell(net, scope='cell_stem_{}'.format(cell_num), filter_scaling=filter_scaling, stride=2, prev_layer= cell_outputs[-2], cell_num=cell_num, current_step=current_step) cell_outputs.append(net) filter_scaling *= hparams.filter_scaling_rate return net, cell_outputs 
utils.bool|str def str2bool(x): return x.lower() in 'true' 
nets.mobilenet.mobilenet.training|scope def training_scope(is_training=True, weight_decay=4e-05, stddev=0.09, dropout_keep_prob=0.8, bn_decay=0.997): """Defines Mobilenet training scope.  Usage: with tf.contrib.slim.arg_scope(mobilenet.training_scope()): logits, endpoints = mobilenet_v2.mobilenet(input_tensor)  # the network created will be trainble with dropout/batch norm # initialized appropriately. Args: is_training: if set to False this will ensure that all customizations are set to non-training mode. This might be helpful for code that is reused across both training/evaluation, but most of the time training_scope with value False is not needed. If this is set to None, the parameters is not added to the batch_norm arg_scope.  weight_decay: The weight decay to use for regularizing the model. stddev: Standard deviation for initialization, if negative uses xavier. dropout_keep_prob: dropout keep probability (not set if equals to None). bn_decay: decay for the batch norm moving averages (not set if equals to None).  Returns: An argument scope to use via arg_scope. """ batch_norm_params = {'decay': bn_decay, 'is_training': is_training} if stddev < 0: weight_intitializer = slim.initializers.xavier_initializer() else: weight_intitializer = tf.truncated_normal_initializer(stddev=stddev) with slim.arg_scope([slim.conv2d, slim.fully_connected, slim. separable_conv2d], weights_initializer=weight_intitializer, normalizer_fn=slim.batch_norm), slim.arg_scope([mobilenet_base, mobilenet], is_training=is_training), safe_arg_scope([slim. batch_norm], **batch_norm_params), safe_arg_scope([slim.dropout], is_training=is_training, keep_prob=dropout_keep_prob), slim.arg_scope([ slim.conv2d], weights_regularizer=slim.l2_regularizer(weight_decay) ), slim.arg_scope([slim.separable_conv2d], weights_regularizer=None ) as s: return s 
nets.vgg_test.VGGATest.Classes|test|No def testNoClasses(self): batch_size = 5 height, width = 224, 224 num_classes = None with self.test_session(): inputs = tf.random_uniform((batch_size, height, width, 3)) net, end_points = vgg.vgg_a(inputs, num_classes) expected_names = ['vgg_a/conv1/conv1_1', 'vgg_a/pool1', 'vgg_a/conv2/conv2_1', 'vgg_a/pool2', 'vgg_a/conv3/conv3_1', 'vgg_a/conv3/conv3_2', 'vgg_a/pool3', 'vgg_a/conv4/conv4_1', 'vgg_a/conv4/conv4_2', 'vgg_a/pool4', 'vgg_a/conv5/conv5_1', 'vgg_a/conv5/conv5_2', 'vgg_a/pool5', 'vgg_a/fc6', 'vgg_a/fc7'] self.assertSetEqual(set(end_points.keys()), set(expected_names)) self.assertTrue(net.op.name.startswith('vgg_a/fc7')) 
retrofit.retrofit def retrofit(wordVecs, lexicon, numIters): newWordVecs = deepcopy(wordVecs) wvVocab = set(newWordVecs.keys()) loopVocab = wvVocab.intersection(set(lexicon.keys())) for it in range(numIters): for word in loopVocab: wordNeighbours = set(lexicon[word]).intersection(wvVocab) numNeighbours = len(wordNeighbours) if numNeighbours == 0: continue newVec = numNeighbours * wordVecs[word] for ppWord in wordNeighbours: newVec += newWordVecs[ppWord] newWordVecs[word] = newVec / (2 * numNeighbours) return newWordVecs 
utils.ideal|kernel def ideal_kernel(labels): """ Compute the ideal kernel K An entry k_ij = 0 if i and j have different class k_ij = 1 if i and j have same class """ K = np.zeros([labels.shape[0], labels.shape[0]]) for i in range(labels.shape[0]): k = labels[i] == labels k.astype(int) K[:, (i)] = k[:, (0)] return K 
attack.deepfool.should|continue def should_continue(cond, i, x, r): return tf.logical_and(cond, tf.less(i, max_iter)) 
texar.modules.decoders.tf_helpers.CustomHelper.sample|ids|dtype @property def sample_ids_dtype(self): return self._sample_ids_dtype 
rasterize_triangles_test.RenderTest.Gradient|test|Computation|Render|Internal def testInternalRenderGradientComputation(self): """Isolates and verifies the Jacobian matrix for the custom kernel.""" image_height = 21 image_width = 28 clip_coordinates = tf.placeholder(tf.float32, shape=[8, 4]) barycentric_coordinates, _, _ = (rasterize_triangles. rasterize_triangles_module.rasterize_triangles(clip_coordinates, self.cube_triangles, image_width, image_height)) with self.test_session(): ndc_init = np.array([[-0.43889722, -0.53184521, 0.85293502, 1.0], [ -0.37635487, 0.22206162, 0.90555805, 1.0], [-0.22849123, 0.76811147, 0.80993629, 1.0], [-0.2805393, -0.14092168, 0.71602166, 1.0], [0.18631913, -0.62634289, 0.88603103, 1.0], [ 0.16183566, 0.08129397, 0.93020856, 1.0], [0.44147962, 0.53497446, 0.85076219, 1.0], [0.53008741, -0.31276882, 0.77620775, 1.0]], dtype=np.float32) theoretical, numerical = tf.test.compute_gradient(clip_coordinates, (8, 4), barycentric_coordinates, (image_height, image_width, 3), x_init_value=ndc_init, delta=0.04) jacobians_match, message = test_utils.check_jacobians_are_nearly_equal( theoretical, numerical, 0.01, 0.01) self.assertTrue(jacobians_match, message) 
cond_utils.model|ex|sdn|params def sdn_model_params_ex5(yy, iso, gain_init, cam, param_inits): with tf.variable_scope('sdn_gain', reuse=tf.AUTO_REUSE): c_i, beta1_i, beta2_i, gain_params_i, cam_params_i = param_inits n_param_per_cam = 3 cam_vals = tf.constant([0, 1, 2, 3, 4], dtype=tf.float32) cam_params = tf.get_variable('cam_params', [n_param_per_cam, cam_vals.shape[0]], tf.float32, initializer=tf. constant_initializer(cam_params_i)) cam_idx = tf.where(tf.equal(cam_vals, cam)) cam_idx = cam_idx[0] cam_one_hot = tf.one_hot(cam_idx, cam_vals.shape[0]) one_cam_params = tf.reduce_sum(cam_one_hot * cam_params, axis=1) one_cam_params = tf.exp(c_i * one_cam_params) gain = tf.get_variable('gain_val', [1], tf.float32, initializer=tf. constant_initializer(1.0)) iso_vals = tf.constant([100, 400, 800, 1600, 3200], dtype=tf.float32) gain_params = tf.get_variable('gain_params', [iso_vals.shape[0]], tf.float32, initializer=tf.constant_initializer(gain_params_i)) iso_idx = tf.where(tf.equal(iso_vals, iso)) gain_one_hot = tf.one_hot(iso_idx, iso_vals.shape[0]) g = tf.reduce_sum(gain_one_hot * gain_params) gain = tf.exp(c_i * g * one_cam_params[2]) * iso beta1 = tf.get_variable('beta1', [1], tf.float32, initializer=tf. constant_initializer(beta1_i)) beta2 = tf.get_variable('beta2', [1], tf.float32, initializer=tf. constant_initializer(beta2_i)) beta1 = tf.exp(c_i * beta1 * one_cam_params[0]) beta2 = tf.exp(c_i * beta2 * one_cam_params[1]) scale = tf.sqrt(beta1 * yy / gain + beta2) return scale 
nets.vgg_test.VGG16Test.test|Build def testBuild(self): batch_size = 5 height, width = 224, 224 num_classes = 1000 with self.test_session(): inputs = tf.random_uniform((batch_size, height, width, 3)) logits, _ = vgg.vgg_16(inputs, num_classes) self.assertEquals(logits.op.name, 'vgg_16/fc8/squeezed') self.assertListEqual(logits.get_shape().as_list(), [batch_size, num_classes]) 
models.triplet.Triplet.by|norm|ops|clip def _clip_by_norm_ops(self): return [tf.assign(self.user_embeddings, tf.clip_by_norm(self. user_embeddings, self.clip_norm, axes=[1])), tf.assign(self. item_embeddings, tf.clip_by_norm(self.item_embeddings, self. clip_norm, axes=[1]))] 
facenet-master.src.validate_on_lfw.parse|arguments def parse_arguments(argv): parser = argparse.ArgumentParser() parser.add_argument('lfw_dir', type=str, help= 'Path to the data directory containing aligned LFW face patches.') parser.add_argument('--lfw_batch_size', type=int, help= 'Number of images to process in a batch in the LFW test set.', default=100) parser.add_argument('model', type=str, help= 'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file' ) parser.add_argument('--image_size', type=int, help= 'Image size (height, width) in pixels.', default=160) parser.add_argument('--lfw_pairs', type=str, help= 'The file containing the pairs to use for validation.', default= 'data/pairs.txt') parser.add_argument('--lfw_nrof_folds', type=int, help= 'Number of folds to use for cross validation. Mainly used for testing.' , default=10) parser.add_argument('--distance_metric', type=int, help= 'Distance metric  0:euclidian, 1:cosine similarity.', default=0) parser.add_argument('--use_flipped_images', help= 'Concatenates embeddings for the image and its horizontally flipped counterpart.' , action='store_true') parser.add_argument('--subtract_mean', help= 'Subtract feature mean before calculating distance.', action= 'store_true') parser.add_argument('--use_fixed_image_standardization', help= 'Performs fixed standardization of images.', action='store_true') return parser.parse_args(argv) 
gym_pycolab.prefab_parts.sprites.MazeWalker.teleport def _teleport(self, virtual_position): """Set the new virtual position of the agent, applying side-effects.  This method is a somewhat "low level" method: it doesn't check whether the new location has an impassible character in it, nor does it apply any scrolling orders that may be current (if called during a game iteration). This method is only grudgingly "protected" (and not "private"), mainly to allow `MazeWalker` subclasses to initialise their location at a place somewhere off the board. Use at your own risk.  This method does handle entering and exiting the board in the conventional way. Virtual positions off of the board yield a true position of `(0, 0)`, and `_on_board_exit` and `_on_board_enter` are called as appropriate.  Args: virtual_position: A 2-tuple containing the intended virtual position for this `MazeWalker`. """ new_row, new_col = virtual_position old_row, old_col = self._virtual_row, self._virtual_col old_on_board = self._on_board(old_row, old_col) new_on_board = self._on_board(new_row, new_col) if old_on_board and not new_on_board: self._on_board_exit() self._virtual_row, self._virtual_col = new_row, new_col if new_on_board: self._position = self.Position(new_row, new_col) else: self._position = self.Position(0, 0) if not old_on_board and new_on_board: self._on_board_enter() 
extensions.multi_modal_application.MultiModalApplication.sampler|grid|initialise def initialise_grid_sampler(self): self.sampler = [[GridSampler(reader=reader, window_sizes=self. data_param, batch_size=self.net_param.batch_size, spatial_window_size=self.action_param.spatial_window_size, window_border=self.action_param.border, smaller_final_batch_mode= self.net_param.smaller_final_batch_mode, queue_length=self. net_param.queue_length) for reader in self.readers]] 
models.aae.Autoencoder.train def train(self, sess, input_features, input_labels, learning_rate, iteration, summary_rate=250): """Single step training of the autoencoder @param sess (tf.Session) the current session @param input_feature (np.array) matrix of features @param input_labels (np.array) array of labels @param learning_rate @param lambda_e explicit mixing coefficient @param lambda_i implicit mixing coefficient @param iteration the current iteration (used for the summary index) @param summary_rate summary written at this rate (iterations) @return (float) the global loss """ _, _, _, loss, summ = sess.run([self.train_op, self. discriminator_train_op, self.generator_train_op, self. loss_reconstruction, self.summaries], feed_dict={self.x: input_features, self.labels_placeholder: input_labels, self. learning_rate: learning_rate}) if self.train_iteration % summary_rate == 0: self.tf_summary_writer.add_summary(summ, global_step=self. train_iteration) self.tf_summary_writer.flush() self.train_iteration = iteration return loss 
translate.beam_search.search|rnn|beam def rnn_beam_search(update_funs, initial_states, sequence_length, beam_size, len_normalization=None, temperature=None, parallel_iterations=16, swap_memory=True): """ :param update_funs: function to compute the next state and logits given the current state and previous ids :param initial_states: recurrent model states :param sequence_length: maximum output length :param beam_size: beam size :param len_normalization: length normalization coefficient (0 or None for no length normalization) :return: tensor of size (batch_size, beam_size, seq_len) containing the beam-search hypotheses sorted by best score (axis 1), and tensor of size (batch_size, beam_size) containing the said scores. """ batch_size = tf.shape(initial_states[0])[0] state_sizes = [tf.shape(state)[1] for state in initial_states] states = [] for initial_state in initial_states: state = tf.tile(tf.expand_dims(initial_state, axis=1), [1, beam_size, 1]) states.append(state) states = tf.concat(states, axis=2) scores = tf.concat([tf.ones(shape=[batch_size, 1]), tf.zeros(shape=[ batch_size, beam_size - 1])], axis=1) scores = tf.log(scores) ids = tf.tile([[utils.BOS_ID]], [batch_size, beam_size]) hypotheses = tf.expand_dims(ids, axis=2) mask = tf.ones([batch_size, beam_size], dtype=tf.float32) time = tf.constant(0, dtype=tf.int32, name='time')  def time_step(time, mask, hypotheses, states, token_ids, scores): token_ids = tf.reshape(token_ids, [batch_size * beam_size]) token_scores = tf.zeros([batch_size, beam_size, 1]) new_states = [] states = tf.split(states, num_or_size_splits=state_sizes, axis=2) for k, (state, state_size, update_fun) in enumerate(zip(states, state_sizes, update_funs)): state = tf.reshape(state, [batch_size * beam_size, state_size]) scope = tf.get_variable_scope() if len(update_funs ) == 1 else 'model_{}'.format(k + 1) with tf.variable_scope(scope, reuse=True): state, logits = update_fun(state, token_ids, time) state = tf.reshape(state, [batch_size, beam_size, state_size]) new_states.append(state) num_classes = tf.shape(logits)[1] logits = tf.reshape(logits, [batch_size, beam_size, num_classes]) token_scores += log_softmax(logits, axis=2, temperature=temperature ) num_classes = tf.shape(token_scores)[2] mask1 = tf.expand_dims(mask, axis=2) mask2 = tf.one_hot(indices=[[utils.EOS_ID]], depth=num_classes) token_scores = token_scores * mask1 + (1 - mask1) * (1 - mask2 ) * -1e+30 sum_logprobs = tf.expand_dims(scores, axis=2) + token_scores scores, indices = tf.nn.top_k(tf.reshape(sum_logprobs, [batch_size, num_classes * beam_size]), k=beam_size) beam_ids = indices // num_classes token_ids = indices % num_classes states = tf.concat([batch_gather(state, beam_ids) for state in new_states], axis=2) hypotheses = tf.concat([batch_gather(hypotheses, beam_ids), tf. expand_dims(token_ids, axis=2)], axis=2) mask = batch_gather(mask, beam_ids) * tf.to_float(tf.not_equal( token_ids, utils.EOS_ID)) return time + 1, mask, hypotheses, states, token_ids, scores loop_vars = [time, mask, hypotheses, states, ids, scores] shapes = [tf.TensorShape([None] * len(var.shape)) for var in loop_vars]  def cond(time, mask, *_): p1 = time < sequence_length p2 = tf.to_int32(tf.reduce_sum(1 - mask)) < batch_size * beam_size return tf.logical_and(p1, p2) _, mask, hypotheses, states, ids, scores = tf.while_loop(cond=cond, body=time_step, loop_vars=loop_vars, shape_invariants=shapes, parallel_iterations=parallel_iterations, swap_memory=swap_memory) hypotheses = hypotheses[:, :, 1:] if len_normalization: n = tf.shape(hypotheses)[1] sequence_length = tf.shape(hypotheses)[2] sel_ids_ = tf.reshape(hypotheses, shape=[batch_size * n, sequence_length]) mask = get_weights(sel_ids_, utils.EOS_ID, include_first_eos=True) length = tf.reduce_sum(mask, axis=1) length = tf.reshape(length, shape=[batch_size, n]) scores /= length ** len_normalization scores, indices = tf.nn.top_k(scores, k=beam_size, sorted=True) indices = tf.stack([tf.tile(tf.expand_dims(tf.range(batch_size), axis=1), [1, beam_size]), indices], axis=2) hypotheses = tf.gather_nd(hypotheses, indices) return hypotheses, scores 
normalize_embeddings.main def main(): parser = argparse.ArgumentParser(description='Normalize word embeddings') parser.add_argument('actions', choices=['unit', 'center', 'unitdim', 'centeremb'], nargs='*', default=[], help= 'the actions to perform in order') parser.add_argument('-i', '--input', default=sys.stdin.fileno(), help= 'the input word embedding file (defaults to stdin)') parser.add_argument('-o', '--output', default=sys.stdout.fileno(), help ='the output word embedding file (defaults to stdout)') parser.add_argument('--encoding', default='utf-8', help= 'the character encoding for input/output (defaults to utf-8)') args = parser.parse_args() f = open(args.input, encoding=args.encoding, errors='surrogateescape') words, matrix = embeddings.read(f) embeddings.normalize(matrix, args.actions) f = open(args.output, mode='w', encoding=args.encoding, errors= 'surrogateescape') embeddings.write(words, matrix, f) 
texar.data.data.multi_aligned_data.MultiAlignedData.utterance|name|cnt def utterance_cnt_name(self, name_or_id): """The name of utterance count tensor of text dataset by its name or id. If the dataset is not variable utterance text data, returns `None`. """ i = self._maybe_name_to_id(name_or_id) if not _is_text_data(self._hparams.datasets[i]['data_type'] ) or not self._hparams.datasets[i]['variable_utterance']: return None name = dsutils._connect_name(self._data_spec.name_prefix[i], self. _data_spec.decoder[i].utterance_cnt_tensor_name) return name 
env.R2REnv.reset def _reset(self): """Reset the environment with new data.  Returns: A instance of common.EnvOutput, which is the initial Observation. """ self._current_idx = self._get_next_idx(self._current_idx) current_scan_id = self._paths[self._current_idx]['scan_id'] current_pano_id = self._scan_info[current_scan_id].pano_name_to_id[self ._paths[self._current_idx]['path'][0]] self._path_history = [current_pano_id] return common.EnvOutput(reward=np.float32(0.0), done=False, observation =self._get_current_observation(current_pano_id, current_scan_id, 0), info='') 
utils.loss_utils.Loss|PSNR def PSNRLoss(y_true, y_pred): """ PSNR is Peek Signal to Noise Ratio, which is similar to mean squared error. PSNR = 20 * log10(MAXp) - 10 * log10(MSE) """ return -10.0 * K.log(K.mean(K.square(y_pred - y_true))) / K.log(10.0) 
prune_vgg11.PruneVgg11.get|pruned|weights|cared def get_pruned_cared_weights(self, weights_dict): return collections.OrderedDict([(key, weight) for key, weight in weights_dict.items() if 'kernel' in key]) 
train_eval.pad|videos|and|concatenate def pad_and_concatenate_videos(videos): max_episode_length = max([len(video) for video in videos]) for video in videos: if len(video) < max_episode_length: video.extend([np.zeros_like(video[-1])] * (max_episode_length - len(video))) videos = [np.concatenate(frames, axis=1) for frames in zip(*videos)] return videos 
ops.loss|cam def cam_loss(source, non_source): identity_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits( labels=tf.ones_like(source), logits=source)) non_identity_loss = tf.reduce_mean(tf.nn. sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(non_source), logits=non_source)) loss = identity_loss + non_identity_loss return loss 
tokenization_test.TokenizationTest.test|is|whitespace def test_is_whitespace(self): self.assertTrue(_is_whitespace(' ')) self.assertTrue(_is_whitespace('\t')) self.assertTrue(_is_whitespace('\r')) self.assertTrue(_is_whitespace('\n')) self.assertTrue(_is_whitespace('\xa0')) self.assertFalse(_is_whitespace('A')) self.assertFalse(_is_whitespace('-')) 
find_related_triples.relation|read|id def read_relation2id(): """ Reads relation-to-id mapping from file. :return: Tuple containing relation-to-id and id-to-relation map. """ relation_to_id = {} id_to_relation = {} with open(PATH_relation2id, 'r', encoding='utf8') as f: for line in f.readlines(): elements = line.strip().split() if len(elements) < 2: continue relation_str = elements[0].split('/')[-1] relation_id = elements[1] relation_to_id[relation_str] = relation_id id_to_relation[relation_id] = relation_str return relation_to_id, id_to_relation 
darkflow.net.build.TFNet.savepb def savepb(self): """ Create a standalone const graph def that C++	can load and run. """ darknet_pb = self.to_darknet() flags_pb = self.FLAGS flags_pb.verbalise = False flags_pb.train = False tfnet_pb = TFNet(flags_pb, darknet_pb) tfnet_pb.sess = tf.Session(graph=tfnet_pb.graph) name = 'built_graph/{}.pb'.format(self.meta['name']) os.makedirs(os.path.dirname(name), exist_ok=True) with open('built_graph/{}.meta'.format(self.meta['name']), 'w') as fp: json.dump(self.meta, fp) self.say('Saving const graph def to {}'.format(name)) graph_def = tfnet_pb.sess.graph_def tf.train.write_graph(graph_def, './', name, False) 
dataset.TOICOCODataset.load|objs def load_objs(self, image_id, is_normalized): info = self.image_info[image_id] meta_path = info['path'] + '_meta.txt' inst_dict = info['inst_dict'] with open(meta_path, 'r') as f: lines = f.readlines() Vs = [] Fs = [] for i, line in enumerate(lines): words = line[:-1].split(' ') inst_id = int(words[0]) if not inst_id in inst_dict: continue if len(words) == 3: if words[2][-3:] == 'npz': obj_name = words[2].replace('.npz', '_norm.obj') mesh_file = os.path.join(self.config.OBJ_MODEL_DIR, 'real_val', obj_name) else: mesh_file = os.path.join(self.config.OBJ_MODEL_DIR, 'real_' + self.subset, words[2] + '.obj') flip_flag = False else: assert len(words) == 4 mesh_file = os.path.join(self.config.OBJ_MODEL_DIR, self.subset, words[2], words[3], 'model.obj') flip_flag = True vertices, faces = utils.load_mesh(mesh_file, is_normalized, flip_flag) Vs.append(vertices) Fs.append(faces) return Vs, Fs 
texar.modules.embedders.embedders.WordEmbedder.dim @property def dim(self): """The embedding dimension. """ return self._dim 
SRGANs-Spectral-Regularization-GANs--master.gen_models.resnet_32.ResNetGenerator.Res|Generator|Net def __init__(self, ch=256, dim_z=128, bottom_width=4, activation=F.relu, n_classes=0, distribution='normal'): super(ResNetGenerator, self).__init__() self.bottom_width = bottom_width self.activation = activation self.distribution = distribution self.dim_z = dim_z self.n_classes = n_classes with self.init_scope(): self.l1 = L.Linear(dim_z, bottom_width ** 2 * ch, initialW=chainer. initializers.GlorotUniform()) self.block2 = Block(ch, ch, activation=activation, upsample=True, n_classes=n_classes) self.block3 = Block(ch, ch, activation=activation, upsample=True, n_classes=n_classes) self.block4 = Block(ch, ch, activation=activation, upsample=True, n_classes=n_classes) self.b5 = L.BatchNormalization(ch) self.c5 = L.Convolution2D(ch, 3, ksize=3, stride=1, pad=1, initialW =chainer.initializers.GlorotUniform()) 
extensions.multi_modal_application.MultiModalApplication.loader|initialise|dataset def initialise_dataset_loader(self, data_param=None, task_param=None, data_partitioner=None): self.data_param = data_param self.segmentation_param = task_param if self.is_training: reader_names = 'image', 'label', 'weight', 'sampler' elif self.is_inference: reader_names = 'image', elif self.is_evaluation: reader_names = 'image', 'label', 'inferred' else: tf.logging.fatal('Action `%s` not supported. Expected one of %s', self.action, self.SUPPORTED_PHASES) raise ValueError try: reader_phase = self.action_param.dataset_to_infer except AttributeError: reader_phase = None file_lists = data_partitioner.get_file_lists_by(phase=reader_phase, action=self.action) self.readers = [ImageReader(reader_names).initialise(data_param, task_param, file_list) for file_list in file_lists] foreground_masking_layer = BinaryMaskingLayer(type_str=self.net_param. foreground_type, multimod_fusion=self.net_param. multimod_foreground_type, threshold=0.0 ) if self.net_param.normalise_foreground_only else None mean_var_normaliser = MeanVarNormalisationLayer(image_name='image', binary_masking_func=foreground_masking_layer ) if self.net_param.whitening else None histogram_normaliser = (HistogramNormalisationLayer(image_name='image', modalities=vars(task_param).get('image'), model_filename=self. net_param.histogram_ref_file, binary_masking_func= foreground_masking_layer, norm_type=self.net_param.norm_type, cutoff=self.net_param.cutoff, name='hist_norm_layer') if self. net_param.histogram_ref_file and self.net_param.normalisation else None ) label_normalisers = None if self.net_param.histogram_ref_file and task_param.label_normalisation: label_normalisers = [DiscreteLabelNormalisationLayer(image_name= 'label', modalities=vars(task_param).get('label'), model_filename=self.net_param.histogram_ref_file)] if self.is_evaluation: label_normalisers.append(DiscreteLabelNormalisationLayer( image_name='inferred', modalities=vars(task_param).get( 'inferred'), model_filename=self.net_param.histogram_ref_file)) label_normalisers[-1].key = label_normalisers[0].key normalisation_layers = [] if histogram_normaliser is not None: normalisation_layers.append(histogram_normaliser) if mean_var_normaliser is not None: normalisation_layers.append(mean_var_normaliser) if task_param.label_normalisation and (self.is_training or not task_param.output_prob): normalisation_layers.extend(label_normalisers) volume_padding_layer = [] if self.net_param.volume_padding_size: volume_padding_layer.append(PadLayer(image_name=SUPPORTED_INPUT, border=self.net_param.volume_padding_size, mode=self.net_param. volume_padding_mode)) augmentation_layers = [] if self.is_training: train_param = self.action_param if train_param.random_flipping_axes != -1: augmentation_layers.append(RandomFlipLayer(flip_axes= train_param.random_flipping_axes)) if train_param.scaling_percentage: augmentation_layers.append(RandomSpatialScalingLayer( min_percentage=train_param.scaling_percentage[0], max_percentage=train_param.scaling_percentage[1], antialiasing=train_param.antialiasing)) if (train_param.rotation_angle or train_param.rotation_angle_x or train_param.rotation_angle_y or train_param.rotation_angle_z): rotation_layer = RandomRotationLayer() if train_param.rotation_angle: rotation_layer.init_uniform_angle(train_param.rotation_angle) else: rotation_layer.init_non_uniform_angle(train_param. rotation_angle_x, train_param.rotation_angle_y, train_param.rotation_angle_z) augmentation_layers.append(rotation_layer) if train_param.do_elastic_deformation: spatial_rank = list(self.readers[0].spatial_ranks.values())[0] augmentation_layers.append(RandomElasticDeformationLayer( spatial_rank=spatial_rank, num_controlpoints=train_param. num_ctrl_points, std_deformation_sigma=train_param. deformation_sigma, proportion_to_augment=train_param. proportion_to_deform)) self.readers[0].add_preprocessing_layers(volume_padding_layer + normalisation_layers + augmentation_layers) for reader in self.readers[1:]: reader.add_preprocessing_layers(volume_padding_layer + normalisation_layers) 
cpplint._IncludeState.Order|Is|Alphabetical|In def IsInAlphabeticalOrder(self, clean_lines, linenum, header_path): """Check if a header is in alphabetical order with the previous header.  Args: clean_lines: A CleansedLines instance containing the file. linenum: The number of the line to check. header_path: Canonicalized header to be checked.  Returns: Returns true if the header is in alphabetical order. """ if self._last_header > header_path and Match('^\\s*#\\s*include\\b', clean_lines.elided[linenum - 1]): return False return True 
texar.modules.decoders.rnn_decoders.AttentionRNNDecoder.initialize def initialize(self, name=None): helper_init = self._helper.initialize() flat_initial_state = nest.flatten(self._initial_state) dtype = flat_initial_state[0].dtype initial_state = self._cell.zero_state(batch_size=tf.shape( flat_initial_state[0])[0], dtype=dtype) initial_state = initial_state.clone(cell_state=self._initial_state) return [helper_init[0], helper_init[1], initial_state] 
model.DeepWalk.feed|generator|dict def feed_dict_generator(self, a_random_walk, step, gamma): """ Method to generate random walk features, gamma and proper time index. """ batch_inputs = batch_input_generator(a_random_walk, self.args. random_walk_length, self.args.window_size) batch_labels = batch_label_generator(a_random_walk, self.args. random_walk_length, self.args.window_size) feed_dict = {self.walker_layer.train_labels: batch_labels, self. walker_layer.train_inputs: batch_inputs, self.gamma: gamma, self. step: float(step)} return feed_dict 
plato.agent.component.dialogue_manager.dialogue_manager.DialogueManager.receive|input def receive_input(self, inpt): """ Receive input and update the dialogue state.  :return: Nothing """ self.DSTracker.update_state(inpt) if self.domain and self.domain in ['CamRest', 'SFH', 'SlotFilling']: if self.agent_role == 'system': db_result, sys_req_slot_entropies = self.db_lookup() self.DSTracker.update_state_db(db_result=db_result, sys_req_slot_entropies=sys_req_slot_entropies) else: self.DSTracker.update_state_db(db_result=None, sys_acts=inpt) return inpt 
plato.agent.component.dialogue_policy.reinforcement_learning.minimax_q_policy.MinimaxQPolicy.maxmin def maxmin(self, state_enc, retry=False): """ Solve the maxmin problem  :param state_enc: the encoding to the state :param retry: :return: """ c = np.zeros(self.NActions + 1) c[0] = -1 A_ub = np.ones((self.NOtherActions, self.NActions + 1)) A_ub[:, 1:] = -np.asarray(self.Q[state_enc]) b_ub = np.zeros(self.NOtherActions) A_eq = np.ones((1, self.NActions + 1)) A_eq[0, 0] = 0 b_eq = [1] bounds = ((None, None),) + ((0, 1),) * self.NActions res = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds) if res.success: self.pi[state_enc] = res.x[1:] elif not retry: return self.maxmin(state_enc, retry=True) else: print('Alert : %s' % res.message) if state_enc in self.V: return self.V[state_enc] else: print('Warning, state not in V, returning 0.') return 0 return res.x[0] 
environment_test.EnvironmentTest.step|test|run def test_run_step(self): env = config.create_environment(FLAGS.task) env.reset() env.step(0) env.close() 
layers.BatchNormLayer.rate|learning|set def set_learning_rate(self, session, new_rate): session.run(self.set_lr_op, {self.lr_ph: new_rate}) 
xlnet-master.tpu_estimator.variables|ops|sync def _sync_variables_ops(ctx): """Create varriables synchronization ops.  Gets the variables back from TPU nodes. This means the variables updated by TPU will now be *synced* to host memory. In BROADCAST mode, we skip this sync since the variables are ususally too big to transmit via RPC.  Args: ctx: A `_InternalTPUContext` instance with mode.  Returns: A list of sync ops. """ if not ctx.is_input_broadcast_with_iterators(): return [array_ops.check_numerics(v.read_value(), 'Gradient for %s is NaN' % v.name).op for v in variables. trainable_variables()] else: return [control_flow_ops.no_op()] 
neural_tangents.predict.momentum.ufl def ufl(fx): """Unflatten outputs.""" return np.reshape(fx, (-1, output_dimension)) 
darc.DataArchive.items def items(self): """Returns a generator that iterates over the stored (object name, Metadata) pairs.  Does not iterate over objects which have been stored without a name. """ for key in self.keys(): yield key, self[key] 
data_utils.Dataset.bow|print def print_bow(self, output_dict, batch_dict): """Print the bow prediction: the input sentence, the target bow, and the predicted bow. """ enc_sentences = batch_dict['enc_inputs'] enc_targets = batch_dict['enc_targets'] enc_outputs = output_dict['pred_ind']  def _decode_set(s, shared): output = [] for si in s: if si in shared: output.append('[' + self.id2word[si] + ']') else: output.append(self.id2word[si]) return for i, (s, t, o) in enumerate(zip(enc_sentences, enc_targets, enc_outputs) ): if i in [0, 1, 5, 6, 10, 11, 15, 16]: print('inputs:') print('    ' + self.decode_sent(s)) shared = set(t) & set(o) print('targets:') print('    ' + _decode_set(set(t) - set([self.pad_id]), shared)) print('outputs:') print('    ' + _decode_set(set(o), shared)) print('') if i == 16: break return 
layers.TransitionMatrixApplication.shape|output|compute def compute_output_shape(self, input_shapes): transition_matrices_shape, label_predictions_shape = input_shapes return label_predictions_shape 
xlnet-master.run_squad.get|spm|basename def _get_spm_basename(): spm_basename = os.path.basename(FLAGS.spiece_model_file) return spm_basename 
bert-master.tokenization_test.TokenizationTest.test|control|is def test_is_control(self): self.assertTrue(tokenization._is_control('\x05')) self.assertFalse(tokenization._is_control('A')) self.assertFalse(tokenization._is_control(' ')) self.assertFalse(tokenization._is_control('\t')) self.assertFalse(tokenization._is_control('\r')) self.assertFalse(tokenization._is_control('')) 
deepcp.get|output|corr def get_corr_output(x_input_corr): (x_tar_one_hot_corr, x_tar_mul_hot_corr, x_input_one_hot_dict_corr, x_input_mul_hot_dict_corr) = partition_input_corr(x_input_corr) data_embed_tar = get_concate_embed(x_tar_one_hot_corr, x_tar_mul_hot_corr) data_vec_tar = tf.reshape(data_embed_tar, [-1, (n_one_hot_slot_corr + n_mul_hot_slot_corr) * k]) n_layer_corr = len(layer_dim_corr) cur_layer = data_vec_tar for i in range(0, n_layer_corr): if i == n_layer_corr - 1: cur_layer = tf.nn.tanh(tf.matmul(cur_layer, weight_dict_corr[i] ) + bias_dict_corr[i]) else: cur_layer = tf.nn.relu(tf.matmul(cur_layer, weight_dict_corr[i] ) + bias_dict_corr[i]) data_rep_tar = cur_layer inner_prod_dict = {} for mm in range(n_neg_used_corr + 1): cur_data_embed = get_concate_embed(x_input_one_hot_dict_corr[mm], x_input_mul_hot_dict_corr[mm]) cur_data_vec = tf.reshape(cur_data_embed, [-1, (n_one_hot_slot_corr + n_mul_hot_slot_corr) * k]) cur_layer = cur_data_vec for i in range(0, n_layer_corr): if i == n_layer_corr - 1: cur_layer = tf.nn.tanh(tf.matmul(cur_layer, weight_dict_corr[i]) + bias_dict_corr[i]) else: cur_layer = tf.nn.relu(tf.matmul(cur_layer, weight_dict_corr[i]) + bias_dict_corr[i]) cur_data_rep = cur_layer inner_prod_dict[mm] = tf.reduce_sum(tf.multiply(data_rep_tar, cur_data_rep), 1, keep_dims=True) return inner_prod_dict 
bert-master.run_classifier.DataProcessor.get|examples|dev def get_dev_examples(self, data_dir): """Gets a collection of `InputExample`s for the dev set.""" raise NotImplementedError() 
sensor_correction.gp_cpu.GPRegressor.save def save(self, fname): joblib.dump(self.gpr, fname) 
run_nvidia.main def main(): parser = argparse.ArgumentParser() parser.add_argument('mode') parser.add_argument('--trial', required=True) parser.add_argument('--version', required=True) parser.add_argument('--retrain-epochs', type=int) parser.add_argument('--iteration-count', type=int, default=1) parser.add_argument('--density', type=float) parser.add_argument('--base-dir') args = parser.parse_args() if args.mode == 'train': train(args.trial, args.version) else: assert args.retrain_epochs and args.density and args.base_dir if args.mode == 'finetune': finetune(args.trial, args.version, args.retrain_epochs, args. density, args.base_dir, args.iteration_count) elif args.mode == 'lottery': lottery(args.trial, args.version, args.retrain_epochs, args. density, args.base_dir) elif args.mode == 'lr_finetune': lr_finetune(args.trial, args.version, args.retrain_epochs, args .density, args.base_dir) elif args.mode == 'lr_lottery': lr_lottery(args.trial, args.version, args.retrain_epochs, args. density, args.base_dir) elif args.mode == 'reinit': reinit(args.trial, args.version, args.retrain_epochs, args. density, args.base_dir) else: raise ValueError() 
src.net_factory.conv|dw def conv_dw(inputs, out_channels, kernel_size=3, stride=1, dilation=1, training=True, name=''): net = tf.layers.separable_conv2d(inputs, filters=out_channels, kernel_size=kernel_size, strides=stride, dilation_rate=dilation, padding='same', use_bias=False, name=name) net = tf.layers.batch_normalization(net, training=training) net = tf.nn.relu(net) return net 
elpips.networks.Network.conv def _conv(self, tensor, weight, bias, w_shape, padding, stride=[1, 2, 2, 1], data_format='NHWC'): """Give 'tensor' as a tuple to run the same dropout to multiple tensors.""" if self.use_net_dropout: dropout_random = tf.random_uniform(tf.shape(as_tuple(tensor)[0]), dtype=tf.float32) dropout_weights = tf.cast(tf.less(dropout_random, self. net_dropout_keep_prob), self.dtype) / float(self. net_dropout_keep_prob) tensor = for_each(tensor, lambda X: dropout_weights * X) tensor = for_each(tensor, lambda X: tf.nn.conv2d(X, f32_to_dtype(weight, self.dtype), strides=stride, padding=padding, data_format= data_format) + f32_to_dtype(bias, self.dtype)) tensor = for_each(tensor, lambda X: tf.nn.relu(X)) return tensor 
cleverhans.attacks.VirtualAdversarialMethod.parse|params def parse_params(self, eps=2.0, num_iterations=1, xi=1e-06, clip_min=None, clip_max=None, **kwargs): """ Take in a dictionary of parameters and applies attack-specific checks before saving them as attributes.  Attack-specific parameters: :param eps: (optional float )the epsilon (input variation parameter) :param num_iterations: (optional) the number of iterations :param xi: (optional float) the finite difference parameter :param clip_min: (optional float) Minimum input component value :param clip_max: (optional float) Maximum input component value """ self.eps = eps self.num_iterations = num_iterations self.xi = xi self.clip_min = clip_min self.clip_max = clip_max return True 
xlnet-master.tpu_estimator._SetEvalIterationsHook.session|create|after def after_create_session(self, session, coord): self._iterations_per_loop_var.load(self._num_steps, session=session) 
vkge.models.base.BilinearModel.Model|Bilinear def __init__(self, *args, **kwargs): """ Implementation of the Bilinear model [1] [1] Nickel, M. et al. - A Three-Way Model for Collective Learning on Multi-Relational Data - ICML 2011  :param similarity_function: Similarity function. """ super().__init__(*args, **kwargs) 
sidd_utils.sample|one|patch def sample_one_patch(input_image, gt_image, var_image, patch_height, patch_width): H = input_image.shape[1] W = input_image.shape[2] i = np.random.randint(0, H - patch_height + 1) j = np.random.randint(0, W - patch_width + 1) input_patch = input_image[:, i:i + patch_height, j:j + patch_width, :] gt_patch = gt_image[:, i:i + patch_height, j:j + patch_width, :] return input_patch, gt_patch, None 
generate-vmulcaddc-test.generate|cases|test def generate_test_cases(ukernel, channel_tile, row_tile, isa): """Generates all tests cases for a VMULCADDC micro-kernel.  Args: ukernel: C name of the micro-kernel function. channel_tile: Number of channels processed per one iteration of the inner loop of the micro-kernel. row_tile: Number of rows processed per one iteration of the outer loop of the micro-kernel. isa: instruction set required to run the micro-kernel. Generated unit test will skip execution if the host processor doesn't support this ISA.  Returns: Code for the test case. """ _, test_name = ukernel.split('_', 1) _, datatype, ukernel_type, _ = ukernel.split('_', 3) test_args = [ukernel] if not isa or isa == 'psimd': test_args.append('VMulCAddCMicrokernelTester::Variant::Scalar') return xngen.preprocess(VMULCADDC_TEST_TEMPLATE, {'TEST_NAME': test_name.upper().replace('UKERNEL_', ''), 'TEST_ARGS': test_args, 'DATATYPE': datatype, 'CHANNEL_TILE': channel_tile, 'ROW_TILE': row_tile, 'ISA_CHECK': xnncommon.generate_isa_check_macro(isa), 'next_prime': next_prime}) 
nmt.model_helper.MaskedNasCell.Masked|Nas|Cell def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) 
pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.encode|sents def encode_sents(self, sents, ordered=False, verbose=False): if verbose: print('encoding {} sents ...'.format(len(sents))) encoded = [] for idx, symbols in enumerate(sents): if verbose and idx > 0 and idx % 500000 == 0: print('    line {}'.format(idx)) encoded.append(self.convert_to_tensor(symbols)) if ordered: encoded = torch.cat(encoded) return encoded 
deepctr.input_embedding.get|inputs|embedding def get_inputs_embedding(feature_dim_dict, embedding_size, l2_reg_embedding, l2_reg_linear, init_std, seed, sparse_input_dict, dense_input_dict, sequence_input_dict, sequence_input_len_dict, sequence_max_len_dict, include_linear, prefix=''): deep_sparse_emb_dict = create_embedding_dict(feature_dim_dict, embedding_size, init_std, seed, l2_reg_embedding, prefix=prefix + 'sparse') deep_emb_list = get_embedding_vec_list(deep_sparse_emb_dict, sparse_input_dict, feature_dim_dict['sparse']) deep_emb_list = merge_sequence_input(deep_sparse_emb_dict, deep_emb_list, sequence_input_dict, sequence_input_len_dict, sequence_max_len_dict, feature_dim_dict['sequence']) deep_emb_list = merge_dense_input(dense_input_dict, deep_emb_list, embedding_size, l2_reg_embedding) if include_linear: linear_sparse_emb_dict = create_embedding_dict(feature_dim_dict, 1, init_std, seed, l2_reg_linear, prefix + 'linear') linear_emb_list = get_embedding_vec_list(linear_sparse_emb_dict, sparse_input_dict, feature_dim_dict['sparse']) linear_emb_list = merge_sequence_input(linear_sparse_emb_dict, linear_emb_list, sequence_input_dict, sequence_input_len_dict, sequence_max_len_dict, feature_dim_dict['sequence']) else: linear_emb_list = None inputs_list = get_inputs_list([sparse_input_dict, dense_input_dict, sequence_input_dict, sequence_input_len_dict]) return inputs_list, deep_emb_list, linear_emb_list 
losses.eq|calc def calc_eq(x, y, phi, psi, cost, lambda_eq): ux, vy = phi(x), psi(y) return eq_loss(x, y, ux, vy, cost, lambda_eq) 
facenet-master.tmp.network.lppool def lppool(inpOp, pnorm, kH, kW, dH, dW, padding, name): with tf.variable_scope(name): if pnorm == 2: pwr = tf.square(inpOp) else: pwr = tf.pow(inpOp, pnorm) subsamp = tf.nn.avg_pool(pwr, ksize=[1, kH, kW, 1], strides=[1, dH, dW, 1], padding=padding) subsamp_sum = tf.multiply(subsamp, kH * kW) if pnorm == 2: out = tf.sqrt(subsamp_sum) else: out = tf.pow(subsamp_sum, 1 / pnorm) return out 
cleverhans.devtools.tests.docscrape.NumpyDocString.str|see|also def _str_see_also(self): if not self['See Also']: return [] out = [] out += self._str_header('See Also') last_had_desc = True for func, desc in self['See Also']: if desc or last_had_desc: out += [''] out += ['`%s`_' % func] else: out[-1] += ', `%s`_' % func if desc: out += self._str_indent(desc) last_had_desc = True else: last_had_desc = False out += [''] return out 
predict_test.momentum @optimizers.optimizer def momentum(learning_rate, momentum=0.9): """A standard momentum optimizer for testing.  Different from `jax.experimental.optimizers.momentum` (Nesterov). """ learning_rate = optimizers.make_schedule(learning_rate)  def init_fn(x0): v0 = np.zeros_like(x0) return x0, v0  def update_fn(i, g, state): x, velocity = state velocity = momentum * velocity + g x = x - learning_rate(i) * velocity return x, velocity  def get_params(state): x, _ = state return x return init_fn, update_fn, get_params 
plyfile.PlyElement.sanity|check def _check_sanity(self): for prop in self.properties: if prop.name not in self._data.dtype.fields: raise ValueError('dangling property %r' % prop.name) 
texar.losses.entropy_test.EntropyTest.entropy|test def _test_entropy(self, entropy_fn, logits, sequence_length=None): with self.test_session() as sess: if sequence_length is None: entropy = entropy_fn(logits) rank = sess.run(tf.rank(entropy)) self.assertEqual(rank, 0) entropy = entropy_fn(logits, average_across_batch=False) rank = sess.run(tf.rank(entropy)) self.assertEqual(rank, 1) self.assertEqual(entropy.shape, tf.TensorShape([self._batch_size])) else: entropy = entropy_fn(logits, sequence_length=sequence_length) rank = sess.run(tf.rank(entropy)) self.assertEqual(rank, 0) entropy = entropy_fn(logits, sequence_length=sequence_length, sum_over_timesteps=False) rank = sess.run(tf.rank(entropy)) self.assertEqual(rank, 1) self.assertEqual(entropy.shape, tf.TensorShape([self._max_time])) entropy = entropy_fn(logits, sequence_length=sequence_length, sum_over_timesteps=False, average_across_timesteps=True, average_across_batch=False) rank = sess.run(tf.rank(entropy)) self.assertEqual(rank, 1) self.assertEqual(entropy.shape, tf.TensorShape([self._batch_size])) entropy = entropy_fn(logits, sequence_length=sequence_length, sum_over_timesteps=False, average_across_batch=False) rank = sess.run(tf.rank(entropy)) self.assertEqual(rank, 2) self.assertEqual(entropy.shape, tf.TensorShape([self. _batch_size, self._max_time])) sequence_length_time = tf.random_uniform([self._max_time], maxval=self._batch_size, dtype=tf.int32) entropy = entropy_fn(logits, sequence_length= sequence_length_time, sum_over_timesteps=False, average_across_batch=False, time_major=True) self.assertEqual(entropy.shape, tf.TensorShape([self. _batch_size, self._max_time])) 
query_methods.BayesianUncertaintySampling.dropout|predict def dropout_predict(self, data): f = K.function([self.model.layers[0].input, K.learning_phase()], [self. model.layers[-1].output]) predictions = np.zeros((self.T, data.shape[0], self.num_labels)) for t in range(self.T): predictions[(t), :, :] = f([data, 1])[0] final_prediction = np.mean(predictions, axis=0) prediction_uncertainty = np.std(predictions, axis=0) return final_prediction, prediction_uncertainty 
texar.data.data.tfrecord_data_test.TFRecordDataTest.tear|Down def tearDown(self): """Remove the downloaded files after the test """ shutil.rmtree(self._test_dir) 
t_sgan_sn_celeba_64.spectral|norm def spectral_norm(w, r=5): w_shape = K.int_shape(w) in_dim = np.prod(w_shape[:-1]).astype(int) out_dim = w_shape[-1] w = K.reshape(w, (in_dim, out_dim)) u = K.ones((1, in_dim)) for i in range(r): v = K.l2_normalize(K.dot(u, w)) u = K.l2_normalize(K.dot(v, K.transpose(w))) return K.sum(K.dot(K.dot(u, w), K.transpose(v))) 
data.utils.Subset.len def __len__(self): return self.n_samples 
get_bounds_ours.grad|compute|max|fast|norm def fast_compute_max_grad_norm(weights, neuron_states, numlayer, norm): assert numlayer >= 2 const, l, u = fast_compute_max_grad_norm_2layer(weights[-1], weights[-2 ], neuron_states[-1]) for i in list(range(numlayer - 2))[::-1]: const, l, u = fast_compute_max_grad_norm_2layer_next(const, l, u, weights[i], neuron_states[i]) l += const u += const l = np.abs(l) u = np.abs(u) max_l_u = np.maximum(l, u) if norm == 1: return np.sum(max_l_u, axis=1) elif norm == 2: return np.sqrt(np.sum(max_l_u ** 2, axis=1)) elif norm == 105: max_ele = np.zeros((max_l_u.shape[0],)) for i in range(max_l_u.shape[0]): for ii in range(max_l_u.shape[1]): if max_l_u[i][ii] > max_ele[i]: max_ele[i] = max_l_u[i][ii] return max_ele 
texar.modules.memory.memory_network.MemNetBase.build def _build(self, memory, query, **kwargs): raise NotImplementedError 
las.ops.bilstm def bilstm(inputs, sequence_length, num_units, dropout, mode, unidirectional=False): with tf.variable_scope('fw_cell'): forward_cell = lstm_cell(num_units, dropout, mode) if not unidirectional: with tf.variable_scope('bw_cell'): backward_cell = lstm_cell(num_units, dropout, mode) return tf.nn.bidirectional_dynamic_rnn(forward_cell, backward_cell, inputs, sequence_length=sequence_length, dtype=tf.float32) else: return tf.nn.dynamic_rnn(forward_cell, inputs, sequence_length= sequence_length, dtype=tf.float32) 
tests.layers.sequence_test.test|Sequence|Layer|Pooling @pytest.mark.parametrize('mode,supports_masking,input_shape', [('sum', False, [(BATCH_SIZE, SEQ_LENGTH, EMBEDDING_SIZE), (BATCH_SIZE, 1)]), ( 'mean', True, (BATCH_SIZE, SEQ_LENGTH, EMBEDDING_SIZE)), ('max', True, (BATCH_SIZE, SEQ_LENGTH, EMBEDDING_SIZE))]) def test_SequencePoolingLayer(mode, supports_masking, input_shape): with CustomObjectScope({'SequencePoolingLayer': sequence. SequencePoolingLayer}): layer_test(sequence.SequencePoolingLayer, kwargs={'mode': mode, 'supports_masking': supports_masking}, input_shape=input_shape, supports_masking=supports_masking) 
aligning.Uniform|test|Scale|Non def testNonUniformScale(SourceHom, TargetHom): OutTransform = np.matmul(TargetHom, np.linalg.pinv(SourceHom)) ScaledRotation = OutTransform[:3, :3] Translation = OutTransform[:3, (3)] Sx = np.linalg.norm(ScaledRotation[(0), :]) Sy = np.linalg.norm(ScaledRotation[(1), :]) Sz = np.linalg.norm(ScaledRotation[(2), :]) Rotation = np.vstack([ScaledRotation[(0), :] / Sx, ScaledRotation[(1), :] / Sy, ScaledRotation[(2), :] / Sz]) print('Rotation matrix norm:', np.linalg.norm(Rotation)) Scales = np.array([Sx, Sy, Sz]) return Scales, Rotation, Translation, OutTransform 
models.get|model|Le|Net def get_LeNet_model(input_shape, labels=10): """ A LeNet model for MNIST. """ model = Sequential() model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape)) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(64, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Flatten()) model.add(Dense(128, activation='relu', name='embedding')) model.add(Dropout(0.5)) model.add(Dense(labels, activation='softmax', name='softmax')) return model 
tflib.model|settings|print def print_model_settings(locals_): print('Uppercase local vars:') all_vars = [(k, v) for k, v in list(locals_.items()) if k.isupper() and k != 'T' and k != 'SETTINGS' and k != 'ALL_SETTINGS'] all_vars = sorted(all_vars, key=lambda x: x[0]) for var_name, var_value in all_vars: print('\t{}: {}'.format(var_name, var_value)) 
ndh_problem.NDHProblem.get|study|loss|types def get_study_loss_types(self): return [common.AC_LOSS, common.CE_LOSS] 
datasets.image|to|files|array def image_files_to_array(image_files, max_pixels=default_max_pixels): import cv2 return np.array([np.swapaxes(scale_down_and_round_even(cv2.imread(str(f )), max_pixels)[:, :, ::-1], 0, 2) for f in image_files]) 
bert_classifier_main_v2.main.head|is def _is_head(): if not FLAGS.distributed: return True return hvd.rank() == 0 
neural_tangents.stax.get|covariance def _get_covariance(x1, x2, marginal_type): """Computes uncentred covariance (nngp) between two sets of inputs  Args: x1: a (2+k)D (k = 0, 2) `np.ndarray` of shape `[n1, <k inner dimensions>, n_features]`. x2: an optional `np.ndarray` that has the same shape as `a` apart from possibly different leading (`n2`) dimension. `None` means `x2 == x1`. marginal_type: an instance of `Marginalisation` specifying between which dimensions should the covariances be computed.  Returns: an `np.ndarray` with uncentred batch covariance with shape `[n1, n2]` `+ [<k inner dimensions>]` (if `covar_type` is `OVER_PIXELS`) `+ [<k inner dimensions>, <k spatial dimensions>]` (if `covar_type` is `OVER_POINTS` or `NO`) """ x2 = x1 if x2 is None else x2 if marginal_type in (M.OVER_ALL, M.OVER_PIXELS): ret = np.matmul(np.moveaxis(x1, 0, -2), np.moveaxis(x2, 0, -1)) ret = np.moveaxis(ret, (-2, -1), (0, 1)) elif marginal_type in (M.OVER_POINTS, M.NO): ret = np.squeeze(np.dot(x1, x2[..., None]), -1) ret = np.transpose(ret, (0, 3, 1, 4, 2, 5)) else: raise NotImplementedError( 'Only implemented for `OVER_ALL`, `OVER_PIXELS`, `OVER_POINTS` and `NO`; supplied {}' .format(marginal_type)) return ret / x1.shape[-1] 
nets.nasnet.nasnet.nasnet|build|cifar def build_nasnet_cifar(images, num_classes, is_training=True, config=None, current_step=None): """Build NASNet model for the Cifar Dataset.""" hparams = cifar_config() if config is None else copy.deepcopy(config) _update_hparams(hparams, is_training) if tf.test.is_gpu_available() and hparams.data_format == 'NHWC': tf.logging.info( 'A GPU is available on the machine, consider using NCHW data format for increased speed on GPU.' ) if hparams.data_format == 'NCHW': images = tf.transpose(images, [0, 3, 1, 2]) total_num_cells = hparams.num_cells + 2 normal_cell = nasnet_utils.NasNetANormalCell(hparams.num_conv_filters, hparams.drop_path_keep_prob, total_num_cells, hparams. total_training_steps) reduction_cell = nasnet_utils.NasNetAReductionCell(hparams. num_conv_filters, hparams.drop_path_keep_prob, total_num_cells, hparams.total_training_steps) with arg_scope([slim.dropout, nasnet_utils.drop_path, slim.batch_norm], is_training=is_training): with arg_scope([slim.avg_pool2d, slim.max_pool2d, slim.conv2d, slim .batch_norm, slim.separable_conv2d, nasnet_utils. factorized_reduction, nasnet_utils.global_avg_pool, nasnet_utils.get_channel_index, nasnet_utils.get_channel_dim], data_format=hparams.data_format): return _build_nasnet_base(images, normal_cell=normal_cell, reduction_cell=reduction_cell, num_classes=num_classes, hparams=hparams, is_training=is_training, stem_type='cifar', current_step=current_step) 
transformer_hparams.batch|transformer|k|chatbot @registry.register_hparams def chatbot_transformer_batch_16k(): hparams = chatbot_cornell_base() hparams.batch_size = 16384 return hparams 
gym_pycolab.plot.Plot.group|update @update_group.setter def update_group(self, group): """Set the current update group. Only `Engine` and tests may do this.""" self._update_group = group 
gym_pycolab.prefab_parts.sprites.MazeWalker.on|board|exit def _on_board_exit(self): """Code to run just before a `MazeWalker` exits the board.  Whatever is in this method is executed immediately prior to a `MazeWalker` exiting the game board, either under its own power or due to scrolling. ("Exiting" refers to the `MazeWalker`'s "virtual position"---see class docstring---since a `Sprite`'s true position cannot be outside of the game board.)  Note that on certain rare occasions, it's possible for this method to run alongside `_on_board_enter` in the same game iteration. On these occasions, the `MazeWalker` is scrolled off the board, but then it performs a move in the opposite direction (at least in part) that brings it right back on. Or, vice versa: the `MazeWalker` gets scrolled onto the board and then walks back off.  By default, this method caches the `MazeWalker`'s previous visibility and then makes the `MazeWalker` invisible---a reasonable thing to do, since it will be moved to "real" position `(0, 0)` as long as its virtual position is not on the game board. If you would like to preserve this behaviour but trigger additional actions on board exit, override this method, but be sure to call this class's own implementation of it, too. Copy and paste:  super(MyCoolMazeWalker, self)._on_board_exit() """ self._prior_visible = self._visible self._visible = False 
gym_pycolab.protocols.logging.log def log(the_plot, message): """Log a message for eventual disposal by the game engine user.  Here, "game engine user" means a user interface or an environment interface, for example. (Clients are not required to deal with messages, but if they do, this is how to get a message to them.)  Most game implementations will not need to call this function directly--- logging is so fundamental that the Plot object expresses a `log` method that's syntactic sugar for this function.  Args: the_plot: the pycolab game's `Plot` object. message: A string message to convey to the game engine user. """ the_plot.setdefault('log_messages', []).append(message) 
translate.models.attention|execution|decoder def attention_execution_decoder(decoder_inputs, initial_state, attention_states, encoders, decoder, encoder_input_length, feed_previous=0.0, align_encoder_id=0, feed_argmax=True, training=True, **kwargs): """ :param decoder_inputs: int32 tensor of shape (batch_size, output_length) :param initial_state: initial state of the decoder (usually the final state of the encoder), as a float32 tensor of shape (batch_size, initial_state_size). This state is mapped to the correct state size for the decoder. :param attention_states: list of tensors of shape (batch_size, input_length, encoder_cell_size), the hidden states of the encoder(s) (one tensor for each encoder). :param encoders: configuration of the encoders :param decoder: configuration of the decoder :param encoder_input_length: list of int32 tensors of shape (batch_size,), tells for each encoder, the true length of each sequence in the batch (sequences in the same batch are padded to all have the same length). :param feed_previous: scalar tensor corresponding to the probability to use previous decoder output instead of the ground truth as input for the decoder (1 when decoding, between 0 and 1 when training) :param feed_argmax: boolean tensor, when True the greedy decoder outputs the word with the highest probability (argmax). When False, it samples a word from the probability distribution (softmax). :param align_encoder_id: outputs attention weights for this encoder. Also used when predicting edit operations (pred_edits), to specifify which encoder reads the sequence to post-edit (MT).  :return: outputs of the decoder as a tensor of shape (batch_size, output_length, decoder_cell_size) attention weights as a tensor of shape (output_length, encoders, batch_size, input_length) """ cell_output_size, cell_state_size = get_state_size(decoder.cell_type, decoder.cell_size, decoder.lstm_proj_size, decoder.layers) assert not decoder.pred_maxout_layer or cell_output_size % 2 == 0, 'cell size must be a multiple of 2' if decoder.use_lstm is False: decoder.cell_type = 'GRU' embedding_shape = [decoder.vocab_size, decoder.embedding_size] weight_scale = decoder.embedding_weight_scale or decoder.weight_scale if weight_scale is None: initializer = None elif decoder.embedding_initializer == 'uniform' or decoder.embedding_initializer is None and decoder.initializer == 'uniform': initializer = tf.random_uniform_initializer(minval=-weight_scale, maxval=weight_scale) else: initializer = tf.random_normal_initializer(stddev=weight_scale) with tf.device('/cpu:0'): if decoder.name == 'edits': embedding_name = 'mt' else: embedding_name = decoder.name embedding = get_variable('embedding_{}'.format(embedding_name), shape=embedding_shape, initializer=initializer) input_shape = tf.shape(decoder_inputs) batch_size = input_shape[0] time_steps = input_shape[1] scope_name = 'decoder_{}'.format(decoder.name) scope_name += '/' + '_'.join(encoder.name for encoder in encoders)  def embed(input_): embedded_input = tf.nn.embedding_lookup(embedding, input_) if decoder.use_dropout and decoder.word_keep_prob is not None: noise_shape = [1, 1] if decoder.pervasive_dropout else [tf. shape(input_)[0], 1] embedded_input = tf.nn.dropout(embedded_input, keep_prob= decoder.word_keep_prob, noise_shape=noise_shape) if decoder.use_dropout and decoder.embedding_keep_prob is not None: size = tf.shape(embedded_input)[1] noise_shape = [1, size] if decoder.pervasive_dropout else [tf. shape(input_)[0], size] embedded_input = tf.nn.dropout(embedded_input, keep_prob= decoder.embedding_keep_prob, noise_shape=noise_shape) return embedded_input  def get_cell(input_size=None, reuse=False): cells = [] for j in range(decoder.layers): input_size_ = input_size if j == 0 else cell_output_size if decoder.cell_type.lower() == 'lstm': cell = CellWrapper(BasicLSTMCell(decoder.cell_size, reuse= reuse)) elif decoder.cell_type.lower() == 'plstm': cell = PLSTM(decoder.cell_size, reuse=reuse, fact_size= decoder.lstm_fact_size, proj_size=decoder.lstm_proj_size) elif decoder.cell_type.lower() == 'dropoutgru': cell = DropoutGRUCell(decoder.cell_size, reuse=reuse, layer_norm=decoder.layer_norm, input_size=input_size_, input_keep_prob=decoder.rnn_input_keep_prob, state_keep_prob=decoder.rnn_state_keep_prob) else: cell = GRUCell(decoder.cell_size, reuse=reuse, layer_norm= decoder.layer_norm) if decoder.use_dropout and decoder.cell_type.lower( ) != 'dropoutgru': cell = DropoutWrapper(cell, input_keep_prob=decoder. rnn_input_keep_prob, output_keep_prob=decoder. rnn_output_keep_prob, state_keep_prob=decoder. rnn_state_keep_prob, variational_recurrent=decoder. pervasive_dropout, dtype=tf.float32, input_size=input_size_ ) cells.append(cell) if len(cells) == 1: return cells[0] else: return CellWrapper(MultiRNNCell(cells))  def look(time, state, input_, prev_weights=None, pos=None, context=None): prev_weights_ = [(prev_weights if i == align_encoder_id else None) for i in range(len(encoders))] pos_ = None if decoder.pred_edits: pos_ = [(pos if i == align_encoder_id else None) for i in range (len(encoders))] if decoder.attn_prev_word: state = tf.concat([state, input_], axis=1) if decoder.attn_prev_attn and context is not None: state = tf.concat([state, context], axis=1) if decoder.hidden_state_scaling: attention_states_ = [(states * decoder.hidden_state_scaling) for states in attention_states] else: attention_states_ = attention_states parameters = dict(hidden_states=attention_states_, encoder_input_length=encoder_input_length, encoders=encoders, aggregation_method=decoder.aggregation_method) context, new_weights = multi_attention(state, time=time, pos=pos_, prev_weights=prev_weights_, **parameters) if decoder.context_mapping: with tf.variable_scope(scope_name): activation = (tf.nn.tanh if decoder. context_mapping_activation == 'tanh' else None) use_bias = not decoder.context_mapping_no_bias context = dense(context, decoder.context_mapping, use_bias= use_bias, activation=activation, name='context_mapping') return context, new_weights[align_encoder_id]  def update(state, input_, context=None, symbol=None, lm_state=None): if (context is not None and lm_state is not None and decoder. rnn_feed_attn): input_ = tf.concat([input_, context, lm_state], axis=1) input_size = input_.get_shape()[1].value initializer = CellInitializer(decoder.cell_size ) if decoder.orthogonal_init else None with tf.variable_scope(tf.get_variable_scope(), initializer=initializer ): try: output, new_state = get_cell(input_size)(input_, state) except ValueError: output, new_state = get_cell(input_size, reuse=True)(input_, state) if decoder.skip_update and decoder.pred_edits and symbol is not None: is_del = tf.equal(symbol, utils.DEL_ID) new_state = tf.where(is_del, state, new_state) if decoder.cell_type.lower() == 'lstm' and decoder.use_lstm_full_state: output = new_state return output, new_state  def update_pos(pos, symbol, max_pos=None): if not decoder.pred_edits: return pos is_keep = tf.equal(symbol, utils.KEEP_ID) is_del = tf.equal(symbol, utils.DEL_ID) is_not_ins = tf.logical_or(is_keep, is_del) pos = beam_search.resize_like(pos, symbol) max_pos = beam_search.resize_like(max_pos, symbol) pos += tf.to_float(is_not_ins) if max_pos is not None: pos = tf.minimum(pos, tf.to_float(max_pos)) return pos  def generate(state, input_, context): if decoder.pred_use_lstm_state is False: state = state[:, -cell_output_size:] projection_input = [state, context] if decoder.use_previous_word: projection_input.insert(1, input_) output_ = tf.concat(projection_input, axis=1) if decoder.pred_deep_layer: deep_layer_size = (decoder.pred_deep_layer_size or decoder. embedding_size) if decoder.layer_norm: output_ = dense(output_, deep_layer_size, use_bias=False, name='deep_output') output_ = tf.contrib.layers.layer_norm(output_, activation_fn=tf.nn.tanh, scope='output_layer_norm') else: output_ = dense(output_, deep_layer_size, activation=tf. tanh, use_bias=True, name='deep_output') if decoder.use_dropout: size = tf.shape(output_)[1] noise_shape = [1, size] if decoder.pervasive_dropout else None output_ = tf.nn.dropout(output_, keep_prob=decoder. deep_layer_keep_prob, noise_shape=noise_shape) else: if decoder.pred_maxout_layer: maxout_size = decoder.maxout_size or cell_output_size output_ = dense(output_, maxout_size, use_bias=True, name= 'maxout') if decoder.old_maxout: output_ = tf.nn.pool(tf.expand_dims(output_, axis=2), window_shape=[2], pooling_type='MAX', padding= 'SAME', strides=[2]) output_ = tf.squeeze(output_, axis=2) else: output_ = tf.maximum(*tf.split(output_, num_or_size_splits=2, axis=1)) if decoder.pred_embed_proj: output_ = dense(output_, decoder.embedding_size, use_bias= False, name='softmax0') if decoder.tie_embeddings and (decoder.pred_embed_proj or decoder. pred_deep_layer): bias = get_variable('softmax1/bias', shape=[decoder.vocab_size]) output_ = tf.matmul(output_, tf.transpose(embedding)) + bias else: output_ = dense(output_, decoder.vocab_size, use_bias=True, name='softmax1') return output_  def execute(symbol, input, lm_state): is_keep = tf.equal(symbol, utils.KEEP_ID) is_del = tf.equal(symbol, utils.DEL_ID) is_not_ins = tf.logical_or(is_keep, is_del) new_input = tf.where(is_not_ins, embed(input), embed(symbol)) input_size = new_input.get_shape()[1].value initializer = CellInitializer(decoder.cell_size ) if decoder.orthogonal_init else None with tf.variable_scope(tf.get_variable_scope(), initializer=initializer ): try: lm_output, new_lm_state = get_cell(input_size)(new_input, lm_state) except ValueError: lm_output, new_lm_state = get_cell(input_size, reuse=True)( new_input, lm_state) if decoder.skip_update and decoder.pred_edits and symbol is not None: new_lm_state = tf.where(is_del, lm_state, new_lm_state) return lm_output, new_lm_state if decoder.use_dropout: initial_state = tf.nn.dropout(initial_state, keep_prob=decoder. initial_state_keep_prob) with tf.variable_scope(scope_name): activation_fn = (None if decoder.initial_state == 'linear' else tf. nn.tanh) if decoder.initial_state == 'trained': initial_state = get_variable(shape=[cell_state_size], name= 'initial_state') initial_state = tf.tile(tf.expand_dims(initial_state, axis=0), [batch_size, 1]) elif decoder.initial_state == 'zero': initial_state = tf.zeros(shape=[batch_size, cell_state_size]) elif decoder.layer_norm: initial_state = dense(initial_state, cell_state_size, use_bias= False, name='initial_state_projection') initial_state = tf.contrib.layers.layer_norm(initial_state, activation_fn=activation_fn, scope='initial_state_layer_norm') else: initial_state = dense(initial_state, cell_state_size, use_bias= True, name='initial_state_projection', activation=activation_fn ) if decoder.cell_type.lower() == 'lstm' and decoder.use_lstm_full_state: initial_output = initial_state else: initial_output = initial_state[:, -cell_output_size:] time = tf.constant(0, dtype=tf.int32, name='time') outputs = tf.TensorArray(dtype=tf.float32, size=time_steps) samples = tf.TensorArray(dtype=tf.int64, size=time_steps) inputs = tf.TensorArray(dtype=tf.int64, size=time_steps).unstack(tf. to_int64(tf.transpose(decoder_inputs))) states = tf.TensorArray(dtype=tf.float32, size=time_steps) weights = tf.TensorArray(dtype=tf.float32, size=time_steps) attns = tf.TensorArray(dtype=tf.float32, size=time_steps) encoder_inputs = kwargs['encoder_inputs'][0] mt_inputs = decoder_inputs max_pos = time_steps index_range = tf.range(batch_size) initial_lm_state = tf.zeros(shape=[batch_size, cell_state_size]) initial_symbol = inputs.read(0) initial_input = embed(initial_symbol) initial_pos = tf.zeros([batch_size], tf.float32) initial_weights = tf.zeros(tf.shape(attention_states[align_encoder_id])[:2] ) zero_context = tf.zeros(shape=tf.shape(attention_states[ align_encoder_id][:, (0)])) with tf.variable_scope('decoder_{}'.format(decoder.name)): initial_context, _ = look(0, initial_output, initial_input, pos= initial_pos, prev_weights=initial_weights, context=zero_context) initial_data = tf.concat([initial_state, initial_context, initial_lm_state, tf.expand_dims(initial_pos, axis=1), initial_weights], axis=1) context_size = initial_context.shape[1].value lm_state_size = initial_lm_state.shape[1].value  def get_logits(state, ids, time): with tf.variable_scope('decoder_{}'.format(decoder.name)): state, context, lm_state, pos, prev_weights = tf.split(state, [ cell_state_size, context_size, lm_state_size, 1, -1], axis=1) input_ = embed(ids) pos = tf.squeeze(pos, axis=1) pos = tf.cond(tf.equal(time, 0), lambda : pos, lambda : update_pos(pos, ids, encoder_input_length[align_encoder_id])) if decoder.cell_type.lower( ) == 'lstm' and decoder.use_lstm_full_state: output = state else: output = state[:, -cell_output_size:] if decoder.conditional_rnn: with tf.variable_scope('conditional_1'): output, state = update(state, input_) elif decoder.update_first: output, state = update(state, input_, None, ids, lm_state) elif decoder.generate_first: output, state = tf.cond(tf.equal(time, 0), lambda : (output, state), lambda : update(state, input_, context, ids, lm_state)) context, new_weights = look(time, output, input_, pos=pos, prev_weights=prev_weights, context=context) if decoder.conditional_rnn: with tf.variable_scope('conditional_2'): output, state = update(state, context) elif not decoder.generate_first: output, state = update(state, input_, context, ids) logits = generate(output, input_, context) argmax = lambda : tf.argmax(logits, 1) target = lambda : inputs.read(time + 1) softmax = lambda : tf.squeeze(tf.multinomial(tf.log(tf.nn. softmax(logits)), num_samples=1), axis=1) use_target = tf.logical_and(time < time_steps - 1, tf. random_uniform([]) >= feed_previous) predicted_symbol = tf.case([(use_target, target), (tf. logical_not(feed_argmax), softmax)], default=argmax) with tf.variable_scope('rnn_lm'): lm_output, lm_state = execute(predicted_symbol, ids, lm_state) pos = tf.expand_dims(pos, axis=1) state = tf.concat([state, context, lm_state, pos, new_weights], axis=1) return state, logits  def _time_step(time, input_, input_symbol, pos, state, output, outputs, states, weights, attns, prev_weights, samples, context, lm_state): if decoder.conditional_rnn: with tf.variable_scope('conditional_1'): output, state = update(state, input_) elif decoder.update_first: with tf.variable_scope('op_decoder_rnn'): output, state = update(state, input_, None, input_symbol, lm_state=lm_state) context, new_weights = look(time, output, input_, pos=pos, prev_weights=prev_weights, context=context) if decoder.conditional_rnn: with tf.variable_scope('conditional_2'): output, state = update(state, context) elif not decoder.generate_first: with tf.variable_scope('op_decoder_rnn'): output, state = update(state, input_, context, input_symbol, lm_state=lm_state) output_ = generate(output, input_, context) argmax = lambda : tf.argmax(output_, 1) target = lambda : inputs.read(time + 1) softmax = lambda : tf.squeeze(tf.multinomial(tf.log(tf.nn.softmax( output_)), num_samples=1), axis=1) use_target = tf.logical_and(time < time_steps - 1, tf. random_uniform([]) >= feed_previous) predicted_symbol = tf.case([(use_target, target), (tf.logical_not( feed_argmax), softmax)], default=argmax) predicted_symbol.set_shape([None]) predicted_symbol = tf.stop_gradient(predicted_symbol) index = tf.stack([index_range, tf.cast(pos, tf.int32)], axis=1) current_mt_symbol = tf.gather_nd(mt_inputs, index) with tf.variable_scope('rnn_lm'): lm_output, lm_state = execute(predicted_symbol, current_mt_symbol, lm_state) input_ = embed(predicted_symbol) pos = update_pos(pos, predicted_symbol, encoder_input_length[ align_encoder_id]) samples = samples.write(time, predicted_symbol) attns = attns.write(time, context) weights = weights.write(time, new_weights) states = states.write(time, state) outputs = outputs.write(time, output_) if (not decoder.conditional_rnn and not decoder.update_first and decoder.generate_first): output, state = update(state, input_, context, predicted_symbol, lm_state=lm_state) return (time + 1, input_, predicted_symbol, pos, state, output, outputs, states, weights, attns, new_weights, samples, context, lm_state) with tf.variable_scope('decoder_{}'.format(decoder.name)): (_, _, _, new_pos, new_state, _, outputs, states, weights, attns, new_weights, samples, _, _) = (tf.while_loop(cond=lambda time, *_: time < time_steps, body=_time_step, loop_vars=(time, initial_input, initial_symbol, initial_pos, initial_state, initial_output, outputs, weights, states, attns, initial_weights, samples, initial_context, initial_lm_state), parallel_iterations=decoder.parallel_iterations, swap_memory= decoder.swap_memory)) outputs = outputs.stack() weights = weights.stack() states = states.stack() attns = attns.stack() samples = samples.stack() outputs = tf.transpose(outputs, perm=(1, 0, 2)) weights = tf.transpose(weights, perm=(1, 0, 2)) states = tf.transpose(states, perm=(1, 0, 2)) attns = tf.transpose(attns, perm=(1, 0, 2)) samples = tf.transpose(samples) return outputs, weights, states, attns, samples, get_logits, initial_data 
utils.merge def merge(images, size): h, w = images.shape[1], images.shape[2] if images.shape[3] in (3, 4): c = images.shape[3] img = np.zeros((h * size[0], w * size[1], c)) for idx, image in enumerate(images): i = idx % size[1] j = idx // size[1] img[j * h:j * h + h, i * w:i * w + w, :] = image return img elif images.shape[3] == 1: img = np.zeros((h * size[0], w * size[1])) for idx, image in enumerate(images): i = idx % size[1] j = idx // size[1] img[j * h:j * h + h, i * w:i * w + w] = image[:, :, (0)] return img else: raise ValueError( 'in merge(images,size) images parameter must have dimensions: HxW or HxWx3 or HxWx4' ) 
operations.norm|instance def instance_norm(x, phase=False, name='instance_norm'): epsilon = 1e-09 mean, var = tf.nn.moments(x, [1, 2, 3], keep_dims=True) return tf.div(tf.subtract(x, mean), tf.sqrt(tf.add(var, epsilon))) 
translate.seq2seq_model.Seq2SeqModel.beam|op|create def create_beam_op(self, models, len_normalization): self.len_normalization = len_normalization self.models = models beam_funs = [model.beam_fun for model in models] initial_data = [model.initial_data for model in models] beam_output = beam_search.rnn_beam_search(beam_funs, initial_data, self .max_output_len[0], self.beam_size, len_normalization, temperature= self.temperature, parallel_iterations=self.decoders[0]. parallel_iterations, swap_memory=self.decoders[0].swap_memory) self.beam_outputs, self.beam_scores = beam_output 
make_tfrecord_shapenet.int|feature def _int64_feature(value): """Returns an int64_list from a bool / enum / int / uint.""" return tf.train.Feature(int64_list=tf.train.Int64List(value=[value])) 
fasterai.filters.MasterFilter.Filter|Master def __init__(self, filters: [IFilter], render_factor: int): self.filters = filters self.render_factor = render_factor 
gym_pycolab.tests.test_things.PycolabTestCase.assert|Board def assertBoard(self, actual_board, art, err_msg=''): """Assert that a pycolab game board matches expected ASCII art.  Args: actual_board: a pycolab game board, in its 2-D `uint8` nparray manifestation. This is the `board` member of a `rendering.Observation` namedtuple. art: an ASCII-art diagram, as a list of same-length ASCII strings, portraying the expected game board. err_msg: optional error message to include in `AssertionError`s raised by this method.  Raises: AssertionError: `art` does not match `actual_board`. """ np.testing.assert_array_equal(actual_board, ascii_art. ascii_art_to_uint8_nparray(art), err_msg) 
xlnet-master.prepro_utils.encode|pieces def encode_pieces(sp_model, text, return_unicode=True, sample=False): if six.PY2 and isinstance(text, unicode): text = text.encode('utf-8') if not sample: pieces = sp_model.EncodeAsPieces(text) else: pieces = sp_model.SampleEncodeAsPieces(text, 64, 0.1) new_pieces = [] for piece in pieces: if len(piece) > 1 and piece[-1] == ',' and piece[-2].isdigit(): cur_pieces = sp_model.EncodeAsPieces(piece[:-1].replace( SPIECE_UNDERLINE, '')) if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0 ] == SPIECE_UNDERLINE: if len(cur_pieces[0]) == 1: cur_pieces = cur_pieces[1:] else: cur_pieces[0] = cur_pieces[0][1:] cur_pieces.append(piece[-1]) new_pieces.extend(cur_pieces) else: new_pieces.append(piece) if six.PY2 and return_unicode: ret_pieces = [] for piece in new_pieces: if isinstance(piece, str): piece = piece.decode('utf-8') ret_pieces.append(piece) new_pieces = ret_pieces return new_pieces 
pytorch_pretrained_bert.modeling_transfo_xl.RelMultiHeadAttn.shift def _shift(self, x, qlen, klen, mask, left=False): if qlen > 1: zero_pad = torch.zeros((x.size(0), qlen - 1, x.size(2), x.size(3)), device=x.device, dtype=x.dtype) else: zero_pad = torch.zeros(0, device=x.device, dtype=x.dtype) if left: mask = mask.flip(1) x_padded = torch.cat([zero_pad, x], dim=1).expand(qlen, -1, -1, -1) else: x_padded = torch.cat([x, zero_pad], dim=1).expand(qlen, -1, -1, -1) x = x_padded.masked_select(mask[:, :, (None), (None)]).view(qlen, klen, x.size(2), x.size(3)) return x 
extensions.multi_modal_application.MultiModalApplication.connect_data_and_network.sampler|switch def switch_sampler(for_training): with tf.name_scope('train' if for_training else 'validation'): sampler = self.get_sampler()[0][0 if for_training else -1] return sampler.pop_batch_op() 
neural_tangents.stax.ab|relu def _ab_relu(x, a, b, **kwargs): return a * np.minimum(x, 0) + b * np.maximum(x, 0) 
data_loader.USPS.USPS def __init__(self, root, train=True, transform=None): """Init USPS dataset.""" self.root = os.path.expanduser(root) self.train = train self.transform = transform self.dataset_size = None self.train_data, self.train_labels = self.load_samples() if self.train: total_num_samples = self.train_labels.shape[0] indices = np.arange(total_num_samples) np.random.shuffle(indices) self.train_data = self.train_data[(indices[0:self.dataset_size]), :] self.train_labels = self.train_labels[indices[0:self.dataset_size]] self.train_data = self.train_data.reshape(-1, 16, 16, 1) 
deepctr.layers.sequence.KMaxPooling.call def call(self, inputs): perm = list(range(self.dims)) perm[-1], perm[self.axis] = perm[self.axis], perm[-1] shifted_input = tf.transpose(inputs, perm) top_k = tf.nn.top_k(shifted_input, k=self.k, sorted=True, name=None)[0] output = tf.transpose(top_k, perm) return output 
borealisflows.noise_flow_model.NoiseFlow.prior def prior(self, name, x): with tf.variable_scope(name): n_z = self.hps.x_shape[-1] h = tf.zeros([tf.shape(x)[0]] + self.hps.x_shape[1:3] + [2 * n_z]) pz = gaussian_diag(h[:, :, :, :n_z], h[:, :, :, n_z:])  def logp(z1): objective = pz.logp(z1) return objective  def sample(eps_std=None): if eps_std is not None: z = pz.sample2(pz.eps * tf.reshape(eps_std, [-1, 1, 1, 1])) else: z = pz.sample return z return logp, sample 
data_prep_util.h|save def save_h5(h5_filename, data, label, data_dtype='uint8', label_dtype='uint8'): h5_fout = h5py.File(h5_filename) h5_fout.create_dataset('data', data=data, compression='gzip', compression_opts=4, dtype=data_dtype) h5_fout.create_dataset('label', data=label, compression='gzip', compression_opts=1, dtype=label_dtype) h5_fout.close() 
data_utils.bow|build|seq|eval|batch def build_batch_bow_seq2seq_eval(data_batch, len_batch, stop_words, max_enc_bow, max_dec_bow, pad_id, single_ref=False): """Build evaluation batch, basically the same as the seq2seq setting""" enc_inputs = [] enc_lens = [] references = [] ref_lens = [] dec_golden_bow = [] dec_bow = [] dec_bow_len = []  def _pad(s_set, max_len, pad_id): s_set = list(s_set)[:max_len] for i in range(max_len - len(s_set)): s_set.append(pad_id) return s_set  def _pad_golden(s_set, max_len): s_set_ = list(s_set) s_set = list(s_set)[:max_len] for i in range(max_len - len(s_set)): s_set.append(np.random.choice(s_set_)) return s_set for st, slen in zip(data_batch, len_batch): inp = st[0][:-1] len_inp = slen[0] if single_ref: ref = [s_[1:s_len_] for s_, s_len_ in zip(st[-1:], slen[-1:])] else: ref = [s_[1:s_len_] for s_, s_len_ in zip(st[1:], slen[1:])] golden_bow = [] for r in ref: golden_bow.extend(r) golden_bow = set(golden_bow) - stop_words golden_bow = _pad_golden(golden_bow, max_enc_bow) d_in = st[1][:-1] d_bow = set(d_in) - stop_words d_bow_len = len(d_bow) d_bow_ = d_bow d_bow = _pad(d_bow, max_dec_bow, pad_id) dec_bow.append(d_bow) dec_bow_len.append(d_bow_len) enc_inputs.append(inp) enc_lens.append(len_inp) references.append(ref) dec_golden_bow.append(golden_bow) batch_dict = {'enc_inputs': np.array(enc_inputs), 'enc_lens': np.array( enc_lens), 'golden_bow': np.array(dec_golden_bow), 'dec_bow': np. array(dec_bow), 'dec_bow_len': np.array(dec_bow_len), 'references': references} return batch_dict 
house_utils.angle|compute|pitch def compute_pitch_angle(source_coordinates, target_coordinates, radians=True): """Computes pitch (elevation) angle given two xyz coordinates.  Args: source_coordinates: Source triplet with xyz coordinates. target_coordinates: Target triplet with xyz coordinates. radians: Whether to output in radians or degrees.  Returns: Pitch angle. 0 means neutral elevation, angles increase towards positive z coordinates. In radians, pi means looking at the ceiling and -pi means looking at the floor. """ source_x, source_y, source_z = source_coordinates target_x, target_y, target_z = target_coordinates delta_x = target_x - source_x delta_y = target_y - source_y delta_z = target_z - source_z delta_xy = math.sqrt(delta_x ** 2 + delta_y ** 2) pitch = math.atan2(delta_z, delta_xy) if radians: return pitch return math.degrees(pitch) 
neural_tangents.predict.mat|gp|inference @get_namedtuple('Gaussians') def _gp_inference_mat(kdd, ktd, ktt, y_train, get, diag_reg=0.0): """Compute the mean and variance of the `posterior` of NNGP and NTK.  Args: kdd: A train-train `Kernel` namedtuple. ktd: A test-train `Kernel` namedtuple. ktt: A test-test `Kernel` namedtuple. y_train: A `np.ndarray`, representing the train targets. get: string, the mode of the Gaussian process, either "nngp" or "ntk", or a tuple, or `None`. If `None` then both `nngp` and `ntk` predictions are returned. diag_reg: A float, representing the strength of the regularization.  Returns: Either a Gaussian(`mean`, `variance`) namedtuple or `mean` of the GP posterior. """ out = {} if get is None: get = 'nngp', 'ntk' if 'nngp' in get: op = _inv_operator(kdd.nngp, diag_reg) pred_mean = _mean_prediction(op, ktd.nngp, y_train) if ktt is not None: pred_cov = _nngp_cov(op, ktd.nngp, ktt) out['nngp'] = Gaussian(pred_mean, pred_cov ) if ktt is not None else pred_mean if 'ntk' in get: op = _inv_operator(kdd.ntk, diag_reg) pred_mean = _mean_prediction(op, ktd.ntk, y_train) if ktt is not None: pred_cov = _ntk_cov(op, ktd.ntk, kdd.nngp, ktd.nngp, ktt) out['ntk'] = Gaussian(pred_mean, pred_cov ) if ktt is not None else pred_mean return out 
avod.builders.avod_loss_builder.off|ang|cls|build|loss def _build_cls_off_ang_loss(model, prediction_dict): """Builds classification, offset, and angle vector losses.  Args: model: network model prediction_dict: prediction dictionary  Returns: losses_output: losses dictionary """ mb_cls_logits = prediction_dict[model.PRED_MB_CLASSIFICATION_LOGITS] mb_cls_softmax = prediction_dict[model.PRED_MB_CLASSIFICATION_SOFTMAX] mb_offsets = prediction_dict[model.PRED_MB_OFFSETS] mb_angle_vectors = prediction_dict[model.PRED_MB_ANGLE_VECTORS] mb_cls_gt = prediction_dict[model.PRED_MB_CLASSIFICATIONS_GT] mb_offsets_gt = prediction_dict[model.PRED_MB_OFFSETS_GT] mb_orientations_gt = prediction_dict[model.PRED_MB_ORIENTATIONS_GT] with tf.variable_scope('avod_gt_angle_vectors'): mb_angle_vectors_gt = (orientation_encoder. tf_orientation_to_angle_vector(mb_orientations_gt)) with tf.variable_scope('avod_losses'): with tf.variable_scope('classification'): cls_loss = _get_cls_loss(model, mb_cls_logits, mb_cls_gt) with tf.variable_scope('regression'): final_reg_loss, offset_loss_norm, ang_loss_norm = ( _get_off_ang_loss(model, mb_offsets, mb_offsets_gt, mb_angle_vectors, mb_angle_vectors_gt, mb_cls_softmax, mb_cls_gt)) with tf.variable_scope('avod_loss'): avod_loss = cls_loss + final_reg_loss tf.summary.scalar('avod_loss', avod_loss) losses_output = dict() losses_output[KEY_CLASSIFICATION_LOSS] = cls_loss losses_output[KEY_REGRESSION_LOSS] = final_reg_loss losses_output[KEY_AVOD_LOSS] = avod_loss losses_output[KEY_OFFSET_LOSS_NORM] = offset_loss_norm losses_output[KEY_ANG_LOSS_NORM] = ang_loss_norm return losses_output 
AffineCouplingGain.AffineCouplingGain.det|jacobian|log|inverse def _inverse_log_det_jacobian(self, z, yy, nlf0=None, nlf1=None, iso=None, cam=None): scale = gain_model_params(iso) if scale is None: return tf.constant(0.0, dtype=z.dtype, name='ildj') return -tf.log(scale) 
avod.core.models.rpn_model_test.RpnModelTest.drop|masks|test|path|create def test_create_path_drop_masks(self): rpn_model = RpnModel(self.model_config, train_val_test='val', dataset= self.dataset) rpn_model.build() p_img = tf.constant(0.6) p_bev = tf.constant(0.85) rand_choice = [0.53, 0.83, 0.05] rand_choice_tensor = tf.convert_to_tensor(rand_choice) img_mask, bev_mask = rpn_model.create_path_drop_masks(p_img, p_bev, rand_choice_tensor) with self.test_session(): img_mask_out = img_mask.eval() bev_mask_out = bev_mask.eval() np.testing.assert_array_equal(img_mask_out, 1.0) np.testing.assert_array_equal(bev_mask_out, 1.0) p_img = tf.constant(0.2) p_bev = tf.constant(0.85) img_mask, bev_mask = rpn_model.create_path_drop_masks(p_img, p_bev, rand_choice_tensor) with self.test_session(): img_mask_out = img_mask.eval() bev_mask_out = bev_mask.eval() np.testing.assert_array_equal(img_mask_out, 0.0) np.testing.assert_array_equal(bev_mask_out, 1.0) p_img = tf.constant(0.9) p_bev = tf.constant(0.1) img_mask, bev_mask = rpn_model.create_path_drop_masks(p_img, p_bev, rand_choice_tensor) with self.test_session(): img_mask_out = img_mask.eval() bev_mask_out = bev_mask.eval() np.testing.assert_array_equal(img_mask_out, 1.0) np.testing.assert_array_equal(bev_mask_out, 0.0) p_img = tf.constant(0.0) p_bev = tf.constant(0.1) img_mask, bev_mask = rpn_model.create_path_drop_masks(p_img, p_bev, rand_choice_tensor) with self.test_session(): img_mask_out = img_mask.eval() bev_mask_out = bev_mask.eval() np.testing.assert_array_equal(img_mask_out, 0.0) np.testing.assert_array_equal(bev_mask_out, 1.0) rand_choice = [0.53, 0.83, 0.61] rand_choice_tensor = tf.convert_to_tensor(rand_choice) p_img = tf.constant(0.0) p_bev = tf.constant(0.1) img_mask, bev_mask = rpn_model.create_path_drop_masks(p_img, p_bev, rand_choice_tensor) with self.test_session(): img_mask_out = img_mask.eval() bev_mask_out = bev_mask.eval() np.testing.assert_array_equal(img_mask_out, 1.0) np.testing.assert_array_equal(bev_mask_out, 0.0) 
dataset.Dataset.Dataset def __init__(self, features_path=None, labels_path=None, tot_labels=0, verbose=True): if features_path is not None and labels_path is not None: if tot_labels <= 0: raise ValueError( "[ERROR] Please specify a valid value for 'tot_labels'") self.load(features_path, labels_path, tot_labels, verbose=verbose) else: self.size = 0 self.tot_labels = 0 self.features = None self.labels = None 
cpplint.Multi|Find|Comment|Start|Next|Line def FindNextMultiLineCommentStart(lines, lineix): """Find the beginning marker for a multiline comment.""" while lineix < len(lines): if lines[lineix].strip().startswith('/*'): if lines[lineix].strip().find('*/', 2) < 0: return lineix lineix += 1 return len(lines) 
func_model_81.disparityregression def disparityregression(input): shape = K.shape(input) disparity_values = np.linspace(-4, 4, 9) x = K.constant(disparity_values, shape=[9]) x = K.expand_dims(K.expand_dims(K.expand_dims(x, 0), 0), 0) x = tf.tile(x, [shape[0], shape[1], shape[2], 1]) out = K.sum(multiply([input, x]), -1) return out 
user.get|photo|conf def get_conf_photo(score_matrix, label, rank_range): score_matrix = softmax(score_matrix) photos_label_scores = score_matrix[:, (label)] keep_ids_rank = np.argsort(photos_label_scores)[::-1][:rank_range] return keep_ids_rank 
cpplint.FileInfo.Is|Source def IsSource(self): """File has a source file extension.""" return self.Extension()[1:] in ('c', 'cc', 'cpp', 'cxx') 
classification.ops.loss_functions.CategoricalLogitsNegativeLogProbLoss.minibatch|register|additional def register_additional_minibatch(self, logits, targets=None): """Register an additiona minibatch's worth of parameters. Args: logits: Tensor of shape [batch_size, output_size]. Parameters for underlying distribution. targets: None or Tensor of shape [batch_size, output_size].  Each row must be a one-hot vector. """ self._logits_components.append(logits) self._targets_components.append(targets) 
facenet-master.src.align.detect_face.bbreg def bbreg(boundingbox, reg): """Calibrate bounding boxes""" if reg.shape[1] == 1: reg = np.reshape(reg, (reg.shape[2], reg.shape[3])) w = boundingbox[:, (2)] - boundingbox[:, (0)] + 1 h = boundingbox[:, (3)] - boundingbox[:, (1)] + 1 b1 = boundingbox[:, (0)] + reg[:, (0)] * w b2 = boundingbox[:, (1)] + reg[:, (1)] * h b3 = boundingbox[:, (2)] + reg[:, (2)] * w b4 = boundingbox[:, (3)] + reg[:, (3)] * h boundingbox[:, 0:4] = np.transpose(np.vstack([b1, b2, b3, b4])) return boundingbox 
xlnet-master.modeling.transformer|xl def transformer_xl(inp_k, n_token, n_layer, d_model, n_head, d_head, d_inner, dropout, dropatt, attn_type, bi_data, initializer, is_training, mem_len=None, inp_q=None, mems=None, same_length=False, clamp_len=-1, untie_r=False, use_tpu=True, input_mask=None, perm_mask=None, seg_id= None, reuse_len=None, ff_activation='relu', target_mapping=None, use_bfloat16=False, scope='transformer', **kwargs): """ Defines a Transformer-XL computation graph with additional support for XLNet.  Args:  inp_k: int32 Tensor in shape [len, bsz], the input token IDs. seg_id: int32 Tensor in shape [len, bsz], the input segment IDs. input_mask: float32 Tensor in shape [len, bsz], the input mask. 0 for real tokens and 1 for padding. mems: a list of float32 Tensors in shape [mem_len, bsz, d_model], memory from previous batches. The length of the list equals n_layer. If None, no memory is used. perm_mask: float32 Tensor in shape [len, len, bsz]. If perm_mask[i, j, k] = 0, i attend to j in batch k; if perm_mask[i, j, k] = 1, i does not attend to j in batch k. If None, each position attends to all the others. target_mapping: float32 Tensor in shape [num_predict, len, bsz]. If target_mapping[i, j, k] = 1, the i-th predict in batch k is on the j-th token. Only used during pretraining for partial prediction. Set to None during finetuning. inp_q: float32 Tensor in shape [len, bsz]. 1 for tokens with losses and 0 for tokens without losses. Only used during pretraining for two-stream attention. Set to None during finetuning.  n_layer: int, the number of layers. d_model: int, the hidden size. n_head: int, the number of attention heads. d_head: int, the dimension size of each attention head. d_inner: int, the hidden size in feed-forward layers. ff_activation: str, "relu" or "gelu". untie_r: bool, whether to untie the biases in attention. n_token: int, the vocab size.  is_training: bool, whether in training mode. use_tpu: bool, whether TPUs are used. use_bfloat16: bool, use bfloat16 instead of float32. dropout: float, dropout rate. dropatt: float, dropout rate on attention probabilities. init: str, the initialization scheme, either "normal" or "uniform". init_range: float, initialize the parameters with a uniform distribution in [-init_range, init_range]. Only effective when init="uniform". init_std: float, initialize the parameters with a normal distribution with mean 0 and stddev init_std. Only effective when init="normal". mem_len: int, the number of tokens to cache. reuse_len: int, the number of tokens in the currect batch to be cached and reused in the future. bi_data: bool, whether to use bidirectional input pipeline. Usually set to True during pretraining and False during finetuning. clamp_len: int, clamp all relative distances larger than clamp_len. -1 means no clamping. same_length: bool, whether to use the same attention length for each token. summary_type: str, "last", "first", "mean", or "attn". The method to pool the input to get a vector representation. initializer: A tf initializer. scope: scope name for the computation graph. """ tf.logging.info('memory input {}'.format(mems)) tf_float = tf.bfloat16 if use_bfloat16 else tf.float32 tf.logging.info('Use float type {}'.format(tf_float)) new_mems = [] with tf.variable_scope(scope): if untie_r: r_w_bias = tf.get_variable('r_w_bias', [n_layer, n_head, d_head ], dtype=tf_float, initializer=initializer) r_r_bias = tf.get_variable('r_r_bias', [n_layer, n_head, d_head ], dtype=tf_float, initializer=initializer) else: r_w_bias = tf.get_variable('r_w_bias', [n_head, d_head], dtype= tf_float, initializer=initializer) r_r_bias = tf.get_variable('r_r_bias', [n_head, d_head], dtype= tf_float, initializer=initializer) bsz = tf.shape(inp_k)[1] qlen = tf.shape(inp_k)[0] mlen = tf.shape(mems[0])[0] if mems is not None else 0 klen = mlen + qlen if attn_type == 'uni': attn_mask = _create_mask(qlen, mlen, tf_float, same_length) attn_mask = attn_mask[:, :, (None), (None)] elif attn_type == 'bi': attn_mask = None else: raise ValueError('Unsupported attention type: {}'.format(attn_type) ) if input_mask is not None and perm_mask is not None: data_mask = input_mask[None] + perm_mask elif input_mask is not None and perm_mask is None: data_mask = input_mask[None] elif input_mask is None and perm_mask is not None: data_mask = perm_mask else: data_mask = None if data_mask is not None: mems_mask = tf.zeros([tf.shape(data_mask)[0], mlen, bsz], dtype =tf_float) data_mask = tf.concat([mems_mask, data_mask], 1) if attn_mask is None: attn_mask = data_mask[:, :, :, (None)] else: attn_mask += data_mask[:, :, :, (None)] if attn_mask is not None: attn_mask = tf.cast(attn_mask > 0, dtype=tf_float) if attn_mask is not None: non_tgt_mask = -tf.eye(qlen, dtype=tf_float) non_tgt_mask = tf.concat([tf.zeros([qlen, mlen], dtype=tf_float ), non_tgt_mask], axis=-1) non_tgt_mask = tf.cast(attn_mask + non_tgt_mask[:, :, (None), ( None)] > 0, dtype=tf_float) else: non_tgt_mask = None word_emb_k, lookup_table = embedding_lookup(x=inp_k, n_token= n_token, d_embed=d_model, initializer=initializer, use_tpu= use_tpu, dtype=tf_float, scope='word_embedding') if inp_q is not None: with tf.variable_scope('mask_emb'): mask_emb = tf.get_variable('mask_emb', [1, 1, d_model], dtype=tf_float) if target_mapping is not None: word_emb_q = tf.tile(mask_emb, [tf.shape(target_mapping )[0], bsz, 1]) else: inp_q_ext = inp_q[:, :, (None)] word_emb_q = inp_q_ext * mask_emb + (1 - inp_q_ext ) * word_emb_k output_h = tf.layers.dropout(word_emb_k, dropout, training=is_training) if inp_q is not None: output_g = tf.layers.dropout(word_emb_q, dropout, training= is_training) if seg_id is not None: if untie_r: r_s_bias = tf.get_variable('r_s_bias', [n_layer, n_head, d_head], dtype=tf_float, initializer=initializer) else: r_s_bias = tf.get_variable('r_s_bias', [n_head, d_head], dtype=tf_float, initializer=initializer) seg_embed = tf.get_variable('seg_embed', [n_layer, 2, n_head, d_head], dtype=tf_float, initializer=initializer) mem_pad = tf.zeros([mlen, bsz], dtype=tf.int32) cat_ids = tf.concat([mem_pad, seg_id], 0) seg_mat = tf.cast(tf.logical_not(tf.equal(seg_id[:, (None)], cat_ids[(None), :])), tf.int32) seg_mat = tf.one_hot(seg_mat, 2, dtype=tf_float) else: seg_mat = None pos_emb = relative_positional_encoding(qlen, klen, d_model, clamp_len, attn_type, bi_data, bsz=bsz, dtype=tf_float) pos_emb = tf.layers.dropout(pos_emb, dropout, training=is_training) if mems is None: mems = [None] * n_layer for i in range(n_layer): new_mems.append(_cache_mem(output_h, mems[i], mem_len, reuse_len)) if seg_id is None: r_s_bias_i = None seg_embed_i = None else: r_s_bias_i = r_s_bias if not untie_r else r_s_bias[i] seg_embed_i = seg_embed[i] with tf.variable_scope('layer_{}'.format(i)): if inp_q is not None: output_h, output_g = two_stream_rel_attn(h=output_h, g= output_g, r=pos_emb, r_w_bias=r_w_bias if not untie_r else r_w_bias[i], r_r_bias=r_r_bias if not untie_r else r_r_bias[i], seg_mat=seg_mat, r_s_bias =r_s_bias_i, seg_embed=seg_embed_i, attn_mask_h= non_tgt_mask, attn_mask_g=attn_mask, mems=mems[i], target_mapping=target_mapping, d_model=d_model, n_head=n_head, d_head=d_head, dropout=dropout, dropatt=dropatt, is_training=is_training, kernel_initializer=initializer) reuse = True else: reuse = False output_h = rel_multihead_attn(h=output_h, r=pos_emb, r_w_bias=r_w_bias if not untie_r else r_w_bias[i], r_r_bias=r_r_bias if not untie_r else r_r_bias[i], seg_mat=seg_mat, r_s_bias=r_s_bias_i, seg_embed= seg_embed_i, attn_mask=non_tgt_mask, mems=mems[i], d_model=d_model, n_head=n_head, d_head=d_head, dropout=dropout, dropatt=dropatt, is_training= is_training, kernel_initializer=initializer, reuse= reuse) if inp_q is not None: output_g = positionwise_ffn(inp=output_g, d_model= d_model, d_inner=d_inner, dropout=dropout, kernel_initializer=initializer, activation_type= ff_activation, is_training=is_training) output_h = positionwise_ffn(inp=output_h, d_model=d_model, d_inner=d_inner, dropout=dropout, kernel_initializer= initializer, activation_type=ff_activation, is_training =is_training, reuse=reuse) if inp_q is not None: output = tf.layers.dropout(output_g, dropout, training=is_training) else: output = tf.layers.dropout(output_h, dropout, training=is_training) return output, new_mems, lookup_table 
attention_model._BaseModel.combine|single def combine_single(self, att_layers, early_fusion=0): x = concatenate(att_layers) if len(att_layers) > 1 else att_layers[0] if early_fusion > 0: x = Dense(early_fusion, activation='relu')(x) return Dense(self.nb_classes, activation='softmax')(x) 
data_utils.build_batch_seq2seq_bow2seq.pad def _pad(s_set, max_len, pad_id): s_set = list(s_set)[:max_len] for i in range(max_len - len(s_set)): s_set.append(pad_id) return s_set 
texar.core.layers.layer|normalize def layer_normalize(inputs, scope=None, **kwargs): """Applies layer normalization. Normalizes over the last dimension.  Args: inputs: A tensor with 2 or more dimensions, where the first dimension must be `batch_size`. scope (optional): variable scope.  Returns: A tensor with the same shape and data dtype as `inputs`. """ return tf.contrib.layers.layer_norm(inputs=inputs, begin_norm_axis=-1, begin_params_axis=-1, scope=scope, **kwargs) 
texar.data.data.tfrecord_data_test.TFRecordDataTest.setUp.int|feature def _int64_feature(value=None): """Returns an int64_list from a bool / enum / int / uint. """ return tf.train.Feature(int64_list=tf.train.Int64List(value=[value])) 
classification.ops.utils.eig|posdef|inv def posdef_inv_eig(tensor, identity, damping): """Computes inverse(tensor + damping * identity) with eigendecomposition.""" eigenvalues, eigenvectors = linalg_ops.self_adjoint_eig(tensor + damping * identity) eigenvalues = gen_math_ops.maximum(eigenvalues, damping) return math_ops.matmul(eigenvectors / eigenvalues, eigenvectors, transpose_b=True) 
utils.get|dice def get_dice(pred, gt): total_dice = 0.0 pred = pred.long() gt = gt.long() for i in range(len(pred)): pred_tmp = pred[i] gt_tmp = gt[i] dice = 2.0 * torch.sum(pred_tmp * gt_tmp).item() / (1.0 + torch.sum (pred_tmp ** 2) + torch.sum(gt_tmp ** 2)).item() print(dice) total_dice += dice return total_dice 
classification.ops.fisher_blocks.EigenCorrectedFullyConnectedKFACBasicFB.minibatch|register|additional def register_additional_minibatch(self, inputs, outputs): """Registers an additional minibatch to the FisherBlock. Args: inputs: Tensor of shape [batch_size, input_size]. Inputs to the matrix-multiply. outputs: Tensor of shape [batch_size, output_size]. Layer preactivations. """ self._inputs.append(inputs) self._outputs.append(outputs) 
pvae.tflib.param def param(name, *args, **kwargs): """ A wrapper for `tf.Variable` which enables parameter sharing in models.  Creates and returns theano shared variables similarly to `tf.Variable`, except if you try to create a param with the same name as a previously-created one, `param(...)` will just return the old one instead of making a new one.  This constructor also adds a `param` attribute to the shared variables it creates, so that you can easily search a graph for all params. """ if name not in _params: kwargs['name'] = name param = tf.Variable(*args, **kwargs) param.param = True _params[name] = param return _params[name] 
gym_pycolab.engine.Engine.backdrop|prefilled|set def set_prefilled_backdrop(self, characters, prefill, backdrop_class, *args, **kwargs): """Add a `Backdrop` to this `Engine`, with a custom initial pattern.  Much the same as `set_backdrop`, this method also allows callers to "prefill" the background with an arbitrary pattern. This method is mainly intended for use by the `ascii_art` tools; most `Backdrop` subclasses should fill their `curtain` on their own in the constructor (or in `update()`).  This method does NOT check to make certain that `prefill` contains only ASCII values corresponding to characters in `characters`; your `Backdrop` class should ensure that only valid characters are present in the curtain after the first call to its `update` method returns.  Args: characters: A collection of ASCII characters that the `Backdrop` is allowed to use. (A string will work as an argument here.) prefill: 2-D `uint8` numpy array of the same dimensions as this `Engine`. The `Backdrop`'s curtain will be initialised with this pattern. backdrop_class: A subclass of `Backdrop` (including `Backdrop` itself) that will be constructed by this method. *args: Additional positional arguments for the `backdrop_class` constructor. **kwargs: Additional keyword arguments for the `backdrop_class` constructor.  Returns: the newly-created `Backdrop`.  Raises: RuntimeError: if gameplay has already begun, if `set_backdrop` has already been called for this engine, or if any characters in `characters` has already been claimed by a preceding call to the `add` method. TypeError: if `backdrop_class` is not a `Backdrop` subclass. ValueError: if `characters` are not ASCII characters. """ self._runtime_error_if_called_during_showtime('set_backdrop') self._value_error_if_characters_are_bad(characters) self._runtime_error_if_characters_claimed_already(characters) if self._backdrop: raise RuntimeError( 'A backdrop of type {} has already been supplied to this Engine.' .format(type(self._backdrop))) if not issubclass(backdrop_class, things.Backdrop): raise TypeError( 'backdrop_class arguments to Engine.set_backdrop must either be a Backdrop class or one of its subclasses.' ) curtain = np.zeros((self._rows, self._cols), dtype=np.uint8) palette = Palette(characters) np.copyto(dst=curtain, src=prefill, casting='equiv') self._backdrop = backdrop_class(curtain, palette, *args, **kwargs) return self._backdrop 
read_net_file.net|read def read_net(net_file, in_len, is_trained_with_pytorch): mean = 0.0 std = 0.0 net = open(net_file, 'r') x = tf.placeholder(tf.float64, [in_len], name='x') y = None z1 = None z2 = None last_layer = None h, w, c = None, None, None is_conv = False while True: curr_line = net.readline()[:-1] if 'Normalize' in curr_line: mean = extract_mean(curr_line) std = extract_std(curr_line) elif 'ParSum1' in curr_line: z1 = x print('par sum1') elif 'ParSum2' in curr_line: z2 = x x = z1 elif 'ParSumComplete' in curr_line: x = tf.add(z2, x) elif 'ParSumReLU' in curr_line: x = tf.nn.relu(tf.add(z2, x)) elif 'SkipNet1' in curr_line: y = x print('skip net1') elif 'SkipNet2' in curr_line: print('skip net2') tmp = x x = y y = tmp elif 'SkipCat' in curr_line: print('skip concatenation ', x.shape[0], x.shape[1], y.shape[0], y.shape[1]) x = tf.concat([y, x], 1) elif curr_line == 'ReLU' or curr_line == 'Sigmoid' or curr_line == 'Tanh' or curr_line == 'Affine': print(curr_line) W = None if (last_layer == 'Conv2D' or last_layer == 'ParSumComplete' or last_layer == 'ParSumReLU') and is_trained_with_pytorch: W = myConst(permutation(parseVec(net), h, w, c).transpose()) else: W = myConst(parseVec(net).transpose()) b = parseVec(net) b = myConst(b) if curr_line == 'Affine': x = tf.nn.bias_add(tf.matmul(tf.reshape(x, [1, numel(x)]), W), b) elif curr_line == 'ReLU': x = tf.nn.relu(tf.nn.bias_add(tf.matmul(tf.reshape(x, [1, numel(x)]), W), b)) elif curr_line == 'Sigmoid': x = tf.nn.sigmoid(tf.nn.bias_add(tf.matmul(tf.reshape(x, [1, numel(x)]), W), b)) else: x = tf.nn.tanh(tf.nn.bias_add(tf.matmul(tf.reshape(x, [1, numel(x)]), W), b)) print('\tOutShape: ', x.shape) print('\tWShape: ', W.shape) print('\tBShape: ', b.shape) elif curr_line == 'MaxPooling2D': maxpool_line = net.readline()[:-1] if 'stride' in maxpool_line: args = runRepl(maxpool_line, ['input_shape', 'pool_size', 'stride', 'padding']) stride = [1] + args['stride'] + [1] else: args = runRepl(maxpool_line, ['input_shape', 'pool_size']) stride = [1] + args['pool_size'] + [1] if 'padding' in maxpool_line: if args['padding'] == 1: padding_arg = 'SAME' else: padding_arg = 'VALID' else: padding_arg = 'VALID' ksize = [1] + args['pool_size'] + [1] print('MaxPool', args) x = tf.nn.max_pool(tf.reshape(x, [1] + args['input_shape']), padding=padding_arg, strides=stride, ksize=ksize) print('\tOutShape: ', x.shape) elif curr_line == 'Conv2D': is_conv = True line = net.readline() args = None start = 0 if 'ReLU' in line: start = 5 elif 'Sigmoid' in line: start = 8 elif 'Tanh' in line: start = 5 elif 'Affine' in line: start = 7 if 'padding' in line: args = runRepl(line[start:-1], ['filters', 'input_shape', 'kernel_size', 'stride', 'padding']) else: args = runRepl(line[start:-1], ['filters', 'input_shape', 'kernel_size']) W = myConst(parseVec(net)) print('W shape', W.shape) b = None if 'padding' in line: if args['padding'] == 1: padding_arg = 'SAME' else: padding_arg = 'VALID' else: padding_arg = 'VALID' if 'stride' in line: stride_arg = [1] + args['stride'] + [1] else: stride_arg = [1, 1, 1, 1] x = tf.nn.conv2d(tf.reshape(x, [1] + args['input_shape']), filter=W, strides=stride_arg, padding=padding_arg) b = myConst(parseVec(net)) h, w, c = [int(i) for i in x.shape][1:] print('Conv2D', args, 'W.shape:', W.shape, 'b.shape:', b.shape) print('\tOutShape: ', x.shape) if 'ReLU' in line: x = tf.nn.relu(tf.nn.bias_add(x, b)) elif 'Sigmoid' in line: x = tf.nn.sigmoid(tf.nn.bias_add(x, b)) elif 'Tanh' in line: x = tf.nn.tanh(tf.nn.bias_add(x, b)) elif 'Affine' in line: x = tf.nn.bias_add(x, b) else: raise Exception('Unsupported activation: ', curr_line) elif curr_line == '': break else: raise Exception('Unsupported Operation: ', curr_line) last_layer = curr_line model = x return model, is_conv, mean, std 
nmt.estimator.api|and|with|train|eval|level|low def train_and_eval_with_low_level_api(hparams): """Train and evaluation function using tpu low level api.""" hparams.tgt_sos_id, hparams.tgt_eos_id = 1, 2 model_fn = make_model_fn(hparams, []) train_runner = create_train_runner(hparams) eval_runner = create_eval_runner(hparams) mlperf_log.gnmt_print(key=mlperf_log.RUN_START) params = {'batch_size': int(hparams.batch_size / hparams.num_shards), 'infer_batch_size': int(hparams.infer_batch_size / hparams.num_shards)} train_input_fn = make_input_fn(hparams, tf.contrib.learn.ModeKeys.TRAIN) train_runner.initialize(train_input_fn, params) train_runner.build_model(model_fn, params) eval_input_fn = make_input_fn(hparams, tf.contrib.learn.ModeKeys.INFER) eval_runner.initialize(eval_input_fn, params) eval_runner.build_model(model_fn, params) score = 0.0 mlperf_log.gnmt_print(key=mlperf_log.TRAIN_LOOP) mlperf_log.gnmt_print(key=mlperf_log.EVAL_TARGET, value=hparams.target_bleu ) current_step = train_runner.get_global_step() steps_per_epoch = int(hparams.num_examples_per_epoch / hparams.batch_size) for i in range(int(round(current_step / steps_per_epoch)), hparams. max_train_epochs): mlperf_log.gnmt_print(key=mlperf_log.TRAIN_EPOCH, value=i) tf.logging.info('Start training epoch %d', i) mlperf_log.gnmt_print(key=mlperf_log.INPUT_SIZE, value=hparams. num_examples_per_epoch) train_runner.train(current_step, steps_per_epoch) current_step = current_step + steps_per_epoch mlperf_log.gnmt_print(key=mlperf_log.TRAIN_CHECKPOINT, value= 'Under ' + hparams.out_dir) tf.logging.info('End training epoch %d', i) mlperf_log.gnmt_print(key=mlperf_log.EVAL_START) predictions = list(eval_runner.predict()) score = get_metric(hparams, predictions, current_step) tf.logging.info('Score after epoch %d: %f', i, score) mlperf_log.gnmt_print(key=mlperf_log.EVAL_ACCURACY, value={'value': score, 'epoch': i}) mlperf_log.gnmt_print(key=mlperf_log.EVAL_STOP, value=i) mlperf_log.gnmt_print(mlperf_log.RUN_STOP, {'success': False}) return score 
EndToEndClassification.EnvClassification.loader.ClassifierLoader.batch|indices|make def make_batch_indices(self, batch_size): """ Generates randomized batch indices for a single train epoch.  Args: batch_size (int): size of the batch.  Returns: (list): containing the indices for each of the batches. """ indices = list(range(self.train[0].shape[0])) random.shuffle(indices) nr_batches = int(np.floor(self.train[0].shape[0] / batch_size)) if nr_batches == 0: raise ValueError('batch size is too big') else: return [indices[b * batch_size:(b + 1) * batch_size] for b in range (nr_batches)] 
generate-vbinary-test.ukernel|split|name def split_ukernel_name(name): match = re.match( '^xnn_(f16|f32)_v(add|div|max|min|mul|sub|addc|divc|rdivc|maxc|minc|mulc|subc|rsubc)_ukernel__(.+)_x(\\d+)$' , name) if match is None: raise ValueError('Unexpected microkernel name: ' + name) op_type = {'add': 'Add', 'div': 'Div', 'max': 'Max', 'min': 'Min', 'mul': 'Mul', 'sub': 'Sub', 'addc': 'AddC', 'divc': 'DivC', 'rdivc': 'RDivC', 'maxc': 'MaxC', 'minc': 'MinC', 'mulc': 'MulC', 'subc': 'SubC', 'rsubc': 'RSubC'}[match.group(2)] batch_tile = int(match.group(4)) arch, isa = xnncommon.parse_target_name(target_name=match.group(3)) return op_type, batch_tile, arch, isa 
aggregator_mnist.get|metrics def get_metrics(hparams): incpt_pkl = hparams.incpt_pkl data = basic_utils.load_if_pickled(incpt_pkl) metric_names = ['inception_score_mean'] metrics = {metric_name: data[metric_name] for metric_name in metric_names} return metrics 
src.model.cnn.BN|Conv|Relu def ConvReluBN(incoming, num_filters, filter_size, name, is_training, padding_type='SAME'): """ Convolution -> Batch normalization -> Relu :param incoming: :param num_filters: :param filter_size: :param name: :param is_training: :param padding_type: :return: """ num_filters_from = incoming.get_shape().as_list()[3] with tf.variable_scope(name): conv_W = var_random('W', tuple(filter_size) + (num_filters_from, num_filters), regularizable=True) after_conv = tf.nn.conv2d(incoming, conv_W, strides=(1, 1, 1, 1), padding=padding_type) after_bn = batch_norm(after_conv, is_training) return tf.nn.relu(after_bn) 
nets.resnet_v2.v|resnet def resnet_v2_152(inputs, num_classes=None, is_training=True, global_pool= True, output_stride=None, spatial_squeeze=True, reuse=None, scope= 'resnet_v2_152'): """ResNet-152 model of [1]. See resnet_v2() for arg and return description.""" blocks = [resnet_v2_block('block1', base_depth=64, num_units=3, stride= 2), resnet_v2_block('block2', base_depth=128, num_units=8, stride=2 ), resnet_v2_block('block3', base_depth=256, num_units=36, stride=2 ), resnet_v2_block('block4', base_depth=512, num_units=3, stride=1)] return resnet_v2(inputs, blocks, num_classes, is_training=is_training, global_pool=global_pool, output_stride=output_stride, include_root_block=True, spatial_squeeze=spatial_squeeze, reuse= reuse, scope=scope) 
nmt.estimator.convert|strings|ids|to def _convert_ids_to_strings(tgt_vocab_file, ids): """Convert prediction ids to words.""" with tf.Session() as sess: reverse_target_vocab_table = (lookup_ops. index_to_string_table_from_file(tgt_vocab_file, default_value= vocab_utils.UNK)) sess.run(tf.tables_initializer()) translations = sess.run(reverse_target_vocab_table.lookup(tf. to_int64(tf.convert_to_tensor(np.asarray(ids))))) return translations 
train.parser|feedforward|create def create_feedforward_parser(parser): """Add the TD3 goal-conditioned policy hyperparameters to the parser.""" parser.add_argument('--buffer_size', type=int, default= FEEDFORWARD_PARAMS['buffer_size'], help= 'the max number of transitions to store') parser.add_argument('--batch_size', type=int, default= FEEDFORWARD_PARAMS['batch_size'], help= 'the size of the batch for learning the policy') parser.add_argument('--actor_lr', type=float, default= FEEDFORWARD_PARAMS['actor_lr'], help='the actor learning rate') parser.add_argument('--critic_lr', type=float, default= FEEDFORWARD_PARAMS['critic_lr'], help='the critic learning rate') parser.add_argument('--tau', type=float, default=FEEDFORWARD_PARAMS[ 'tau'], help= 'the soft update coefficient (keep old values, between 0 and 1)') parser.add_argument('--gamma', type=float, default=FEEDFORWARD_PARAMS[ 'gamma'], help='the discount rate') parser.add_argument('--noise', type=float, default=FEEDFORWARD_PARAMS[ 'noise'], help= 'scaling term to the range of the action space, that is subsequently used as the standard deviation of Gaussian noise added to the action if `apply_noise` is set to True in `get_action`' ) parser.add_argument('--target_policy_noise', type=float, default= FEEDFORWARD_PARAMS['target_policy_noise'], help= 'standard deviation term to the noise from the output of the target actor policy. See TD3 paper for more.' ) parser.add_argument('--target_noise_clip', type=float, default= FEEDFORWARD_PARAMS['target_noise_clip'], help= 'clipping term for the noise injected in the target actor policy') parser.add_argument('--layer_norm', action='store_true', help= 'enable layer normalisation') parser.add_argument('--use_huber', action='store_true', help= 'specifies whether to use the huber distance function as the loss for the critic. If set to False, the mean-squared error metric is used instead' ) return parser 
xlnet-master.function_builder.get|loss|race def get_race_loss(FLAGS, features, is_training): """Loss for downstream multi-choice QA tasks such as RACE.""" bsz_per_core = tf.shape(features['input_ids'])[0]  def _transform_features(feature): out = tf.reshape(feature, [bsz_per_core, 4, -1]) out = tf.transpose(out, [2, 0, 1]) out = tf.reshape(out, [-1, bsz_per_core * 4]) return out inp = _transform_features(features['input_ids']) seg_id = _transform_features(features['segment_ids']) inp_mask = _transform_features(features['input_mask']) label = tf.reshape(features['label_ids'], [bsz_per_core]) xlnet_config = xlnet.XLNetConfig(json_path=FLAGS.model_config_path) run_config = xlnet.create_run_config(is_training, True, FLAGS) xlnet_model = xlnet.XLNetModel(xlnet_config=xlnet_config, run_config= run_config, input_ids=inp, seg_ids=seg_id, input_mask=inp_mask) summary = xlnet_model.get_pooled_out(FLAGS.summary_type, FLAGS. use_summ_proj) with tf.variable_scope('logits'): logits = tf.layers.dense(summary, 1, kernel_initializer=xlnet_model .get_initializer()) logits = tf.reshape(logits, [bsz_per_core, 4]) one_hot_target = tf.one_hot(label, 4) per_example_loss = -tf.reduce_sum(tf.nn.log_softmax(logits) * one_hot_target, -1) total_loss = tf.reduce_mean(per_example_loss) return total_loss, per_example_loss, logits 
decoder.decoding_train.loop|attn|dec|fn def _dec_loop_attn_fn(i, dec_state, dec_outputs, dec_logits_train, dec_pointers, dec_prob_train, dec_g_train): dec_in = dec_inputs[i] query = dec_state[-1].h if multi_source: context, _ = multi_source_attention(query, memory, mem_lens, max_mem_len, state_size) else: context, dist = attention(query, memory, mem_lens, max_mem_len) attn_vec = context + query bow_cond_g = 1.0 if bow_cond_gate_proj is not None: bow_cond_g = bow_cond_gate_proj(query + bow_cond) if bow_cond is not None: dec_in = dec_in + bow_cond_g * bow_cond dec_out, dec_state = dec_cell(dec_in + attn_vec, dec_state) dec_logits = dec_proj(dec_out) vocab_dist = tf.nn.softmax(dec_logits) if copy: pointers = [] print('use %d pointers' % len(dec_ptr_k_proj)) for proj_i in dec_ptr_k_proj: ptr_query = proj_i(dec_out) _, ptr = attention(ptr_query, memory[0], mem_lens[0], max_mem_len[0]) pointers.append(ptr) pointers = tf.stack(pointers) pointers = tf.reduce_mean(tf.transpose(pointers, [1, 0, 2]), 1) g = dec_ptr_g_proj(dec_out) mixed_dist, ptr_dist = _mix_dist(vocab_dist, pointers, copy_ind, g) dec_pointers = dec_pointers.write(i, pointers) dec_prob_train = dec_prob_train.write(i, mixed_dist) dec_g_train = dec_g_train.write(i, g) dec_outputs = dec_outputs.write(i, dec_out) dec_logits_train = dec_logits_train.write(i, dec_logits) return (i + 1, dec_state, dec_outputs, dec_logits_train, dec_pointers, dec_prob_train, dec_g_train) 
ops.dil|conv|d def dil_conv2d(input_, output_dim, k_h=3, k_w=3, rate=2, stddev=0.02, name= 'conv2d'): with tf.variable_scope(name): w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim], initializer=tf.truncated_normal_initializer(stddev =stddev)) conv = tf.nn.atrous_conv2d(input_, w, rate=rate, padding='SAME') biases = tf.get_variable('biases', [output_dim], initializer=tf. constant_initializer(0.0)) conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape()) return conv 
enas.ptb.ptb_ops.batch|norm def batch_norm(x, is_training, name='batch_norm', decay=0.999, epsilon=1.0): shape = x.get_shape()[1] with tf.variable_scope(name, reuse=None if is_training else True): offset = tf.get_variable('offset', shape, initializer=tf. constant_initializer(0.0, dtype=tf.float32)) scale = tf.get_variable('scale', shape, initializer=tf. constant_initializer(1.0, dtype=tf.float32)) moving_mean = tf.get_variable('moving_mean', shape, trainable=False, initializer=tf.constant_initializer(0.0, dtype=tf.float32)) moving_variance = tf.get_variable('moving_variance', shape, trainable=False, initializer=tf.constant_initializer(1.0, dtype =tf.float32)) if is_training: mean, variance = tf.nn.moments(x, [0]) update_mean = moving_averages.assign_moving_average(moving_mean, mean, decay) update_variance = moving_averages.assign_moving_average( moving_variance, variance, decay) with tf.control_dependencies([update_mean, update_variance]): x = scale * (x - mean) / tf.sqrt(epsilon + variance) + offset else: x = scale * (x - moving_mean) / tf.sqrt(epsilon + moving_variance ) + offset return x 
official.utils.logs.hooks_test.ExamplesPerSecondHookTest.examples|sec|every|per|test|secs def test_examples_per_sec_every_1_secs(self): with self.graph.as_default(): self._validate_log_every_n_secs(1) 
model.BasicModel.Basic|Model def __init__(self, config): self.config = config 
bnn_trainer.Trainer.step|single|run def run_single_step(self, batch_chunk): _start_time = time.time() fetch = [self.global_step, self.summary_op, self.model.kl_loss, self. model.rmse, self.model.ll, self.train_op] fetch_values = self.session.run(fetch, feed_dict=self.model. get_feed_dict(batch_chunk)) [step, summary, kl_loss, rmse, ll] = fetch_values[:5] _end_time = time.time() return step, summary, kl_loss, rmse, ll, _end_time - _start_time 
pvae.pixelvae_bbans_two_layer.rec|net def rec_net1(x): _mu, _sig, h = session.run([mu1, sig1, h1], feed_dict={images_tf: x, bn_is_training: False, bn_stats_iter: 0, total_iters: 99999}) return _mu, _sig, h 
plyfile.PlyElement.name @property def name(self): return self._name 
task.SortedMerge.Sorted|Merge def __init__(self, alphabet: list): """ :param alphabet: Alphabet used. Last element is used as delimiter. Should be at least 5 """ a_len = len(alphabet) self.delimiter = alphabet[a_len - 1] self.delimiter1 = alphabet[a_len - 2] self.word_gen = WordSorting(alphabet[:-1]) 
cleverhans.attacks_tf.CarliniWagnerL2.batch|attack def attack_batch(self, imgs, labs, mask=None): """ Run the attack on a batch of instance and labels. Mask is an argument that allows modifying only a subset of the pixels, and is used by the CW L0 attack algorithm.    """  def compare(x, y): if not isinstance(x, (float, int, np.int64)): x = np.copy(x) if self.TARGETED: x[y] -= self.CONFIDENCE else: x[y] += self.CONFIDENCE x = np.argmax(x) if self.TARGETED: return x == y else: return x != y batch_size = self.batch_size if mask is None: mask = np.ones(imgs.shape, dtype=imgs.dtype) oimgs = np.clip(imgs, self.clip_min, self.clip_max) imgs = (imgs - self.clip_min) / (self.clip_max - self.clip_min) imgs = np.clip(imgs, 0, 1) imgs = imgs * 2 - 1 imgs = np.arctanh(imgs * 0.999999) lower_bound = np.zeros(batch_size) CONST = np.ones(batch_size) * self.initial_const upper_bound = np.ones(batch_size) * 10000000000.0 o_bestl2 = [10000000000.0] * batch_size o_bestscore = [-1] * batch_size o_bestattack = np.copy(oimgs) for outer_step in range(self.BINARY_SEARCH_STEPS): self.sess.run(self.init) batch = imgs[:batch_size] batchlab = labs[:batch_size] bestl2 = [10000000000.0] * batch_size bestscore = [-1] * batch_size _logger.debug('  Binary search step {} of {}'.format(outer_step, self.BINARY_SEARCH_STEPS)) if self.repeat and outer_step == self.BINARY_SEARCH_STEPS - 1: CONST = upper_bound self.sess.run(self.setup, {self.assign_timg: batch, self. assign_mask: mask, self.assign_simg: batch, self.assign_tlab: batchlab, self.assign_const: CONST}) prev = 1000000.0 for iteration in range(self.MAX_ITERATIONS): _, l, l2s, scores, nimg = self.sess.run([self.train, self.loss, self.l2dist, self.output, self.newimg]) if iteration % (self.MAX_ITERATIONS // 10 or 1) == 0: _logger.debug(('    Iteration {} of {}: loss={:.3g} ' + 'l2={:.3g} f={:.3g}').format(iteration, self. MAX_ITERATIONS, l, np.mean(l2s), np.mean(scores))) if self.ABORT_EARLY and iteration % (self.MAX_ITERATIONS // 10 or 1 ) == 0: if l > prev * 0.9999: msg = '    Failed to make progress; stop early' _logger.debug(msg) break prev = l for e, (l2, sc, ii) in enumerate(zip(l2s, scores, nimg)): lab = np.argmax(batchlab[e]) if l2 < bestl2[e] and compare(sc, lab): bestl2[e] = l2 bestscore[e] = np.argmax(sc) if l2 < o_bestl2[e] and compare(sc, lab): o_bestl2[e] = l2 o_bestscore[e] = np.argmax(sc) o_bestattack[e] = ii for e in range(batch_size): if compare(bestscore[e], np.argmax(batchlab[e])) and bestscore[e ] != -1: upper_bound[e] = min(upper_bound[e], CONST[e]) if upper_bound[e] < 1000000000.0: CONST[e] = (lower_bound[e] + upper_bound[e]) / 2 else: lower_bound[e] = max(lower_bound[e], CONST[e]) if upper_bound[e] < 1000000000.0: CONST[e] = (lower_bound[e] + upper_bound[e]) / 2 else: CONST[e] *= 10 _logger.debug('  Successfully generated adversarial examples ' + 'on {} of {} instances.'.format(sum(upper_bound < 1000000000.0), batch_size)) o_bestl2 = np.array(o_bestl2) mean = np.mean(np.sqrt(o_bestl2[o_bestl2 < 1000000000.0])) _logger.debug('   Mean successful distortion: {:.4g}'.format(mean)) o_bestl2 = np.array(o_bestl2) if self.extension == 0: if o_bestscore[0] == -1: return None gradients = self.sess.run(self.outgrad) return gradients, o_bestscore, o_bestattack return o_bestattack 
Model.get|neg def get_neg(ILL, output_layer, k): neg = [] t = len(ILL) ILL_vec = np.array([output_layer[e1] for e1 in ILL]) KG_vec = np.array(output_layer) sim = scipy.spatial.distance.cdist(ILL_vec, KG_vec, metric='cityblock') for i in range(t): rank = sim[(i), :].argsort() neg.append(rank[0:k]) neg = np.array(neg) neg = neg.reshape((t * k,)) return neg 
darkflow.net.yolo.predict.preprocess def preprocess(self, im, allobj=None): """ Takes an image, return it as a numpy tensor that is readily to be fed into tfnet. If there is an accompanied annotation (allobj), meaning this preprocessing is serving the train process, then this image will be transformed with random noise to augment training data, using scale, translation, flipping and recolor. The accompanied parsed annotation (allobj) will also be modified accordingly. """ if type(im) is not np.ndarray: im = cv2.imread(im) if allobj is not None: result = imcv2_affine_trans(im) im, dims, trans_param = result scale, offs, flip = trans_param for obj in allobj: _fix(obj, dims, scale, offs) if not flip: continue obj_1_ = obj[1] obj[1] = dims[0] - obj[3] obj[3] = dims[0] - obj_1_ im = imcv2_recolor(im) im = self.resize_input(im) if allobj is None: return im return im 
model.feature|build|cascade|coord|graph|bins def build_cascade_coord_bins_feature_graph(mrcnn_coord_x_bin, mrcnn_coord_y_bin, mrcnn_coord_z_bin, mrcnn_coord_feature_x, mrcnn_coord_feature_y, mrcnn_coord_feature_z, num_bins, num_classes): """Builds the computation graph of the second-stage coordinate map.  Inputs: mrcnn_coord_x_bin, mrcnn_coord_y_bin, mrcnn_coord_z_bin: [batch, roi_count, height, width, num_classes, num_bins] image_shape: [height, width, depth] pool_size: The width of the square feature map generated from ROI Pooling. num_classes: number of classes, which determines the depth of the results  Returns: x, y, and z, Coordinate maps [batch, roi_count, height, width, num_classes] """ x1 = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 28, 28, num_bins * num_classes]), name='mrcnn_coord_cascade_final_reshape_x')( mrcnn_coord_x_bin) x1 = KL.TimeDistributed(KL.Conv2D(512, (1, 1), padding='same'), name= 'mrcnn_coord_cascade_conv1')(x1) x1 = KL.Activation('relu')(x1) x2 = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 28, 28, num_bins * num_classes]), name='mrcnn_coord_cascade_final_reshape_y')( mrcnn_coord_y_bin) x2 = KL.TimeDistributed(KL.Conv2D(512, (1, 1), padding='same'), name= 'mrcnn_coord_cascade_conv2')(x2) x2 = KL.Activation('relu')(x2) x3 = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 28, 28, num_bins * num_classes]), name='mrcnn_coord_cascade_final_reshape_z')( mrcnn_coord_z_bin) x3 = KL.TimeDistributed(KL.Conv2D(512, (1, 1), padding='same'), name= 'mrcnn_coord_cascade_conv3')(x3) x3 = KL.Activation('relu')(x3) f1 = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 28, 28, 256 ]), name='mrcnn_coord_cascade_final_reshape_feature_x')( mrcnn_coord_feature_x) f1 = KL.TimeDistributed(KL.Conv2D(512, (1, 1), padding='same'), name= 'mrcnn_coord_cascade_conv4')(f1) f1 = KL.Activation('relu')(f1) f2 = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 28, 28, 256 ]), name='mrcnn_coord_cascade_final_reshape_feature_y')( mrcnn_coord_feature_y) f2 = KL.TimeDistributed(KL.Conv2D(512, (1, 1), padding='same'), name= 'mrcnn_coord_cascade_conv5')(f2) f2 = KL.Activation('relu')(f2) f3 = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 28, 28, 256 ]), name='mrcnn_coord_cascade_final_reshape_feature_z')( mrcnn_coord_feature_z) f3 = KL.TimeDistributed(KL.Conv2D(512, (1, 1), padding='same'), name= 'mrcnn_coord_cascade_conv6')(f3) f3 = KL.Activation('relu')(f3) x = KL.Add(name='mrcnn_coord_cascade_add')([x1, x2, x3, f1, f2, f3]) x = KL.TimeDistributed(KL.Conv2D(512, (1, 1), padding='same'), name= 'mrcnn_coord_cascade_conv7')(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(num_classes * 3, (1, 1), padding= 'same'), name='mrcnn_coord_cascade_conv_final')(x) x = KL.Activation('sigmoid', name='mrcnn_coord_cascade_sigmoid')(x) x = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], tf.shape(t)[1], tf.shape(t)[2], tf.shape(t)[3], num_classes, 3]), name= 'mrcnn_coord_cascade_final_reshape')(x) mrcnn_coord_x = KL.Lambda(lambda x: x[:, :, :, :, :, (0)], name= 'mrcnn_coord_x_final')(x) mrcnn_coord_y = KL.Lambda(lambda x: x[:, :, :, :, :, (1)], name= 'mrcnn_coord_y_final')(x) mrcnn_coord_z = KL.Lambda(lambda x: x[:, :, :, :, :, (2)], name= 'mrcnn_coord_z_final')(x) return mrcnn_coord_x, mrcnn_coord_y, mrcnn_coord_z 
train_shapenet.placeholder|inputs def placeholder_inputs(batch_size, num_point): xyz_pl = tf.placeholder(tf.float32, shape=(batch_size, num_point, 3)) label_pl = tf.placeholder(tf.int32, shape=(batch_size, num_point)) return xyz_pl, label_pl 
bert-master.extract_features.examples|read def read_examples(input_file): """Read a list of `InputExample`s from an input file.""" examples = [] unique_id = 0 with tf.gfile.GFile(input_file, 'r') as reader: while True: line = tokenization.convert_to_unicode(reader.readline()) if not line: break line = line.strip() text_a = None text_b = None m = re.match('^(.*) \\|\\|\\| (.*)$', line) if m is None: text_a = line else: text_a = m.group(1) text_b = m.group(2) examples.append(InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b)) unique_id += 1 return examples 
facenet-master.tmp.mnist_center_loss.maybe|download def maybe_download(filename): """Download the data from Yann's website, unless it's already here.""" if not tf.gfile.Exists(WORK_DIRECTORY): tf.gfile.MakeDirs(WORK_DIRECTORY) filepath = os.path.join(WORK_DIRECTORY, filename) if not tf.gfile.Exists(filepath): filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath) with tf.gfile.GFile(filepath) as f: size = f.size() print('Successfully downloaded', filename, size, 'bytes.') return filepath 
pvae.model.TwoLayerModel.dec def dec1(self, latents1, images): theta1 = self.dec1_partial(latents1) return self.dec1_pixelcnn(theta1, images) 
deepctr.models.ccpm.CCPM def CCPM(feature_dim_dict, embedding_size=8, conv_kernel_width=(6, 5), conv_filters=(4, 4), dnn_hidden_units=(256,), l2_reg_linear=1e-05, l2_reg_embedding=1e-05, l2_reg_dnn=0, dnn_dropout=0, init_std=0.0001, seed=1024, task='binary'): """Instantiates the Convolutional Click Prediction Model architecture.  :param feature_dim_dict: dict,to indicate sparse field and dense field like {'sparse':{'field_1':4,'field_2':3,'field_3':2},'dense':['field_4','field_5']} :param embedding_size: positive integer,sparse feature embedding_size :param conv_kernel_width: list,list of positive integer or empty list,the width of filter in each conv layer. :param conv_filters: list,list of positive integer or empty list,the number of filters in each conv layer. :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN. :param l2_reg_linear: float. L2 regularizer strength applied to linear part :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector :param l2_reg_dnn: float. L2 regularizer strength applied to DNN :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate. :param init_std: float,to use as the initialize std of embedding vector :param seed: integer ,to use as random seed. :param task: str, ``"binary"`` for  binary logloss or  ``"regression"`` for regression loss :return: A Keras model instance. """ check_feature_config_dict(feature_dim_dict) if len(conv_kernel_width) != len(conv_filters): raise ValueError( 'conv_kernel_width must have same element with conv_filters') deep_emb_list, linear_emb_list, dense_input_dict, inputs_list = ( preprocess_input_embedding(feature_dim_dict, embedding_size, l2_reg_embedding, l2_reg_linear, init_std, seed, create_linear_weight=True)) linear_logit = get_linear_logit(linear_emb_list, dense_input_dict, l2_reg_linear) n = len(deep_emb_list) l = len(conv_filters) conv_input = concat_fun(deep_emb_list, axis=1) pooling_result = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=3) )(conv_input) for i in range(1, l + 1): filters = conv_filters[i - 1] width = conv_kernel_width[i - 1] k = max(1, int((1 - pow(i / l, l - i)) * n)) if i < l else 3 conv_result = tf.keras.layers.Conv2D(filters=filters, kernel_size=( width, 1), strides=(1, 1), padding='same', activation='tanh', use_bias=True)(pooling_result) pooling_result = KMaxPooling(k=min(k, conv_result.shape[1].value), axis=1)(conv_result) flatten_result = tf.keras.layers.Flatten()(pooling_result) final_logit = DNN(dnn_hidden_units, l2_reg=l2_reg_dnn, dropout_rate= dnn_dropout)(flatten_result) final_logit = tf.keras.layers.Dense(1, use_bias=False)(final_logit) final_logit = tf.keras.layers.add([final_logit, linear_logit]) output = PredictionLayer(task)(final_logit) model = tf.keras.models.Model(inputs=inputs_list, outputs=output) return model 
evaluate_shapenet_onehot.parse|fn def parse_fn(item): features = tf.parse_single_example(item, features={'xyz_raw': tf. FixedLenFeature([], dtype=tf.string), 'seg_label': tf. FixedLenFeature([], dtype=tf.string), 'cls_label': tf. FixedLenFeature([], dtype=tf.int64)}) xyz = tf.decode_raw(features['xyz_raw'], tf.float32) seg_label = tf.decode_raw(features['seg_label'], tf.int32) cls_label = tf.cast(features['cls_label'], tf.int32) xyz = tf.reshape(xyz, [-1, 3]) seg_label = tf.reshape(seg_label, [-1, 1]) print(cls_label) cls_label = tf.cast([cls_label, cls_label, cls_label, cls_label], tf. float32) cls_label = tf.reshape(cls_label, [1, -1]) all_in_one = tf.concat((xyz, tf.cast(seg_label, tf.float32)), axis=-1) all_in_one = tf.concat((all_in_one, cls_label), axis=0) return all_in_one 
prep_data.initialize|vocabulary def initialize_vocabulary(vocabulary_path): if gfile.Exists(vocabulary_path): rev_vocab = [] with gfile.GFile(vocabulary_path, mode='r') as f: rev_vocab.extend(f.readlines()) rev_vocab = [line.strip() for line in rev_vocab] vocab = dict([(x, y) for y, x in enumerate(rev_vocab)]) return vocab, rev_vocab else: raise ValueError('Vocabulary file %s not found.', vocabulary_path) 
m_phate.kernel.kernel|multislice def _multislice_kernel(data, intraslice_knn=2, interslice_knn=25, decay=5, n_pca=100, p=list, kernel_fn=graph_kernel, pdist_fn=square_pdist, knn_fn=knn_dist, kernel_dist_fn=distance_to_kernel, **kwargs): n = data.shape[0] m = data.shape[1] N = n * m K = sparse.lil_matrix((N, N)) kernels = p(kernel_fn(x, knn=intraslice_knn, decay=decay, **kwargs, n_jobs=1) for x in data) for i, G_K in enumerate(kernels): K = graphtools.utils.set_submatrix(K, np.arange(i * m, (i + 1) * m), np.arange(i * m, (i + 1) * m), G_K) interslice_dist = p(pdist_fn(data[:, (vertex), :]) for vertex in range( data.shape[1])) bandwidths = p(knn_fn(D, interslice_knn) for D in interslice_dist) bandwidth = np.mean(bandwidths) interslice_kernel = p(kernel_dist_fn(D, bandwidth) for D in interslice_dist ) for vertex, A in enumerate(interslice_kernel): K = graphtools.utils.set_submatrix(K, np.arange(n) * m + vertex, np .arange(n) * m + vertex, A) return K 
classification.ops.fisher_factors.ConvOutputKroneckerFactor.num|sources @property def _num_sources(self): return len(self._outputs_grads) 
lambada.Detokenizer.detokenize|sentence def detokenize_sentence(self, tokens) ->List[str]: sentence = [self.detokenize_token(token) for token in tokens] return sentence 
data_utils.exp|safe def safe_exp(x): perp = 10000 if x < 100: perp = math.exp(x) if perp > 10000: return 10000 return perp 
topKProtect.h def h(F, true_label, k): S = softmax(F) idx_sort = np.argsort(S[:, (true_label)])[::-1] n, m = np.shape(S) F = F[(idx_sort), :] for i in range(0, n): F_r = F[i:, :] f_v = np.sum(F_r, axis=0) topk = np.argsort(f_v)[::-1][:k] if true_label not in topk: return i return n 
invariant_l0_attack.transform|find def find_transform(): global x_train, x_test x_train = (x_train > 0.5) + 0 x_test = (x_test > 0.5) + 0 UID = random.randint(0, 1000000) transformation_matrix = tf.placeholder(tf.float32, [8]) inverse_matrix = tf.placeholder(tf.float32, [8]) darkena = tf.placeholder(DTYPE, []) darkenb = tf.placeholder(DTYPE, []) print('shape', x_train.shape) dataset = tf.constant(x_train, dtype=DTYPE) labels = tf.constant(y_train, dtype=tf.int32) print('a1') transformed_dataset = tf.contrib.image.transform(dataset, transformation_matrix, 'BILINEAR') inverted_dataset = tf.contrib.image.transform(transformed_dataset, inverse_matrix, 'BILINEAR') ok_transform = tf.reduce_sum(inverted_dataset, axis=(1, 2, 3) ) > tf.reduce_sum(dataset, axis=(1, 2, 3)) * 0.85 transformed_dataset = (1 - (1 - transformed_dataset) ** darkenb) ** ( 1.0 / darkenb) print('a2') flat_transformed = tf.cast(tf.reshape(transformed_dataset, [-1, 28 * 28 ]), dtype=DTYPE) query = tf.placeholder(DTYPE, (None, 28, 28, 1)) query_y = tf.placeholder(tf.int32, [None]) query_t = tf.transpose(tf.reshape(query, [-1, 28 * 28])) query_t = (1 - (1 - query_t) ** darkena) ** (1.0 / darkena) print('a3') norms = tf.reduce_sum(tf.square(flat_transformed), axis=1)[:, (tf.newaxis) ] - 2 * tf.matmul(flat_transformed, query_t) badness1 = 1000 * tf.reshape(1 - tf.cast(ok_transform, dtype=DTYPE), [- 1, 1]) badness2 = 1000 * tf.cast(tf.equal(tf.reshape(query_y, [1, -1]), tf. reshape(labels, [-1, 1])), dtype=DTYPE) print(norms, badness1, badness2, query_y, labels) norms = norms + badness1 + badness2 _, topk_indices = tf.nn.top_k(-tf.transpose(norms), k=1, sorted=False) print('done')  def rand(low, high): return random.random() * (high - low) + low sess = tf.Session() best = np.zeros((100, 28, 28, 1)) l0 = np.zeros(100) + 10000 best_idx = np.zeros(100) best_transforms = [None] * 100 for tick in range(10000000): angle = rand(-0.25, 0.25) sx, sy = rand(0.8, 1.2), rand(0.8, 1.2) ax, ay = rand(-0.2, 0.2), rand(-0.2, 0.2) tx, ty = rand(-8, 8), rand(-8, 8) da, db = rand(-0.25, 4), rand(-0.25, 4) mat, inv = compute_mat(angle, sx, sy, ax, ay, tx, ty, da, db) now = time.time() ns, topk, dat, is_ok = sess.run((norms, topk_indices, transformed_dataset, ok_transform), {transformation_matrix: mat .flatten()[:-1], inverse_matrix: inv.flatten()[:-1], query: x_test[use_idx], query_y: y_test[use_idx], darkena: db, darkenb: db}) for i in range(100): e = topk[i][0] v = ns[e, i] dd = np.sum((x_test[use_idx[i]] > 0.5) ^ (dat[e] > 0.5)) if dd < l0[i]: l0[i] = min(l0[i], dd) best[i] = dat[e] best_idx[i] = e best_transforms[i] = [angle, sx, sy, ax, ay, tx, ty, da, db] if tick % 1000 == 0: print('mean', np.mean(l0), 'median', np.median(l0)) print(sorted(l0)) np.save('best/best_%d_%d.npy' % (UID, tick), best) np.save('best/best_%d_%d_idx.npy' % (UID, tick), best_idx) np.save('best/best_%d_transforms_%d.npy' % (UID, tick), best_transforms) if tick % 10000 == 0: for i in range(100): print('is', l0[i]) show(x_test[use_idx[i]]) show(best[i]) show((x_test[use_idx[i]] > 0.5) ^ (best[i] > 0.5)) 
utils.refinement|box def box_refinement(box, gt_box): """Compute refinement needed to transform box to gt_box. box and gt_box are [N, (y1, x1, y2, x2)]. (y2, x2) is assumed to be outside the box. """ box = box.astype(np.float32) gt_box = gt_box.astype(np.float32) height = box[:, (2)] - box[:, (0)] width = box[:, (3)] - box[:, (1)] center_y = box[:, (0)] + 0.5 * height center_x = box[:, (1)] + 0.5 * width gt_height = gt_box[:, (2)] - gt_box[:, (0)] gt_width = gt_box[:, (3)] - gt_box[:, (1)] gt_center_y = gt_box[:, (0)] + 0.5 * gt_height gt_center_x = gt_box[:, (1)] + 0.5 * gt_width dy = (gt_center_y - center_y) / height dx = (gt_center_x - center_x) / width dh = np.log(gt_height / height) dw = np.log(gt_width / width) return np.stack([dy, dx, dh, dw], axis=1) 
fasterai.visualize.get|colorizer|video def get_video_colorizer(render_factor: int=21) ->VideoColorizer: return get_stable_video_colorizer(render_factor=render_factor) 
r2r_problem.R2RProblem.eval def eval(self, action_list, env_output_list): result = {} for key, fn in self._eval_dict.items(): score = fn(action_list, env_output_list, self._env) result[key] = score return result 
texar.modules.networks.network_base.FeedForwardNetworkBase.build def _build(self, inputs, mode=None): """Feeds forward inputs through the network layers and returns outputs.  Args: inputs: The inputs to the network. The requirements on inputs depends on the first layer and subsequent layers in the network. mode (optional): A tensor taking value in :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, including `TRAIN`, `EVAL`, and `PREDICT`. If `None`, :func:`texar.global_mode` is used.  Returns: The output of the network. """ training = is_train_mode(mode) prev_outputs = inputs for layer_id, layer in enumerate(self._layers): if isinstance(layer, tf.layers.Dropout) or isinstance(layer, tf. layers.BatchNormalization): outputs = layer(prev_outputs, training=training) else: outputs = layer(prev_outputs) self._layer_outputs.append(outputs) self._layer_outputs_by_name[self._layer_names[layer_id]] = outputs prev_outputs = outputs if not self._built: self._add_internal_trainable_variables() for layer in self._layers: self._add_trainable_variable(layer.trainable_variables) self._built = True return outputs 
tokenization_transfo_xl_test.TransfoXLTokenizationTest.no|lower|full|test|tokenizer def test_full_tokenizer_no_lower(self): tokenizer = TransfoXLTokenizer(lower_case=False) self.assertListEqual(tokenizer.tokenize(' \tHeLLo!how  \n Are yoU?  '), ['HeLLo', '!', 'how', 'Are', 'yoU', '?']) 
losses.loss|magnet def magnet_loss(features, labels, margin=1.0, unique_labels=None): """Simple unimodal magnet loss.  See::  Rippel, Paluri, Dollar, Bourdev: Metric Learning With Adaptive Density Discrimination. ICLR, 2016.  Parameters ---------- features : tf.Tensor A matrix of shape NxM that contains the M-dimensional feature vectors of N objects (floating type). labels : tf.Tensor The one-dimensional array of length N that contains for each feature the associated class label (integer type). margin : float A scalar margin hyperparameter. unique_labels : Optional[tf.Tensor] Optional tensor of unique values in `labels`. If None given, computed from data.  Returns ------- tf.Tensor A scalar loss tensor.  """ nil = tf.constant(0.0, tf.float32) one = tf.constant(1.0, tf.float32) minus_two = tf.constant(-2.0, tf.float32) eps = tf.constant(0.0001, tf.float32) margin = tf.constant(margin, tf.float32) num_per_class = None if unique_labels is None: unique_labels, sample_to_unique_y, num_per_class = (tf. unique_with_counts(labels)) num_per_class = tf.cast(num_per_class, tf.float32) y_mat = tf.cast(tf.equal(tf.reshape(labels, (-1, 1)), tf.reshape( unique_labels, (1, -1))), dtype=tf.float32) if num_per_class is None: num_per_class = tf.reduce_sum(y_mat, reduction_indices=[0]) class_means = tf.reduce_sum(tf.expand_dims(tf.transpose(y_mat), -1) * tf.expand_dims(features, 0), reduction_indices=[1]) / tf.expand_dims( num_per_class, -1) squared_distance = _pdist(features, class_means) num_samples = tf.cast(tf.shape(labels)[0], tf.float32) variance = tf.reduce_sum(y_mat * squared_distance) / (num_samples - one) const = one / (minus_two * (variance + eps)) linear = const * squared_distance - y_mat * margin maxi = tf.reduce_max(linear, reduction_indices=[1], keepdims=True) loss_mat = tf.exp(linear - maxi) a = tf.reduce_sum(y_mat * loss_mat, reduction_indices=[1]) b = tf.reduce_sum((one - y_mat) * loss_mat, reduction_indices=[1]) loss = tf.maximum(nil, -tf.log(eps + a / (eps + b))) return tf.reduce_mean(loss), class_means, variance 
tflib.ops.linear.Linear.sample def sample(shape): if len(shape) < 2: raise RuntimeError('Only shapes of length 2 or more are supported.') flat_shape = shape[0], np.prod(shape[1:]) a = np.random.normal(0.0, 1.0, flat_shape) u, _, v = np.linalg.svd(a, full_matrices=False) q = u if u.shape == flat_shape else v q = q.reshape(shape) return q.astype('float32') 
Micro-Net-master.Models._test_micro_net.micro|test|net def test_micro_net(): """ micro_net() test case  :return: """ model = micro_net(nb_classes=2, inputs_shape=(500, 500, 3)) model.summary() 
cifar10.Cifar10DataSet.preprocess def preprocess(self, image): """Preprocess a single image in [height, width, depth] layout.""" if self.subset == 'train' and self.use_distortion: image = tf.image.resize_image_with_crop_or_pad(image, 40, 40) image = tf.random_crop(image, [HEIGHT, WIDTH, DEPTH]) image = tf.image.random_flip_left_right(image) return image 
infinite_fcn_test.InfiniteFcnTest.test|infinite|fcn def test_infinite_fcn(self): infinite_fcn.main(None) 
cnn.get|sent|from|idx def get_idx_from_sent(sent, word_idx_map, maxlen, padding): x = [] for i in range(padding): x.append(0) words = sent.split() for word in words: if word in word_idx_map: x.append(word_idx_map[word]) while len(x) < maxlen + 2 * padding: x.append(0) return x 
construct_delta_patch.graph|build def build_graph(sess, num_steps, step_size, dog_imgs, loc, dog_mask, func_name='resnet_model/final_dense:0', input_name='input_tensor:0', model_name='scene_only', image_bounds=[0 - _B_MEAN, 255 - _R_MEAN], alpha=0.01, beta=0.01): """Construct the graph to compute ||f(x) - f(x+d)|| num_steps times.  During each iteration, delta is updated using gradient descent to minimize the difference between f(x) and f(x+d). """ dog_mask_resized = img_as_bool(resize(dog_mask.astype(np.bool), (loc[2] - loc[0], loc[3] - loc[1]))) dog_mask_resized = np.expand_dims(dog_mask_resized, axis=2) dog_mask_resized = np.array([np.tile(dog_mask_resized, [1, 1, 3])]) tf.saved_model.loader.load(sess, ['serve'], os.path.join(os.getcwd(), 'models', model_name)) graph = tf.get_default_graph() gdef_1 = tf.graph_util.convert_variables_to_constants(sess, tf. get_default_graph().as_graph_def(), ['softmax_tensor'], variable_names_blacklist=[input_name, func_name])  def update_delta(delta, i): tf.import_graph_def(gdef_1, input_map={input_name: tf. get_default_graph().get_tensor_by_name(input_name) + delta}) graph = tf.get_default_graph() image = graph.get_tensor_by_name(input_name) fx = graph.get_tensor_by_name(func_name) fxd = graph.get_tensor_by_name('while/import/' + func_name) loss = tf.norm(fxd - fx) loss = loss - alpha * tf.norm(tf.reshape(delta, [-1])) relu = tf.reduce_sum(tf.nn.relu(image + delta - image_bounds[0]) + tf.nn.relu(tf.fill(tf.shape(image), image_bounds[1]) - image - delta)) loss = loss + beta * relu new_delta = delta - step_size * tf.squeeze(tf.gradients(loss, delta), 0 ) new_delta = tf.pad(new_delta[:, loc[0]:loc[2], loc[1]:loc[3], :] * dog_mask_resized, tf.constant([[0, 0], [loc[0], 224 - loc[2]], [loc[1], 224 - loc[3]], [0, 0]])) return new_delta, i + 1  def cond(unused_delta, i): return tf.less(i, num_steps) i = tf.constant(0) delta = tf.constant(dog_imgs) new_delta, _ = tf.while_loop(cond, update_delta, loop_vars=[delta, i]) image = tf.get_default_graph().get_tensor_by_name(input_name) new_image = image + new_delta return tf.stop_gradient(new_image) 
official.resnet.imagenet_preprocessing.size|smallest|least|at def _smallest_size_at_least(height, width, resize_min): """Computes new shape with the smallest side equal to `smallest_side`.  Computes new shape with the smallest side equal to `smallest_side` while preserving the original aspect ratio.  Args: height: an int32 scalar tensor indicating the current height. width: an int32 scalar tensor indicating the current width. resize_min: A python integer or scalar `Tensor` indicating the size of the smallest side after resize.  Returns: new_height: an int32 scalar tensor indicating the new height. new_width: an int32 scalar tensor indicating the new width. """ resize_min = tf.cast(resize_min, tf.float32) height, width = tf.cast(height, tf.float32), tf.cast(width, tf.float32) smaller_dim = tf.minimum(height, width) scale_ratio = resize_min / smaller_dim new_height = tf.cast(height * scale_ratio, tf.int32) new_width = tf.cast(width * scale_ratio, tf.int32) return new_height, new_width 
nmt.model.BaseModel.model|build def _build_model(self, hparams): """Builds a sequence-to-sequence model.  Args: hparams: Hyperparameter configurations.  Returns: For infrence, A tuple of the form (logits, decoder_cell_outputs, predicted_ids), where: logits: logits output of the decoder. decoder_cell_outputs: the output of decoder. predicted_ids: predicted ids from beam search. For training, returns the final loss  Raises: ValueError: if encoder_type differs from mono and bi, or attention_option is not (luong | scaled_luong | bahdanau | normed_bahdanau). """ if hparams.language_model: utils.print_out('  language modeling: no encoder') self.encoder_outputs = None encoder_state = None else: self.encoder_outputs, encoder_state = self._build_encoder(hparams) return self._build_decoder(self.encoder_outputs, encoder_state, hparams) 
sgan_sn_cifar10.spectral|normalization def spectral_normalization(w): return w / spectral_norm(w) 
gym_pycolab.engine.Engine.its|showtime def its_showtime(self): """Finalise `Engine` set-up and compute the first observation of the game.  Switches the `Engine` from set-up mode, where more `Sprite`s and `Drape`s can be added, to "play" mode, where gameplay iterates via the `play` method. After this permanent modal switch, no further calls to `add_drape` or `add_sprite` can be made.  Once in "play" mode, consults the `Backdrop` and all `Sprite`s and `Drape`s for updates, and uses these to compute the episode's first observation.  Returns: A three-tuple with the following members: * A `rendering.Observation` object containing single-array and multi-array feature-map representations of the game board. * An initial reward given to the player (or players) (before it/they even gets/get a chance to play!). This reward can be any type---it all depends on what the `Backdrop`, `Sprite`s, and `Drape`s have communicated to the `Plot`. If none have communicated anything at all, this will be None. * A reinforcement learning discount factor value. By default, it will be 1.0 if the game is still ongoing; if the game has just terminated (before the player got a chance to do anything!), `discount` will be 0.0 unless the game has chosen to supply a non-standard value to the `Plot`'s `terminate_episode` method.  Raises: RuntimeError: if this method is called more than once, or if no `Backdrop` class has ever been provided to the Engine. """ self._runtime_error_if_called_during_showtime('its_showtime') self._showtime = True self._update_groups = [(key, self._update_groups[key]) for key in sorted(self._update_groups.keys())] self._current_update_group = None chars = set(self._sprites_and_drapes.keys()).union(self._backdrop.palette) self._renderer = rendering.BaseObservationRenderer(self._rows, self. _cols, chars) self._render() return self.play(None) 
td_or_not_td.alg.replaymemory.ReplayMemory.save|pre|next def next_pre_save(self, state, vector=None): """Pre-save next observation until full information is available.  Args: state: Image observation. vector: Additional measurement observations. """ assert self.next_s is None self.next_s = copy.deepcopy(state) if vector is not None: self.next_v = copy.deepcopy(vector) 
network.NeuralNetwork.layer|add def add_layer(self, layer_object): """Add a layer to network  Arguments: layer_object {layers.AbstractLayer} -- Layer object """ assert not self.finalized, 'Graph already finalized.' layer_object.set_vars(self.out, self.n_layers) self.out = layer_object.forward(self.out) self.xs.append(self.out) self.regularization_loss += layer_object.regularizer() self.layers.append(layer_object) self.n_layers += 1 
models.attacks.SaliencyMapMethod.Saliency|Method|Map def __init__(self, model, back='tf', sess=None): """ Create a SaliencyMapMethod instance. Note: the model parameter should be an instance of the cleverhans.model.Model abstraction provided by CleverHans. """ super(SaliencyMapMethod, self).__init__(model, back, sess) if not isinstance(self.model, Model): self.model = CallableModelWrapper(self.model, 'probs') import tensorflow as tf self.feedable_kwargs = {'y_target': tf.float32} self.structural_kwargs = ['theta', 'gamma', 'clip_max', 'clip_min'] 
amb_basic_utils.up|dir|set def set_up_dir(directory, clean=False): if os.path.exists(directory): if clean: shutil.rmtree(directory) else: os.makedirs(directory) 
output.Output.get|options|run def get_run_options(self): return self.run_options 
nmt.model_helper.CellWrapper.size|state @property def state_size(self): return self._cell.state_size 
models.differential_evolution.DifferentialEvolutionSolver.ensure|constraint def _ensure_constraint(self, trial): """ make sure the parameters lie between the limits """ for index in np.where((trial < 0) | (trial > 1))[0]: trial[index] = self.random_number_generator.rand() 
SRGANs-Spectral-Regularization-GANs--master.evaluation.calc_inception.evaluation @chainer.training.make_extension() def evaluation(trainer=None): model = load_inception_model(path) ims = gen_images(gen, n_ims, batchsize=batchsize).astype('f') mean, std = inception_score(model, ims, splits=splits) chainer.reporter.report({'inception_mean': mean, 'inception_std': std}) if dst is not None: preview_dir = '{}/IS.txt'.format(dst) with open(preview_dir, 'a', encoding='ascii') as f: f.write(str(trainer.updater.iteration)) f.write(':') f.write(str(mean)) f.write(',  ') f.write(str(std)) f.write('\n') 
model_hvc.towers|run def run_towers(optimizer, global_step, is_training, training_data, validation_data, count_classes, num_gpus): with tf.device('/device:CPU:0'), tf.name_scope('input/train_or_eval'): images, labels = tf.cond(is_training, lambda : training_data, lambda : validation_data) labels_list = [] logits_list = [] loss_list = [] grads = [] for i in range(num_gpus): tower_name = 'tower%d' % i with tf.device('/device:GPU:%d' % i): with tf.name_scope(tower_name): these_logits, this_loss = make_tower(images, labels, is_training, count_classes) logits_list.append(these_logits) loss_list.append(this_loss) labels_list.append(labels) grads.append(optimizer.compute_gradients(this_loss)) train_op, loss, acc_top_1, acc_top_5 = merge_towers_and_optimize(optimizer, global_step, grads, logits_list, loss_list, labels_list) return train_op, loss, acc_top_1, acc_top_5 
generate-raddextexp-test.generate|cases|test def generate_test_cases(ukernel, elements_tile, isa): """Generates all tests cases for a RAddExtExp micro-kernel.  Args: ukernel: C name of the micro-kernel function. elements_tile: Number of batch elements processed per one iteration of the inner loop of the micro-kernel. isa: instruction set required to run the micro-kernel. Generated unit test will skip execution if the host processor doesn't support this ISA.  Returns: Code for the test case. """ _, test_name = ukernel.split('_', 1) _, datatype, _ = ukernel.split('_', 2) return xngen.preprocess(RADDEXTEXP_TEST_TEMPLATE, {'TEST_FUNCTION': ukernel, 'TEST_NAME': test_name.upper().replace('UKERNEL_', ''), 'DATATYPE': datatype, 'ELEMENTS_TILE': elements_tile, 'ISA_CHECK': xnncommon.generate_isa_check_macro(isa)}) 
deepctr.layers.core.LocalActivationUnit.get|config def get_config(self): config = {'activation': self.activation, 'hidden_units': self. hidden_units, 'l2_reg': self.l2_reg, 'dropout_rate': self. dropout_rate, 'use_bn': self.use_bn, 'seed': self.seed} base_config = super(LocalActivationUnit, self).get_config() return dict(list(base_config.items()) + list(config.items())) 
utils.get|test|images def get_images_test(path, mod_type='L', height=None, width=None): image = imread(path, mode=mod_type) if height is not None and width is not None: image = imresize(image, [height, width], interp='nearest') if mod_type == 'L': d = image.shape image = np.reshape(image, [d[0], d[1], 1]) return image 
tests.layers.sequence_test.test|Transformer def test_Transformer(): with CustomObjectScope({'Transformer': sequence.Transformer}): layer_test(sequence.Transformer, kwargs={'att_embedding_size': 1, 'head_num': 8, 'use_layer_norm': True, 'supports_masking': False, 'dropout_rate': 0.5}, input_shape=[(BATCH_SIZE, SEQ_LENGTH, EMBEDDING_SIZE), (BATCH_SIZE, SEQ_LENGTH, EMBEDDING_SIZE), (BATCH_SIZE, 1), (BATCH_SIZE, 1)]) 
vnet.Upsampling.forward def forward(self, x): x = self.conv(x) return x 
core.base_model.BaseModel.load def load(self, sess): """ Loads latest checkpoint from the experiment path defined in the config file. :param sess: TensorFlow Session :return: None """ latest_checkpoint = tf.train.latest_checkpoint(self.config.checkpoint_dir) if latest_checkpoint: print('Loading model checkpoint {} ...\n'.format(latest_checkpoint)) self.saver.restore(sess, latest_checkpoint) print('Model loaded') 
europilot.train.Writer.write def _write(self, file_, image_filename, sensor_data): """Synchronously write sensor data to disk. :param image_filename: filename of corresponding training image :param sensor_data: `SensorData` object.  CSV format seq_id,filename,sensor_data1,sensor_data2,sensor_data3,...  Order of sensor data depends on `OrderedDict` defined in `controllerstate.ControllerState`.  """ if not self._csv_initialized: sensor_header = ','.join(sensor_data.raw.keys()) csv_header = 'img,' + sensor_header file_.write(csv_header + '\n') self._csv_initialized = True values = [image_filename] + [str(x) for x in sensor_data.raw.values()] data = ','.join(values) self._data_seq += 1 file_.write(data + '\n') 
PatchStatsCalculator.PatchStatsCalculator.calc|stats def calc_stats(self, just_save=False): if just_save: logging.trace('just_save') save(os.path.join(self.save_dir, 'pat_stats%s.npy') % self. file_postfix, self.stats) return self.stats t0 = time.time() divs = self.divide_parts(self.hps.train_its, self.n_threads) out_que = queue.Queue(self.n_threads) cnt_mb_que = queue.Queue() for t in range(self.n_threads): self.threads.append(Thread(target=self.calc_patch_stats, args=(t, self.mb_queue, divs[t], out_que, cnt_mb_que))) self.threads[t].start() print('') for t in range(self.n_threads): self.threads[t].join() self.weighted_stats(out_que) self.calc_scalar_stats() save(os.path.join(self.save_dir, 'pat_stats%s.npy') % self.file_postfix, self.stats) logging.trace('calc. stats: time = %3.0f s ' % (time.time() - t0)) return self.stats 
classification.ops.fisher_factors.FisherFactor.op|update|make|covariance def make_covariance_update_op(self, ema_decay): """Constructs and returns the covariance update Op. Args: ema_decay: The exponential moving average decay (float or Tensor). Returns: An Op for updating the covariance Variable referenced by _cov. """ new_cov = math_ops.add_n(tuple(self._compute_new_cov(idx) for idx in range(self._num_sources))) return moving_averages.assign_moving_average(self._cov, new_cov, ema_decay, zero_debias=ZERO_DEBIAS) 
gan.latent.UniformLatent.Latent|Uniform def __init__(self, in_dim, out_dim, low=-1.0, high=1.0, q_std=1.0, *args, **kwargs): super(UniformLatent, self).__init__(*args, **kwargs) self.low = low self.high = high self.q_std = q_std self._in_dim = in_dim self._out_dim = out_dim 
extensions.u_hved.application.U_HVEDApplication.iteration|update|set def set_iteration_update(self, iteration_message): if self.is_training: choices = [] nb_choices = np.random.randint(4) choices = np.random.choice(4, nb_choices + 1, replace=False, p=[1 / 4, 1 / 4, 1 / 4, 1 / 4]) choices = [(True if k in choices else False) for k in range(4)] iteration_message.data_feed_dict[self.choices] = choices n_iter = iteration_message.current_iter decay = 4 leng = 10 iteration_message.data_feed_dict[self.lr ] = self.action_param.lr / decay ** int(n_iter / leng) 
td_or_not_td.alg.graph.Graph.network def network(self, perception, vector, action_input, name, data_format, reuse=False, print_shape=False): nc = NetworkCreator(name, data_format, reuse, perception, print_shape) if self.p.use_vector_input and self.p.use_screen: assert vector is not None tmp = nc.extend(vector) elif self.p.use_vector_input: tmp = vector elif self.p.use_screen: tmp = perception else: raise ValueError('both use_vector_input and use_screen are False') if self.p.algorithm == 'a3c': pol = nc.fc_layer(self.p.number_of_actions, tmp) v = tf.reshape(nc.fc_layer(1, tmp), [-1]) return [pol, v] elif self.p.algorithm == 'Qmc': assert action_input is not None if self.p.use_screen: nc.fc_layer(512, tmp) nc.relu() r_a = nc.fc_layer(self.p.number_of_actions * self.p. number_of_predictions) r_a = tf.reshape(r_a, [-1, self.p.number_of_actions, self.p. number_of_predictions]) a = tf.argmax(tf.reduce_sum(self.p.prediction_steps_usage * r_a, 2), 1) r_a = r_a - tf.reduce_mean(r_a, reduction_indices=1, keep_dims=True ) action_input = tf.reshape(action_input, tf.concat([tf.shape( action_input), [1]], 0)) r_a = tf.reduce_sum(r_a * action_input, 1) nc.fc_layer(512, tmp) nc.relu() r = nc.fc_layer(self.p.number_of_predictions) r_list = r + r_a return [r_list, a] else: r_a = nc.fc_layer(self.p.number_of_actions * self.p. number_of_predictions, tmp) r_a = tf.reshape(r_a, [-1, self.p.number_of_actions, self.p. number_of_predictions]) action_input = tf.reshape(action_input, tf.concat([tf.shape( action_input), [1]], 0)) r_list = tf.reduce_sum(r_a * action_input, 1) a = tf.argmax(tf.reduce_sum(self.p.prediction_steps_usage * r_a, 2), 1) return [r_list, a] elif self.p.algorithm == 'Q': assert action_input is not None if self.p.use_screen: nc.fc_layer(512, tmp) nc.relu() q_a = nc.fc_layer(self.p.number_of_actions) a = tf.argmax(q_a, 1) q_a = q_a - tf.reduce_mean(q_a, reduction_indices=1, keep_dims=True ) q_a = tf.reduce_sum(q_a * action_input, 1) nc.fc_layer(512, tmp) nc.relu() q = nc.fc_layer(1) q = tf.reshape(q, [-1]) q_list = q + q_a return [q_list, a] else: q_a = nc.fc_layer(self.p.number_of_actions, tmp) a = tf.argmax(q_a, 1) q_list = tf.reduce_sum(q_a * action_input, 1) return [q_list, a] else: raise ValueError('unknown algorithm') 
functions.load|data def load_data(dataset, opt): if dataset == 'atomic': data_loader = load_atomic_data(opt) elif dataset == 'conceptnet': data_loader = load_conceptnet_data(opt) encoder_path = 'model/encoder_bpe_40000.json' bpe_path = 'model/vocab_40000.bpe' text_encoder = TextEncoder(encoder_path, bpe_path) text_encoder.encoder = data_loader.vocab_encoder text_encoder.decoder = data_loader.vocab_decoder return data_loader, text_encoder 
pytorch_pretrained_bert.modeling_openai.OpenAIGPTMultipleChoiceHead.forward def forward(self, hidden_states, mc_token_ids): mc_token_ids = mc_token_ids.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, -1, hidden_states.size(-1)) multiple_choice_h = hidden_states.gather(2, mc_token_ids).squeeze(2) multiple_choice_logits = self.linear(multiple_choice_h).squeeze(-1) return multiple_choice_logits 
celebA.gen.wgan_utils.summary|and|to|regularization|add def add_to_regularization_and_summary(var): if var is not None: tf.summary.histogram(var.op.name, var) tf.add_to_collection('reg_loss', tf.nn.l2_loss(var)) 
GPSig.kernels.Sequential.K|compute|full @autoflow((settings.float_type, [None, None]), (settings.float_type, [None, None])) def compute_K_full(self, X, Y): return self.K(X, Y, override_full=True) 
instruction_encoder.InstructionEncoder.get|layer|embedding def _get_embedding_layer(self, pretrained_embed_path, oov_buckets_size, vocab_size, embed_dim): """Get word embedding layer.  Args: pretrained_embed_path: Pretrained glove embedding path. oov_buckets_size: Out-of-vocabularies bucket size. vocab_size: vocabulary size (used if pretrained_embed_path is None). embed_dim: the dimension of word embeddings ( used if pretrained_embed_path is None).  Returns: A tf.keras.layers.Embedding instance. """ if pretrained_embed_path: with tf.io.gfile.GFile(pretrained_embed_path, 'rb') as f: floats_np = np.load(f) vocab_size = floats_np.shape[0] embed_dim = floats_np.shape[1] init_tensor = tf.constant(floats_np) oov_init = tf.compat.v1.truncated_normal_initializer(stddev=0.01)(shape =(oov_buckets_size, embed_dim), dtype=tf.float32) init_tensor = tf.concat([init_tensor, oov_init], axis=0) else: init_tensor = tf.compat.v1.truncated_normal_initializer(stddev=0.01)( shape=(vocab_size + oov_buckets_size, embed_dim), dtype=tf.float32) embeddings_initializer = tf.constant_initializer(init_tensor.numpy()) return tf.keras.layers.Embedding(vocab_size + oov_buckets_size, embed_dim, embeddings_initializer=embeddings_initializer, mask_zero =True, name='embedding') 
elpips.pnetlin.tensor|normalize def normalize_tensor(in_feat, eps=1e-10): """Normalizes a tensor to unit length in the depth/feature dimension.""" norm_factor = tf.sqrt(tf.reduce_sum(tf.square(in_feat), axis=3, keepdims=True)) return in_feat / (norm_factor + eps) 
model.image|parse|meta def parse_image_meta(meta): """Parses an image info Numpy array to its components. See compose_image_meta() for more details. """ image_id = meta[:, (0)] image_shape = meta[:, 1:4] window = meta[:, 4:8] active_class_ids = meta[:, 8:] return image_id, image_shape, window, active_class_ids 
official.utils.logs.logger.environment|test|collect def _collect_test_environment(run_info): """Detect the local environment, eg GCE, AWS or DGX, etc.""" if cloud_lib.on_gcp(): run_info['test_environment'] = GCP_TEST_ENV 
model.layerwise.contexts|prior def prior_contexts(hps): return np.zeros(latent_shape(hps)), np.zeros(latent_shape(hps)), np.zeros( hidden_shape(hps)) 
avod.datasets.kitti.kitti_utils.KittiUtils.Kitti|Utils def __init__(self, dataset): self.dataset = dataset self.label_cluster_utils = LabelClusterUtils(self.dataset) self.clusters, self.std_devs = [None, None] self.bev_source = self.dataset.bev_source self.config = dataset.config.kitti_utils_config self.area_extents = np.reshape(self.config.area_extents, (3, 2)) self.bev_extents = self.area_extents[[0, 2]] self.voxel_size = self.config.voxel_size self.anchor_strides = np.reshape(self.config.anchor_strides, (-1, 2)) self.bev_generator = bev_generator_builder.build(self.config. bev_generator, self) self._density_threshold = self.config.density_threshold if self.bev_source == 'depth' and not os.path.exists(self.dataset.depth_dir ): raise FileNotFoundError( 'Could not find depth maps, please run demos/save_lidar_depth_maps.py in wavedata first' ) self.mini_batch_utils = MiniBatchUtils(self.dataset) self._mini_batch_dir = self.mini_batch_utils.mini_batch_dir self.clusters, self.std_devs = self.label_cluster_utils.get_clusters() 
dataset.DataSet.Set|Data def __init__(self, in_dir, exts='.jpg'): """ Create a data-set consisting of the filenames in the given directory and sub-dirs that match the given filename-extensions.  For example, the knifey-spoony data-set (see knifey.py) has the following dir-structure:  knifey-spoony/forky/ knifey-spoony/knifey/ knifey-spoony/spoony/ knifey-spoony/forky/test/ knifey-spoony/knifey/test/ knifey-spoony/spoony/test/  This means there are 3 classes called: forky, knifey, and spoony.  If we set in_dir = "knifey-spoony/" and create a new DataSet-object then it will scan through these directories and create a training-set and test-set for each of these classes.  The training-set will contain a list of all the *.jpg filenames in the following directories:  knifey-spoony/forky/ knifey-spoony/knifey/ knifey-spoony/spoony/  The test-set will contain a list of all the *.jpg filenames in the following directories:  knifey-spoony/forky/test/ knifey-spoony/knifey/test/ knifey-spoony/spoony/test/  See the TensorFlow Tutorial #09 for a usage example.  :param in_dir: Root-dir for the files in the data-set. This would be 'knifey-spoony/' in the example above.  :param exts: String or tuple of strings with valid filename-extensions. Not case-sensitive.  :return: Object instance. """ in_dir = os.path.abspath(in_dir) self.in_dir = in_dir self.exts = tuple(ext.lower() for ext in exts) self.class_names = [] self.filenames = [] self.filenames_test = [] self.class_numbers = [] self.class_numbers_test = [] self.num_classes = 0 for name in os.listdir(in_dir): current_dir = os.path.join(in_dir, name) if os.path.isdir(current_dir): self.class_names.append(name) filenames = self._get_filenames(current_dir) self.filenames.extend(filenames) class_number = self.num_classes class_numbers = [class_number] * len(filenames) self.class_numbers.extend(class_numbers) filenames_test = self._get_filenames(os.path.join(current_dir, 'test')) self.filenames_test.extend(filenames_test) class_numbers = [class_number] * len(filenames_test) self.class_numbers_test.extend(class_numbers) self.num_classes += 1 
lottery.lottery.mask|is|name def is_mask_name(mask_name): return mask_name.endswith(_MASK_SUFFIX) 
layers.TanhLayer.Layer|Tanh def __init__(self): super().__init__() self.name = 'Tanh' 
thumt.layers.attention.signal|timing|add def add_timing_signal(x, min_timescale=1.0, max_timescale=10000.0, name=None): """ This function adds a bunch of sinusoids of different frequencies to a Tensor. See paper: `Attention is all you need'  :param x: A tensor with shape [batch, length, channels] :param min_timescale: A floating point number :param max_timescale: A floating point number :param name: An optional string  :returns: a Tensor the same shape as x. """ with tf.name_scope(name, default_name='add_timing_signal', values=[x]): length = tf.shape(x)[1] channels = tf.shape(x)[2] position = tf.to_float(tf.range(length)) num_timescales = channels // 2 log_timescale_increment = math.log(float(max_timescale) / float( min_timescale)) / (tf.to_float(num_timescales) - 1) inv_timescales = min_timescale * tf.exp(tf.to_float(tf.range( num_timescales)) * -log_timescale_increment) scaled_time = tf.expand_dims(position, 1) * tf.expand_dims( inv_timescales, 0) signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1) signal = tf.pad(signal, [[0, 0], [0, tf.mod(channels, 2)]]) signal = tf.reshape(signal, [1, length, channels]) return x + tf.cast(signal, x.dtype) 
facenet-master.src.generative.models.dfc_vae.Vae.encoder def encoder(self, images, is_training): activation_fn = leaky_relu weight_decay = 0.0 with tf.variable_scope('encoder'): with slim.arg_scope([slim.batch_norm], is_training=is_training): with slim.arg_scope([slim.conv2d, slim.fully_connected], weights_initializer=tf.truncated_normal_initializer(stddev= 0.1), weights_regularizer=slim.l2_regularizer(weight_decay), normalizer_fn=slim.batch_norm, normalizer_params=self. batch_norm_params): net = slim.conv2d(images, 32, [4, 4], 2, activation_fn= activation_fn, scope='Conv2d_1') net = slim.conv2d(net, 64, [4, 4], 2, activation_fn= activation_fn, scope='Conv2d_2') net = slim.conv2d(net, 128, [4, 4], 2, activation_fn= activation_fn, scope='Conv2d_3') net = slim.conv2d(net, 256, [4, 4], 2, activation_fn= activation_fn, scope='Conv2d_4') net = slim.flatten(net) fc1 = slim.fully_connected(net, self.latent_variable_dim, activation_fn=None, normalizer_fn=None, scope='Fc_1') fc2 = slim.fully_connected(net, self.latent_variable_dim, activation_fn=None, normalizer_fn=None, scope='Fc_2') return fc1, fc2 
evaluate_s3dis_with_overlap.train def train(): testset = input_fn(TESTLIST, BATCH_SIZE) test_iterator = testset.make_initializable_iterator() next_test_element = test_iterator.get_next() with tf.device('/gpu:0'): input_pl, label_pl, inner_label_pl = placeholder_inputs(BATCH_SIZE, NUM_POINT) training_pl = tf.placeholder(tf.bool, shape=()) pred, end_points = MODEL.get_model(input_pl, training_pl, config= net_config) MODEL.get_loss(pred, label_pl, end_points, inner_label_pl) if net_config.weight_decay is not None: reg_loss = tf.multiply(tf.losses.get_regularization_loss(), net_config.weight_decay, name='reg_loss') tf.add_to_collection('losses', reg_loss) losses = tf.get_collection('losses') total_loss = tf.add_n(losses, name='total_loss') saver = tf.train.Saver() n = len([n.name for n in tf.get_default_graph().as_graph_def().node]) print('*****************The Graph has %d nodes*****************' % n) config = tf.ConfigProto() config.gpu_options.allow_growth = True config.allow_soft_placement = True config.log_device_placement = False with tf.Session(config=config) as sess: ops = {'input_pl': input_pl, 'label_pl': label_pl, 'inner_label_pl': inner_label_pl, 'training_pl': training_pl, 'pred': pred, 'loss': total_loss} saver.restore(sess, os.path.join(LOG_DIR, FLAGS.model_name)) sess.run(test_iterator.initializer) eval_one_epoch(sess, ops, next_test_element) 
gym_daisy_custom.control.gaits.DaisyTripod27DPolicy.get|action def get_action(self, obs, t): return self.actions[t] 
bert.tokenization.convert|unicode|to def convert_to_unicode(text): """Converts `text` to Unicode (if it's not already), assuming utf-8 input.""" if six.PY3: if isinstance(text, str): return text elif isinstance(text, bytes): return text.decode('utf-8', 'ignore') else: raise ValueError('Unsupported string type: %s' % type(text)) elif six.PY2: if isinstance(text, str): return text.decode('utf-8', 'ignore') elif isinstance(text, unicode): return text else: raise ValueError('Unsupported string type: %s' % type(text)) else: raise ValueError('Not running on Python2 or Python 3?') 
texar.utils.utils.get|class def get_class(class_name, module_paths=None): """Returns the class based on class name.  Args: class_name (str): Name or full path to the class. module_paths (list): Paths to candidate modules to search for the class. This is used if the class cannot be located solely based on `class_name`. The first module in the list that contains the class is used.  Returns: The target class.  Raises: ValueError: If class is not found based on :attr:`class_name` and :attr:`module_paths`. """ class_ = locate(class_name) if class_ is None and module_paths is not None: for module_path in module_paths: class_ = locate('.'.join([module_path, class_name])) if class_ is not None: break if class_ is None: raise ValueError('Class not found in {}: {}'.format(module_paths, class_name)) return class_ 
UGATIT.UGATIT.generate|b|a def generate_b2a(self, x_B, reuse=False): out, cam, _ = self.generator(x_B, reuse=reuse, scope='generator_A') return out, cam 
neural_tangents.utils.batch._get_jit_or_pmap_broadcast.jit_or_pmap_broadcast.f_pmapped.f def _f(_x_or_kernel_np, *_args_np): if is_input_kernel: _x_or_kernel_np = _merge_dicts(_x_or_kernel_np, x_or_kernel_other) _x_or_kernel_np = Kernel(**_x_or_kernel_np) _args_np = {i: _arg_np for i, _arg_np in zip(args_np_idxs, _args_np)} _args = _merge_dicts(_args_np, args_other) _args = tuple(v for k, v in sorted(_args.items())) return f(_x_or_kernel_np, *_args, **kwargs) 
svae_dc.utils.bo_utils.GPRegressionModel.forward def forward(self, x): mean_x = self.mean_module(x) covar_x = self.covar_module(x) output = gpytorch.distributions.MultivariateNormal(mean_x, covar_x) return output 
texar.agents.seq_pg_agent.SeqPGAgent.get|pg|loss def _get_pg_loss(self): loss_hparams = self._hparams.loss pg_loss = pg_loss_with_logits(actions=self._samples, logits=self. _logits, sequence_length=self._sequence_length, advantages=self. _qvalue_inputs, batched=True, average_across_batch=loss_hparams. average_across_batch, average_across_timesteps=loss_hparams. average_across_timesteps, sum_over_batch=loss_hparams. sum_over_batch, sum_over_timesteps=loss_hparams.sum_over_timesteps, time_major=loss_hparams.time_major) if self._hparams.entropy_weight > 0: entropy = self._get_entropy() pg_loss -= self._hparams.entropy_weight * entropy return pg_loss 
atomic_train.AtomicGenerationIteratorTrainer.batch def batch(self, opt, *args): outputs = batch.batch_atomic_generate(opt, *args) token_loss = outputs['loss'] nums = outputs['nums'] reset = outputs['reset'] return token_loss, nums, reset 
pytorch_pretrained_bert.tokenization.tokenize|whitespace def whitespace_tokenize(text): """Runs basic whitespace cleaning and splitting on a piece of text.""" text = text.strip() if not text: return [] tokens = text.split() return tokens 
official.vgg.vgg_run_loop.vgg_model_fn.batch|exclude|norm def exclude_batch_norm(name): return 'batch_normalization' not in name 
bert.modeling.BertConfig.json|to|string def to_json_string(self): """Serializes this instance to a JSON string.""" return json.dumps(self.to_dict(), indent=2, sort_keys=True) + '\n' 
misc.utils.numel def numel(x): return np.prod(var_shape(x)) 
deepctr.contrib.rnn._rnn_step.through|one|copy def _copy_one_through(output, new_output): if output.shape.ndims == 0: return new_output copy_cond = time >= sequence_length with ops.colocate_with(new_output): return array_ops.where(copy_cond, output, new_output) 
utils.training.metrics|numpy def numpy_metrics(y_pred, y_true, n_classes=11, void_labels=[11]): """ Similar to theano_metrics to metrics but instead y_pred and y_true are now numpy arrays from: https://github.com/SimJeg/FC-DenseNet/blob/master/metrics.py void label is 11 by default """ y_pred = np.argmax(y_pred, axis=1) not_void = ~np.any([(y_true == label) for label in void_labels], axis=0) I = np.zeros(n_classes) U = np.zeros(n_classes) for i in range(n_classes): y_true_i = y_true == i y_pred_i = y_pred == i I[i] = np.sum(y_true_i & y_pred_i) U[i] = np.sum((y_true_i | y_pred_i) & not_void) accuracy = np.sum(I) / np.sum(not_void) return I, U, accuracy 
facenet-master.tmp.test_invariance_on_lfw.scale|images def scale_images(images, scale, image_size): images_scale_list = [None] * images.shape[0] for i in range(images.shape[0]): images_scale_list[i] = misc.imresize(images[(i), :, :, :], scale) images_scale = np.stack(images_scale_list, axis=0) sz1 = images_scale.shape[1] / 2 sz2 = image_size / 2 images_crop = images_scale[:, sz1 - sz2:sz1 + sz2, sz1 - sz2:sz1 + sz2, :] return images_crop 
deepMOT-master.tracking_on_mot.main def main(args, sot_tracker, sst): train_test = ['/train/', '/test/'] for t_t in train_test: pth = args.data_root + args.dataset + t_t nmspth = args.dets_path + args.dataset + t_t videos = os.listdir(pth) for vname in videos: if os.path.exists(args.save_path + args.save_dir + '/' + vname + '.txt'): continue (to_refine, to_combine, DAN_th, death_count, birth_wait, loose_assignment, case1_interpolate, interpolate_flag, CMC, birth_iou) = tracking_config(vname, args.dataset) print('tracking video: ') print(vname) csv_towrite = [] track_init = [] imgs_path = pth + vname + '/img1/' imgs = sorted(os.listdir(imgs_path)) if os.path.exists(nmspth + vname + '/det/det.npy'): frames_det = np.load(nmspth + vname + '/det/det.npy', allow_pickle=True).item() else: frames_det = read_txt_detV2(nmspth + vname + '/det/det.txt') if len(frames_det.keys()) == 0: print('cannot load detections') break img_prev = None count_ids = 0 bbox_track = dict() id_track = list() states = dict() prev_frame_id = 0 birth_candidates = dict() death_candidates = dict() collect_prev_pos = dict() bbox_track[prev_frame_id] = None to_interpolate = dict() pre_warp_matrix = None w_matrix = None for frameid, im_pth in enumerate(imgs): print('frameid: ', frameid + 1) img_curr = cv2.imread(os.path.join(imgs_path, im_pth)) h, w, _ = img_curr.shape if len(states) > 0: tmp = [] im_prev_features = TrackUtil.convert_image(img_prev.copy()) if img_prev is not None and CMC: w_matrix = getWarpMatrix(img_curr, img_prev) for key, state_curr in states.items(): prev_pos = state_curr['target_pos'].copy() prev_size = state_curr['target_sz'].copy() prev_xyxy = [prev_pos[0] - 0.5 * prev_size[0], prev_pos[1] - 0.5 * prev_size[1], prev_pos[0] + 0.5 * prev_size[0], prev_pos[1] + 0.5 * prev_size[1]] if state_curr['gt_id'] not in collect_prev_pos.keys(): prev_xywh = [prev_pos[0] - 0.5 * prev_size[0], prev_pos[1] - 0.5 * prev_size[1], prev_size [0], prev_size[1]] prev_xywh = np.array([prev_xywh], dtype=np.float32) prev_xywh[:, ([0, 2])] /= float(w) prev_xywh[:, ([1, 3])] /= float(h) track_norm_center = TrackUtil.convert_detection( prev_xywh) tracks_features = sst.forward_feature_extracter( im_prev_features, track_norm_center).detach_() collect_prev_pos[state_curr['gt_id']] = [[[ frameid - 1, np.array(prev_xyxy)]], [[ frameid - 1, tracks_features.clone()]], 0, list(), list(), 'active', [0.0, -1.0, -1.0], np.zeros(4) - 1] del tracks_features elif collect_prev_pos[state_curr['gt_id']][5 ] == 'active': prev_xywh = [prev_pos[0] - 0.5 * prev_size[0], prev_pos[1] - 0.5 * prev_size[1], prev_size [0], prev_size[1]] prev_xywh = np.array([prev_xywh], dtype=np.float32) prev_xywh[:, ([0, 2])] /= float(w) prev_xywh[:, ([1, 3])] /= float(h) track_norm_center = TrackUtil.convert_detection( prev_xywh) tracks_features = sst.forward_feature_extracter( im_prev_features, track_norm_center).detach_() collect_prev_pos[state_curr['gt_id']][0].append([ frameid - 1, np.array(prev_xyxy)]) if len(collect_prev_pos[state_curr['gt_id']][0] ) > 10: collect_prev_pos[state_curr['gt_id']][0].pop(0) collect_prev_pos[state_curr['gt_id']][1].append([ frameid - 1, tracks_features.clone()]) if len(collect_prev_pos[state_curr['gt_id']][1] ) > 3: collect_prev_pos[state_curr['gt_id']][1].pop(0) del tracks_features collect_prev_pos[state_curr['gt_id']][7 ] = np.zeros(4) - 1 if len(collect_prev_pos[state_curr['gt_id']][0] ) == 10: avg_h = 0.0 avg_w = 0.0 for f, pos in collect_prev_pos[state_curr[ 'gt_id']][0]: avg_h += pos[3] - pos[1] avg_w += pos[2] - pos[0] avg_h /= len(collect_prev_pos[state_curr[ 'gt_id']][0]) avg_w /= len(collect_prev_pos[state_curr[ 'gt_id']][0]) last_t, last_pos = collect_prev_pos[ state_curr['gt_id']][0][-1] first_t, first_pos = collect_prev_pos[ state_curr['gt_id']][0][0] first_pos_center = np.array([0.5 * ( first_pos[0] + first_pos[2]), 0.5 * ( first_pos[1] + first_pos[3])]) last_pos_center = np.array([0.5 * (last_pos [0] + last_pos[2]), 0.5 * (last_pos[1] + last_pos[3])]) velocity = (last_pos_center - first_pos_center ) / (last_t - first_t) collect_prev_pos[state_curr['gt_id']][6] = [ velocity, avg_h, avg_w] collect_prev_pos[state_curr['gt_id']][0] = [ collect_prev_pos[state_curr['gt_id']][0 ][-1]] else: pass target_pos, target_sz, state_curr, _ = SiamRPN_track( state_curr, img_curr.copy(), sot_tracker, train =True, CMC=img_prev is not None and CMC, prev_xyxy=prev_xyxy, w_matrix=w_matrix) tmp.append(torch.stack([target_pos[0] - target_sz[0 ] * 0.5, target_pos[1] - target_sz[1] * 0.5, target_pos[0] + target_sz[0] * 0.5, target_pos[ 1] + target_sz[1] * 0.5], dim=0).detach_(). unsqueeze(0)) del _ del target_pos del target_sz torch.cuda.empty_cache() bbox_track[frameid] = torch.cat(tmp, dim=0).detach_() del bbox_track[prev_frame_id] del tmp torch.cuda.empty_cache() else: bbox_track[frameid] = None if str(frameid + 1) in frames_det.keys(): distance = [] if bbox_track[frameid] is not None: bboxes = bbox_track[frameid].detach().cpu().numpy( ).tolist() for bbox in bboxes: IOU = bb_fast_IOU_v1(bbox, frames_det[str( frameid + 1)]) distance.append(IOU.tolist()) distance = np.vstack(distance) if to_combine: del bboxes bbox_track[frameid] = mix_track_detV2(torch. FloatTensor(distance).cuda(), torch. FloatTensor(frames_det[str(frameid + 1)]). cuda(), bbox_track[frameid]) boxes = bbox_track[frameid].detach().cpu().numpy( ).tolist() for idx, [key, state] in enumerate(states.items()): box = boxes[idx] state['target_pos'] = np.array([0.5 * (box[ 2] + box[0]), 0.5 * (box[3] + box[1])]) state['target_sz'] = np.array([box[2] - box [0], box[3] - box[1]]) states[key] = state distance = [] bboxes = bbox_track[frameid].detach().cpu().numpy( ).tolist() for bbox in bboxes: IOU = bb_fast_IOU_v1(bbox, frames_det[str( frameid + 1)]) distance.append(IOU.tolist()) distance = np.vstack(distance) else: distance = np.array(distance) bbox_track[frameid], count_ids = tracking_birth_death( distance, bbox_track[frameid], frames_det, img_curr, id_track, count_ids, frameid, birth_candidates, track_init, death_candidates, states, sot_tracker, collect_prev_pos, sst, th=0.5, birth_iou=birth_iou, to_refine=to_refine, DAN_th=DAN_th, death_count= death_count, birth_wait=birth_wait, to_interpolate= to_interpolate, interpolate_flag=interpolate_flag, loose_assignment=loose_assignment, case1_interpolate=case1_interpolate) del distance if bbox_track[frameid] is not None: bbox_torecord = bbox_track[frameid].detach().cpu( ).numpy().tolist() for j in range(len(bbox_torecord)): if id_track[j] not in death_candidates.keys(): torecord = copy.deepcopy(bbox_torecord[j]) torecord[2] = torecord[2] - torecord[0] torecord[3] = torecord[3] - torecord[1] towrite = [str(frameid + 1)] towrite += [str(elem) for elem in [id_track [j] + 1] + torecord] towrite += ['-1', '-1', '-1', '-1'] csv_towrite.append(towrite) else: print('no detections! all tracks killed.') bbox_track[frameid] = None id_track = list() states = dict() death_candidates = dict() collect_prev_pos = dict() img_prev = img_curr.copy() prev_frame_id = frameid torch.cuda.empty_cache() for lst in track_init: for frame_id, det_id, track_id in lst: if not isinstance(det_id, np.ndarray): tmp_box = copy.deepcopy(frames_det[str(frame_id + 1 )][det_id]) else: tmp_box = det_id.tolist() tmp_box[2] = tmp_box[2] - tmp_box[0] tmp_box[3] = tmp_box[3] - tmp_box[1] towrite = [str(frame_id + 1)] towrite += [str(elem) for elem in [track_id + 1] + tmp_box] towrite += ['-1', '-1', '-1', '-1'] csv_towrite.append(towrite) if not os.path.exists(args.save_path + args.save_dir + '/'): os.makedirs(args.save_path + args.save_dir + '/') with open(args.save_path + args.save_dir + '/' + vname + '.txt', 'w', encoding='utf-8') as f: writer = csv.writer(f, delimiter=',') for row in csv_towrite: writer.writerow(row) print('tracking for {:s} is done.'.format(vname)) 
predict_test.PredictTest.test|Learning|Max|Rate @jtu.parameterized.named_parameters(jtu.cases_from_list({'testcase_name': '_train={}_network={}_logits={}_{}'.format(train, network, out_logits, name), 'train_shape': train, 'network': network, 'out_logits': out_logits, 'fn_and_kernel': fn, 'name': name} for train, network in zip(TRAIN_SHAPES, NETWORK) for out_logits in OUTPUT_LOGITS for name, fn in KERNELS.items())) def testMaxLearningRate(self, train_shape, network, out_logits, fn_and_kernel, name): key = random.PRNGKey(0) key, split = random.split(key) if len(train_shape) == 2: train_shape = train_shape[0] * 5, train_shape[1] * 10 else: train_shape = 16, 8, 8, 3 x_train = random.normal(split, train_shape) key, split = random.split(key) y_train = np.array(random.bernoulli(split, shape=(train_shape[0], out_logits)), np.float32) for lr_factor in [0.5, 3.0]: params, f, ntk = fn_and_kernel(key, train_shape[1:], network, out_logits) loss = lambda params, x: 0.5 * np.mean((f(params, x) - y_train) ** 2) grad_loss = jit(grad(loss)) g_dd = ntk(x_train, None, 'ntk') steps = 20 if name == 'theoretical': step_size = predict.max_learning_rate(g_dd, num_outputs=out_logits ) * lr_factor else: step_size = predict.max_learning_rate(g_dd, num_outputs=-1 ) * lr_factor opt_init, opt_update, get_params = optimizers.sgd(step_size) opt_state = opt_init(params)  def get_loss(opt_state): return loss(get_params(opt_state), x_train) init_loss = get_loss(opt_state) for i in range(steps): params = get_params(opt_state) opt_state = opt_update(i, grad_loss(params, x_train), opt_state) trained_loss = get_loss(opt_state) loss_ratio = trained_loss / (init_loss + 1e-12) if lr_factor == 3.0: if not math.isnan(loss_ratio): self.assertGreater(loss_ratio, 10.0) else: self.assertLess(loss_ratio, 0.1) 
csqa_dataset.data_with_graphs_and_paths.len def __len__(self): return self.n_samples 
texar.utils.average_recorder.AverageRecorder.reset def reset(self, id_or_name=None): """Resets the record.  Args: id_or_name (optional): A list or a single element. Each element is the index (if the record type is `list`) or name (if the record type is `dict`) of the field to reset. If `None`, all fields are reset. """ keys = id_or_name if keys is None: keys = list(self._recorders.keys()) elif not isinstance(keys, (list, tuple)): keys = [keys] for key in keys: self._recorders[key].reset() 
pytorch_pretrained_bert.tokenization_gpt2.GPT2Tokenizer.bpe def bpe(self, token): if token in self.cache: return self.cache[token] word = tuple(token) pairs = get_pairs(word) if not pairs: return token while True: bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float ('inf'))) if bigram not in self.bpe_ranks: break first, second = bigram new_word = [] i = 0 while i < len(word): try: j = word.index(first, i) new_word.extend(word[i:j]) i = j except: new_word.extend(word[i:]) break if word[i] == first and i < len(word) - 1 and word[i + 1 ] == second: new_word.append(first + second) i += 2 else: new_word.append(word[i]) i += 1 new_word = tuple(new_word) word = new_word if len(word) == 1: break else: pairs = get_pairs(word) word = ' '.join(word) self.cache[token] = word return word 
regression.ops.mvg_optimizer.MVGOptimizer.mu @property def mu(self): return self._mu 
texar.data.data.data_iterators.TrainTestDataIterator.to|val|data|switch def switch_to_val_data(self, sess): """Starts to iterate through val data (from the beginning).  Args: sess: The current tf session. """ if self._val_name not in self._datasets: raise ValueError('Val data not provided.') self.switch_to_dataset(sess, self._val_name) 
get_bounds_ours.get|weights|list def get_weights_list(model): weights = [] bias = [] U = model.U for i, Ui in enumerate(U): [weight_Ui, bias_Ui] = Ui.get_weights() print('Hidden layer {} weight shape: {}'.format(i, weight_Ui.shape)) weights.append(np.ascontiguousarray(np.transpose(weight_Ui))) bias.append(np.ascontiguousarray(np.transpose(bias_Ui))) print('Hidden layer {} bias shape: {}'.format(i, bias_Ui.shape)) [W, bias_W] = model.W.get_weights() weights.append(np.ascontiguousarray(np.transpose(W))) bias.append(np.ascontiguousarray(np.transpose(bias_W))) print('Last layer weight shape: {}'.format(W.shape)) print('Last layer bias shape: {}'.format(bias_W.shape)) return weights, bias 
nets.nasnet.nasnet_utils.calc|layers|reduction def calc_reduction_layers(num_cells, num_reduction_layers): """Figure out what layers should have reductions.""" reduction_layers = [] for pool_num in range(1, num_reduction_layers + 1): layer_num = float(pool_num) / (num_reduction_layers + 1) * num_cells layer_num = int(layer_num) reduction_layers.append(layer_num) return reduction_layers 
prune_mobilenet_for_cifar.PruneMobileNetForCifar.get|pruned|weights def get_pruned_weights(self, cut_channels): for name, cut_channel in cut_channels.items(): _, next_layer_name = self._get_last_and_next_block_name(name) assert next_layer_name is not None weight = self.pruned_weights_dict[name] weight = prune_channel(weight, cut_channel, cut_type='output') self.pruned_weights_dict[name] = weight cut_content = ['kernel', 'bias', 'beta', 'gamma', 'moving_mean', 'moving_variance'] name = name.rstrip(cut_content[0]) for content in cut_content[1:]: if self.pruned_weights_dict.get(name + content) is not None: weight = self.pruned_weights_dict[name + content] weight = prune_channel(weight, cut_channel, cut_type='flatten') self.pruned_weights_dict[name + content] = weight if 'conv2d' in next_layer_name: next_dw_name = next_layer_name.replace('conv2d', 'dw') next_dw_weight = self.pruned_weights_dict[next_dw_name] next_weight = self.pruned_weights_dict[next_layer_name] next_dw_weight = prune_channel(next_dw_weight, cut_channel, cut_type='input') self.pruned_weights_dict[next_dw_name] = next_dw_weight next_dw_name = next_dw_name.rstrip('kernel') for content in cut_content[1:]: if self.pruned_weights_dict.get(next_dw_name + content ) is not None: weight = self.pruned_weights_dict[next_dw_name + content] weight = prune_channel(weight, cut_channel, cut_type= 'flatten') self.pruned_weights_dict[next_dw_name + content] = weight weight = self.pruned_weights_dict[next_layer_name] weight = prune_channel(weight, cut_channel, cut_type='input') self.pruned_weights_dict[next_layer_name] = weight elif 'dense' in next_layer_name: weight = self.pruned_weights_dict[next_layer_name] weight = np.reshape(weight, [1, 1] + list(weight.shape)) weight = prune_channel(weight, cut_channel, cut_type='input') weight = np.squeeze(weight) self.pruned_weights_dict[next_layer_name] = weight else: raise ValueError('unknown layer name for prune weights: ' + next_layer_name) return self.pruned_weights_dict 
facenet-master.tmp.mnist_center_loss.main def main(argv=None): if FLAGS.self_test: print('Running self-test.') train_data, train_labels = fake_data(256) validation_data, validation_labels = fake_data(EVAL_BATCH_SIZE) test_data, test_labels = fake_data(EVAL_BATCH_SIZE) num_epochs = 1 else: train_data_filename = maybe_download('train-images-idx3-ubyte.gz') train_labels_filename = maybe_download('train-labels-idx1-ubyte.gz') test_data_filename = maybe_download('t10k-images-idx3-ubyte.gz') test_labels_filename = maybe_download('t10k-labels-idx1-ubyte.gz') train_data = extract_data(train_data_filename, 60000) train_labels = extract_labels(train_labels_filename, 60000) test_data = extract_data(test_data_filename, 10000) test_labels = extract_labels(test_labels_filename, 10000) validation_data = train_data[:VALIDATION_SIZE, (...)] validation_labels = train_labels[:VALIDATION_SIZE] train_data = train_data[VALIDATION_SIZE:, (...)] train_labels = train_labels[VALIDATION_SIZE:] num_epochs = NUM_EPOCHS train_size = train_labels.shape[0] train_data_node = tf.placeholder(data_type(), shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)) train_labels_node = tf.placeholder(tf.int64, shape=(BATCH_SIZE,)) eval_data = tf.placeholder(data_type(), shape=(EVAL_BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)) conv1_weights = tf.Variable(tf.truncated_normal([5, 5, NUM_CHANNELS, 32 ], stddev=0.1, seed=SEED, dtype=data_type())) conv1_biases = tf.Variable(tf.zeros([32], dtype=data_type())) conv2_weights = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev= 0.1, seed=SEED, dtype=data_type())) conv2_biases = tf.Variable(tf.constant(0.1, shape=[64], dtype=data_type())) fc1_weights = tf.Variable(tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512], stddev=0.1, seed=SEED, dtype=data_type())) fc1_biases = tf.Variable(tf.constant(0.1, shape=[512], dtype=data_type())) fc1p_weights = tf.Variable(tf.truncated_normal([512, 2], stddev=0.1, seed=SEED, dtype=data_type())) fc1p_biases = tf.Variable(tf.constant(0.1, shape=[2], dtype=data_type())) fc2_weights = tf.Variable(tf.truncated_normal([2, NUM_LABELS], stddev= 0.1, seed=SEED, dtype=data_type())) fc2_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS], dtype= data_type()))  def batch_norm(x, phase_train): """ Batch normalization on convolutional maps. Args: x:           Tensor, 4D BHWD input maps n_out:       integer, depth of input maps phase_train: boolean tf.Variable, true indicates training phase scope:       string, variable scope affn:      whether to affn-transform outputs Return: normed:      batch-normalized maps Ref: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177 """ name = 'batch_norm' with tf.variable_scope(name): phase_train = tf.convert_to_tensor(phase_train, dtype=tf.bool) n_out = int(x.get_shape()[-1]) beta = tf.Variable(tf.constant(0.0, shape=[n_out], dtype=x. dtype), name=name + '/beta', trainable=True, dtype=x.dtype) gamma = tf.Variable(tf.constant(1.0, shape=[n_out], dtype=x. dtype), name=name + '/gamma', trainable=True, dtype=x.dtype) batch_mean, batch_var = tf.nn.moments(x, [0], name='moments') ema = tf.train.ExponentialMovingAverage(decay=0.9)  def mean_var_with_update(): ema_apply_op = ema.apply([batch_mean, batch_var]) with tf.control_dependencies([ema_apply_op]): return tf.identity(batch_mean), tf.identity(batch_var) mean, var = control_flow_ops.cond(phase_train, mean_var_with_update, lambda : (ema.average(batch_mean), ema.average(batch_var))) normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 0.001 ) return normed  def model(data, train=False): """The Model definition.""" conv = tf.nn.conv2d(data, conv1_weights, strides=[1, 1, 1, 1], padding='SAME') relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases)) pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1 ], padding='SAME') conv = tf.nn.conv2d(pool, conv2_weights, strides=[1, 1, 1, 1], padding='SAME') relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases)) pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1 ], padding='SAME') pool_shape = pool.get_shape().as_list() reshape = tf.reshape(pool, [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]]) hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases) if train: hidden = tf.nn.dropout(hidden, 0.5, seed=SEED) hidden = tf.matmul(hidden, fc1p_weights) + fc1p_biases return tf.nn.relu(tf.matmul(hidden, fc2_weights) + fc2_biases), hidden logits, hidden = model(train_data_node, True) xent_loss = tf.reduce_mean(tf.nn. sparse_softmax_cross_entropy_with_logits(logits, train_labels_node)) beta = 0.001 center_loss, _ = facenet.center_loss(hidden, train_labels_node, 0.95, NUM_LABELS) loss = xent_loss + beta * center_loss regularizers = tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases ) + tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases) loss += 0.0005 * regularizers batch = tf.Variable(0, dtype=data_type()) learning_rate = tf.train.exponential_decay(0.01, batch * BATCH_SIZE, train_size, 0.95, staircase=True) optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(loss, global_step=batch) train_prediction = tf.nn.softmax(logits) eval_logits, eval_embeddings = model(eval_data) eval_prediction = tf.nn.softmax(eval_logits)  def eval_in_batches(data, sess): """Get all predictions for a dataset by running it in small batches.""" size = data.shape[0] if size < EVAL_BATCH_SIZE: raise ValueError('batch size for evals larger than dataset: %d' % size) predictions = np.ndarray(shape=(size, NUM_LABELS), dtype=np.float32) for begin in xrange(0, size, EVAL_BATCH_SIZE): end = begin + EVAL_BATCH_SIZE if end <= size: predictions[begin:end, :] = sess.run(eval_prediction, feed_dict={eval_data: data[begin:end, (...)]}) else: batch_predictions = sess.run(eval_prediction, feed_dict={ eval_data: data[-EVAL_BATCH_SIZE:, (...)]}) predictions[begin:, :] = batch_predictions[begin - size:, :] return predictions  def calculate_embeddings(data, sess): """Get all predictions for a dataset by running it in small batches.""" size = data.shape[0] if size < EVAL_BATCH_SIZE: raise ValueError('batch size for evals larger than dataset: %d' % size) predictions = np.ndarray(shape=(size, 2), dtype=np.float32) for begin in xrange(0, size, EVAL_BATCH_SIZE): end = begin + EVAL_BATCH_SIZE if end <= size: predictions[begin:end, :] = sess.run(eval_embeddings, feed_dict={eval_data: data[begin:end, (...)]}) else: batch_predictions = sess.run(eval_embeddings, feed_dict={ eval_data: data[-EVAL_BATCH_SIZE:, (...)]}) predictions[begin:, :] = batch_predictions[begin - size:, :] return predictions start_time = time.time() with tf.Session() as sess: tf.global_variables_initializer().run() print('Initialized!') for step in xrange(int(num_epochs * train_size) // BATCH_SIZE): offset = step * BATCH_SIZE % (train_size - BATCH_SIZE) batch_data = train_data[offset:offset + BATCH_SIZE, (...)] batch_labels = train_labels[offset:offset + BATCH_SIZE] feed_dict = {train_data_node: batch_data, train_labels_node: batch_labels} _, cl, l, lr, predictions = sess.run([optimizer, center_loss, loss, learning_rate, train_prediction], feed_dict=feed_dict) if step % EVAL_FREQUENCY == 0: elapsed_time = time.time() - start_time start_time = time.time() print('Step %d (epoch %.2f), %.1f ms' % (step, float(step) * BATCH_SIZE / train_size, 1000 * elapsed_time / EVAL_FREQUENCY)) print('Minibatch loss: %.3f  %.3f, learning rate: %.6f' % ( l, cl * beta, lr)) print('Minibatch error: %.1f%%' % error_rate(predictions, batch_labels)) print('Validation error: %.1f%%' % error_rate( eval_in_batches(validation_data, sess), validation_labels)) sys.stdout.flush() test_error = error_rate(eval_in_batches(test_data, sess), test_labels) print('Test error: %.1f%%' % test_error) if FLAGS.self_test: print('test_error', test_error) assert test_error == 0.0, 'expected 0.0 test_error, got %.2f' % ( test_error,) train_embeddings = calculate_embeddings(train_data, sess) color_list = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'b', 'g', 'r', 'c'] plt.figure(1) for n in range(0, 10): idx = np.where(train_labels[0:10000] == n) plt.plot(train_embeddings[idx, 0], train_embeddings[idx, 1], color_list[n] + '.') plt.show() 
modeling_openai_test.OpenAIGPTModelTest.OpenAIGPTModelTester.double|check|heads|output|openai def check_openai_double_heads_output(self, result): total_voc = self.n_special + self.vocab_size self.parent.assertListEqual(list(result['lm_logits'].size()), [self. batch_size, self.n_choices, self.seq_length, total_voc]) self.parent.assertListEqual(list(result['mc_logits'].size()), [self. batch_size, self.n_choices]) 
models.attacks.BasicIterativeMethod.Method|Basic|Iterative def __init__(self, model, back='tf', sess=None): """ Create a BasicIterativeMethod instance. Note: the model parameter should be an instance of the cleverhans.model.Model abstraction provided by CleverHans. """ super(BasicIterativeMethod, self).__init__(model, back, sess) self.feedable_kwargs = {'eps': np.float32, 'eps_iter': np.float32, 'y': np.float32, 'y_target': np.float32, 'clip_min': np.float32, 'clip_max': np.float32} self.structural_kwargs = ['ord', 'nb_iter'] if not isinstance(self.model, Model): self.model = CallableModelWrapper(self.model, 'probs') 
classification.train.Trainer.test|final def test_final(self): self.model.load(self.sess) loss_list = [] acc_list = [] labels_list = [] logits_list = [] for x, y in self.test_loader: feed_dict = {self.model.inputs: x, self.model.targets: y, self. model.is_training: False, self.model.n_particles: 30} loss, acc, logits_batch = self.sess.run([self.model.loss, self. model.acc, self.model.logits], feed_dict=feed_dict) loss_list.append(loss) acc_list.append(acc) labels_list.append(y) logits_list.append(torch.from_numpy(logits_batch)) avg_loss = np.mean(loss_list) avg_acc = np.mean(acc_list) ece = _ECELoss()(torch.cat(logits_list, 0), torch.cat(labels_list, 0), 'experiments/' + self.config.dataset + '/' + self.config.exp_name) print('test | loss: %5.6f | accuracy: %5.6f | ece: %5.6f\n' % (float( avg_loss), float(avg_acc), ece)) 
utils.Dataset.prepare.clean|name def clean_name(name): """Returns a shorter version of object names for cleaner display.""" return ','.join(name.split(',')[:1]) 
nets.EDSRGAN.EDSR_model.vgg|build def build_vgg(self): vgg = VGG19(weights='imagenet') vgg.outputs = [vgg.layers[9].output] img = Input(shape=self.hr_shape) img_features = vgg(img) return Model(img, img_features) 
nets.mobilenet_v1.v|mobilenet def mobilenet_v1(inputs, num_classes=1000, dropout_keep_prob=0.999, is_training=True, min_depth=8, depth_multiplier=1.0, conv_defs=None, prediction_fn=tf.contrib.layers.softmax, spatial_squeeze=True, reuse= None, scope='MobilenetV1', global_pool=False): """Mobilenet v1 model for classification.  Args: inputs: a tensor of shape [batch_size, height, width, channels]. num_classes: number of predicted classes. If 0 or None, the logits layer is omitted and the input features to the logits layer (before dropout) are returned instead. dropout_keep_prob: the percentage of activation values that are retained. is_training: whether is training or not. min_depth: Minimum depth value (number of channels) for all convolution ops. Enforced when depth_multiplier < 1, and not an active constraint when depth_multiplier >= 1. depth_multiplier: Float multiplier for the depth (number of channels) for all convolution ops. The value must be greater than zero. Typical usage will be to set this value in (0, 1) to reduce the number of parameters or computation cost of the model. conv_defs: A list of ConvDef namedtuples specifying the net architecture. prediction_fn: a function to get predictions out of logits. spatial_squeeze: if True, logits is of shape is [B, C], if false logits is of shape [B, 1, 1, C], where B is batch_size and C is number of classes. reuse: whether or not the network and its variables should be reused. To be able to reuse 'scope' must be given. scope: Optional variable_scope. global_pool: Optional boolean flag to control the avgpooling before the logits layer. If false or unset, pooling is done with a fixed window that reduces default-sized inputs to 1x1, while larger inputs lead to larger outputs. If true, any input size is pooled down to 1x1.  Returns: net: a 2D Tensor with the logits (pre-softmax activations) if num_classes is a non-zero integer, or the non-dropped-out input to the logits layer if num_classes is 0 or None. end_points: a dictionary from components of the network to the corresponding activation.  Raises: ValueError: Input rank is invalid. """ input_shape = inputs.get_shape().as_list() if len(input_shape) != 4: raise ValueError('Invalid input tensor rank, expected 4, was: %d' % len(input_shape)) with tf.variable_scope(scope, 'MobilenetV1', [inputs], reuse=reuse ) as scope: with slim.arg_scope([slim.batch_norm, slim.dropout], is_training= is_training): net, end_points = mobilenet_v1_base(inputs, scope=scope, min_depth=min_depth, depth_multiplier=depth_multiplier, conv_defs=conv_defs) with tf.variable_scope('Logits'): if global_pool: net = tf.reduce_mean(net, [1, 2], keep_dims=True, name= 'global_pool') end_points['global_pool'] = net else: kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7]) net = slim.avg_pool2d(net, kernel_size, padding='VALID', scope='AvgPool_1a') end_points['AvgPool_1a'] = net if not num_classes: return net, end_points net = slim.dropout(net, keep_prob=dropout_keep_prob, scope= 'Dropout_1b') logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, normalizer_fn=None, scope= 'Conv2d_1c_1x1') if spatial_squeeze: logits = tf.squeeze(logits, [1, 2], name='SpatialSqueeze') end_points['Logits'] = logits if prediction_fn: end_points['Predictions'] = prediction_fn(logits, scope= 'Predictions') return logits, end_points 
avod.experiments.run_training.train def train(model_config, train_config, dataset_config): dataset = DatasetBuilder.build_kitti_dataset(dataset_config, use_defaults=False) train_val_test = 'train' model_name = model_config.model_name with tf.Graph().as_default(): if model_name == 'rpn_model': model = RpnModel(model_config, train_val_test=train_val_test, dataset=dataset) elif model_name == 'avod_model': model = AvodModel(model_config, train_val_test=train_val_test, dataset=dataset) else: raise ValueError('Invalid model_name') trainer.train(model, train_config) 
generate-raddextexp-test.main def main(args): options = parser.parse_args(args) with codecs.open(options.spec, 'r', encoding='utf-8') as spec_file: spec_yaml = yaml.safe_load(spec_file) if not isinstance(spec_yaml, list): raise ValueError('expected a list of micro-kernels in the spec') tests = ( """// Copyright 2019 Google LLC // // This source code is licensed under the BSD-style license found in the // LICENSE file in the root directory of this source tree. // // Auto-generated file. Do not edit! //   Specification: {specification} //   Generator: {generator}   #include <gtest/gtest.h>  #include <xnnpack/common.h> #include <xnnpack/isa-checks.h>  #include <xnnpack/raddextexp.h> #include "raddextexp-microkernel-tester.h\" """ .format(specification=options.spec, generator=sys.argv[0])) for ukernel_spec in spec_yaml: name = ukernel_spec['name'] elements_tile, arch, isa = split_ukernel_name(name) arch = ukernel_spec.get('arch', arch) test_case = generate_test_cases(name, elements_tile, isa) tests += '\n\n' + xnncommon.postprocess_test_case(test_case, arch, isa) with codecs.open(options.output, 'w', encoding='utf-8') as output_file: output_file.write(tests) 
neural_tangents.utils.empirical.taylor_expand.taylorize|r def taylorize_r(f, params, dparams, degree, current_degree): """Recursive function to accumulate contributions to the Taylor series.""" if current_degree == degree: return f(params)  def f_jvp(p): _, val_jvp = jvp(f, (p,), (dparams,)) return val_jvp df = taylorize_r(f_jvp, params, dparams, degree, current_degree + 1) return f(params) + df / (current_degree + 1) 
classification.ops.loss_functions.NormalMeanVarianceNegativeLogProbLoss.targets @property def targets(self): return self._targets 
testing_utils.MockEnv.Env|Mock def __init__(self, state_space_size, unroll_length=1): self._state_space_size = state_space_size self._action_space = [-1, 1] self._current_state = None self._env_spec = common.EnvOutput(reward=tf.TensorSpec(shape=[ unroll_length + 1], dtype=tf.float32), done=tf.TensorSpec(shape=[ unroll_length + 1], dtype=tf.bool), observation={'f1': tf. TensorSpec(shape=[unroll_length + 1, 4, 10], dtype=tf.float32), 'f2': tf.TensorSpec(shape=[unroll_length + 1, 7, 10, 2], dtype=tf. float32)}, info=tf.TensorSpec(shape=[unroll_length + 1], dtype=tf. string)) 
classification.ops.fisher_blocks.NaiveDiagonalFB.multiply|inverse def multiply_inverse(self, vector): vector_flat = utils.tensors_to_column(vector) out_flat = vector_flat / (self._factor.get_cov() + self._damping) return utils.column_to_tensors(vector, out_flat) 
classification.ops.fisher_factors.maybe|colocate|with @contextlib.contextmanager def _maybe_colocate_with(op, colocate_cov_ops_with_inputs): """Context to colocate with `op` if `colocate_cov_ops_with_inputs`.""" if colocate_cov_ops_with_inputs: if isinstance(op, (list, tuple)): with tf_ops.colocate_with(op[0]): yield else: with tf_ops.colocate_with(op): yield else: yield 
nets.inception_v3_test.InceptionV3Test.Only|Upto|Endpoint|test|Build|Final def testBuildOnlyUptoFinalEndpoint(self): batch_size = 5 height, width = 299, 299 endpoints = ['Conv2d_1a_3x3', 'Conv2d_2a_3x3', 'Conv2d_2b_3x3', 'MaxPool_3a_3x3', 'Conv2d_3b_1x1', 'Conv2d_4a_3x3', 'MaxPool_5a_3x3', 'Mixed_5b', 'Mixed_5c', 'Mixed_5d', 'Mixed_6a', 'Mixed_6b', 'Mixed_6c', 'Mixed_6d', 'Mixed_6e', 'Mixed_7a', 'Mixed_7b', 'Mixed_7c'] for index, endpoint in enumerate(endpoints): with tf.Graph().as_default(): inputs = tf.random_uniform((batch_size, height, width, 3)) out_tensor, end_points = inception.inception_v3_base(inputs, final_endpoint=endpoint) self.assertTrue(out_tensor.op.name.startswith('InceptionV3/' + endpoint)) self.assertItemsEqual(endpoints[:index + 1], end_points.keys()) 
t_sagan_celeba.Attention.Attention def __init__(self, **kwargs): super(Attention, self).__init__(**kwargs) 
a3c.A3C.process def process(self, sess): """ process grabs a rollout that's been produced by the thread runner, and updates the parameters.  The update is then sent to the parameter server. """ sess.run(self.sync) rollout = self.pull_batch_from_queue() batch = process_rollout(rollout, gamma=GAMMA, lambda_=1.0) should_compute_summary = (self.worker_task < self.tasks and self. local_steps >= 11) if should_compute_summary: fetches = [self.summary_op, self.train_op, self.global_step] self.local_steps -= 11 else: fetches = [self.train_op, self.global_step] feed_dict = {self.local_network.x: batch.si, self.ac: batch.a, self.adv: batch.adv, self.r: batch.r, self.local_network.task: batch.task} fetched = sess.run(fetches, feed_dict=feed_dict) if should_compute_summary and self.summary_writer is not None: self.summary_writer.add_summary(tf.Summary.FromString(fetched[0]), fetched[-1]) self.summary_writer.flush() self.local_steps += self.worker_per_task 
models.triplet.Triplet.placeholders|create def _create_placeholders(self): logger.debug('--> Define Triplet placeholders') self.anchor_ids = tf.placeholder(name='anchor_ids', dtype=tf.int32, shape=[None]) self.pos_ids = tf.placeholder(name='pos_ids', dtype=tf.int32, shape=[None]) self.neg_ids = tf.placeholder(name='neg_ids', dtype=tf.int32, shape=[ None, self.n_negatives]) self.eval_ids = tf.placeholder(tf.int32, [None]) 
utils.training.get|predictions def get_predictions(output_batch): bs, c, h, w = output_batch.size() tensor = output_batch.data values, indices = tensor.cpu().max(1) indices = indices.view(bs, h, w) return indices 
tf_util.get|edge|feature def get_edge_feature(point_cloud, nn_idx, k): """Construct edge feature for each point Args: point_cloud: (batch_size, num_points, 1, num_dims) nn_idx: (batch_size, num_points, k) k: int  Returns: edge features: (batch_size, num_points, k, num_dims) """ og_batch_size = point_cloud.get_shape().as_list()[0] point_cloud = tf.squeeze(point_cloud) if og_batch_size == 1: point_cloud = tf.expand_dims(point_cloud, 0) point_cloud_central = point_cloud point_cloud_shape = point_cloud.get_shape() batch_size = point_cloud_shape[0].value num_points = point_cloud_shape[1].value num_dims = point_cloud_shape[2].value idx_ = tf.range(batch_size) * num_points idx_ = tf.reshape(idx_, [batch_size, 1, 1]) point_cloud_flat = tf.reshape(point_cloud, [-1, num_dims]) point_cloud_neighbors = tf.gather(point_cloud_flat, nn_idx + idx_) point_cloud_central = tf.expand_dims(point_cloud_central, axis=-2) point_cloud_central = tf.tile(point_cloud_central, [1, 1, k, 1]) edge_feature = tf.concat([point_cloud_central, point_cloud_neighbors - point_cloud_central], axis=-1) return edge_feature 
xlnet-master.function_builder.get|regression|loss def get_regression_loss(FLAGS, features, is_training): """Loss for downstream regression tasks.""" bsz_per_core = tf.shape(features['input_ids'])[0] inp = tf.transpose(features['input_ids'], [1, 0]) seg_id = tf.transpose(features['segment_ids'], [1, 0]) inp_mask = tf.transpose(features['input_mask'], [1, 0]) label = tf.reshape(features['label_ids'], [bsz_per_core]) xlnet_config = xlnet.XLNetConfig(json_path=FLAGS.model_config_path) run_config = xlnet.create_run_config(is_training, True, FLAGS) xlnet_model = xlnet.XLNetModel(xlnet_config=xlnet_config, run_config= run_config, input_ids=inp, seg_ids=seg_id, input_mask=inp_mask) summary = xlnet_model.get_pooled_out(FLAGS.summary_type, FLAGS. use_summ_proj) with tf.variable_scope('model', reuse=tf.AUTO_REUSE): per_example_loss, logits = modeling.regression_loss(hidden=summary, labels=label, initializer=xlnet_model.get_initializer(), scope= 'regression_{}'.format(FLAGS.task_name.lower()), return_logits=True ) total_loss = tf.reduce_mean(per_example_loss) return total_loss, per_example_loss, logits 
dataset.TOICOCODataset.load|scenes def load_scenes(self, dataset_dir, if_calculate_mean=False): """Load a subset of the ShapeNetScene dataset. dataset_dir: The root directory of the ShapeNetScene dataset. subset: What to load (train, val) if_calculate_mean: if calculate the mean color of the images in this dataset """ image_dir = os.path.join(dataset_dir, self.subset) source = 'ShapeNetTOI' num_images_before_load = len(self.image_info) folder_list = [name for name in os.listdir(image_dir) if os.path.isdir( os.path.join(image_dir, name))] num_total_folders = len(folder_list) image_ids = range(10 * num_total_folders) color_mean = np.zeros((0, 3), dtype=np.float32) for i in image_ids: image_id = int(i) % 10 folder_id = int(i) // 10 image_path = os.path.join(image_dir, '{:05d}'.format(folder_id), '{:04d}'.format(image_id)) color_path = image_path + '_color.png' if not os.path.exists(color_path): continue meta_path = os.path.join(image_dir, '{:05d}'.format(folder_id), '{:04d}_meta.txt'.format(image_id)) inst_dict = {} with open(meta_path, 'r') as f: for line in f: line_info = line.split(' ') inst_id = int(line_info[0]) cls_id = int(line_info[1]) inst_dict[inst_id] = cls_id width = self.config.IMAGE_MAX_DIM height = self.config.IMAGE_MIN_DIM self.add_image(source=source, image_id=image_id, path=image_path, width=width, height=height, inst_dict=inst_dict) if if_calculate_mean: image_file = image_path + '_color.png' image = cv2.imread(image_file).astype(np.float32) print(i) color_mean_image = np.mean(image, axis=(0, 1))[:3] color_mean_image = np.expand_dims(color_mean_image, axis=0) color_mean = np.append(color_mean, color_mean_image, axis=0) if if_calculate_mean: dataset_color_mean = np.mean(color_mean[::-1], axis=0) print('The mean color of this dataset is ', dataset_color_mean) num_images_after_load = len(self.image_info) self.source_image_ids[source] = np.arange(num_images_before_load, num_images_after_load) print('{} images are loaded into the dataset from {}.'.format( num_images_after_load - num_images_before_load, source)) 
neural_tangents.utils.batch._serial.kernel|fn def kernel_fn(x1, x2=None, *args, **kwargs): return _move_kernel_to_cpu(_kernel_fn(x1, x2, *args, **kwargs)) 
translate.beam_search.get|shape def get_shape(tensor): """Returns static shape if available and dynamic shape otherwise.""" static_shape = tensor.shape.as_list() dynamic_shape = tf.unstack(tf.shape(tensor)) dims = [(s[1] if s[0] is None else s[0]) for s in zip(static_shape, dynamic_shape)] return dims 
official.utils.logs.logger.environment|tensorflow|variables|collect def _collect_tensorflow_environment_variables(run_info): run_info['tensorflow_environment_variables'] = [{'name': k, 'value': v} for k, v in sorted(os.environ.items()) if k.startswith('TF_')] 
optimizers.get|sgd|w|optim def get_w_optim_sgd(losses, w_list, w_lr, global_w_step, args): with tf.variable_scope('w_opt'): w_optim = [] for idx, w, loss in zip(range(len(w_list)), w_list, losses.w_opt_list): w_fun = get_sgd_fun(kind=args.w_optim, name='w_optim_{}'.format (idx), lr=w_lr) min_op = w_fun.minimize(loss, var_list=[w], global_step= global_w_step) w_optim.append(min_op) return w_optim 
tf_train.run_bbans.codec|shape|from @lru_cache() def codec_from_shape(shape): print('Creating codec for shape ' + str(shape)) hps.image_size = shape[2], shape[3] z_shape = latent_shape(hps) z_size = np.prod(z_shape) graph = tf.Graph() with graph.as_default(): with tf.variable_scope('model', reuse=tf.AUTO_REUSE): x = tf.placeholder(tf.float32, shape, 'x') model = CVAE1(hps, 'eval', x) stepwise_model = LayerwiseCVAE(model) saver = tf.train.Saver(model.avg_dict) config = tf.ConfigProto(allow_soft_placement=True, intra_op_parallelism_threads=4, inter_op_parallelism_threads=4, device_count={'GPU': 0}) sess = tf.Session(config=config, graph=graph) saver.restore(sess, restore_path()) (run_all_contexts, run_top_prior, runs_down_prior, run_top_posterior, runs_down_posterior, run_reconstruction ) = stepwise_model.get_model_parts_as_numpy_functions(sess)  def vae_view(head): return ag_tuple((np.reshape(head[:z_size], z_shape), np.reshape( head[z_size:], shape))) obs_codec = lambda h, z1: codecs.Logistic_UnifBins(*run_reconstruction( h, z1), obs_precision, bin_prec=8) return codecs.substack(ResNetVAE(run_all_contexts, run_top_posterior, runs_down_posterior, run_top_prior, runs_down_prior, obs_codec, prior_precision, q_precision), vae_view) 
classification.ops.loss_functions.NormalMeanVarianceNegativeLogProbLoss.hessian|multiply def multiply_hessian(self, vector): raise NotImplementedError() 
data_util.point|cloud|shift def shift_point_cloud(batch_data, shift_range=0.1): """ Randomly shift point cloud. Shift is per point cloud. Input: BxNx3 array, original batch of point clouds Return: BxNx3 array, shifted batch of point clouds """ B, N, C = batch_data.shape shifts = np.random.uniform(-shift_range, shift_range, (B, 3)) for batch_index in range(B): batch_data[(batch_index), :, :] += shifts[(batch_index), :] return batch_data 
deepMOT-master.models.DAN.SST.get|similarity def get_similarity(self, image1, detection1, image2, detection2): feature1 = self.forward_feature_extracter(image1, detection1) feature2 = self.forward_feature_extracter(image2, detection2) return self.forward_stacker_features(feature1, feature2, False) 
plato.run_plato_rds.run @click.command() @click.option('--config', default='') @click.option('--test', default=False) def run(config, test): if test: basic_controller.run(None, True) elif config: basic_controller.run(config) else: print_usage() 
agents_test.AgentsTest.agent|input|create def _create_agent_input(self, batch_size, unroll_length): done = tf.cast(tf.random.uniform([unroll_length, batch_size], maxval=2, dtype=tf.int32), tf.bool) return tf.random.uniform([unroll_length, batch_size], maxval=2, dtype= tf.int32), utils.EnvOutput(reward=tf.random.uniform([unroll_length, batch_size]), done=done, observation=self._random_obs(batch_size, unroll_length)) 
ghost_net.GhostNet.get|logits def get_logits(self, inputs): sc = ghostnet_arg_scope(data_format=self.data_format, weight_decay=self .weight_decay, use_batch_norm=True, batch_norm_decay=0.9997, batch_norm_epsilon=0.001, regularize_depthwise=False) with slim.arg_scope(sc): with argscope(Conv2D, kernel_initializer=kernel_initializer): with argscope([Conv2D, BatchNorm], data_format=self.data_format): logits, end_points = mobilenet_v2(inputs, dw_code=self. dw_code, ratio_code=self.ratio_code, se=self.se, num_classes=self.num_classes, dropout_keep_prob=self. dropout_keep_prob, min_depth=8, depth_multiplier=self. depth_multiplier, depth=self.depth, conv_defs=None, prediction_fn=tf.contrib.layers.softmax, spatial_squeeze=True, reuse=None, scope=self.scope, global_pool=False) return logits 
models.neural_network.NeuralNetwork.get|wrt|params|gradients def get_gradients_wrt_params(self, X, Y): """Get gradients of Loss(X,Y) wrt network params""" num_params = self.num_params inp_size = X.shape[0] gradient = np.zeros((inp_size, num_params), dtype=np.float32) for i in range(inp_size): grad = self.sess.run(self.grad_loss_wrt_param, feed_dict={self. input_placeholder: X[i].reshape(self.input_shape), self. labels_placeholder: Y[i].reshape(self.label_shape), K. learning_phase(): 0}) temp = np.array([]) for j in range(len(grad)): layer_size = np.prod(grad[j].shape) temp = np.concatenate((temp, np.reshape(grad[j], layer_size)), axis=0) gradient[(i), :] = temp return gradient 
download_speech_corpus.RegExPattern.repr def __repr__(self): return '{}({})'.format(self.__class__.__name__, repr(self.expected.pattern) ) 
regression.ops.ng_optimizer.NGOptimizer.NG|Optimizer def __init__(self, shape, N, lam, alpha, beta, w_name): self.shape = shape self.N = N self.lam = lam self.alpha = alpha self.beta = beta self.w_name = w_name 
seld_dcase2019_master.metrics.evaluation_metrics.cart|sph def cart2sph(x, y, z): """ Convert cartesian to spherical coordinates  :param x: :param y: :param z: :return: azi, ele in radians and r in meters """ azimuth = np.arctan2(y, x) elevation = np.arctan2(z, np.sqrt(x ** 2 + y ** 2)) r = np.sqrt(x ** 2 + y ** 2 + z ** 2) return azimuth, elevation, r 
nets.nasnet.nasnet_test.NASNetTest.Step|Model|Cifar|test|Current def testCurrentStepCifarModel(self): batch_size = 5 height, width = 32, 32 num_classes = 10 inputs = tf.random_uniform((batch_size, height, width, 3)) global_step = tf.train.create_global_step() with slim.arg_scope(nasnet.nasnet_cifar_arg_scope()): logits, end_points = nasnet.build_nasnet_cifar(inputs, num_classes, current_step=global_step) auxlogits = end_points['AuxLogits'] predictions = end_points['Predictions'] self.assertListEqual(auxlogits.get_shape().as_list(), [batch_size, num_classes]) self.assertListEqual(logits.get_shape().as_list(), [batch_size, num_classes]) self.assertListEqual(predictions.get_shape().as_list(), [batch_size, num_classes]) 
nets.cyclegan_test.CycleganTest.four|multiple|of|width|if|helper|not|error def _error_if_width_not_multiple_of_four_helper(self, width): self.assertRaisesRegexp(ValueError, 'The input width must be a multiple of 4.', cyclegan. cyclegan_generator_resnet, tf.placeholder(tf.float32, shape=[None, 32, width, 3])) 
enas.cifar10.general_child.GeneralChild.get|HW def _get_HW(self, x): """ Args: x: tensor of shape [N, H, W, C] or [N, C, H, W] """ return x.get_shape()[2].value 
utils.get|image def get_image(path, height=256, width=256, set_mode='L'): image = imread(path, mode=set_mode) if height is not None and width is not None: image = imresize(image, [height, width], interp='nearest') return image 
vocab.VocabEntry.word|id def id2word(self, wid): return self.id2word[wid] 
pvae.tflib.imagenet64.load def load(batch_size, datapath, rs=np.random): return make_generator(os.path.join(datapath, 'imagenet64/train_64x64'), 1281149, batch_size, rs), make_generator(os.path.join(datapath, 'imagenet64/valid_64x64'), 49999, batch_size, rs) 
avod.core.evaluator_utils.strip|checkpoint|id def strip_checkpoint_id(checkpoint_dir): """Helper function to return the checkpoint index number.  Args: checkpoint_dir: Path directory of the checkpoints  Returns: checkpoint_id: An int representing the checkpoint index """ checkpoint_name = checkpoint_dir.split('/')[-1] return int(checkpoint_name.split('-')[-1]) 
texar.modules.decoders.rnn_decoder_base.RNNDecoderBase.output|dtype @property def output_dtype(self): """Types of output of one step. """ raise NotImplementedError 
dp_sgd_autoencoder.model def model(img, bs): dims = [256, 300, 100, 20, 100, 300, 256] h0 = tf.concat([img, tf.ones((bs, 1))], axis=1) w_list = [] z_list = [] h_list = [h0] h = h0 for idx in range(1, len(dims)): d_in = dims[idx - 1] d_out = dims[idx] with tf.variable_scope('fc{}'.format(idx)): w = tf.get_variable('w', (d_in + 1, d_out), initializer=tf. glorot_normal_initializer(), dtype=tf.float32) z = tf.matmul(h, w) pre_h = tf.nn.relu(z) if idx < len(dims) - 1 else z h = tf.concat([pre_h, tf.ones((bs, 1))], axis=1) if idx < len(dims ) - 1 else pre_h w_list.append(w) z_list.append(z) h_list.append(h) return w_list, z_list, h_list 
SMILESX_utils.smiles|tokenizer def smiles_tokenizer(smiles): smiles = smiles.replace('\n', '') smiles = smiles.replace(bracket[0], ' ' + bracket[0]).replace(bracket[1 ], bracket[1] + ' ') lrb_print = [smiles[ic:ic + 3] for ic, ichar in enumerate(smiles) if ichar == lrb[0]] if len(lrb_print) != 0: for ichar in lrb_print: smiles = smiles.replace(ichar, ' ' + ichar + ' ') smiles = smiles.split(' ') splitted_smiles = list() for ifrag in smiles: ifrag_tag = False for inac in (bracket + lrb): if inac in ifrag: ifrag_tag = True break if ifrag_tag == False: for iaa in aliphatic_organic[7:9]: ifrag = ifrag.replace(iaa, ' ' + iaa + ' ') ifrag_tmp = ifrag.split(' ') for iifrag_tmp in ifrag_tmp: if iifrag_tmp != aliphatic_organic[7 ] and iifrag_tmp != aliphatic_organic[8]: splitted_smiles.extend(iifrag_tmp) else: splitted_smiles.extend([iifrag_tmp]) else: splitted_smiles.extend([ifrag]) return terminator + splitted_smiles + terminator 
lanenet_data_processor.DataSet.len def __len__(self): return self._len 
utils.reformat def reformat(images, labels, num_classes=10): """Convert labels to one-hot Encoding and image matrix to favourable dimensions  Arguments: images {np float} -- images size [channel, dim, dim, batch] labels {np int} -- labels size [batch, 1]  Keyword Arguments: num_classes {int} -- number of classes (default: {10})  Returns: tuples of np float -- images, labels with dimensions [batch, dim, dim, channel], [batch, num_classes] """ images = images.transpose([3, 0, 1, 2]) batch_size = labels.size onehot_labels = np.zeros((batch_size, num_classes)) onehot_labels[np.arange(batch_size), labels.squeeze() % num_classes] = 1 return images, onehot_labels 
pytorch_pretrained_bert.modeling.swish def swish(x): return x * torch.sigmoid(x) 
resnet18.ResNet18.weights|restore def restore_weights(self, scope, layer_type, weights_dict): """ prefix: scope[9:] layer_type: conv, bn, dense """ if layer_type == 'conv' or layer_type == 'dense': prefix = '/conv2d' if layer_type == 'conv' else '/dense' sk_prefix = '/conv2d' if layer_type == 'conv' else '' saved_kernel = weights_dict.get(scope.name[9:] + sk_prefix + '/kernel') if saved_kernel is not None: weight = tf.get_default_graph().get_tensor_by_name(scope.name + prefix + '/kernel:0') weight = tf.assign(weight, saved_kernel) tf.add_to_collection('init', weight) if layer_type == 'dense': saved_bias = weights_dict.get(scope.name[9:] + '/bias') if saved_bias is not None: bias = tf.get_default_graph().get_tensor_by_name(scope.name + prefix + '/bias:0') bias = tf.assign(bias, saved_bias) tf.add_to_collection('init', bias) elif layer_type == 'bn': saved_beta = weights_dict.get(scope.name[9:] + '/conv2d/beta') saved_gamma = weights_dict.get(scope.name[9:] + '/conv2d/gamma') saved_moving_mean = weights_dict.get(scope.name[9:] + '/conv2d/moving_mean') saved_moving_variance = weights_dict.get(scope.name[9:] + '/conv2d/moving_variance') if saved_beta is not None: beta = tf.get_default_graph().get_tensor_by_name(scope.name + '/batch_normalization/beta:0') gamma = tf.get_default_graph().get_tensor_by_name(scope.name + '/batch_normalization/gamma:0') moving_mean = tf.get_default_graph().get_tensor_by_name(scope. name + '/batch_normalization/moving_mean:0') moving_variance = tf.get_default_graph().get_tensor_by_name( scope.name + '/batch_normalization/moving_variance:0') beta = tf.assign(beta, saved_beta) gamma = tf.assign(gamma, saved_gamma) moving_mean = tf.assign(moving_mean, saved_moving_mean) moving_variance = tf.assign(moving_variance, saved_moving_variance) tf.add_to_collection('init', beta) tf.add_to_collection('init', gamma) tf.add_to_collection('init', moving_mean) tf.add_to_collection('init', moving_variance) else: raise ValueError('unknown layer type') return 
embedding.Embedding.frequent|most def most_frequent(self, k): nvocabulary = self.words[:k] nvectors = np.asarray([self.word_to_vector(w) for w in nvocabulary]) return Embedding(vectors=nvectors, vocabulary=Vocabulary(nvocabulary, False, False)) 
build_imagenet_data.dataset|process def _process_dataset(name, directory, num_shards, synset_to_human, image_to_bboxes): """Process a complete data set and save it as a TFRecord.  Args: name: string, unique identifier specifying the data set. directory: string, root path to the data set. num_shards: integer number of shards for this data set. synset_to_human: dict of synset to human labels, e.g., 'n02119022' --> 'red fox, Vulpes vulpes' image_to_bboxes: dictionary mapping image file names to a list of bounding boxes. This list contains 0+ bounding boxes. """ filenames, synsets, labels = _find_image_files(directory, FLAGS.labels_file ) humans = _find_human_readable_labels(synsets, synset_to_human) bboxes = _find_image_bounding_boxes(filenames, image_to_bboxes) _process_image_files(name, filenames, synsets, labels, humans, bboxes, num_shards) 
models.attacks.vatm def vatm(model, x, logits, eps, back='tf', num_iterations=1, xi=1e-06, clip_min=None, clip_max=None): """ A wrapper for the perturbation methods used for virtual adversarial training : https://arxiv.org/abs/1507.00677 It calls the right function, depending on the user's backend. :param model: the model which returns the network unnormalized logits :param x: the input placeholder :param logits: the model's unnormalized output tensor :param eps: the epsilon (input variation parameter) :param num_iterations: the number of iterations :param xi: the finite difference parameter :param clip_min: optional parameter that can be used to set a minimum value for components of the example returned :param clip_max: optional parameter that can be used to set a maximum value for components of the example returned :return: a tensor for the adversarial example  """ assert back == 'tf' from .attacks_tf import vatm as vatm_tf return vatm_tf(model, x, logits, eps, num_iterations=num_iterations, xi =xi, clip_min=clip_min, clip_max=clip_max) 
m_phate.train.BatchTraceHistory.Trace|Batch|History def __init__(self, data, model, save_weights=False, *args, **kwargs): self.trace_data = data self.trace_model = model self.save_weights = save_weights if save_weights: self.weights = [] self.trace = [] super().__init__(*args, **kwargs) 
texar.modules.networks.conv_networks_test.Conv1DNetworkTest.length|test|seq|unknown def test_unknown_seq_length(self): """Tests use of pooling layer when the seq_length dimension of inputs is `None`. """ network_1 = Conv1DNetwork() inputs_1 = tf.placeholder(tf.float32, [64, None, 300]) outputs_1 = network_1(inputs_1) self.assertEqual(outputs_1.shape, [64, 128]) hparams = {'num_conv_layers': 2, 'filters': 128, 'kernel_size': [[3, 4, 5], 4], 'pooling': 'AveragePooling', 'pool_size': [2, None], 'num_dense_layers': 1, 'dense_size': 10} network = Conv1DNetwork(hparams) self.assertEqual(len(network.layers), 1 + 1 + 1 + 1 + 1 + 1) self.assertTrue(isinstance(network.layer_by_name('pool_2'), tx.core. AverageReducePooling1D)) inputs = tf.placeholder(tf.float32, [64, None, 300]) outputs = network(inputs) self.assertEqual(outputs.shape, [64, 10]) hparams_2 = {'num_conv_layers': 1, 'filters': 128, 'kernel_size': 4, 'other_conv_kwargs': {'data_format': 'channels_first'}, 'pooling': 'MaxPooling', 'other_pool_kwargs': {'data_format': 'channels_first' }, 'num_dense_layers': 1, 'dense_size': 10} network_2 = Conv1DNetwork(hparams_2) inputs_2 = tf.placeholder(tf.float32, [64, 300, None]) outputs_2 = network_2(inputs_2) self.assertEqual(outputs_2.shape, [64, 10]) 
AffineCouplingCondXY.AffineCouplingCondXY.det|jacobian|log|forward def _forward_log_det_jacobian(self, x, yy, nlf0=None, nlf1=None, iso=None, cam=None): if self._last_layer: x = tf.reshape(x, (-1, self.i0, self.i1, self.ic)) yy = tf.reshape(yy, (-1, self.i0, self.i1, self.ic)) x0 = x[:, :, :, :self.ic // 2] x0yy = tf.concat([x0, yy], axis=-1) _, log_scale = self._shift_and_log_scale_fn(x0yy) log_scale = self.scale * tf.tanh(log_scale) if log_scale is None: return tf.constant(0.0, dtype=x.dtype, name='fldj') return -tf.reduce_sum(log_scale, axis=[1, 2, 3]) 
helper.show|Class|Table def showClassTable(number_of_list, title='Number of samples'): import pandas as pd print('\n+------------Show Table---------------+') lenth = len(number_of_list) column1 = range(1, lenth + 1) table = {'Class#': column1, title: number_of_list} table_df = pd.DataFrame(table).to_string(index=False) print(table_df) print('+-----------Close Table---------------+') 
models.attacks_tf.CarliniWagnerL2.batch|attack def attack_batch(self, imgs, labs, mask=None): """ Run the attack on a batch of instance and labels. Mask is an argument that allows modifying only a subset of the pixels, and is used by the CW L0 attack algorithm.    """  def compare(x, y): if not isinstance(x, (float, int, np.int64)): x = np.copy(x) if self.TARGETED: x[y] -= self.CONFIDENCE else: x[y] += self.CONFIDENCE x = np.argmax(x) if self.TARGETED: return x == y else: return x != y batch_size = self.batch_size if mask is None: mask = np.ones(imgs.shape, dtype=imgs.dtype) oimgs = np.clip(imgs, self.clip_min, self.clip_max) imgs = (imgs - self.clip_min) / (self.clip_max - self.clip_min) imgs = np.clip(imgs, 0, 1) imgs = imgs * 2 - 1 imgs = np.arctanh(imgs * 0.999999) lower_bound = np.zeros(batch_size) CONST = np.ones(batch_size) * self.initial_const upper_bound = np.ones(batch_size) * 10000000000.0 o_bestl2 = [10000000000.0] * batch_size o_bestscore = [-1] * batch_size o_bestattack = np.copy(oimgs) for outer_step in range(self.BINARY_SEARCH_STEPS): self.sess.run(self.init) batch = imgs[:batch_size] batchlab = labs[:batch_size] bestl2 = [10000000000.0] * batch_size bestscore = [-1] * batch_size _logger.debug('  Binary search step {} of {}'.format(outer_step, self.BINARY_SEARCH_STEPS)) if self.repeat and outer_step == self.BINARY_SEARCH_STEPS - 1: CONST = upper_bound self.sess.run(self.setup, {self.assign_timg: batch, self. assign_mask: mask, self.assign_simg: batch, self.assign_tlab: batchlab, self.assign_const: CONST}) prev = 1000000.0 for iteration in range(self.MAX_ITERATIONS): _, l, l2s, scores, nimg = self.sess.run([self.train, self.loss, self.l2dist, self.output, self.newimg]) if iteration % (self.MAX_ITERATIONS // 10 or 1) == 0: _logger.debug(('    Iteration {} of {}: loss={:.3g} ' + 'l2={:.3g} f={:.3g}').format(iteration, self. MAX_ITERATIONS, l, np.mean(l2s), np.mean(scores))) if self.ABORT_EARLY and iteration % (self.MAX_ITERATIONS // 10 or 1 ) == 0: if l > prev * 0.9999: msg = '    Failed to make progress; stop early' _logger.debug(msg) break prev = l for e, (l2, sc, ii) in enumerate(zip(l2s, scores, nimg)): lab = np.argmax(batchlab[e]) if l2 < bestl2[e] and compare(sc, lab): bestl2[e] = l2 bestscore[e] = np.argmax(sc) if l2 < o_bestl2[e] and compare(sc, lab): o_bestl2[e] = l2 o_bestscore[e] = np.argmax(sc) o_bestattack[e] = ii for e in range(batch_size): if compare(bestscore[e], np.argmax(batchlab[e])) and bestscore[e ] != -1: upper_bound[e] = min(upper_bound[e], CONST[e]) if upper_bound[e] < 1000000000.0: CONST[e] = (lower_bound[e] + upper_bound[e]) / 2 else: lower_bound[e] = max(lower_bound[e], CONST[e]) if upper_bound[e] < 1000000000.0: CONST[e] = (lower_bound[e] + upper_bound[e]) / 2 else: CONST[e] *= 10 _logger.debug('  Successfully generated adversarial examples ' + 'on {} of {} instances.'.format(sum(upper_bound < 1000000000.0), batch_size)) o_bestl2 = np.array(o_bestl2) mean = np.mean(np.sqrt(o_bestl2[o_bestl2 < 1000000000.0])) _logger.debug('   Mean successful distortion: {:.4g}'.format(mean)) o_bestl2 = np.array(o_bestl2) if self.extension == 0: if o_bestscore[0] == -1: return None gradients = self.sess.run(self.outgrad) return gradients, o_bestscore, o_bestattack return o_bestattack 
dump_embedding.get|Embedding def getEmbedding(sess, dh, batch_size, ss, sense_mat, selected_indices, scws=False): if scws == True: testData = dh.scws_testData testLocation = dh.scws_testLocation else: testData = dh.testData testLocation = dh.testLocation senseVec = [[] for i in range(len(testData))] senseScore = [[] for i in range(len(testData))] currentIndex = 0 while currentIndex < len(testData): c_indices = [] while len(c_indices) != batch_size and currentIndex != len(testData): c_indices.append(testData[currentIndex]) currentIndex += 1 if len(c_indices) != batch_size and currentIndex == len(testData): while len(c_indices) != batch_size: c_indices.append(c_indices[-1]) currentIndex += 1 c_indices += [c_indices[-1] for i in range(context_window * 2)] sense_probability = sess.run(ss.sense_prob, feed_dict={ss. context_indices: c_indices, ss.eval_mode: True, ss.bi_info: tf. SparseTensorValue([[0, 0]], [0], [1, 1]), ss.lengths: [(0) for _ in range(context_window * 2 + batch_size)], ss.major_weight: 0.0} ) for k in range(sense_dim): selected_s_input_indices = [] for i in range(batch_size): if currentIndex - batch_size + i < len(testData): wordIndex = testData[currentIndex - batch_size + i][ testLocation[currentIndex - batch_size + i]] selected_s_input_indices.append(wordIndex * sense_dim + k) else: selected_s_input_indices.append(0) embedded_sense = sess.run(sense_mat, feed_dict={ selected_indices: selected_s_input_indices}) for i in range(batch_size): index = currentIndex - batch_size + i if index < len(testData): senseVec[index].append(embedded_sense[i]) selected_w_input_indices = [] for i in range(batch_size): if currentIndex - batch_size + i < len(testData): wordIndex = testData[currentIndex - batch_size + i][ testLocation[currentIndex - batch_size + i]] selected_w_input_indices.append(wordIndex) else: selected_w_input_indices.append(0) for i in range(batch_size): index = currentIndex - batch_size + i if index < len(testData): senseScore[index] = sense_probability[i] return np.asarray(senseVec), np.asarray(senseScore) 
SRGANs-Spectral-Regularization-GANs--master.evaluation.sample_generate.image|make @chainer.training.make_extension() def make_image(trainer): np.random.seed(seed) n_images = rows * cols x = gen_images(gen, n_images, batchsize=n_images) _, _, h, w = x.shape x = x.reshape((rows, cols, 3, h, w)) x = x.transpose(0, 3, 1, 4, 2) x = x.reshape((rows * h, cols * w, 3)) preview_dir = '{}/preview'.format(dst) preview_path = preview_dir + '/image{:0>8}.png'.format(trainer.updater. iteration) if not os.path.exists(preview_dir): os.makedirs(preview_dir) Image.fromarray(x).save(preview_path) 
gan.metric.DSpritesInceptionScore.build def build(self): self.build_connection() if self.do_training: self.build_label() self.build_loss() self.build_summary() 
ctr_funcs.wo|input|label|tf|pipeline def tf_input_pipeline_wo_label(file_names, batch_size, num_epochs=1, record_defaults=record_defaults): file_name_queue = tf.train.string_input_producer(file_names, num_epochs =num_epochs, shuffle=True) feature = tf_read_data_wo_label(file_name_queue, record_defaults) min_after_dequeue = 5000 capacity = min_after_dequeue + 3 * batch_size feature_batch = tf.train.shuffle_batch([feature], batch_size=batch_size, capacity=capacity, min_after_dequeue=min_after_dequeue) return feature_batch 
gym_pycolab.engine.Engine.characters|bad|if|value|error|are def _value_error_if_characters_are_bad(self, characters, mandatory_len=None): if mandatory_len is not None and len(characters) != mandatory_len: raise ValueError( '{}, a string of length {}, was used where a string of length {} was required' .format(repr(characters), len(characters), mandatory_len)) for char in characters: try: ord(char) except TypeError: raise ValueError('Character {} is not an ASCII character'. format(char)) 
UGATIT.UGATIT.panalty|gradient def gradient_panalty(self, real, fake, scope='discriminator_A'): if self.gan_type.__contains__('dragan'): eps = tf.random_uniform(shape=tf.shape(real), minval=0.0, maxval=1.0) _, x_var = tf.nn.moments(real, axes=[0, 1, 2, 3]) x_std = tf.sqrt(x_var) fake = real + 0.5 * x_std * eps alpha = tf.random_uniform(shape=[self.batch_size, 1, 1, 1], minval=0.0, maxval=1.0) interpolated = real + alpha * (fake - real) logit, cam_logit, _, _ = self.discriminator(interpolated, reuse=True, scope=scope) GP = [] cam_GP = [] for i in range(2): grad = tf.gradients(logit[i], interpolated)[0] grad_norm = tf.norm(flatten(grad), axis=1) if self.gan_type == 'wgan-lp': GP.append(self.ld * tf.reduce_mean(tf.square(tf.maximum(0.0, grad_norm - 1.0)))) elif self.gan_type == 'wgan-gp' or self.gan_type == 'dragan': GP.append(self.ld * tf.reduce_mean(tf.square(grad_norm - 1.0))) for i in range(2): grad = tf.gradients(cam_logit[i], interpolated)[0] grad_norm = tf.norm(flatten(grad), axis=1) if self.gan_type == 'wgan-lp': cam_GP.append(self.ld * tf.reduce_mean(tf.square(tf.maximum(0.0, grad_norm - 1.0)))) elif self.gan_type == 'wgan-gp' or self.gan_type == 'dragan': cam_GP.append(self.ld * tf.reduce_mean(tf.square(grad_norm - 1.0))) return sum(GP), sum(cam_GP) 
classification.ops.layer_collection.LayerCollection.total|loss def total_loss(self): return math_ops.add_n(tuple(loss.evaluate() for loss in self.losses)) 
instruction_encoder.InstructionEncoder.get|embeddings def get_embeddings(self, ids): """Compute word embeddings.  Args: ids: A tensor of type int64.  Returns: A tensor of type int64 and the shape should be [ids.shape, embedding_dim]. """ return self._word_embeddings(ids) 
graphsage.aggregators.MeanPoolingAggregator.call def _call(self, inputs): self_vecs, neigh_vecs = inputs neigh_h = neigh_vecs dims = tf.shape(neigh_h) batch_size = dims[0] num_neighbors = dims[1] h_reshaped = tf.reshape(neigh_h, (batch_size * num_neighbors, self. neigh_input_dim)) for l in self.mlp_layers: h_reshaped = l(h_reshaped) neigh_h = tf.reshape(h_reshaped, (batch_size, num_neighbors, self. hidden_dim)) neigh_h = tf.reduce_mean(neigh_h, axis=1) from_neighs = tf.matmul(neigh_h, self.vars['neigh_weights']) from_self = tf.matmul(self_vecs, self.vars['self_weights']) if not self.concat: output = tf.add_n([from_self, from_neighs]) else: output = tf.concat([from_self, from_neighs], axis=1) if self.bias: output += self.vars['bias'] return self.act(output) 
deepctr.layers.interaction.CrossNet.shape|output|compute def compute_output_shape(self, input_shape): return input_shape 
train_bnn.main def main(): parser = argparse.ArgumentParser() parser.add_argument('--batchSz', type=int, default=64) parser.add_argument('--nEpochs', type=int, default=100) parser.add_argument('--no-cuda', action='store_true') parser.add_argument('--save') parser.add_argument('--run', type=str, default='bnn_dbsn_0') parser.add_argument('--seed', type=int, default=1) parser.add_argument('--dataset', type=str, default='cifar10') parser.add_argument('--opt', type=str, default='sgd', choices=('sgd', 'adam', 'rmsprop')) parser.add_argument('--net_learning_rate', type=float, default=0.1, help='learning rate for net') parser.add_argument('--net_weight_decay', type=float, default=0.0, help ='wd for arch encoding') parser.add_argument('--arch_learning_rate', type=float, default=0.0003, help='learning rate for arch encoding') parser.add_argument('--init_type', type=str, default='origin') parser.add_argument('--grad_clip', type=float, default=5.0, help= 'gradient clipping') parser.add_argument('--nesterov', action='store_true', default=False, help='use nesterov for net') parser.add_argument('--ncells', type=int, default=4) parser.add_argument('--nlayers', type=int, default=7) parser.add_argument('--beta1', type=float, default=0.5, help='beta1') parser.add_argument('--drop_rate', type=float, default=0.0, help= 'drop_rate') parser.add_argument('--droppath_rate', type=float, default=0.0, help= 'droppath_rate') parser.add_argument('--after_norm_type', type=str, default='bn') parser.add_argument('--tau0', type=float, default=3.0, help='tau0') parser.add_argument('--tau_min', type=float, default=1.0, help='tau_min') parser.add_argument('--tau_anneal_rate', type=float, default=1.5e-05, help='tau_anneal_rate') parser.add_argument('--ps', action='store_true', default=False, help= 'point estimation') parser.add_argument('--prior_sigma', type=float, default=0.1, help= 'prior_sigma') parser.add_argument('--restore_arch', action='store_true', default= False, help='restore_arch') parser.add_argument('--w_kl_weight', type=float, default=0.001, help= 'w_kl_weight') args = parser.parse_args() args.cuda = not args.no_cuda and torch.cuda.is_available() args.save = '../work/run{}'.format(args.run) create_exp_dir(args.save, scripts_to_save=glob.glob('*.py')) log_format = '%(asctime)s %(message)s' logging.basicConfig(stream=sys.stdout, level=logging.INFO, format= log_format, datefmt='%m/%d %I:%M:%S %p') fh = logging.FileHandler(os.path.join(args.save, 'log.txt'), mode='w+') fh.setFormatter(logging.Formatter(log_format)) logging.getLogger().addHandler(fh) logging.info(str(args)) np.random.seed(args.seed) torch.cuda.set_device(0) cudnn.benchmark = True torch.manual_seed(args.seed) cudnn.enabled = True torch.cuda.manual_seed(args.seed) ngpus = int(torch.cuda.device_count()) logging.info('# of GPUs: ' + str(ngpus)) if args.dataset == 'cifar10': normMean = [0.49139968, 0.48215827, 0.44653124] normStd = [0.24703233, 0.24348505, 0.26158768] normTransform = transforms.Normalize(normMean, normStd) nClasses = 10 dataset_all = dset.CIFAR10 elif args.dataset == 'cifar100': normMean = [0.5071, 0.4867, 0.4408] normStd = [0.2675, 0.2565, 0.2761] normTransform = transforms.Normalize(normMean, normStd) nClasses = 100 dataset_all = dset.CIFAR100 trainTransform = transforms.Compose([transforms.RandomCrop(32, padding= 4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normTransform]) testTransform = transforms.Compose([transforms.ToTensor(), normTransform]) trainLoader = DataLoader(dataset_all(root='../cifar', train=True, download=True, transform=trainTransform), batch_size=args.batchSz, shuffle=True, pin_memory=True, num_workers=2, drop_last=True) trainLoader_arch = None testLoader = DataLoader(dataset_all(root='../cifar', train=False, download=True, transform=testTransform), batch_size=args.batchSz, shuffle=False, pin_memory=True, num_workers=2) num_batch = len(trainLoader) logging.info(str(num_batch)) start_epoch = 1 net = stochastic_nn.StochasticNet(growthRate=16, reduction=0.4, nClasses=nClasses, args=args).cuda() restore_alpha_path = ('../work/runadags_lr3_decayto0.5_0/alphas100.pth' if args.dataset == 'cifar10' else '../work/runadags_lr3_d205_cifar100_0/alphas100.pth') alphas = torch.load(restore_alpha_path) if args.restore_arch else Variable( 0.001 * torch.randn(int((args.nlayers - 1) * args.nlayers / 2), 4). cuda(), requires_grad=True) betas = Variable(torch.ones(int((args.nlayers - 1) * args.nlayers / 2), 1).cuda() * math.log(1.0), requires_grad=True) logging.info('  + Number of params: {}'.format(sum([p.data.nelement() for p in net.parameters()]))) if args.opt == 'sgd': optimizer = optim.SGD(net.parameters(), lr=args.net_learning_rate, nesterov=args.nesterov, momentum=0.9, weight_decay=args. net_weight_decay) elif args.opt == 'adam': optimizer = optim.Adam(net.parameters(), weight_decay=0.0001) elif args.opt == 'rmsprop': optimizer = optim.RMSprop(net.parameters(), weight_decay=0.0001) if args.ps: biOptimizer = PSOptimizer(net, args, num_batch, alphas, betas, ngpus) else: biOptimizer = GumbelSoftmaxMOptimizer(net, args, num_batch, alphas, betas, ngpus) for epoch in range(start_epoch, args.nEpochs + 1): adjust_opt(args.opt, optimizer, biOptimizer, epoch, args. arch_learning_rate, args.nEpochs) train_arch(args, epoch, net, trainLoader, trainLoader_arch, optimizer, biOptimizer, ngpus) if epoch % 10 == 1: print(torch.cat([F.softmax(alphas, 1), betas.exp()], 1).data. cpu().numpy()) if epoch % 20 == 0: test(args, epoch, num_batch, net, testLoader, biOptimizer, alphas, betas, ngpus) torch.save(net, os.path.join(args.save, 'epoch{}.pth'.format( epoch))) torch.save(alphas, os.path.join(args.save, 'alphas{}.pth'. format(epoch))) torch.save(betas, os.path.join(args.save, 'betas{}.pth'.format( epoch))) 
commons.measure.PadRotateProjectDevice.np|unmeasure def unmeasure_np(self, hparams, x_measured_val, theta_val): raise NotImplementedError 
SMILESX_utils.get|tokens def get_tokens(smiles_array, split_l=1): tokenized_smiles_list = list() for ismiles in smiles_array.tolist(): tokenized_smiles_tmp = smiles_tokenizer(ismiles) tokenized_smiles_list.append([''.join(tokenized_smiles_tmp[i:i + split_l]) for i in range(0, len(tokenized_smiles_tmp) - split_l + 1, 1)]) return tokenized_smiles_list 
gen.epoch|oneshot|core|sparse|plot|resnet|for def plot_resnet56_sparse_oneshot_epoch_for_epoch_core(): common.epoch_for_epoch_plot(network=common.RESNET56, is_iterative=False, prune_method=common.UNSTRUCTURED, min_max_y=(-0.03, 0.01), to_ignore=['lr_lottery', 'reinit'], nbins=5, nybins=4) 
instruction_encoder_test.InstructionEncoderTest.ndh|call|test def test_call_ndh(self): encoder = instruction_encoder.InstructionEncoder(num_hidden_layers=2, output_dim=256, pretrained_embed_path=None, oov_bucket_size=1, vocab_size=1082, word_embed_dim=300) tokens = tf.constant([[3, 4, 5, 1, 6, 0]]) result = encoder(tokens) self.assertEqual(result[0].shape, [1, 6, 512]) self.assertEqual(result[1][0][0].shape, [1, 512]) self.assertEqual(result[1][0][1].shape, [1, 512]) self.assertEqual(result[1][1][0].shape, [1, 512]) self.assertEqual(result[1][1][1].shape, [1, 512]) 
acwgangp.ACWGANGP.discriminator def discriminator(self, x, is_training=True, reuse=False): with tf.variable_scope('discriminator', reuse=reuse): net = lrelu(conv2d(x, 64, 4, 4, 2, 2, name='d_conv1')) net = lrelu(bn(conv2d(net, 128, 4, 4, 2, 2, name='d_conv2'), is_training=is_training, scope='d_bn2')) net = tf.reshape(net, [self.batch_size, -1]) net = lrelu(bn(linear(net, 1024, scope='d_fc3'), is_training= is_training, scope='d_bn3')) out_logit = linear(net, 1, scope='d_fc4') out = tf.nn.sigmoid(out_logit) return out, out_logit, net 
neural_tangents.stax._set_covariances_req_attr.get|element|maximal def _get_maximal_element(covs_req, comparison_op): for f in kernel_fns: if hasattr(f, _COVARIANCES_REQ): marginal = getattr(f, _COVARIANCES_REQ)['marginal'] cross = getattr(f, _COVARIANCES_REQ)['cross'] if comparison_op(marginal, covs_req['marginal']): covs_req['marginal'] = marginal if comparison_op(cross, covs_req['cross']): covs_req['cross'] = cross return covs_req 
empirical_test.EmpiricalTest.testTaylorExpansion.exact|f def f_2_exact(x0, x, params): w1, w2, b = params dx = x - x0 return f_lin_exact(x0, x, params) + 0.5 * np.dot(np.dot(dx.T, w1), dx) 
nets.resnet_utils.arg|scope|resnet def resnet_arg_scope(weight_decay=0.0001, batch_norm_decay=0.997, batch_norm_epsilon=1e-05, batch_norm_scale=True, activation_fn=tf.nn. relu, use_batch_norm=True, batch_norm_updates_collections=tf.GraphKeys. UPDATE_OPS): """Defines the default ResNet arg scope.  TODO(gpapan): The batch-normalization related default values above are appropriate for use in conjunction with the reference ResNet models released at https://github.com/KaimingHe/deep-residual-networks. When training ResNets from scratch, they might need to be tuned.  Args: weight_decay: The weight decay to use for regularizing the model. batch_norm_decay: The moving average decay when estimating layer activation statistics in batch normalization. batch_norm_epsilon: Small constant to prevent division by zero when normalizing activations by their variance in batch normalization. batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the activations in the batch normalization layer. activation_fn: The activation function which is used in ResNet. use_batch_norm: Whether or not to use batch normalization. batch_norm_updates_collections: Collection for the update ops for batch norm.  Returns: An `arg_scope` to use for the resnet models. """ batch_norm_params = {'decay': batch_norm_decay, 'epsilon': batch_norm_epsilon, 'scale': batch_norm_scale, 'updates_collections': batch_norm_updates_collections, 'fused': None} with slim.arg_scope([slim.conv2d], weights_regularizer=slim. l2_regularizer(weight_decay), weights_initializer=slim. variance_scaling_initializer(), activation_fn=activation_fn, normalizer_fn=slim.batch_norm if use_batch_norm else None, normalizer_params=batch_norm_params): with slim.arg_scope([slim.batch_norm], **batch_norm_params): with slim.arg_scope([slim.max_pool2d], padding='SAME') as arg_sc: return arg_sc 
sampler.BeamSampler.batch|make def make_batch(self, X): X = np.array(X) assert X.ndim in [1, 2] if X.ndim == 1: X = np.expand_dims(X, axis=0) pos_enc = np.arange(n_vocab + n_special, n_vocab + n_special + X.shape[-1]) pos_enc = np.expand_dims(pos_enc, axis=0) batch = np.stack([X, pos_enc], axis=-1) batch = torch.tensor(batch, dtype=torch.long).to(device) return batch 
fasterai.generators.learner|deep|gen def gen_learner_deep(data: ImageDataBunch, gen_loss=FeatureLoss(), arch= models.resnet34, nf_factor: float=1.5) ->Learner: return unet_learner_deep(data, arch, wd=0.001, blur=True, norm_type= NormType.Spectral, self_attention=True, y_range=(-3.0, 3.0), loss_func=gen_loss, nf_factor=nf_factor) 
pytorch_pretrained_bert.modeling_openai.swish def swish(x): return x * torch.sigmoid(x) 
plato.agent.component.dialogue_policy.deep_learning.supervised_policy.SupervisedPolicy.encode|state def encode_state(self, state): """ Encodes the dialogue state into a vector.  :param state: the state to encode :return: int - a unique state encoding """ temp = [int(state.is_terminal_state)] temp.append(1) if state.system_made_offer else temp.append(0) if self.agent_role == 'user': if state.user_goal: for c in self.informable_slots: if c != 'name': if c in state.user_goal.constraints: temp.append(1) else: temp.append(0) for c in self.informable_slots: if c != 'name': if (c in state.user_goal.actual_constraints and state. user_goal.actual_constraints[c].value): temp.append(1) else: temp.append(0) for r in self.requestable_slots: if r in state.user_goal.requests: temp.append(1) else: temp.append(0) for r in self.requestable_slots: if (r in state.user_goal.actual_requests and state. user_goal.actual_requests[r].value): temp.append(1) else: temp.append(0) else: temp += [0] * 2 * (len(self.informable_slots) - 1 + len(self. requestable_slots)) if self.agent_role == 'system': for value in state.slots_filled.values(): temp.append(1) if value else temp.append(0) for r in self.requestable_slots: temp.append(1) if r == state.requested_slot else temp.append(0) return temp 
seed_rl-master.grpc.python.ops_test.OpsTest.upvalue|test @parameterized.parameters(([], False), ([1], True)) def test_upvalue(self, dim, batched): address = self.get_unix_address() server = ops.Server([address]) a = tf.constant(2)  @tf.function(input_signature=[tf.TensorSpec(dim, tf.int32)]) def foo(x): return x / a server.bind(foo, batched=batched) server.start() client = ops.Client(address) self.assertAllEqual(21, client.foo(42)) server.shutdown() 
run_BERT.train.train|step def train_step(x_batch, T_batch, Ts_batch, x_len_batch, T_len_batch, Ts_len_batch, R_Self_batch, R_Cross_batxh, T_W_batch, T_P_batch, Ts_P_batch, y_batch): """ A single training step """ feed_dict = {model.input_x: x_batch, model.input_target: T_batch, model .input_targets_all: Ts_batch, model.sen_len: x_len_batch, model. target_len: T_len_batch, model.targets_all_len_a: Ts_len_batch, model.relate_self: R_Self_batch, model.relate_cross: R_Cross_batxh, model.target_which: T_W_batch, model.target_position: T_P_batch, model.targets_all_position_a: Ts_P_batch, model.input_y: y_batch, model.dropout_keep_prob: FLAGS.dropout_keep_prob} _, step, summaries, loss, accuracy = sess.run([train_op, global_step, train_summary_op, model.loss, model.accuracy], feed_dict) time_str = datetime.datetime.now().isoformat() print('{}: step {}, loss {:g}, acc {:g}'.format(time_str, step, loss, accuracy)) train_summary_writer.add_summary(summaries, step) 
dimensionality_reduction_directory.main def main(): parser = argparse.ArgumentParser() parser.add_argument('-d', '--directory', type=str) parser.add_argument('-o', '--output_directory', type=str) parser.add_argument('-m', '--method', choices=['PCA', 'tSVD', 'DRA'], default='DRA') parser.add_argument('-n', '--reduce_to', type=int, default=300) parser.add_argument('-b', '--do_in_batches', action='store_true') args = parser.parse_args() files = [os.path.join(args.directory, f) for f in os.listdir(args. directory) if os.path.isfile(os.path.join(args.directory, f))] for i_file, file in enumerate(files): printTrace('Dimensionality reduction: Embedding ' + str(i_file) + ' of ' + str(len(files)) + ' : ' + str(file)) excec_com = 'python3 dimensionality_reduction.py -i ' + str(file ) + ' -m ' + str(args.method ) + ' -o ' + args.output_directory + file.split('/')[-1 ] + '_' + str(args.method) + '.vec -n ' + str(args.reduce_to) + ( ' -b ' if args.do_in_batches else '') print(excec_com) os.system(excec_com) 
estimate_gradient_norm.EstimateLipschitz.estimate|Lipschitz|multiplerun def _estimate_Lipschitz_multiplerun(self, num, niters, input_image, target_label, true_label, sample_norm='l2', transform=None, order=1): """ num: number of samples per iteration niters: number of iterations input_image: original image (h*w*c) """ batch_size = self.batch_size shape = (batch_size, self.model.image_size, self.model.image_size, self .model.num_channels) dimension = (self.model.image_size * self.model.image_size * self.model .num_channels) if num < batch_size: print('Increasing num to', batch_size) num = batch_size """ 1. Compute input_image related quantities: """ pred, grad_val, grad_2_norm_val, grad_1_norm_val, grad_inf_norm_val = (self .sess.run([self.output, self.grad_op, self.grad_2_norm_op, self. grad_1_norm_op, self.grad_inf_norm_op], feed_dict={self.img: [ input_image], self.true_label: true_label, self.target_label: target_label})) pred = np.squeeze(pred) c = true_label j = target_label g_x0 = pred[c] - pred[j] g_x0_grad_2_norm = np.squeeze(grad_2_norm_val) g_x0_grad_1_norm = np.squeeze(grad_1_norm_val) g_x0_grad_inf_norm = np.squeeze(grad_inf_norm_val) print('** Evaluating g_x0, grad_2_norm_val on the input image x0: ') print('shape of input_image = {}'.format(input_image.shape)) print( 'g_x0 = {:.3f}, grad_2_norm_val = {:3f}, grad_1_norm_val = {:.3f}, grad_inf_norm_val = {:3f}' .format(g_x0, g_x0_grad_2_norm, g_x0_grad_1_norm, g_x0_grad_inf_norm)) if order == 2: print('** Evaluating hv and hv_norm on the input image x0:') randv = np.random.randn(*input_image.shape) hv, hv_norm = self.sess.run([self.hv_op, self.hv_norm_op], feed_dict={self.img: [input_image], self.randv: [randv], self. true_label: true_label, self.target_label: target_label}) print('hv shape = {}, hv_norm = {}'.format(hv.shape, hv_norm)) """ 2. Prepare for sampling: """  def div_work_to_cores(njobs, nprocs): process_item_list = [] while njobs > 0: process_item_list.append(int(np.ceil(njobs / float(nprocs)))) njobs -= process_item_list[-1] nprocs -= 1 return process_item_list if self.dataset == 'imagenet': total_item_size = batch_size else: total_item_size = num process_item_list = div_work_to_cores(total_item_size, self.n_processes) self.n_processes = len(process_item_list) if sample_norm == 'l2': a = 0 b = 3 elif sample_norm == 'li': a = 0.1 b = 0.1 elif sample_norm == 'l1': a = 0 b = 30 else: raise RuntimeError('Unknown sample_norm ' + sample_norm) print('Using sphere', sample_norm) inputs_0 = np.array(input_image) tag_prefix = str(os.getpid()) + '_' result_arr = NpShmemArray(np.float32, (total_item_size, dimension), tag_prefix + 'randsphere') scale = NpShmemArray(np.float32, num + batch_size, tag_prefix + 'scale') scale[:] = (b - a) * np.random.rand(num + batch_size) + a input_example = NpShmemArray(np.float32, inputs_0.shape, tag_prefix + 'input_example') input_example[:] = inputs_0 all_inputs = NpShmemArray(np.float32, (total_item_size,) + inputs_0. shape, tag_prefix + 'all_inputs') clipped_all_inputs = np.empty(dtype=np.float32, shape=(total_item_size, ) + inputs_0.shape) offset_list = [0] for item in process_item_list[:-1]: offset_list.append(offset_list[-1] + item) print(self.n_processes, 'threads launched with parameter', process_item_list, offset_list) worker_func = partial(randsphere, n=dimension, input_shape=inputs_0. shape, total_size=total_item_size, scale_size=num + batch_size, tag_prefix=tag_prefix, r=1.0, norm=sample_norm, transform=transform) worker_args = list(zip(process_item_list, offset_list, [0] * self. n_processes)) sample_results = self.pool.map_async(worker_func, worker_args) Niters = niters if order == 1: L2_max = np.zeros(Niters) L1_max = np.zeros(Niters) Li_max = np.zeros(Niters) G2_max = np.zeros(Niters) G1_max = np.zeros(Niters) Gi_max = np.zeros(Niters) L2 = np.zeros(num) L1 = np.zeros(num) Li = np.zeros(num) G2 = np.zeros(num) G1 = np.zeros(num) Gi = np.zeros(num) elif order == 2: H2_max = np.zeros(Niters) H2 = np.zeros(num) H2_neg = np.zeros(num) Nbatches = num // batch_size search_begin_time = time.time() """ 3. Start performing sampling: """ for iters in range(Niters): iter_begin_time = time.time() scale[:] = (b - a) * np.random.rand(num + batch_size) + a L_counter = 0 G_counter = 0 H_counter = 0 overhead_time = 0.0 overhead_start = time.time() if self.dataset != 'imagenet': sample_results.get() np.clip(all_inputs, -0.5, 0.5, out=clipped_all_inputs) sample_results = self.pool.map_async(worker_func, worker_args) overhead_time += time.time() - overhead_start for i in range(Nbatches): overhead_start = time.time() if self.dataset == 'imagenet': sample_results.get() np.clip(all_inputs, -0.5, 0.5, out=clipped_all_inputs) worker_args = zip(process_item_list, offset_list, [(i + 1) * batch_size] * self.n_processes) sample_results = self.pool.map_async(worker_func, worker_args) if self.dataset == 'imagenet': batch_inputs = clipped_all_inputs else: batch_inputs = clipped_all_inputs[i * batch_size:(i + 1) * batch_size] overhead_time += time.time() - overhead_start if order == 1: (perturbed_predicts, perturbed_grad_2_norm, perturbed_grad_1_norm, perturbed_grad_inf_norm) = (self .sess.run([self.output, self.grad_2_norm_op, self. grad_1_norm_op, self.grad_inf_norm_op], feed_dict={self .img: batch_inputs, self.target_label: target_label, self.true_label: true_label})) if self.compute_slope: s12_2_norm = np.linalg.norm(s[0:batch_size - 1:2] - s[1 :batch_size:2], axis=1) s12_1_norm = np.linalg.norm(s[0:batch_size - 1:2] - s[1 :batch_size:2], ord=1, axis=1) s12_i_norm = np.linalg.norm(s[0:batch_size - 1:2] - s[1 :batch_size:2], ord=np.inf, axis=1) g_x1 = perturbed_predicts[0:batch_size - 1:2, (c) ] - perturbed_predicts[0:batch_size - 1:2, (j)] g_x2 = perturbed_predicts[1:batch_size:2, (c) ] - perturbed_predicts[1:batch_size:2, (j)] batch_L2 = np.abs(g_x1 - g_x2) / s12_2_norm batch_L1 = np.abs(g_x1 - g_x2) / s12_i_norm batch_Li = np.abs(g_x1 - g_x2) / s12_1_norm L2[L_counter:L_counter + batch_size // 2] = batch_L2 L1[L_counter:L_counter + batch_size // 2] = batch_L1 Li[L_counter:L_counter + batch_size // 2] = batch_Li G2[G_counter:G_counter + batch_size] = perturbed_grad_2_norm G1[G_counter:G_counter + batch_size] = perturbed_grad_1_norm Gi[G_counter:G_counter + batch_size] = perturbed_grad_inf_norm L_counter += batch_size // 2 G_counter += batch_size elif order == 2: randv_batch = np.random.randn(*batch_inputs.shape) perturbed_hv, perturbed_hv_norm = self.sess.run([self.hv_op, self.hv_norm_op], feed_dict={self.img: batch_inputs, self.randv: randv_batch, self.true_label: true_label, self.target_label: target_label}) show_tensor_dim = False if show_tensor_dim: print('====================') print( '** Evaluating perturbed_hv and perturbed_hv_norm in batch {}: ' .format(iters)) print('pertubed_hv_prod shape = {}'.format(perturbed_hv .shape)) print('randv_batch shape = {}'.format(randv_batch.shape)) print('perturbed_hv_norm = {}'.format(perturbed_hv_norm [:, (0)])) print('perturbed_hv_norm shape = {}'.format( perturbed_hv_norm.shape)) pt_hvs = [] pt_hvs.append(perturbed_hv + 0 * randv_batch) temp_hv, temp_eig, niter_eig = self.sess.run([self. while_hv_op, self.while_eig, self.it], feed_dict={self. img: batch_inputs, self.randv: randv_batch, self. true_label: true_label, self.target_label: target_label}) if max(temp_eig) > 0: shiftconst = max(temp_eig) temp_eig_1, niter_eig_1 = self.sess.run([self. while_eig_1, self.it_1], feed_dict={self.img: batch_inputs, self.randv: randv_batch, self. true_label: true_label, self.target_label: target_label, self.shiftconst: shiftconst}) else: temp_eig_1 = temp_eig niter_eig_1 = -1 print( 'temp_eig (abs) converge in {} steps, temp_eig_1 (neg) converge in {} steps' .format(niter_eig, niter_eig_1)) final_est_eig = temp_eig final_est_eig_neg = temp_eig_1 H2[H_counter:H_counter + batch_size] = final_est_eig H2_neg[H_counter:H_counter + batch_size] = final_est_eig_neg H_counter += batch_size if order == 1: if self.compute_slope: L2_max[iters] = np.max(L2) L1_max[iters] = np.max(L1) Li_max[iters] = np.max(Li) G2_max[iters] = np.max(G2) G1_max[iters] = np.max(G1) Gi_max[iters] = np.max(Gi) if self.compute_slope: print( '[STATS][L2] loop = {}, time = {:.5g}, iter_time = {:.5g}, overhead = {:.5g}, L2 = {:.5g}, L1 = {:.5g}, Linf = {:.5g}, G2 = {:.5g}, G1 = {:.5g}, Ginf = {:.5g}' .format(iters, time.time() - search_begin_time, time. time() - iter_begin_time, overhead_time, L2_max[iters], L1_max[iters], Li_max[iters], G2_max[iters], G1_max[ iters], Gi_max[iters])) else: print( '[STATS][L2] loop = {}, time = {:.5g}, iter_time = {:.5g}, overhead = {:.5g}, G2 = {:.5g}, G1 = {:.5g}, Ginf = {:.5g}' .format(iters, time.time() - search_begin_time, time. time() - iter_begin_time, overhead_time, G2_max[iters], G1_max[iters], Gi_max[iters])) sys.stdout.flush() if self.compute_slope: L2.fill(0) L1.fill(0) Li.fill(0) G2.fill(0) G1.fill(0) Gi.fill(0) elif order == 2: idx = H2 > 0 H2[idx] = H2_neg[idx] idx_max = np.argmax(abs(H2)) H2_max[iters] = H2[idx_max] print( '[STATS][L2] loop = {}, time = {:.5g}, iter_time = {:.5g}, overhead = {:.5g}, H2 = {:.5g}' .format(iters, time.time() - search_begin_time, time.time() - iter_begin_time, overhead_time, H2_max[iters])) if order == 1: print( '[STATS][L1] g_x0 = {:.5g}, L2_max = {:.5g}, L1_max = {:.5g}, Linf_max = {:.5g}, G2_max = {:.5g}, G1_max = {:.5g}, Ginf_max = {:.5g}' .format(g_x0, np.max(L2_max), np.max(L1_max), np.max(Li_max), np.max(G2_max), np.max(G1_max), np.max(Gi_max))) if self.compute_slope: print( '[STATS][L1] bnd_L2_max = {:.5g}, bnd_L1_max = {:.5g}, bnd_Linf_max = {:.5g}, bnd_G2_max = {:.5g}, bnd_G1_max = {:.5g}, bnd_Ginf_max = {:.5g}' .format(g_x0 / np.max(L2_max), g_x0 / np.max(Li_max), g_x0 / np.max(L1_max), g_x0 / np.max(G2_max), g_x0 / np.max(Gi_max ), g_x0 / np.max(G1_max))) else: print( '[STATS][L1] bnd_G2_max = {:.5g}, bnd_G1_max = {:.5g}, bnd_Ginf_max = {:.5g}' .format(g_x0 / np.max(G2_max), g_x0 / np.max(Gi_max), g_x0 / np.max(G1_max))) sys.stdout.flush() sample_results.get() return [L2_max, L1_max, Li_max, G2_max, G1_max, Gi_max, g_x0, pred] elif order == 2: H2_max_val = max(abs(H2_max)) print( '[STATS][L1] g_x0 = {:.5g}, g_x0_grad_2_norm = {:.5g}, g_x0_grad_1_norm = {:.5g}, g_x0_grad_inf_norm = {:.5g}, H2_max = {:.5g}' .format(g_x0, g_x0_grad_2_norm, g_x0_grad_1_norm, g_x0_grad_inf_norm, H2_max_val)) bnd = (-g_x0_grad_2_norm + np.sqrt(g_x0_grad_2_norm ** 2 + 2 * g_x0 * H2_max_val)) / H2_max_val print('[STATS][L1] bnd_H2_max = {:.5g}'.format(bnd)) sys.stdout.flush() sample_results.get() return [H2_max, g_x0, g_x0_grad_2_norm, g_x0_grad_1_norm, g_x0_grad_inf_norm, pred] 
texar.agents.agent_base.AgentBase.hparams @property def hparams(self): """A :class:`~texar.hyperparams.HParams` instance. The hyperparameters of the module. """ return self._hparams 
data_helper.data_helper.load|file|csv def load_csv_file(self, filename, num_classes): """ Load CSV file, generate one-hot labels and process text data as Paper did. """ all_data = [] labels = [] with open(filename) as f: reader = csv.DictReader(f, fieldnames=['class'], restkey='fields') for row in reader: one_hot = np.zeros(num_classes) one_hot[int(row['class']) - 1] = 1 labels.append(one_hot) data = np.ones(self.sequence_max_length) * 68 text = row['fields'][-1].lower() all_data.append(self.char2vec(text)) f.close() return np.array(all_data), np.array(labels) 
cifar10_sobolev.conv|apply def apply_conv(x, filters=32, kernel_size=3, he_init=True): if he_init: initializer = tf.contrib.layers.variance_scaling_initializer(uniform =True) else: initializer = tf.contrib.layers.xavier_initializer(uniform=True) return tf.layers.conv2d(x, filters=filters, kernel_size=kernel_size, padding='SAME', kernel_initializer=initializer) 
src.input_data.retrieve|name def retrieve_name(var): callers_local_vars = inspect.currentframe().f_back.f_locals.items() return [var_name for var_name, var_val in callers_local_vars if var_val is var and 'item' not in var_name][0] 
alig.th.alig.AliG.G|Ali def __init__(self, params, max_lr=None, momentum=0, projection_fn=None, eps =1e-05, adjusted_momentum=False): if max_lr is not None and max_lr <= 0.0: raise ValueError('Invalid max_lr: {}'.format(max_lr)) if momentum < 0.0: raise ValueError('Invalid momentum value: {}'.format(momentum)) params_list = list(params) defaults = dict(max_lr=max_lr, momentum=momentum, step_size=None) super(AliG, self).__init__(params_list, defaults) self.adjusted_momentum = adjusted_momentum self.projection = projection_fn self.eps = eps for group in self.param_groups: if group['momentum']: for p in group['params']: self.state[p]['momentum_buffer'] = torch.zeros_like(p.data, requires_grad=False) if self.adjusted_momentum: self.apply_momentum = self.apply_momentum_adjusted else: self.apply_momentum = self.apply_momentum_standard if self.projection is not None: self.projection() 
vdcnn.padding|fixed def fixed_padding(inputs, kernel_size=3): pad_total = kernel_size - 1 pad_beg = pad_total // 2 pad_end = pad_total - pad_beg padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end], [0, 0]]) return padded_inputs 
kaffe.caffe.resolver.CaffeResolver.pycaffe|has def has_pycaffe(self): return self.caffe is not None 
model.layerwise.hidden|shape def hidden_shape(hps): return hps.batch_size * hps.k, hps.h_size, hps.image_size[0 ] // 2, hps.image_size[1] // 2 
translate.models.attention|no def no_attention(state, hidden_states, *args, **kwargs): batch_size = tf.shape(state)[0] weighted_average = tf.zeros(shape=tf.stack([batch_size, 0])) weights = tf.zeros(shape=[batch_size, tf.shape(hidden_states)[1]]) return weighted_average, weights 
AffineCouplingSdnGain.AffineCouplingSdnGain.Affine|Sdn|Coupling|Gain def __init__(self, x_shape, shift_and_log_scale_fn, layer_id=0, last_layer= False, validate_args=False, name='real_nvp'): super(AffineCouplingSdnGain, self).__init__(forward_min_event_ndims=1, is_constant_jacobian=False, validate_args=validate_args, name=name) self.x_shape = x_shape self.i0, self.i1, self.ic = x_shape self._last_layer = last_layer self.id = layer_id self._shift_and_log_scale_fn = shift_and_log_scale_fn self.scale = tf.get_variable('rescaling_scale{}'.format(self.id), [], dtype=DTYPE, initializer=tf.constant_initializer(0.0001)) 
test_envs.TestUR5.Up|set def setUp(self): """Create the UR5 environment.  This follows the ur5.py example located in example_designs/. """ self.env = UR5(use_contexts=True, random_contexts=True, context_range=[ (-np.pi, np.pi), (-np.pi / 4, 0), (-np.pi / 4, np.pi / 4)]) self.env.reset() 
nets.inception_v1_test.InceptionV1Test.With|Reuse|Train|test|Eval def testTrainEvalWithReuse(self): train_batch_size = 5 eval_batch_size = 2 height, width = 224, 224 num_classes = 1000 train_inputs = tf.random_uniform((train_batch_size, height, width, 3)) inception.inception_v1(train_inputs, num_classes) eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3)) logits, _ = inception.inception_v1(eval_inputs, num_classes, reuse=True) predictions = tf.argmax(logits, 1) with self.test_session() as sess: sess.run(tf.global_variables_initializer()) output = sess.run(predictions) self.assertEquals(output.shape, (eval_batch_size,)) 
models.densenet.DenseNet3.forward def forward(self, x): out = self.conv1(x) out = self.trans1(self.block1(out)) out = self.trans2(self.block2(out)) out = self.block3(out) out = self.relu(self.bn1(out)) out = F.avg_pool2d(out, 8) out = out.view(-1, self.in_planes) return self.fc(out) 
commons.im_rotate.image|rotate|tf def tf_image_rotate(images, angles, name=None): with tf.name_scope(name, 'image_rotate', [images, angles]) as name: z = py_func(image_rotate, [images, angles], [tf.float32], name=name, grad=image_rotate_grad) return z[0] 
neural_tangents.utils.batch._serial.x|serial|fn def serial_fn_x1(x1, x2=None, *args, **kwargs): x2_is_none = x2 is None if x2_is_none: x2 = x1 n1 = x1.shape[0] n2 = x2.shape[0] input_shape = x1.shape[1:] n1_batch_size = (batch_size if not is_parallel else batch_size * device_count) n1_batches, ragged = divmod(n1, n1_batch_size) if ragged: msg = ( 'Number of examples in x1 must divide batch size. Found |x1| = {} and batch size = {}.' .format(n1, n1_batch_size)) if is_parallel: msg += ( ' Note that device parallelism was detected and so the batch size was expanded by a factor of {}.' .format(device_count)) raise ValueError(msg) n2_batches, ragged = divmod(n2, batch_size) if ragged: raise ValueError( 'Number of examples in x2 must divide batch size. Found |x2| = {} and batch size = {}' .format(n2, batch_size)) x1s = np.reshape(x1, (n1_batches, n1_batch_size) + input_shape) x2s = np.reshape(x2, (n2_batches, batch_size) + input_shape)  def row_fn(_, x1): return _, _scan(col_fn, x1, x2s, store_on_device)[1]  def col_fn(x1, x2): return x1, kernel_fn(x1, x2, *args, **kwargs) _, kernel = _scan(row_fn, 0, x1s, store_on_device) return flatten(kernel, x2_is_none) 
gan.infogan_cr.INFOGAN_CR.build def build(self): self.build_connection() self.build_loss() self.build_summary() self.build_metric() self.saver = tf.train.Saver() 
query_methods.QueryMethod.model|update def update_model(self, new_model): del self.model gc.collect() self.model = new_model 
plato.agent.component.dialogue_policy.reinforcement_learning.reward_function.SlotFillingGoalAdvancementReward.initialize def initialize(self, **kwargs): """ Initialize the failure penalty and success reward  :param kwargs: dictionary containing failure penalty and success reward :return: Nothing """ if 'failure_penalty' in kwargs: self.failure_penalty = kwargs['failure_penalty'] if 'success_reward' in kwargs: self.success_reward = kwargs['success_reward'] if 'state' in kwargs: self.prev_state = deepcopy(kwargs['state']) else: self.prev_state = None if 'goal' in kwargs: self.prev_goal = deepcopy(kwargs['goal']) else: self.prev_goal = None 
model.graph|build|cascade|coord def build_cascade_coord_graph(mrcnn_coord_x_bin, mrcnn_coord_y_bin, mrcnn_coord_z_bin, num_bins, num_classes): """Builds the computation graph of the second-stage coordinate map.  Inputs: mrcnn_coord_x_bin, mrcnn_coord_y_bin, mrcnn_coord_z_bin: [batch, roi_count, height, width, num_classes, num_bins] image_shape: [height, width, depth] pool_size: The width of the square feature map generated from ROI Pooling. num_classes: number of classes, which determines the depth of the results  Returns: x, y, and z, Coordinate maps [batch, roi_count, height, width, num_classes] """ x1 = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 28, 28, num_bins * num_classes]), name='mrcnn_coord_cascade_final_reshape_x')( mrcnn_coord_x_bin) x2 = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 28, 28, num_bins * num_classes]), name='mrcnn_coord_cascade_final_reshape_y')( mrcnn_coord_y_bin) x3 = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 28, 28, num_bins * num_classes]), name='mrcnn_coord_cascade_final_reshape_z')( mrcnn_coord_z_bin) x = KL.Concatenate(axis=4, name='mrcnn_coord_cascade_combine')([x1, x2, x3] ) x = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding='same'), name= 'mrcnn_coord_cascade_conv1')(x) x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_cascade_bn1')( x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(num_classes * 3, (1, 1), padding= 'same'), name='mrcnn_coord_cascade_conv2')(x) x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_cascade_bn2')( x) x = KL.Activation('sigmoid', name='mrcnn_coord_cascade_sigmoid')(x) x = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], tf.shape(t)[1], tf.shape(t)[2], tf.shape(t)[3], num_classes, 3]), name= 'mrcnn_coord_cascade_final_reshape')(x) mrcnn_coord_x = KL.Lambda(lambda x: x[:, :, :, :, :, (0)], name= 'mrcnn_coord_x_final')(x) mrcnn_coord_y = KL.Lambda(lambda x: x[:, :, :, :, :, (1)], name= 'mrcnn_coord_y_final')(x) mrcnn_coord_z = KL.Lambda(lambda x: x[:, :, :, :, :, (2)], name= 'mrcnn_coord_z_final')(x) return mrcnn_coord_x, mrcnn_coord_y, mrcnn_coord_z 
facenet-master.tmp.seed_test.conv|test|inference def inference_conv_test(images): conv1 = _conv(images, 3, 64, 7, 7, 2, 2, 'SAME') resh1 = tf.reshape(conv1, [-1, 147456]) affn = _affine(resh1, 147456, 128) return affn 
house_parser.R2RHouseParser.R|Parser|House def __init__(self, house_file_path): """Parses regions, panos, categories and objects from house spec file.  For more information see: https://github.com/niessner/Matterport/blob/master/data_organization.md  Args: house_file_path: Path to scan id house specification file. """ self.scan_id = os.path.splitext(os.path.basename(house_file_path))[0] with tf.io.gfile.GFile(house_file_path, 'r') as input_file: assert re.match('^ASCII .*', input_file.readline().strip()) is not None house_info = input_file.readline().strip().split() assert 29 == len(house_info) self.num_images = int(house_info[3]) self.num_panos = int(house_info[4]) self.num_objects = int(house_info[8]) self.num_categories = int(house_info[9]) self.num_regions = int(house_info[10]) self.regions = {} self.panos = {} self.categories = {} self.objects = {} self.images = {} for line in input_file: if line[0] == 'R': r = Region(line) assert r.index not in self.regions self.regions[r.index] = r elif line[0] == 'P': p = Pano(line) assert p.index not in self.panos self.panos[p.index] = p elif line[0] == 'C': c = Category(line) assert c.index not in self.categories self.categories[c.index] = c elif line[0] == 'O': o = Object(line) assert o.index not in self.objects self.objects[o.index] = o elif line[0] == 'I': i = Image(line, self.scan_id) assert i.index not in self.images self.images[i.index] = i assert self.num_regions == len(self.regions) assert self.num_panos == len(self.panos) assert self.num_categories == len(self.categories) assert self.num_objects == len(self.objects) assert self.num_images == len(self.images) self.pano_name_map = {} for p in self.panos.values(): self.pano_name_map[p.name] = p.index self.region_object_map = collections.defaultdict(list) for o in self.objects.values(): self.region_object_map[o.region_index] += [o.index] for image in self.images.values(): pano = self.get_pano_by_name(image.name) pano.images.append(image) 
gym_pycolab.tests.engine_test.EngineTest.testPlotStateVariables.info|add|state def add_state_info(actions, board, layers, backdrop, things, the_plot): del actions, board, layers, backdrop, things state_info.append((the_plot.frame, the_plot.update_group)) 
kaffe.graph.Graph.topologically|sorted def topologically_sorted(self): sorted_nodes = [] unsorted_nodes = list(self.nodes) temp_marked = set() perm_marked = set()  def visit(node): if node in temp_marked: raise KaffeError('Graph is not a DAG.') if node in perm_marked: return temp_marked.add(node) for child in node.children: visit(child) perm_marked.add(node) temp_marked.remove(node) sorted_nodes.insert(0, node) while len(unsorted_nodes): visit(unsorted_nodes.pop()) return sorted_nodes 
enas.cifar10.general_child.GeneralChild.test|build def _build_test(self): print('-' * 80) print('Build test graph') logits = self._model(self.x_test, False, reuse=True) self.test_preds = tf.argmax(logits, axis=1) self.test_preds = tf.to_int32(self.test_preds) self.test_acc = tf.equal(self.test_preds, self.y_test) self.test_acc = tf.to_int32(self.test_acc) self.test_acc = tf.reduce_sum(self.test_acc) 
models.one_pixel_attack.OnePixelAttack.Attack|One|Pixel def __init__(self, dimensions=(32, 32, 3)): self.dimensions = dimensions 
deepcp.get|concate|embed def get_concate_embed(x_input_one_hot, x_input_mul_hot): data_embed_one_hot = get_masked_one_hot(x_input_one_hot) data_embed_mul_hot = get_masked_mul_hot(x_input_mul_hot) data_embed_concat = tf.concat([data_embed_one_hot, data_embed_mul_hot], 1) return data_embed_concat 
empirical_test.kernel|fns def _kernel_fns(key, input_shape, network, out_logits): init_fn, f, _ = _build_network(input_shape, network, out_logits) _, params = init_fn(key, (-1,) + input_shape) implicit_kernel_fn = jit(empirical.empirical_implicit_ntk_fn(f)) direct_kernel_fn = jit(empirical.empirical_direct_ntk_fn(f)) return partial(implicit_kernel_fn, params=params), partial(direct_kernel_fn , params=params) 
avod.datasets.kitti.kitti_dataset.KittiDataset.get|path|map|depth def get_depth_map_path(self, sample_name): return self.depth_dir + '/' + sample_name + '_left_depth.png' 
UGATIT.UGATIT.save def save(self, checkpoint_dir, step): checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir) if not os.path.exists(checkpoint_dir): os.makedirs(checkpoint_dir) self.saver.save(self.sess, os.path.join(checkpoint_dir, self.model_name + '.model'), global_step=step) 
texar.modules.decoders.beam_search_decode_test.BeamSearchDecodeTest.attention|given|test|decoder|state|initial def test_attention_decoder_given_initial_state(self): """Tests beam search with RNNAttentionDecoder given initial state. """ seq_length = np.random.randint(self._max_time, size=[self._batch_size]) + 1 encoder_values_length = tf.constant(seq_length) hparams = {'attention': {'kwargs': {'num_units': self._attention_dim}}, 'rnn_cell': {'kwargs': {'num_units': self._cell_dim}}} decoder = tx.modules.AttentionRNNDecoder(vocab_size=self._vocab_size, memory=self._encoder_output, memory_sequence_length= encoder_values_length, hparams=hparams) state = decoder.cell.zero_state(self._batch_size, tf.float32) cell_state = state.cell_state self._test_beam_search(decoder, initial_state=cell_state) tiled_cell_state = tile_batch(cell_state, multiplier=self._beam_width) self._test_beam_search(decoder, tiled_initial_state=tiled_cell_state, initiated=True) 
mnist_plot.envelope|plot|tradeoff def plot_tradeoff_envelope(mu, sigma, E, title_name, save): l1 = plot(np.arange(0, 1.01, 0.01), norm.cdf(norm.ppf(1 - np.arange(0, 1.01, 0.01)) - mu), color='r', linewidth=2, label=str(mu) + '-GDP by CLT') rowIndex = np.arange(1e-05, 0.1, 0.001) frame = pd.DataFrame(0, index=rowIndex, columns=np.arange(0, 1.01, 0.01)) row = 0 for delta in rowIndex: eps = compute_epsilon(E, sigma, 60000, 256, delta) breaking = (1 - np.exp(-eps)) / (np.exp(eps) - np.exp(-eps)) * (1 - delta) x_array1 = np.arange(0, breaking, 0.01) x_array2 = np.arange(breaking, 1, 0.01) y_array1 = -np.exp(eps) * x_array1 + 1 - delta y_array2 = np.exp(-eps) * (1 - delta - x_array2) frame.iloc[(row), :len(y_array1)] = y_array1 frame.iloc[(row), len(y_array1):] = y_array2 row += 1 l2 = plot(np.concatenate((x_array1, x_array2)), frame.max(), color= 'royalblue', linewidth=2, linestyle='--', label= '($\\epsilon,\\delta$)-DP by MA') xlabel('Type I error', fontsize=15) ylabel('Type II error', fontsize=15) xlim(0, 1) ylim(0, 1) title(title_name, fontsize=16) legend(loc='upper right', fontsize=12) gca().set_aspect('equal', adjustable='box') savefig(fname=save, format='pdf', bbox_inches='tight') show() return None 
gym_pycolab.examples.classics.chain_walk.game|make def make_game(): """Builds and returns a chain-walk game.""" return ascii_art.ascii_art_to_game(GAME_ART, what_lies_beneath='.', sprites={'P': PlayerSprite}) 
datasets.camvid.CamVid.len def __len__(self): return len(self.imgs) 
plot_gain_params.load|iso|nlf|cam def load_cam_iso_nlf(): cin = pd.read_csv('cam_iso_nlf_all.txt') cin = cin.drop_duplicates() cin = cin.set_index('cam_iso', drop=False) return cin 
plato.utilities.parser.run_data_parser.file|path|check def check_file_path(path): if os.path.isfile(path): return path else: import plato plato_path = '/'.join(plato.__file__.split('/')[:-1]) + '/' new_config_path = plato_path + 'example/config/parser/' + path if os.path.isfile(new_config_path): return new_config_path else: raise ValueError(f'Configuration file {path} not found!') 
official.utils.logs.hooks_helper_test.BaseTest.train|get|test|hook|hooks|profiler def test_get_train_hooks_profiler_hook(self): self.validate_train_hook_name('ProfilerHook', 'profilerhook') 
facenet-master.src.train_tripletloss.main def main(args): network = importlib.import_module(args.model_def) subdir = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S') log_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir) if not os.path.isdir(log_dir): os.makedirs(log_dir) model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir) if not os.path.isdir(model_dir): os.makedirs(model_dir) facenet.write_arguments_to_file(args, os.path.join(log_dir, 'arguments.txt')) src_path, _ = os.path.split(os.path.realpath(__file__)) facenet.store_revision_info(src_path, log_dir, ' '.join(sys.argv)) np.random.seed(seed=args.seed) train_set = facenet.get_dataset(args.data_dir) print('Model directory: %s' % model_dir) print('Log directory: %s' % log_dir) if args.pretrained_model: print('Pre-trained model: %s' % os.path.expanduser(args. pretrained_model)) if args.lfw_dir: print('LFW directory: %s' % args.lfw_dir) pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs)) lfw_paths, actual_issame = lfw.get_paths(os.path.expanduser(args. lfw_dir), pairs) with tf.Graph().as_default(): tf.set_random_seed(args.seed) global_step = tf.Variable(0, trainable=False) learning_rate_placeholder = tf.placeholder(tf.float32, name= 'learning_rate') batch_size_placeholder = tf.placeholder(tf.int32, name='batch_size') phase_train_placeholder = tf.placeholder(tf.bool, name='phase_train') image_paths_placeholder = tf.placeholder(tf.string, shape=(None, 3), name='image_paths') labels_placeholder = tf.placeholder(tf.int64, shape=(None, 3), name ='labels') input_queue = data_flow_ops.FIFOQueue(capacity=100000, dtypes=[tf. string, tf.int64], shapes=[(3,), (3,)], shared_name=None, name=None ) enqueue_op = input_queue.enqueue_many([image_paths_placeholder, labels_placeholder]) nrof_preprocess_threads = 4 images_and_labels = [] for _ in range(nrof_preprocess_threads): filenames, label = input_queue.dequeue() images = [] for filename in tf.unstack(filenames): file_contents = tf.read_file(filename) image = tf.image.decode_image(file_contents, channels=3) if args.random_crop: image = tf.random_crop(image, [args.image_size, args. image_size, 3]) else: image = tf.image.resize_image_with_crop_or_pad(image, args.image_size, args.image_size) if args.random_flip: image = tf.image.random_flip_left_right(image) image.set_shape((args.image_size, args.image_size, 3)) images.append(tf.image.per_image_standardization(image)) images_and_labels.append([images, label]) image_batch, labels_batch = tf.train.batch_join(images_and_labels, batch_size=batch_size_placeholder, shapes=[(args.image_size, args.image_size, 3), ()], enqueue_many=True, capacity=4 * nrof_preprocess_threads * args.batch_size, allow_smaller_final_batch=True) image_batch = tf.identity(image_batch, 'image_batch') image_batch = tf.identity(image_batch, 'input') labels_batch = tf.identity(labels_batch, 'label_batch') prelogits, _ = network.inference(image_batch, args.keep_probability, phase_train=phase_train_placeholder, bottleneck_layer_size=args .embedding_size, weight_decay=args.weight_decay) embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name='embeddings') anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1, 3, args.embedding_size]), 3, 1) triplet_loss = facenet.triplet_loss(anchor, positive, negative, args.alpha) learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step, args.learning_rate_decay_epochs * args.epoch_size, args.learning_rate_decay_factor, staircase=True) tf.summary.scalar('learning_rate', learning_rate) regularization_losses = tf.get_collection(tf.GraphKeys. REGULARIZATION_LOSSES) total_loss = tf.add_n([triplet_loss] + regularization_losses, name= 'total_loss') train_op = facenet.train(total_loss, global_step, args.optimizer, learning_rate, args.moving_average_decay, tf.global_variables()) saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3) summary_op = tf.summary.merge_all() gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args. gpu_memory_fraction) sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) sess.run(tf.global_variables_initializer(), feed_dict={ phase_train_placeholder: True}) sess.run(tf.local_variables_initializer(), feed_dict={ phase_train_placeholder: True}) summary_writer = tf.summary.FileWriter(log_dir, sess.graph) coord = tf.train.Coordinator() tf.train.start_queue_runners(coord=coord, sess=sess) with sess.as_default(): if args.pretrained_model: print('Restoring pretrained model: %s' % args.pretrained_model) saver.restore(sess, os.path.expanduser(args.pretrained_model)) epoch = 0 while epoch < args.max_nrof_epochs: step = sess.run(global_step, feed_dict=None) epoch = step // args.epoch_size train(args, sess, train_set, epoch, image_paths_placeholder, labels_placeholder, labels_batch, batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, embeddings, total_loss, train_op, summary_op, summary_writer, args. learning_rate_schedule_file, args.embedding_size, anchor, positive, negative, triplet_loss) save_variables_and_metagraph(sess, saver, summary_writer, model_dir, subdir, step) if args.lfw_dir: evaluate(sess, lfw_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, args.batch_size, args.lfw_nrof_folds, log_dir, step, summary_writer, args.embedding_size) return model_dir 
texar.modules.decoders.tf_helpers.ScheduledOutputTrainingHelper.sample def sample(self, time, outputs, state, name=None): """Gets a sample for one step.""" with ops.name_scope(name, 'ScheduledOutputTrainingHelperSample', [time, outputs, state]): sampler = bernoulli.Bernoulli(probs=self._sampling_probability) return sampler.sample(sample_shape=self.batch_size, seed=self._seed) 
gym_pycolab.envs.pycolab_grid_worlds_env.PycolabGridWorldsLevel4Env.game|make def _make_game(self): self._setup() return grid_worlds.make_game(positive_rewards=1, negative_rewards=1) 
texar.hyperparams.HParams.Params|H def __init__(self, hparams, default_hparams, allow_new_hparam=False): if isinstance(hparams, HParams): hparams = hparams.todict() if default_hparams is not None: parsed_hparams = self._parse(hparams, default_hparams, allow_new_hparam ) else: parsed_hparams = self._parse(hparams, hparams) super(HParams, self).__setattr__('_hparams', parsed_hparams) 
agents_test.AgentsTest.frames|test|stack|done def test_stack_frames_done(self): zero_state = agents.DuelingLSTMDQNNet(2, [1], stack_size=4).initial_state(1 ).frame_stacking_state output, state = stack_frames(frames=[[[1]]], done=[[False]], frame_stacking_state=zero_state, stack_size=4) self.assertAllEqual(output, [[[1, 0, 0, 0]]]) output, state = stack_frames(frames=[[[2]]], done=[[True]], frame_stacking_state=state, stack_size=4) self.assertAllEqual(output, [[[2, 0, 0, 0]]]) output, state = stack_frames(frames=[[[3]], [[4]], [[5]], [[6]], [[7]], [[8]]], done=[[False], [False], [False], [False], [True], [False]], frame_stacking_state=state, stack_size=4) self.assertEqual(output.shape[0], 6) self.assertAllEqual(output[0], [[3, 2, 0, 0]]) self.assertAllEqual(output[5], [[8, 7, 0, 0]]) 
extensions.u_hemis.u_hemis_net.ConvDecoderImg.Img|Conv|Decoder def __init__(self, w_initializer=None, w_regularizer=None, b_initializer= None, b_regularizer=None, name='ConvDecoderImg'): super(ConvDecoderImg, self).__init__(name=name) self.initializers = {'w': w_initializer, 'b': b_initializer} self.regularizers = {'w': w_regularizer, 'b': b_regularizer} self.ini_f = NB_CONV self.layers = [{'name': 'block_1', 'n_features': 4 * self.ini_f, 'kernels': ((3, 3, 3), (3, 3, 3))}, {'name': 'block_2', 'n_features': 2 * self.ini_f, 'kernels': ((3, 3, 3), (3, 3, 3))}, { 'name': 'block_3', 'n_features': self.ini_f, 'kernels': ((3, 3, 3), (3, 3, 3))}] 
model_bayesnn.Model.phi def phi(self, n_samples, lpx, lqx, method, alpha=0): diff = lpx - lqx if method == 'adapted': diff -= tf.reduce_max(diff) dx = tf.exp(diff) prob = tf.sign(tf.expand_dims(dx, 1) - tf.expand_dims(dx, 0)) prob = tf.cast(tf.greater(prob, 0.5), tf.float32) wx = tf.reduce_sum(prob, axis=1) / n_samples wx = (1.0 - wx) ** alpha elif method == 'alpha': diff = alpha * diff diff -= tf.reduce_max(diff) wx = tf.exp(diff) else: raise NotImplementedError wx /= tf.reduce_sum(wx) return wx 
src.layers.FullyConnectedDecoder.call def _call(self, inputs): x = inputs x = tf.nn.dropout(x, 1 - self.dropout) outputs = tf.matmul(x, self.vars['weights']) return outputs 
Init.uniform def uniform(shape, scale=0.05, name=None): """Uniform init.""" initial = tf.random_uniform(shape, minval=-scale, maxval=scale, dtype= tf.float32) return tf.Variable(initial, name=name) 
models.cnn.CNN.cnn|mnist def mnist_cnn(self): layers = [Conv2D(32, (3, 3), padding='valid', input_shape=(self. input_side, self.input_side, self.input_channels), name='conv1'), Activation(self.non_linearity), Conv2D(64, (3, 3), name='conv2'), Activation(self.non_linearity), MaxPooling2D(pool_size=(2, 2)), Dropout(self.dropout_prob), Flatten(), Dense(128, name='dense1'), Activation(self.non_linearity), Dropout(self.dropout_prob), Dense( self.num_classes, name='logits'), Activation('softmax')] model = Sequential() for layer in layers: model.add(layer) return model 
pytorch_pretrained_bert.modeling_gpt2.Attention.heads|merge def merge_heads(self, x): x = x.permute(0, 2, 1, 3).contiguous() new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),) return x.view(*new_x_shape) 
input_converter.write|records def write_records(records, out_filename): """ Write to TensorFlow record """ writer = tf.python_io.TFRecordWriter(out_filename) for count, record in enumerate(records): writer.write(record) if count % 10000 == 0: tf.logging.info('write: %d', count) writer.close() 
utils.make|name def make_name(opt, prefix='', eval_=False, is_dir=True, set_epoch=None, do_epoch=True): string = prefix string += '{}-{}'.format(opt.dataset, opt.exp) string += '/' string += '{}-{}-{}'.format(opt.trainer, opt.cycle, opt.iters) string += '/' string += opt.model if opt.mle: string += '-{}'.format(opt.mle) string += '/' string += make_name_string(opt.data) + '/' string += make_name_string(opt.net) + '/' string += make_name_string(opt.train.static) + '/' if eval_: string += make_name_string(opt.eval) + '/' if not is_dir: mkpath(string) string += make_name_string(opt.train.dynamic, True, do_epoch, set_epoch) if is_dir: mkpath(string) return string 
nlp_pipeline.normalize def normalize(sentence_sets, word2id, max_sent_len): """Normalize the sentences by the following procedure - word to index - add unk - pad/ cut the sentence length - record the sentence length  Args: sentence_sets: the set of sentence paraphrase, a list of sentence list word2id: word index, a dictionary max_sent_len: maximum sentence length, a integer """ sent_sets = [] sent_len_sets = [] max_sent_len = max_sent_len + 1 for st in sentence_sets: st_ = [] st_len = [] for s in st: s_ = [word2id['_GOO']] for w in s: if w in word2id: s_.append(word2id[w]) else: s_.append(word2id['_UNK']) s_.append(word2id['_EOS']) s_ = s_[:max_sent_len] if len(s_) < max_sent_len: s_len = len(s_) - 1 for i in range(max_sent_len - len(s_)): s_.append(word2id['_PAD']) else: s_[-1] = word2id['_EOS'] s_len = max_sent_len - 1 st_.append(s_) st_len.append(s_len) sent_sets.append(st_) sent_len_sets.append(st_len) return sent_sets, sent_len_sets 
darkflow.net.help.camera def camera(self): file = self.FLAGS.demo SaveVideo = self.FLAGS.saveVideo if file == 'camera': file = 0 else: assert os.path.isfile(file), 'file {} does not exist'.format(file) camera = cv2.VideoCapture(file) if file == 0: self.say('Press [ESC] to quit demo') assert camera.isOpened(), 'Cannot capture source' if file == 0: cv2.namedWindow('', 0) _, frame = camera.read() height, width, _ = frame.shape cv2.resizeWindow('', width, height) else: _, frame = camera.read() height, width, _ = frame.shape if SaveVideo: fourcc = cv2.VideoWriter_fourcc(*'XVID') if file == 0: fps = 1 / self._get_fps(frame) if fps < 1: fps = 1 else: fps = round(camera.get(cv2.CAP_PROP_FPS)) videoWriter = cv2.VideoWriter('video.avi', fourcc, fps, (width, height) ) buffer_inp = list() buffer_pre = list() elapsed = int() start = timer() self.say('Press [ESC] to quit demo') while camera.isOpened(): elapsed += 1 _, frame = camera.read() if frame is None: print('\nEnd of Video') break preprocessed = self.framework.preprocess(frame) buffer_inp.append(frame) buffer_pre.append(preprocessed) if elapsed % self.FLAGS.queue == 0: feed_dict = {self.inp: buffer_pre} net_out = self.sess.run(self.out, feed_dict) for img, single_out in zip(buffer_inp, net_out): postprocessed = self.framework.postprocess(single_out, img, False) if SaveVideo: videoWriter.write(postprocessed) if file == 0: cv2.imshow('', postprocessed) buffer_inp = list() buffer_pre = list() if elapsed % 5 == 0: sys.stdout.write('\r') sys.stdout.write('{0:3.3f} FPS'.format(elapsed / (timer() - start)) ) sys.stdout.flush() if file == 0: choice = cv2.waitKey(1) if choice == 27: break sys.stdout.write('\n') if SaveVideo: videoWriter.release() camera.release() if file == 0: cv2.destroyAllWindows() 
official.resnet.resnet_run_loop.image|bytes|fn|input|serving def image_bytes_serving_input_fn(image_shape, dtype=tf.float32): """Serving input fn for raw jpeg images."""  def _preprocess_image(image_bytes): """Preprocess a single raw image.""" bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=dtype, shape=[1, 1, 4]) height, width, num_channels = image_shape image = imagenet_preprocessing.preprocess_image(image_bytes, bbox, height, width, num_channels, is_training=False) return image image_bytes_list = tf.placeholder(shape=[None], dtype=tf.string, name= 'input_tensor') images = tf.map_fn(_preprocess_image, image_bytes_list, back_prop=False, dtype=dtype) return tf.estimator.export.TensorServingInputReceiver(images, { 'image_bytes': image_bytes_list}) 
task.Dyck.Dyck def __init__(self, max_val) ->None: self.bracket_pairs = {i: (i + 1) for i in range(1, max_val, 2)} self.inverse_bracket_pairs = {v: k for k, v in self.bracket_pairs.items()} self.open_brackets = [k for k, _ in self.bracket_pairs.items()] self.brackets = [k for k, _ in self.bracket_pairs.items()] + [v for _, v in self.bracket_pairs.items()] 
neural_tangents.stax.Flatten.kernel_fn.trace def trace(x): count = x.shape[-4] * x.shape[-2] y = np.trace(x, axis1=-2, axis2=-1) z = np.trace(y, axis1=-2, axis2=-1) return z / count 
deeplab_resnet.utils.preprocess|inv def inv_preprocess(imgs, num_images=1): """Inverse preprocessing of the batch of images. Add the mean vector and convert from BGR to RGB.  Args: imgs: batch of input images. num_images: number of images to apply the inverse transformations on.  Returns: The batch of the size num_images with the same spatial dimensions as the input. """ n, h, w, c = imgs.shape assert n >= num_images, 'Batch size %d should be greater or equal than number of images to save %d.' % ( n, num_images) outputs = np.zeros((num_images, h, w, c), dtype=np.uint8) for i in range(num_images): outputs[i] = (imgs[i] + IMG_MEAN)[:, :, ::-1].astype(np.uint8) return outputs 
embeddings.OpenKE.models.TransH.TransH.calc def _calc(self, h, t, r): return abs(h + r - t) 
train_shapenet.string|log def log_string(out_str): LOG_FOUT.write(out_str + '\n') LOG_FOUT.flush() print(out_str) 
pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.file|encode def encode_file(self, path, ordered=False, verbose=False, add_eos=True, add_double_eos=False): if verbose: print('encoding file {} ...'.format(path)) assert os.path.exists(path) encoded = [] with open(path, 'r', encoding='utf-8') as f: for idx, line in enumerate(f): if verbose and idx > 0 and idx % 500000 == 0: print('    line {}'.format(idx)) symbols = self.tokenize(line, add_eos=add_eos, add_double_eos= add_double_eos) encoded.append(self.convert_to_tensor(symbols)) if ordered: encoded = torch.cat(encoded) return encoded 
clas_main.main def _main(_): train_data = tx.data.MultiAlignedData(config.train_data) val_data = tx.data.MultiAlignedData(config.val_data) test_data = tx.data.MultiAlignedData(config.test_data) iterator = tx.data.TrainTestDataIterator(train_data, val_data, test_data) batch = iterator.get_next() embedder = tx.modules.WordEmbedder(vocab_size=train_data.vocab('x'). size, hparams=config.emb) classifier = tx.modules.Conv1DClassifier(config.clas) logits, pred = classifier(embedder(batch['x_text_ids'])) loss = tf.losses.sparse_softmax_cross_entropy(labels=batch['y'], logits =logits) accu = tx.evals.accuracy(batch['y'], pred) train_op = tx.core.get_train_op(loss, hparams=config.opt)  def _run_epoch(sess, mode, epoch=0, verbose=False): is_train = tx.utils.is_train_mode_py(mode) fetches = {'accu': accu, 'batch_size': tx.utils.get_batch_size( batch['y'])} if is_train: fetches['train_op'] = train_op feed_dict = {tx.context.global_mode(): mode} cum_accu = 0.0 nsamples = 0 step = 0 while True: try: rets = sess.run(fetches, feed_dict) step += 1 accu_ = rets['accu'] cum_accu += accu_ * rets['batch_size'] nsamples += rets['batch_size'] if verbose and (step == 1 or step % 100 == 0): tf.logging.info('epoch: {0:2} step: {1:4} accu: {2:.4f}' .format(epoch, step, accu_)) except tf.errors.OutOfRangeError: break return cum_accu / nsamples with tf.Session() as sess: sess.run(tf.global_variables_initializer()) sess.run(tf.local_variables_initializer()) sess.run(tf.tables_initializer()) best_val_accu = -1.0 for epoch in range(config.num_epochs): iterator.switch_to_train_data(sess) train_accu = _run_epoch(sess, tf.estimator.ModeKeys.TRAIN, epoch) iterator.switch_to_val_data(sess) val_accu = _run_epoch(sess, tf.estimator.ModeKeys.EVAL, epoch) tf.logging.info( 'epoch: {0:2} train accu: {1:.4f} val accu: {2:.4f}'.format (epoch + 1, train_accu, val_accu)) if val_accu > best_val_accu: best_val_accu = val_accu iterator.switch_to_test_data(sess) test_accu = _run_epoch(sess, tf.estimator.ModeKeys.EVAL) tf.logging.info('test accu: {0:.4f}'.format(test_accu)) 
thumt.utils.lrp.LegacyGRUCell_encoder_v2n.size|state @property def state_size(self): return self._num_units 
nets.inception_v3.v|inception def inception_v3(inputs, num_classes=1000, is_training=True, dropout_keep_prob=0.8, min_depth=16, depth_multiplier=1.0, prediction_fn=slim.softmax, spatial_squeeze=True, reuse=None, create_aux_logits=True, scope='InceptionV3', global_pool=False): """Inception model from http://arxiv.org/abs/1512.00567.  "Rethinking the Inception Architecture for Computer Vision"  Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna.  With the default arguments this method constructs the exact model defined in the paper. However, one can experiment with variations of the inception_v3 network by changing arguments dropout_keep_prob, min_depth and depth_multiplier.  The default image size used to train this network is 299x299.  Args: inputs: a tensor of size [batch_size, height, width, channels]. num_classes: number of predicted classes. If 0 or None, the logits layer is omitted and the input features to the logits layer (before dropout) are returned instead. is_training: whether is training or not. dropout_keep_prob: the percentage of activation values that are retained. min_depth: Minimum depth value (number of channels) for all convolution ops. Enforced when depth_multiplier < 1, and not an active constraint when depth_multiplier >= 1. depth_multiplier: Float multiplier for the depth (number of channels) for all convolution ops. The value must be greater than zero. Typical usage will be to set this value in (0, 1) to reduce the number of parameters or computation cost of the model. prediction_fn: a function to get predictions out of logits. spatial_squeeze: if True, logits is of shape [B, C], if false logits is of shape [B, 1, 1, C], where B is batch_size and C is number of classes. reuse: whether or not the network and its variables should be reused. To be able to reuse 'scope' must be given. create_aux_logits: Whether to create the auxiliary logits. scope: Optional variable_scope. global_pool: Optional boolean flag to control the avgpooling before the logits layer. If false or unset, pooling is done with a fixed window that reduces default-sized inputs to 1x1, while larger inputs lead to larger outputs. If true, any input size is pooled down to 1x1.  Returns: net: a Tensor with the logits (pre-softmax activations) if num_classes is a non-zero integer, or the non-dropped-out input to the logits layer if num_classes is 0 or None. end_points: a dictionary from components of the network to the corresponding activation.  Raises: ValueError: if 'depth_multiplier' is less than or equal to zero. """ if depth_multiplier <= 0: raise ValueError('depth_multiplier is not greater than zero.') depth = lambda d: max(int(d * depth_multiplier), min_depth) with tf.variable_scope(scope, 'InceptionV3', [inputs], reuse=reuse ) as scope: with slim.arg_scope([slim.batch_norm, slim.dropout], is_training= is_training): net, end_points = inception_v3_base(inputs, scope=scope, min_depth=min_depth, depth_multiplier=depth_multiplier) if create_aux_logits and num_classes: with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim. avg_pool2d], stride=1, padding='SAME'): aux_logits = end_points['Mixed_6e'] with tf.variable_scope('AuxLogits'): aux_logits = slim.avg_pool2d(aux_logits, [5, 5], stride=3, padding='VALID', scope='AvgPool_1a_5x5') aux_logits = slim.conv2d(aux_logits, depth(128), [1, 1], scope='Conv2d_1b_1x1') kernel_size = _reduced_kernel_size_for_small_input( aux_logits, [5, 5]) aux_logits = slim.conv2d(aux_logits, depth(768), kernel_size, weights_initializer=trunc_normal( 0.01), padding='VALID', scope='Conv2d_2a_{}x{}' .format(*kernel_size)) aux_logits = slim.conv2d(aux_logits, num_classes, [ 1, 1], activation_fn=None, normalizer_fn=None, weights_initializer=trunc_normal(0.001), scope= 'Conv2d_2b_1x1') if spatial_squeeze: aux_logits = tf.squeeze(aux_logits, [1, 2], name='SpatialSqueeze') end_points['AuxLogits'] = aux_logits with tf.variable_scope('Logits'): if global_pool: net = tf.reduce_mean(net, [1, 2], keep_dims=True, name= 'GlobalPool') end_points['global_pool'] = net else: kernel_size = _reduced_kernel_size_for_small_input(net, [8, 8]) net = slim.avg_pool2d(net, kernel_size, padding='VALID', scope='AvgPool_1a_{}x{}'.format(*kernel_size)) end_points['AvgPool_1a'] = net if not num_classes: return net, end_points net = slim.dropout(net, keep_prob=dropout_keep_prob, scope= 'Dropout_1b') end_points['PreLogits'] = net logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, normalizer_fn=None, scope= 'Conv2d_1c_1x1') if spatial_squeeze: logits = tf.squeeze(logits, [1, 2], name='SpatialSqueeze') end_points['Logits'] = logits end_points['Predictions'] = prediction_fn(logits, scope= 'Predictions') return logits, end_points 
correct_depth.crop def crop(img, border): return img[border[1]:-border[1], border[0]:-border[0]] 
utils.nprf_knrm_pair_generator.NPRFKNRMPairGenerator.pairs|count def count_pairs(self, qid_list, sample_size):  def count_on_topic(neg_len, pos_len, sample_size): sample_size = min(neg_len, sample_size) if sample_size == 0: return 0 else: return pos_len * sample_size total = 0 for qid in qid_list: relevance = self.relevance_dict.get(qid) relevance_posting = relevance.get_judged_docid_list() res = relevance.get_supervised_docid_list() if len(res) < self.nb_supervised_doc: pass else: rel_0, rel_1, rel_2 = relevance_posting[0], relevance_posting[1 ], relevance_posting[2] count_01 = count_on_topic(len(rel_0), len(rel_1), sample_size) count_12 = count_on_topic(len(rel_1), len(rel_2), sample_size) count = min(self.sample_perquery_limit, count_01 + count_12) total += count total = min(self.sample_total_limit, total) return total 
plato.controller.sgui_controller.SGUIController.human|GUI|update def update_GUI_human(self, sys_agent, human_role='user'): asr_utterance = '' if self.INTERACTION_MODE == 'speech': if not self.GUI_dialogue_initialized: self.GUI_dialogue_initialized = True with speech_rec.Microphone() as source: print('(listening...)') audio = self.asr.listen(source, phrase_time_limit=3) try: asr_utterance = self.asr.recognize_google(audio) print('Google ASR: ' + asr_utterance) except speech_rec.UnknownValueError: print('Google ASR did not understand you') except speech_rec.RequestError as e: print('Google ASR request error: {0}'.format(e)) if human_role == 'user': if self.INTERACTION_MODE == 'speech': self.user_output = asr_utterance if self.user_output_history: self.user_output_history += '\n' + asr_utterance else: self.user_output_history = asr_utterance self.window.Element('_USER_').Update(self.user_output_history) self.window.Refresh() self.sys_output = sys_agent.continue_dialogue({'input': self. user_output}) self.sys_output_history += '\n' + self.sys_output['output_raw'] self.update_text('_SYS_', self.sys_output_history, self.sys_output[ 'output_raw']) if 'bye' in self.user_output: self.GUI_dialogue_initialized = False self.sys_output_history += ( '\n==================================\n\n') self.window.Element('_SYS_').Update(self.sys_output_history) self.window.Refresh() sys_agent.end_dialogue() self.sys_output = '' self.sys_output_dacts = [] self.goal = None 
tica.tICA.partial|fit def partial_fit(self, X): """Fit the model with X.  This method is suitable for online learning. The state of the model will be updated with the new data `X`.  Parameters ---------- X: array-like, shape (n_samples, n_features) Training data, where n_samples in the number of samples and n_features is the number of features.  Returns ------- self : object Returns the instance itself. """ self._fit(X) return self 
official.resnet.imagenet_test.BaseTest.v|shapes|tensor|resnet|test def test_tensor_shapes_resnet_152_v2(self): self.tensor_shapes_helper(152, resnet_version=2) 
layers.TernaryConvolutionLayer.regularizer def regularizer(self): """Regularizer  Returns: tf tensor (rank 0) or float -- Regularization loss """ reg_loss = self.reg_factor * tf.add_n([tf.reduce_sum(tf.abs(v)) for v in self.vars]) return reg_loss 
test_utils.test|space def test_space(gym_space, expected_size, expected_min, expected_max): """Test the shape and bounds of an action or observation space.  Parameters ---------- gym_space : gym.spaces.Box gym space object to be tested expected_size : int expected size expected_min : float or array_like expected minimum value(s) expected_max : float or array_like expected maximum value(s) """ assert gym_space.shape[0] == expected_size, '{}, {}'.format(gym_space. shape[0], expected_size) np.testing.assert_almost_equal(gym_space.high, expected_max, decimal=4) np.testing.assert_almost_equal(gym_space.low, expected_min, decimal=4) 
data.PatchGeneratorPerFile.get|features|patches def get_patches_features(self, feature_dir, file_list): """ Given a directory with precomputed features in files: - create the variable self.features with all the TF patches of all the files in the feature_dir """ assert os.path.isdir(os.path.dirname(feature_dir) ), 'path to feature directory does not exist' self.file_list = [f for f in file_list if f.endswith(self.suffix_in + '.data')] self.nb_files = len(self.file_list) assert self.nb_files > 0, 'there are no features files in the feature directory' self.feature_dir = feature_dir self.nb_inst_cum = np.cumsum(np.array([0] + [self. get_num_instances_per_file(os.path.join(self.feature_dir, f_name)) for f_name in self.file_list], dtype=int)) self.nb_patch_total = self.nb_inst_cum[-1] self.current_f_idx = 0 self.feature_size = self.get_feature_size_per_file(f_name=os.path.join( self.feature_dir, self.file_list[0])) self.features = np.zeros((self.nb_patch_total, self.patch_len, self. feature_size), dtype=self.floatx) for f_id in range(self.nb_files): self.fetch_file_2_tensor(f_id) 
decoder.attention|multi|source def multi_source_attention(query, memory, mem_lens, max_mem_len, state_size): """Attention to multiple sources  query: a query vector [B, S] memory: a list of memory vectors [[B, M, S], [B, M, S], ... ] mem_lens: a list of memory length max_mem_len: a list of maximum memory length """ with tf.variable_scope('multi_source_attention'): num_memory = len(memory) context_vector = [] for i in range(num_memory): query = tf.layers.dense(query, state_size, name='query_proj', kernel_initializer=tf.random_normal_initializer(stddev=0.05 ), bias_initializer=tf.constant_initializer(0.0), reuse=tf. AUTO_REUSE) context, _ = attention(query, memory[i], mem_lens[i], max_mem_len[i]) context_vector.append(context) context_vector = tf.concat(context_vector, axis=1) context_vector = tf.layers.dense(context_vector, state_size, name= 'mem_output_proj', kernel_initializer=tf. random_normal_initializer(stddev=0.05), bias_initializer=tf. constant_initializer(0.0), reuse=tf.AUTO_REUSE) return context_vector, None 
preprocess_unlabeled.main def main(): parser = argparse.ArgumentParser(description=__doc__) parser.add_argument('--data-file', required=True, help= 'Location of input data; see the README for expected data format.') parser.add_argument('--bert-dir', required=True, help= 'Location of the pre-trained BERT model.') parser.add_argument('--num-docs', default=1000, type=int, help= 'Number of documents to use (default=1000).') parser.add_argument('--cased', default=False, action='store_true', help ="Don't lowercase the input.") parser.add_argument('--max_sequence_length', default=128, type=int, help='Maximum input sequence length after tokenization (default=128).') args = parser.parse_args() random.seed(0) current_doc_tokens = [] segments = [] tokenizer = tokenization.FullTokenizer(vocab_file=os.path.join(args. bert_dir, 'vocab.txt'), do_lower_case=not args.cased) with open(args.data_file, 'r') as f: for line in f: line = tokenization.convert_to_unicode(line).strip() if not line: if current_doc_tokens: for segment in prep_document(current_doc_tokens, args. max_sequence_length): segments.append(segment) if len(segments) >= args.num_docs: break if len(segments) >= args.num_docs: break current_doc_tokens = [] tokens = tokenizer.tokenize(line) if tokens: current_doc_tokens.append(tokens) random.shuffle(segments) utils.write_json([{'tokens': s} for s in segments], args.data_file. replace('.txt', '') + '.json') 
mac_model.fwd|model def fwd_model(xy_var, w_list): """ computes simple forward pass of the model :param xy_var: in and out variables :param w_list: weights per layer :return: """ x_in = xy_var.x preds_hid = [] x_hid = x_in for w in w_list[:-1]: x_hid = tf.nn.relu(x_hid @ w[:-1, :] + w[(-1), :]) preds_hid.append(x_hid) w = w_list[-1] pred_out = x_hid @ w[:-1, :] + w[(-1), :] if xy_var.y is not None: ce_term = tf.nn.softmax_cross_entropy_with_logits_v2(labels=xy_var. y, logits=pred_out) nested_loss = tf.reduce_mean(ce_term) accuracy_count = tf.cast(tf.equal(tf.argmax(xy_var.y, axis=1), tf. argmax(pred_out, axis=1)), tf.float32) n_correct = tf.reduce_sum(accuracy_count) else: nested_loss = 0.5 * tf.reduce_mean(tf.reduce_sum((x_in - pred_out) ** 2, axis=-1)) n_correct = None return pred_out, preds_hid, nested_loss, n_correct 
batch_test.BatchTest.analytic|kernel|test|composition def _test_analytic_kernel_composition(self, batching_fn): rng = random.PRNGKey(0) rng_self, rng_other = random.split(rng) x_self = random.normal(rng_self, (8, 10)) x_other = random.normal(rng_other, (20, 10)) Block = stax.serial(stax.Dense(256), stax.Relu()) _, _, ker_fn = Block ker_fn = batching_fn(ker_fn) _, _, composed_ker_fn = stax.serial(Block, Block) ker_out = ker_fn(ker_fn(x_self)) composed_ker_out = composed_ker_fn(x_self) self.assertAllClose(ker_out, composed_ker_out, True) ker_out = ker_fn(ker_fn(x_self, x_other)) composed_ker_out = composed_ker_fn(x_self, x_other) self.assertAllClose(ker_out, composed_ker_out, True) x_self = random.normal(rng, (8, 10, 10, 3)) x_other = random.normal(rng, (10, 10, 10, 3)) Block = stax.serial(stax.Conv(256, (3, 3)), stax.Relu()) Readout = stax.serial(stax.GlobalAvgPool(), stax.Dense(10)) block_ker_fn, readout_ker_fn = Block[2], Readout[2] _, _, composed_ker_fn = stax.serial(Block, Readout) block_ker_fn = batching_fn(block_ker_fn) readout_ker_fn = batching_fn(readout_ker_fn) ker_out = readout_ker_fn(block_ker_fn(x_self, marginalization='none')) composed_ker_out = composed_ker_fn(x_self) self.assertAllClose(ker_out, composed_ker_out, True) ker_out = readout_ker_fn(block_ker_fn(x_self, x_other, marginalization= 'none')) composed_ker_out = composed_ker_fn(x_self, x_other) self.assertAllClose(ker_out, composed_ker_out, True) 
data_learning.NeuralNetworkModel.model|phase|cnn def cnn_model_phase(self, x): x = Conv2D(filters=12, kernel_size=(3, 3), strides=(1, 1), padding= 'valid', activation='relu', kernel_initializer=initializers. glorot_uniform())(x) x = BatchNormalization()(x) x = AveragePooling2D(pool_size=(2, 1), strides=(2, 1))(x) x = Conv2D(filters=12, kernel_size=(4, 4), strides=(1, 1), padding= 'valid', activation='relu', kernel_initializer=initializers. glorot_uniform())(x) x = BatchNormalization()(x) x = AveragePooling2D(pool_size=(3, 1), strides=(3, 1))(x) print('before flatten, shape of the phase data is: ' + str(x.shape)) x = Flatten()(x) x = Dropout(0.5)(x) x = Dense(32, kernel_regularizer=regularizers.l2(0.02), kernel_initializer=initializers.glorot_uniform(), activation='relu')(x) x = BatchNormalization()(x) return x 
calculate_moments_accountant.bounded|integral def integral_bounded(fn, lb, ub): integral, _ = integrate.quad(fn, lb, ub) return integral 
hbaselines.goal_conditioned.policy.ActorCriticPolicy.update def update(self, update_actor=True, **kwargs): """Perform a gradient update step.  Parameters ---------- update_actor : bool specifies whether to update the actor policy. The critic policy is still updated if this value is set to False.  Returns ------- float critic loss float actor loss """ raise NotImplementedError 
data_loader.GaussiansGenerator.get|batch def get_batch(self): batch = [] for i in range(self.batch_size): point = np.random.randn(2) * self.eps_noise center = self.centers[i % 4] point[0] += center[0] point[1] += center[1] batch.append(point) batch = np.array(batch, dtype='float32') batch = self.float_tensor(batch) batch = batch[(torch.randperm(batch.size(0))), :] return batch 
EndToEndClassification.EnvClassification.trainer.ClassifierTrainer.train def train(self, batch_size=500, no_epochs=200, lr=0.005, momentum=0.9, momentum_optimizer=True): """ Performs training of the classifier with a pre-defined number of epochs and a constant learning rate. The momentum optimizer is used by default, else Adam.  Args: batch_size (int): number of train examples in a mini-batch. no_epochs (int): number of epochs (pre-defined, no overfitting test implemented in this set-up). lr (float): constant learning rate. momentum (float): momentum value of the optimizer. momentum_optimizer (bool): whether to use the momentum optimizer (True) or Adam (False). Leave to True to reproduce the paper results. """ self.no_epochs = no_epochs self.batch_size = batch_size if len(self.dataset.train[0].shape) == 4: self.input_batch_shape = batch_size, self.dataset.train[0].shape[1 ], self.dataset.train[0].shape[2], self.dataset.train[0].shape[3] elif len(self.dataset.train[0].shape) == 3: self.input_batch_shape = batch_size, self.dataset.train[0].shape[1 ], self.dataset.train[0].shape[2] else: raise ValueError('Incorrect input data dimensionality') self.label_batch_shape = batch_size self.eval_input_shape = self.dataset.validation[0].shape self.eval_label_shape = self.dataset.validation[1].shape self.test_input_shape = self.dataset.test[0].shape self.test_label_shape = self.dataset.test[1].shape self.lr = lr self.momentum = momentum self.best_loss = np.inf self.input_placeholder = tf.placeholder(tf.float32, shape=self. input_batch_shape) self.label_placeholder = tf.placeholder(tf.int32, shape=self. label_batch_shape) self.evaluation_input_placeholder = tf.placeholder(tf.float32, shape= self.eval_input_shape) self.evaluation_label_placeholder = tf.placeholder(tf.int32, shape=self .eval_label_shape) self.test_input_placeholder = tf.placeholder(tf.float32, shape=self. test_input_shape) self.test_label_placeholder = tf.placeholder(tf.int32, shape=self. test_label_shape) self.is_training_ph = tf.placeholder(tf.bool, name='is_training') with tf.variable_scope('model'): self.train_prediction_op = self.model.build_predict_op(input_tensor =self.input_placeholder, is_training=self.is_training_ph) with tf.variable_scope('model', reuse=True): self.eval_prediction_op = self.model.build_predict_op(input_tensor= self.evaluation_input_placeholder, is_training=self.is_training_ph) self.val_op = tf.argmax(self.eval_prediction_op, axis=1) self.test_prediction_op = self.model.build_predict_op(input_tensor= self.test_input_placeholder, is_training=self.is_training_ph) self.test_op = tf.argmax(self.test_prediction_op, axis=1) self.train_loss_op = self.model.get_loss_op(prediction=self. train_prediction_op, label_tensor=self.label_placeholder) with tf.variable_scope('optimizer'): if momentum_optimizer: optimize = tf.train.MomentumOptimizer(learning_rate=self.lr, momentum=self.momentum, use_nesterov=True) else: optimize = tf.train.AdamOptimizer(learning_rate=self.lr) if self.MSTfrozen: var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'model/piczak') self.grad_op = optimize.compute_gradients(loss=self. train_loss_op, var_list=var_list) elif self.Piczakfrozen: var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'model/MSTmodel') self.grad_op = optimize.compute_gradients(loss=self. train_loss_op, var_list=var_list) else: self.grad_op = optimize.compute_gradients(loss=self.train_loss_op) self.train_op = optimize.apply_gradients(self.grad_op) init = tf.global_variables_initializer() config = tf.ConfigProto(allow_soft_placement=True, log_device_placement =False) config.gpu_options.allow_growth = True self.sess = tf.Session(config=config) self.sess.run(init) if self.MSTmodel_initialized is not False: self.model.load_MSTmodel(self.MSTmodel_initialized, self.sess) if self.piczak_initialized is not False: self.model.load_piczak(self.piczak_initialized, self.sess) results = {'train_losses': [], 'val_accuracy': [], 'test_accuracy': []} epoch_train_losses = [] epoch_val_accuracy = [] epoch_test_accuracy = [] for epoch in range(self.no_epochs): train_batch_losses = [] indices_train_batches = self.dataset.make_batch_indices(batch_size) for i, batch_indices in enumerate(indices_train_batches): input_batch, label_batch = self.dataset.load_batch(batch_indices) _, loss = self.sess.run([self.train_op, self.train_loss_op], feed_dict={self.input_placeholder: input_batch, self. label_placeholder: label_batch, self.is_training_ph: True}) train_batch_losses.append(loss) epoch_train_average = np.average(train_batch_losses) print('train loss: {}'.format(epoch_train_average)) epoch_train_losses.append(epoch_train_average) results['train_losses'] = epoch_train_losses val_input, val_labels = self.dataset.validation predicted_labels_val = self.sess.run([self.val_op], feed_dict={self .evaluation_input_placeholder: val_input, self. evaluation_label_placeholder: val_labels, self.is_training_ph: False}) predicted_labels_val = predicted_labels_val[0] val_accuracy = classification_accuracy(self.dataset.validation_pd, predicted_labels_val) epoch_val_accuracy.append(val_accuracy) results['val_accuracy'] = epoch_val_accuracy print('val accuracy: {}'.format(val_accuracy)) test_input, test_labels = self.dataset.test predicted_labels_test = self.sess.run([self.test_op], feed_dict={ self.test_input_placeholder: test_input, self. test_label_placeholder: test_labels, self.is_training_ph: False}) predicted_labels_test = predicted_labels_test[0] test_accuracy = classification_accuracy(self.dataset.test_pd, predicted_labels_test) epoch_test_accuracy.append(test_accuracy) results['test_accuracy'] = epoch_test_accuracy print('test accuracy: {}'.format(test_accuracy)) dump_pickle(self.save_results, results) if self.save_model: if self.save_separate: self.model.save_MSTmodel(os.path.join(self. save_model_path_mst, self.model.model_name + '_MST'), self.sess) self.model.save_piczak(os.path.join(self. save_model_path_piczak, self.model.model_name + '_piczak'), self.sess) else: self.model.save(os.path.join(self.save_model_path, self. model.model_name), self.sess) 
optimizers.get|learning|rates def get_learning_rates(args): """ :return: constant or decaying learning rates per epoch, depending on settings """ w_lr = args.w_learning_rate z_lr = args.z_learning_rate global_w_step = None if args.w_lr_decay is not None: global_w_step = tf.get_variable('global_w_step', dtype=tf.int32, initializer=0) epoch_steps = args.train_set_size / args.batch_size if args.w_optim != 'nested': epoch_steps *= len(args.topology) + 1 w_lr = tf.train.exponential_decay(w_lr, global_w_step, epoch_steps, args.w_lr_decay, staircase=True) if args.lr_decay_stop is not None: min_lr = args.w_learning_rate * args.w_lr_decay ** (args. lr_decay_stop - 1) w_lr = tf.maximum(w_lr, min_lr) return w_lr, z_lr, global_w_step 
SMILESX_utils.result|median|mean def mean_median_result(x_cardinal_tmp, y_pred_tmp): x_card_cumsum = np.cumsum(x_cardinal_tmp) x_card_cumsum_shift = shift(x_card_cumsum, 1, cval=0) y_mean = np.array([np.mean(y_pred_tmp[x_card_cumsum_shift[cenumcard]: ienumcard]) for cenumcard, ienumcard in enumerate(x_card_cumsum. tolist())]) y_med = np.array([np.median(y_pred_tmp[x_card_cumsum_shift[cenumcard]: ienumcard]) for cenumcard, ienumcard in enumerate(x_card_cumsum. tolist())]) return y_mean, y_med 
deeppoly_nodes.DeeppolySigmoidNodeFirst.transformer def transformer(self, nn, man, element, nlb, nub, use_area_heuristic): """ transformer for the first layer of a neural network, if that first layer is fully connected with sigmoid  Arguments --------- man : ElinaManagerPtr man to which element belongs element : ElinaAbstract0Ptr abstract element onto which the transformer gets applied  Return ------ output : ElinaAbstract0Ptr abstract element after the transformer """ ffn_handle_first_sigmoid_layer(man, element, *self.get_arguments()) return element 
sampler.TopKSampler.batch|append def append_batch(self, X, next_idx, mask): next_pos = X[:, -1:, (1)] + 1 next_x = torch.cat((next_idx, next_pos), -1).unsqueeze(1) next_mask = torch.cat([mask, torch.ones(X.size(0), 1, device=mask. device)], 1) return torch.cat((X, next_x), 1), next_mask 
tests.layers.interaction_test.test|Layer|FGCNN def test_FGCNNLayer(): with CustomObjectScope({'FGCNNLayer': layers.FGCNNLayer}): layer_test(layers.FGCNNLayer, kwargs={'filters': (4, 6), 'kernel_width': (7, 7)}, input_shape=(BATCH_SIZE, FIELD_SIZE, EMBEDDING_SIZE)) 
cleverhans.devtools.tests.test_format.format|test|pep def test_format_pep8(): """ Test if pep8 is respected. """ pep8_checker = StyleGuide() files_to_check = [] for path in list_files('.py'): rel_path = os.path.relpath(path, cleverhans.__path__[0]) if rel_path in whitelist_pep8: continue else: files_to_check.append(path) report = pep8_checker.check_files(files_to_check) if report.total_errors > 0: raise AssertionError('PEP8 Format not respected') 
commons.utils.get|inception|data def get_inception_data(y_hat_val_list): inception_scores = [get_inception_score(y_hat_val) for y_hat_val in y_hat_val_list] score_mean = np.mean(inception_scores) data = {'y_hat_vals_list': y_hat_val_list, 'inception_scores': inception_scores, 'inception_score_mean': score_mean} return data 
run_squad.main def main(): parser = argparse.ArgumentParser() parser.add_argument('--bert_model', default=None, type=str, required= True, help= 'Bert pre-trained model selected in the list: bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese.' ) parser.add_argument('--output_dir', default=None, type=str, required= True, help= 'The output directory where the model checkpoints and predictions will be written.' ) parser.add_argument('--train_file', default=None, type=str, help= 'SQuAD json for training. E.g., train-v1.1.json') parser.add_argument('--predict_file', default=None, type=str, help= 'SQuAD json for predictions. E.g., dev-v1.1.json or test-v1.1.json') parser.add_argument('--max_seq_length', default=384, type=int, help= 'The maximum total input sequence length after WordPiece tokenization. Sequences longer than this will be truncated, and sequences shorter than this will be padded.' ) parser.add_argument('--doc_stride', default=128, type=int, help= 'When splitting up a long document into chunks, how much stride to take between chunks.' ) parser.add_argument('--max_query_length', default=64, type=int, help= 'The maximum number of tokens for the question. Questions longer than this will be truncated to this length.' ) parser.add_argument('--do_train', action='store_true', help= 'Whether to run training.') parser.add_argument('--do_predict', action='store_true', help= 'Whether to run eval on the dev set.') parser.add_argument('--train_batch_size', default=32, type=int, help= 'Total batch size for training.') parser.add_argument('--predict_batch_size', default=8, type=int, help= 'Total batch size for predictions.') parser.add_argument('--learning_rate', default=5e-05, type=float, help= 'The initial learning rate for Adam.') parser.add_argument('--num_train_epochs', default=3.0, type=float, help ='Total number of training epochs to perform.') parser.add_argument('--warmup_proportion', default=0.1, type=float, help= 'Proportion of training to perform linear learning rate warmup for. E.g., 0.1 = 10%% of training.' ) parser.add_argument('--n_best_size', default=20, type=int, help= 'The total number of n-best predictions to generate in the nbest_predictions.json output file.' ) parser.add_argument('--max_answer_length', default=30, type=int, help= 'The maximum length of an answer that can be generated. This is needed because the start and end predictions are not conditioned on one another.' ) parser.add_argument('--verbose_logging', action='store_true', help= 'If true, all of the warnings related to data processing will be printed. A number of warnings are expected for a normal SQuAD evaluation.' ) parser.add_argument('--no_cuda', action='store_true', help= 'Whether not to use CUDA when available') parser.add_argument('--seed', type=int, default=42, help= 'random seed for initialization') parser.add_argument('--gradient_accumulation_steps', type=int, default= 1, help= 'Number of updates steps to accumulate before performing a backward/update pass.' ) parser.add_argument('--do_lower_case', action='store_true', help= 'Whether to lower case the input text. True for uncased models, False for cased models.' ) parser.add_argument('--local_rank', type=int, default=-1, help= 'local_rank for distributed training on gpus') parser.add_argument('--fp16', action='store_true', help= 'Whether to use 16-bit float precision instead of 32-bit') parser.add_argument('--loss_scale', type=float, default=0, help= """Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True. 0 (default value): dynamic loss scaling. Positive power of 2: static loss scaling value. """ ) parser.add_argument('--version_2_with_negative', action='store_true', help= 'If true, the SQuAD examples contain some that do not have an answer.') parser.add_argument('--null_score_diff_threshold', type=float, default= 0.0, help= 'If null_score - best_non_null is greater than the threshold predict null.' ) args = parser.parse_args() if args.local_rank == -1 or args.no_cuda: device = torch.device('cuda' if torch.cuda.is_available() and not args.no_cuda else 'cpu') n_gpu = torch.cuda.device_count() else: torch.cuda.set_device(args.local_rank) device = torch.device('cuda', args.local_rank) n_gpu = 1 torch.distributed.init_process_group(backend='nccl') logger.info( 'device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}' .format(device, n_gpu, bool(args.local_rank != -1), args.fp16)) if args.gradient_accumulation_steps < 1: raise ValueError( 'Invalid gradient_accumulation_steps parameter: {}, should be >= 1' .format(args.gradient_accumulation_steps)) args.train_batch_size = (args.train_batch_size // args. gradient_accumulation_steps) random.seed(args.seed) np.random.seed(args.seed) torch.manual_seed(args.seed) if n_gpu > 0: torch.cuda.manual_seed_all(args.seed) if not args.do_train and not args.do_predict: raise ValueError( 'At least one of `do_train` or `do_predict` must be True.') if args.do_train: if not args.train_file: raise ValueError( 'If `do_train` is True, then `train_file` must be specified.') if args.do_predict: if not args.predict_file: raise ValueError( 'If `do_predict` is True, then `predict_file` must be specified.' ) if os.path.exists(args.output_dir) and os.listdir(args.output_dir ) and args.do_train: raise ValueError('Output directory () already exists and is not empty.' ) if not os.path.exists(args.output_dir): os.makedirs(args.output_dir) tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case) train_examples = None num_train_optimization_steps = None if args.do_train: train_examples = read_squad_examples(input_file=args.train_file, is_training=True, version_2_with_negative=args. version_2_with_negative) num_train_optimization_steps = int(len(train_examples) / args. train_batch_size / args.gradient_accumulation_steps ) * args.num_train_epochs if args.local_rank != -1: num_train_optimization_steps = (num_train_optimization_steps // torch.distributed.get_world_size()) model = BertForQuestionAnswering.from_pretrained(args.bert_model, cache_dir=os.path.join(PYTORCH_PRETRAINED_BERT_CACHE, 'distributed_{}'.format(args.local_rank))) if args.fp16: model.half() model.to(device) if args.local_rank != -1: try: from apex.parallel import DistributedDataParallel as DDP except ImportError: raise ImportError( 'Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.' ) model = DDP(model) elif n_gpu > 1: model = torch.nn.DataParallel(model) param_optimizer = list(model.named_parameters()) param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]] no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight'] optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01}, {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}] if args.fp16: try: from apex.optimizers import FP16_Optimizer from apex.optimizers import FusedAdam except ImportError: raise ImportError( 'Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.' ) optimizer = FusedAdam(optimizer_grouped_parameters, lr=args. learning_rate, bias_correction=False, max_grad_norm=1.0) if args.loss_scale == 0: optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True) else: optimizer = FP16_Optimizer(optimizer, static_loss_scale=args. loss_scale) else: optimizer = BertAdam(optimizer_grouped_parameters, lr=args. learning_rate, warmup=args.warmup_proportion, t_total= num_train_optimization_steps) global_step = 0 if args.do_train: cached_train_features_file = (args.train_file + '_{0}_{1}_{2}_{3}'. format(list(filter(None, args.bert_model.split('/'))).pop(), str(args.max_seq_length), str(args.doc_stride), str(args. max_query_length))) train_features = None try: with open(cached_train_features_file, 'rb') as reader: train_features = pickle.load(reader) except: train_features = convert_examples_to_features(examples= train_examples, tokenizer=tokenizer, max_seq_length=args. max_seq_length, doc_stride=args.doc_stride, max_query_length=args.max_query_length, is_training=True) if args.local_rank == -1 or torch.distributed.get_rank() == 0: logger.info('  Saving train features into cached file %s', cached_train_features_file) with open(cached_train_features_file, 'wb') as writer: pickle.dump(train_features, writer) logger.info('***** Running training *****') logger.info('  Num orig examples = %d', len(train_examples)) logger.info('  Num split examples = %d', len(train_features)) logger.info('  Batch size = %d', args.train_batch_size) logger.info('  Num steps = %d', num_train_optimization_steps) all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long) all_input_mask = torch.tensor([f.input_mask for f in train_features ], dtype=torch.long) all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long) all_start_positions = torch.tensor([f.start_position for f in train_features], dtype=torch.long) all_end_positions = torch.tensor([f.end_position for f in train_features], dtype=torch.long) train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_start_positions, all_end_positions) if args.local_rank == -1: train_sampler = RandomSampler(train_data) else: train_sampler = DistributedSampler(train_data) train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size) model.train() for _ in trange(int(args.num_train_epochs), desc='Epoch'): for step, batch in enumerate(tqdm(train_dataloader, desc= 'Iteration')): if n_gpu == 1: batch = tuple(t.to(device) for t in batch) (input_ids, input_mask, segment_ids, start_positions, end_positions) = batch loss = model(input_ids, segment_ids, input_mask, start_positions, end_positions) if n_gpu > 1: loss = loss.mean() if args.gradient_accumulation_steps > 1: loss = loss / args.gradient_accumulation_steps if args.fp16: optimizer.backward(loss) else: loss.backward() if (step + 1) % args.gradient_accumulation_steps == 0: if args.fp16: lr_this_step = args.learning_rate * warmup_linear( global_step / num_train_optimization_steps, args.warmup_proportion) for param_group in optimizer.param_groups: param_group['lr'] = lr_this_step optimizer.step() optimizer.zero_grad() global_step += 1 if args.do_train: model_to_save = model.module if hasattr(model, 'module') else model output_model_file = os.path.join(args.output_dir, WEIGHTS_NAME) torch.save(model_to_save.state_dict(), output_model_file) output_config_file = os.path.join(args.output_dir, CONFIG_NAME) with open(output_config_file, 'w') as f: f.write(model_to_save.config.to_json_string()) config = BertConfig(output_config_file) model = BertForQuestionAnswering(config) model.load_state_dict(torch.load(output_model_file)) else: model = BertForQuestionAnswering.from_pretrained(args.bert_model) model.to(device) if args.do_predict and (args.local_rank == -1 or torch.distributed. get_rank() == 0): eval_examples = read_squad_examples(input_file=args.predict_file, is_training=False, version_2_with_negative=args. version_2_with_negative) eval_features = convert_examples_to_features(examples=eval_examples, tokenizer=tokenizer, max_seq_length=args.max_seq_length, doc_stride=args.doc_stride, max_query_length=args. max_query_length, is_training=False) logger.info('***** Running predictions *****') logger.info('  Num orig examples = %d', len(eval_examples)) logger.info('  Num split examples = %d', len(eval_features)) logger.info('  Batch size = %d', args.predict_batch_size) all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long) all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long) all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long) all_example_index = torch.arange(all_input_ids.size(0), dtype=torch .long) eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_example_index) eval_sampler = SequentialSampler(eval_data) eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.predict_batch_size) model.eval() all_results = [] logger.info('Start evaluating') for input_ids, input_mask, segment_ids, example_indices in tqdm( eval_dataloader, desc='Evaluating'): if len(all_results) % 1000 == 0: logger.info('Processing example: %d' % len(all_results)) input_ids = input_ids.to(device) input_mask = input_mask.to(device) segment_ids = segment_ids.to(device) with torch.no_grad(): batch_start_logits, batch_end_logits = model(input_ids, segment_ids, input_mask) for i, example_index in enumerate(example_indices): start_logits = batch_start_logits[i].detach().cpu().tolist() end_logits = batch_end_logits[i].detach().cpu().tolist() eval_feature = eval_features[example_index.item()] unique_id = int(eval_feature.unique_id) all_results.append(RawResult(unique_id=unique_id, start_logits=start_logits, end_logits=end_logits)) output_prediction_file = os.path.join(args.output_dir, 'predictions.json') output_nbest_file = os.path.join(args.output_dir, 'nbest_predictions.json') output_null_log_odds_file = os.path.join(args.output_dir, 'null_odds.json') write_predictions(eval_examples, eval_features, all_results, args. n_best_size, args.max_answer_length, args.do_lower_case, output_prediction_file, output_nbest_file, output_null_log_odds_file, args.verbose_logging, args. version_2_with_negative, args.null_score_diff_threshold) 
cleverhans.devtools.tests.docscrape.NumpyDocString.str|indent def _str_indent(self, doc, indent=4): out = [] for line in doc: out += [' ' * indent + line] return out 
generate_dataset.blend def blend(template, template_mask, target_image, target_bbox, steps=3): template = (template * 255).astype(np.uint8) target_image = (target_image * 255).astype(np.uint8) temp_template_mask = template_mask.copy() temp_template_mask = cv2.copyMakeBorder(temp_template_mask, 1, 1, 1, 1, cv2.BORDER_CONSTANT, value=(0, 0, 0)) blend_mask = temp_template_mask.astype(np.float32) * (1.0 / steps) kernel = np.ones((3, 3), np.uint8) for step in range(steps - 1): temp_template_mask = cv2.erode(temp_template_mask, kernel) blend_mask += temp_template_mask * (1.0 / steps) x0 = target_bbox['x0'] y0 = target_bbox['y0'] x1 = target_bbox['x1'] y1 = target_bbox['y1'] blend_mask = blend_mask[1:-1, 1:-1] blended = target_image[y0:y1, x0:x1] * (1 - blend_mask) + template[:, :, ([0, 1, 2])] * blend_mask return blended.astype(np.float32) / 255.0 
avod.core.box_3d_encoder_test.Box3dEncoderTest.anchor|to|tensor|d|test|box def test_anchor_tensor_to_box_3d(self): anchors = np.asarray([[-0.59, 1.9, 25.01, 3.2, 1.66, 1.61], [-0.59, 1.9, 25.01, 1.61, 1.66, 3.2]], dtype=np.float32) exp_3d_box = np.asarray([[-0.59, 1.9, 25.01, 3.2, 1.61, 1.66, 0], [- 0.59, 1.9, 25.01, 3.2, 1.61, 1.66, -1.57]], dtype=np.float32) anchor_tensors = tf.convert_to_tensor(anchors, dtype=tf.float32) boxes_3d = box_3d_encoder.anchors_to_box_3d(anchor_tensors, fix_lw=True) sess = tf.Session() with sess.as_default(): boxes_3d_out = boxes_3d.eval() np.testing.assert_almost_equal(boxes_3d_out, exp_3d_box, decimal=3, err_msg='Wrong tensor anchor to box3D format') 
vkge.training.util.make|prior def make_prior(code_size, distribution, alt_prior): """ Returns the prior on embeddings for tensorflow distributions  (i) MultivariateNormalDiag function  (ii) HypersphericalUniform  with alternative prior on gaussian  (1) Alt: N(0,1/code_size) (2) N(0,1) """ if distribution == 'normal': if alt_prior: loc = tf.zeros(code_size) scale = tf.sqrt(tf.divide(tf.ones(code_size), code_size)) else: loc = tf.zeros(code_size) scale = tf.ones(code_size) dist = tfd.MultivariateNormalDiag(loc, scale) elif distribution == 'vmf': dist = HypersphericalUniform(code_size - 1, dtype=tf.float32) else: raise NotImplemented return dist 
model.tensorpack_model.AttentionOCR.inputs def inputs(self): return [tf.TensorSpec([None, cfg.image_size, cfg.image_size, 3], tf. float32, 'image'), tf.TensorSpec([None, cfg.seq_len + 1], tf.int32, 'label'), tf.TensorSpec([None, cfg.seq_len + 1], tf.float32, 'mask' ), tf.TensorSpec([None, 4], tf.float32, 'normalized_bbox'), tf. TensorSpec([], tf.bool, 'is_training'), tf.TensorSpec([], tf. float32, 'dropout_keep_prob')] 
envs.AntGatherEnv.step def step(self, action): """Advance the simulation by one step.  The done mas here is modified to include the horizon ending. """ obs, reward, done, info = super(AntGatherEnv, self).step(action) self.step_number += 1 done = done or self.step_number == self.horizon return obs, reward, done, info 
seq2seq_hparams.lstm|batch|chatbot @registry.register_hparams def chatbot_lstm_batch_512(): hparams = chatbot_lstm_batch_8k() hparams.batch_size = 512 return hparams 
seld_dcase2019_master.cls_feature_class.FeatureClass.file|read|desc def read_desc_file(self, desc_filename, in_sec=False): desc_file = {'class': list(), 'start': list(), 'end': list(), 'ele': list(), 'azi': list()} fid = open(desc_filename, 'r') next(fid) for line in fid: split_line = line.strip().split(',') desc_file['class'].append(split_line[0]) if in_sec: desc_file['start'].append(float(split_line[1])) desc_file['end'].append(float(split_line[2])) else: desc_file['start'].append(int(np.floor(float(split_line[1]) * self._frame_res))) desc_file['end'].append(int(np.ceil(float(split_line[2]) * self ._frame_res))) desc_file['ele'].append(int(split_line[3])) desc_file['azi'].append(int(split_line[4])) fid.close() return desc_file 
data_utils.Dataset.size|vocab @property def vocab_size(self): return len(self.word2id) 
avod.core.models.rpn_model.RpnModel.create_path_drop_masks.kill|branch def kill_branch(): return tf.constant(0.0) 
VIPPruning.VIPPruning.closer|find|th def find_closer_th(self, percentage=0.1, allowed_layers=[]): scores = None for i in range(0, len(self.score_layer)): if i in allowed_layers: if scores is None: scores = self.score_layer[i] else: scores = np.concatenate((scores, self.score_layer[i])) total = scores.shape[0] closest = np.zeros(total) for i in range(0, total): th = scores[i] idxs = np.where(scores <= th)[0] discarded = len(idxs) / total closest[i] = abs(percentage - discarded) th = scores[np.argmin(closest)] return th 
tf_util.conv|d def conv3d(inputs, num_output_channels, kernel_size, scope, stride=[1, 1, 1 ], padding='SAME', use_xavier=True, stddev=0.001, weight_decay=0.0, activation_fn=tf.nn.relu, bn=False, bn_decay=None, is_training=None, is_dist=False): """ 3D convolution with non-linear operation.  Args: inputs: 5-D tensor variable BxDxHxWxC num_output_channels: int kernel_size: a list of 3 ints scope: string stride: a list of 3 ints padding: 'SAME' or 'VALID' use_xavier: bool, use xavier_initializer if true stddev: float, stddev for truncated_normal init weight_decay: float activation_fn: function bn: bool, whether to use batch norm bn_decay: float or float tensor variable in [0,1] is_training: bool Tensor variable  Returns: Variable tensor """ with tf.variable_scope(scope) as sc: kernel_d, kernel_h, kernel_w = kernel_size num_in_channels = inputs.get_shape()[-1].value kernel_shape = [kernel_d, kernel_h, kernel_w, num_in_channels, num_output_channels] kernel = _variable_with_weight_decay('weights', shape=kernel_shape, use_xavier=use_xavier, stddev=stddev, wd=weight_decay) stride_d, stride_h, stride_w = stride outputs = tf.nn.conv3d(inputs, kernel, [1, stride_d, stride_h, stride_w, 1], padding=padding) biases = _variable_on_cpu('biases', [num_output_channels], tf. constant_initializer(0.0)) outputs = tf.nn.bias_add(outputs, biases) if bn: outputs = batch_norm_for_conv3d(outputs, is_training, bn_decay= bn_decay, scope='bn', is_dist=is_dist) if activation_fn is not None: outputs = activation_fn(outputs) return outputs 
baseline_seq2seq_attn_main.file|stdout|and|print def print_stdout_and_file(content, file): print(content) print(content, file=file) 
empirical_test.EmpiricalTest.Against|test|NTK|Direct @jtu.parameterized.named_parameters(jtu.cases_from_list({'testcase_name': '_train_shape={}_test_shape={}_network={}_{}'.format(train, test, network, name), 'train_shape': train, 'test_shape': test, 'network': network, 'name': name, 'kernel_fn': kernel_fn} for train, test, network in zip(TRAIN_SHAPES, TEST_SHAPES, NETWORK) for name, kernel_fn in KERNELS. items())) def testNTKAgainstDirect(self, train_shape, test_shape, network, name, kernel_fn): key = random.PRNGKey(0) key, self_split, other_split = random.split(key, 3) data_self = random.normal(self_split, train_shape) data_other = random.normal(other_split, test_shape) implicit, direct = kernel_fn(key, train_shape[1:], network) g = implicit(data_self, None) g_direct = direct(data_self, None) self.assertAllClose(g, g_direct, check_dtypes=False) g = implicit(data_other, data_self) g_direct = direct(data_other, data_self) self.assertAllClose(g, g_direct, check_dtypes=False) 
utils_test.SplitStructureTest.test|prefix|zero|length def test_zero_length_prefix(self): prefix, suffix = utils.split_structure(tf.constant([1, 2, 3]), 0) self.assertAllEqual(prefix, tf.constant([])) self.assertAllEqual(suffix, tf.constant([1, 2, 3])) 
data_utils.Dataset.random|walk|print def print_random_walk(self, random_walk_outputs, batch_dict, num_cases=3): """Print the random walk outputs""" inputs = batch_dict['enc_inputs'][:3] references = batch_dict['references'][:3] for i in range(num_cases): print('inputs:') print('    ' + self.decode_sent(inputs[i])) for d in random_walk_outputs: print('->') print('    ' + self.decode_sent(d['predict'][i])) print('references:') for r in references[i]: print('    ' + self.decode_sent(r)) print('') return 
model.batch|graph|pack def batch_pack_graph(x, counts, num_rows): """Picks different number of values from each row in x depending on the values in counts. """ outputs = [] for i in range(num_rows): outputs.append(x[(i), :counts[i]]) return tf.concat(outputs, axis=0) 
model.PCGN_beamsearch.PCGNBeamSearchDecoder.initialize def initialize(self, name=None): """Initialize the decoder.  Args: name: Name scope for any created operations.  Returns: `(finished, start_inputs, initial_state)`. """ finished, start_inputs = self._finished, self._start_inputs initial_state = BeamSearchDecoderState(cell_state=self. _initial_cell_state, log_probs=array_ops.zeros([self._batch_size, self._beam_width], dtype=nest.flatten(self._initial_cell_state)[0]. dtype), finished=finished, lengths=array_ops.zeros([self. _batch_size, self._beam_width], dtype=dtypes.int64)) return finished, start_inputs, initial_state 
nets.nasnet.nasnet_utils.NasNetABaseCell.reduce|layer|prev def _reduce_prev_layer(self, prev_layer, curr_layer): """Matches dimension of prev_layer to the curr_layer.""" if prev_layer is None: return curr_layer curr_num_filters = self._filter_size prev_num_filters = get_channel_dim(prev_layer.shape) curr_filter_shape = int(curr_layer.shape[2]) prev_filter_shape = int(prev_layer.shape[2]) if curr_filter_shape != prev_filter_shape: prev_layer = tf.nn.relu(prev_layer) prev_layer = factorized_reduction(prev_layer, curr_num_filters, stride=2) elif curr_num_filters != prev_num_filters: prev_layer = tf.nn.relu(prev_layer) prev_layer = slim.conv2d(prev_layer, curr_num_filters, 1, scope= 'prev_1x1') prev_layer = slim.batch_norm(prev_layer, scope='prev_bn') return prev_layer 
utils.image|scan def image_scan(window_size, point, row, col, data, is_training=False): image = np.zeros((window_size + 1, window_size + 1, data.shape[-1])) for i in range(point[0] - window_size // 2, point[0] + window_size // 2 + 2 ): for j in range(point[1] - window_size // 2, point[1] + window_size // 2 + 2): if i >= 0 and i < row and j >= 0 and j < col: image[(i - point[0] + window_size // 2), (j - point[1] + window_size // 2), :] = data[(i), (j), :] else: image[(i - point[0] + window_size // 2), (j - point[1] + window_size // 2), :] = np.zeros([data.shape[-1]]) image_rot = image.copy() if is_training: k = random.randint(0, 4) for i in range(image_rot.shape[-1]): image_rot[:, :, (i)] = np.rot90(image_rot[:, :, (i)], k) image_flip = image.copy() for i in range(image_flip.shape[-1]): image_flip[:, :, (i)] = np.fliplr(image_flip[:, :, (i)]) return [image, image_rot, image_flip] else: return image 
amb_measure.DropMaskType1.sample|theta def sample_theta(self, hparams): noise_shape = self.get_noise_shape() mask = np.random.uniform(size=noise_shape) p = hparams.drop_prob mask = np.float32(mask >= p) / (1 - p) theta_val = np.ones(shape=self.batch_dims) theta_val = theta_val * mask return theta_val 
utils.data_utils.convert|example|single def convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer): """Converts a single `InputExample` into a single `InputFeatures`.""" label_map = {} for i, label in enumerate(label_list): label_map[label] = i tokens_a = tokenizer.tokenize(example.text_a) tokens_b = None if example.text_b: tokens_b = tokenizer.tokenize(example.text_b) if tokens_b: _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3) elif len(tokens_a) > max_seq_length - 2: tokens_a = tokens_a[0:max_seq_length - 2] tokens = [] segment_ids = [] tokens.append('[CLS]') segment_ids.append(0) for token in tokens_a: tokens.append(token) segment_ids.append(0) tokens.append('[SEP]') segment_ids.append(0) if tokens_b: for token in tokens_b: tokens.append(token) segment_ids.append(1) tokens.append('[SEP]') segment_ids.append(1) input_ids = tokenizer.convert_tokens_to_ids(tokens) input_mask = [1] * len(input_ids) while len(input_ids) < max_seq_length: input_ids.append(0) input_mask.append(0) segment_ids.append(0) assert len(input_ids) == max_seq_length assert len(input_mask) == max_seq_length assert len(segment_ids) == max_seq_length label_id = label_map[example.label] if ex_index < 0: tf.logging.info('*** Example ***') tf.logging.info('guid: %s' % example.guid) tf.logging.info('tokens: %s' % ' '.join([tokenization. printable_text(x) for x in tokens])) tf.logging.info('input_ids: %s' % ' '.join([str(x) for x in input_ids]) ) tf.logging.info('input_ids length: %d' % len(input_ids)) tf.logging.info('input_mask: %s' % ' '.join([str(x) for x in input_mask])) tf.logging.info('segment_ids: %s' % ' '.join([str(x) for x in segment_ids])) tf.logging.info('label: %s (id = %d)' % (example.label, label_id)) feature = InputFeatures(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, label_id=label_id) return feature 
official.resnet.resnet_preprocessing.x|at|equal|least|are def _at_least_x_are_equal(a, b, x): """At least `x` of `a` and `b` `Tensors` are equal.""" match = tf.equal(a, b) match = tf.cast(match, tf.int32) return tf.greater_equal(tf.reduce_sum(match), x) 
neural_style.preprocess def preprocess(img): imgpre = np.copy(img) imgpre = imgpre[(...), ::-1] imgpre = imgpre[(np.newaxis), :, :, :] imgpre -= np.array([123.68, 116.779, 103.939]).reshape((1, 1, 1, 3)) return imgpre 
cleverhans.model.Model.get|layer def get_layer(self, x, layer): """ Expose the hidden features of a model given a layer name. :param x: A symbolic representation of the network input :param layer: The name of the hidden layer to return features at. :return: A symbolic representation of the hidden features :raise: NoSuchLayerError if `layer` is not in the model. """ output = self.fprop(x) try: requested = output[layer] except KeyError: raise NoSuchLayerError() return requested 
texar.losses.entropy.sequence|logits|entropy|with def sequence_entropy_with_logits(logits, rank=None, sequence_length=None, average_across_batch=True, average_across_timesteps=False, average_across_remaining=False, sum_over_batch=False, sum_over_timesteps=True, sum_over_remaining=True, time_major=False): """Shannon entropy given logits.  Args: logits: Unscaled log probabilities of shape `[batch_size, max_time, d_3, ..., d_{rank-1}, distribution_dim]` and of dtype `float32` or `float64`.  The rank of the tensor is optionally specified by the argument :attr:`rank`.  The tensor is considered as having `[batch_size, .., d_{rank-1}]` elements, each of which has a distribution of length `d_rank` (i.e., `distribution_dim`). So the last dimension is always summed out to compute the entropy.  The batch and time dimensions are exchanged if :attr:`time_major` is `True`. rank (int, optional): The rank of :attr:`logits`. If `None` (default), `rank` is inferred automatically from `logits`. If the inference fails, `rank` is set to 3, i.e., assuming `logits` is of shape `[batch_size, max_time, distribution_dim]` sequence_length (optional): A Tensor of shape `[batch_size]`. Time steps beyond the respective sequence lengths are counted into the entropy. average_across_timesteps (bool): If set, average the entropy across the time dimension. Must not set `average_across_timesteps` and `sum_over_timesteps` at the same time. average_across_batch (bool): If set, average the entropy across the batch dimension. Must not set `average_across_batch`' and `sum_over_batch` at the same time. average_across_remaining (bool): If set, average the entropy across the remaining dimensions. Must not set `average_across_remaining`' and `sum_over_remaining` at the same time. Used only when :attr:`logits` has rank >= 4. sum_over_timesteps (bool): If set, sum the entropy across the time dimension. Must not set `average_across_timesteps` and `sum_over_timesteps` at the same time. sum_over_batch (bool): If set, sum the entropy across the batch dimension. Must not set `average_across_batch` and `sum_over_batch` at the same time. sum_over_remaining (bool): If set, sum the entropy across the remaining dimension. Must not set `average_across_remaining` and `sum_over_remaining` at the same time. Used only when :attr:`logits` has rank >= 4. time_major (bool): The shape format of the inputs. If `True`, :attr:`logits` must have shape `[max_time, batch_size, ...]`. If `False` (default), it must have shape `[batch_size, max_time, ...]`.  Returns: A Tensor containing the shannon entropy. The dimensionality of the Tensor depends on the configuration of reduction arguments. For example, if batch, time, and remaining dimensions are all reduced (by either sum or average), the returned Tensor is a scalar Tensor. """ entropy = _get_entropy(logits) if rank is None: rank = get_rank(logits) if rank is None: rank = 3 rank -= 1 entropy = mask_and_reduce(entropy, sequence_length, rank=rank, average_across_batch=average_across_batch, average_across_timesteps =average_across_timesteps, average_across_remaining= average_across_remaining, sum_over_batch=sum_over_batch, sum_over_timesteps=sum_over_timesteps, sum_over_remaining= sum_over_remaining, time_major=time_major) return entropy 
vkge.training.constraints.renorm|update|clip def renorm_update_clip(log_var_matrix, norm=1.0, axis=0): var_matrix = tf.exp(log_var_matrix) scaled = tf.clip_by_norm(var_matrix, 1.0, axes=1) scaled = tf.log(scaled) return tf.assign(log_var_matrix, scaled) 
nmt.async_checkpoint.AsyncCheckpointSaverHook.Hook|Async|Checkpoint|Saver def __init__(self, checkpoint_dir, save_secs=None, save_steps=None, saver= None, checkpoint_basename='model.ckpt', scaffold=None, listeners=None): """Initializes a `CheckpointSaverHook`.  Args: checkpoint_dir: `str`, base directory for the checkpoint files. save_secs: `int`, save every N secs. save_steps: `int`, save every N steps. saver: `Saver` object, used for saving. checkpoint_basename: `str`, base name for the checkpoint files. scaffold: `Scaffold`, use to get saver object. listeners: List of `CheckpointSaverListener` subclass instances. Used for callbacks that run immediately before or after this hook saves the checkpoint.  Raises: ValueError: One of `save_steps` or `save_secs` should be set. ValueError: At most one of `saver` or `scaffold` should be set. """ logging.info('Create AsyncCheckpointSaverHook.') if saver is not None and scaffold is not None: raise ValueError('You cannot provide both saver and scaffold.') self._saver = saver self._save_thread = None self._write_graph_thread = None self._checkpoint_dir = checkpoint_dir self._save_path = os.path.join(checkpoint_dir, checkpoint_basename) self._scaffold = scaffold self._timer = basic_session_run_hooks.SecondOrStepTimer(every_secs= save_secs, every_steps=save_steps) self._listeners = listeners or [] self._steps_per_run = 1 self._summary_writer = None self._global_step_tensor = None 
tests.models.FNN_test.without|test|FNN|seq @pytest.mark.parametrize('sparse_feature_num,dense_feature_num', [(0, 1), ( 1, 0)]) def test_FNN_without_seq(sparse_feature_num, dense_feature_num): model_name = 'FNN' sample_size = SAMPLE_SIZE x, y, feature_dim_dict = get_test_data(sample_size, sparse_feature_num, dense_feature_num, sequence_feature=()) model = FNN(feature_dim_dict, dnn_hidden_units=[32, 32], dnn_dropout=0.5) check_model(model, model_name, x, y) 
agents.DuelingLSTMDQNNet.torso def _torso(self, prev_action, env_output): conv_out = self._body(env_output.observation) one_hot_prev_action = tf.one_hot(prev_action, self._num_actions) return tf.concat([conv_out, tf.expand_dims(env_output.reward, -1), one_hot_prev_action], axis=1) 
ge.classify.Classifier.evaluate def evaluate(self, X, Y): top_k_list = [len(l) for l in Y] Y_ = self.predict(X, top_k_list) Y = self.binarizer.transform(Y) averages = ['micro', 'macro', 'samples', 'weighted'] results = {} for average in averages: results[average] = f1_score(Y, Y_, average=average) results['acc'] = accuracy_score(Y, Y_) print('-------------------') print(results) return results print('-------------------') 
train_dataset.DataLoader.epoch.task|handle def handle_task(indices): """Constructs a minibatch with the given indices.""" thread_id = threading.get_ident() if not thread_id in _darcs: _darcs[thread_id] = {} judges, p0s, p1s, refs = [], [], [], [] for i in indices: dataset, index = self._getDatasetByIndex(i) dataset_path = dataset.getDarcPath() if not dataset_path in _darcs[thread_id]: _darcs[thread_id][dataset_path] = darc.DataArchive(dataset_path) db = _darcs[thread_id][dataset_path] p_data = db['{}_p'.format(index)] judge_data = db['{}_judge'.format(index)] judges.append(judge_data.data()) p0s.append(p_data[(0), :, :, :]) p1s.append(p_data[(1), :, :, :]) refs.append(p_data[(2), :, :, :]) return {'judge': np.concatenate(judges), 'p0': np.stack(p0s), 'p1': np. stack(p1s), 'ref': np.stack(refs)} 
generate-raddstoreexpminusmax-test.ukernel|split|name def split_ukernel_name(name): match = re.match( '^xnn_(f16|f32)_raddstoreexpminusmax_ukernel__(.+)_x(\\d+)(_acc(\\d+))?$' , name) if match is None: raise ValueError('Unexpected microkernel name: ' + name) elements_tile = int(match.group(3)) arch, isa = xnncommon.parse_target_name(target_name=match.group(2)) return elements_tile, arch, isa 
vgg16.VGG16.get_params_and_calculation_from_channel_num.get|size|input def get_input_size(index): size = ori_size if not isinstance(size, int): size = size[0] if index >= 0 and index <= 1: return size elif index >= 2 and index <= 3: return size / 2 elif index >= 4 and index <= 6: return size / 4 elif index >= 7 and index <= 9: return size / 8 elif index >= 10 and index <= 12: return size / 16 elif index >= 13: return size / 32 return size 
src.data_util.data_gen.DataGen.gen def gen(self, batch_size): valid_target_len = self.valid_target_len with open(self.annotation_path, 'r') as ann_file: lines = ann_file.readlines() random.shuffle(lines) for l in lines: img_path, lex = l.strip().split() try: img_bw, word = self.read_data(img_path, lex) if valid_target_len < float('inf'): word = word[:valid_target_len + 1] width = img_bw.shape[-1] b_idx = min(width, self.bucket_max_width) bs = self.bucket_data[b_idx].append(img_bw, word, os.path. join(self.data_root, img_path)) if bs >= batch_size: b = self.bucket_data[b_idx].flush_out(self.bucket_specs, valid_target_length=valid_target_len, go_shift=1) if b is not None: yield b else: assert False, 'no valid bucket of width %d' % width except IOError: pass self.clear() 
EndToEndClassification.EnvClassification.loader.ClassifierLoader.load|batch def load_batch(self, batch_indices): """ Loads a single batch from the trainset.  Args: batch_indices (list): indices for the batch.  Returns: (np.array): batch examples. (np.array): batch labels. """ return self.train[0][batch_indices], self.train[1][batch_indices] 
cnn_helpers.make|concat def make_concat(op_name, axis, in_tensors): with tf.name_scope(op_name): return tf.concat(axis=axis, values=in_tensors, name=op_name) 
kaffe.tensorflow.network.Network.var|make def make_var(self, name, shape): """Creates a new TensorFlow variable.""" return tf.get_variable(name, shape, trainable=self.trainable) 
nets.cyclegan_test.CycleganTest.four|multiple|height|test|of|if|not|error def test_error_if_height_not_multiple_of_four_height30(self): self._error_if_height_not_multiple_of_four_helper(30) 
deeplab_resnet.image_reader_segment.get|shape|label def get_label_shape(label): """ Returns the shape of a tensor """ shape = np.asarray(label.shape) return shape 
thumt.utils.lrp.LegacyGRUCell_decoder_v2n.size|state @property def state_size(self): return self._num_units 
official.utils.flags.flags_test.BaseTester.parse|test|info|dtype def test_parse_dtype_info(self): for dtype_str, tf_dtype, loss_scale in [['fp16', tf.float16, 128], [ 'fp32', tf.float32, 1]]: flags_core.parse_flags([__file__, '--dtype', dtype_str]) self.assertEqual(flags_core.get_tf_dtype(flags.FLAGS), tf_dtype) self.assertEqual(flags_core.get_loss_scale(flags.FLAGS), loss_scale) flags_core.parse_flags([__file__, '--dtype', dtype_str, '--loss_scale', '5']) self.assertEqual(flags_core.get_loss_scale(flags.FLAGS), 5) with self.assertRaises(SystemExit): flags_core.parse_flags([__file__, '--dtype', 'int8']) 
nmt.low_level_runner.TrainLowLevelRunner.initialize.get_enqueue_ops_fn.enqueue_ops_fn_v1.tpu|ordinal|fn def tpu_ordinal_fn(shard_index_in_host): return shard_index_in_host % self.hparams.num_shards_per_host 
craystack.codecs.repeat.pop def pop(message): symbols = [] for i in range(n): message, symbol = pop_(message) symbols.append(symbol) return message, symbols 
transformer.MultiHeadAttention.heads|split|into def split_into_heads(self, x, batch_size): x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth)) return tf.transpose(x, perm=[0, 2, 1, 3]) 
models.attacks.CarliniWagnerL2.generate.cw|wrap def cw_wrap(x_val, y_val): return np.array(attack.attack(x_val, y_val), dtype=np.float32) 
texar.data.data.dataset_utils.transformation|make|chained def make_chained_transformation(tran_fns, *args, **kwargs): """Returns a dataset transformation function that applies a list of transformations sequentially.  Args: tran_fns (list): A list of dataset transformation function. *args: Extra arguments for each of the transformation function. **kwargs: Extra keyword arguments for each of the transformation function.  Returns: A transformation function to be used in :tf_main:`tf.data.Dataset.map <data/Dataset#map>`. """  def _chained_fn(data): for tran_fns_i in tran_fns: data = tran_fns_i(data, *args, **kwargs) return data return _chained_fn 
nn_layer.rnn|dynamic|bi|diff def bi_dynamic_rnn_diff(cell, inputs_fw, inputs_bw, n_hidden, l_fw, l_bw, max_len, scope_name): with tf.name_scope('forward_lstm'): outputs_fw, state_fw = tf.nn.dynamic_rnn(cell(n_hidden), inputs= inputs_fw, sequence_length=l_fw, dtype=tf.float32, scope=scope_name ) batch_size = tf.shape(outputs_fw)[0] index = tf.range(0, batch_size) * max_len + (l_fw - 1) output_fw = tf.gather(tf.reshape(outputs_fw, [-1, n_hidden]), index) with tf.name_scope('backward_lstm'): outputs_bw, state_bw = tf.nn.dynamic_rnn(cell(n_hidden), inputs= inputs_bw, sequence_length=l_bw, dtype=tf.float32, scope=scope_name ) batch_size = tf.shape(outputs_bw)[0] index = tf.range(0, batch_size) * max_len + (l_bw - 1) output_bw = tf.gather(tf.reshape(outputs_bw, [-1, n_hidden]), index) outputs = tf.concat([output_fw, output_bw], 1) return outputs 
nets.dcgan_test.DCGANTest.test|input|invalid|generator def test_generator_invalid_input(self): wrong_dim_input = tf.zeros([5, 32, 32]) with self.assertRaises(ValueError): dcgan.generator(wrong_dim_input) correct_input = tf.zeros([3, 2]) with self.assertRaisesRegexp(ValueError, 'must be a power of 2'): dcgan.generator(correct_input, final_size=30) with self.assertRaisesRegexp(ValueError, 'must be greater than 8'): dcgan.generator(correct_input, final_size=4) 
preprocess.preprocess|dynamic|unlab def preprocess_dynamic_unlab(dir, extraction_step, patch_shape, num_images_training_unlab): T1_vols = np.empty((num_images_training_unlab, 144, 192, 256), dtype= 'float32') T2_vols = np.empty((num_images_training_unlab, 144, 192, 256), dtype= 'float32') for case_idx in range(11, 11 + num_images_training_unlab): T1_vols[(case_idx - 11), :, :, :] = read_vol(case_idx, 'T1', dir) T2_vols[(case_idx - 11), :, :, :] = read_vol(case_idx, 'T2', dir) T1_mean = T1_vols.mean() T1_std = T1_vols.std() T1_vols = (T1_vols - T1_mean) / T1_std T2_mean = T2_vols.mean() T2_std = T2_vols.std() T2_vols = (T2_vols - T2_mean) / T2_std for i in range(T1_vols.shape[0]): T1_vols[i] = (T1_vols[i] - np.min(T1_vols[i])) / (np.max(T1_vols[i] ) - np.min(T1_vols[i])) * 255 for i in range(T2_vols.shape[0]): T2_vols[i] = (T2_vols[i] - np.min(T2_vols[i])) / (np.max(T2_vols[i] ) - np.min(T2_vols[i])) * 255 T1_vols = T1_vols / 127.5 - 1.0 T2_vols = T2_vols / 127.5 - 1.0 x = get_patches_unlab(T1_vols, T2_vols, extraction_step, patch_shape, dir) print('Total Extracted Unlabelled Patches Shape:', x.shape) return x 
nets.pix2pix.discriminator|pix def pix2pix_discriminator(net, num_filters, padding=2, is_training=False): """Creates the Image2Image Translation Discriminator.  Args: net: A `Tensor` of size [batch_size, height, width, channels] representing the input. num_filters: A list of the filters in the discriminator. The length of the list determines the number of layers in the discriminator. padding: Amount of reflection padding applied before each convolution. is_training: Whether or not the model is training or testing.  Returns: A logits `Tensor` of size [batch_size, N, N, 1] where N is the number of 'patches' we're attempting to discriminate and a dictionary of model end points. """ del is_training end_points = {} num_layers = len(num_filters)  def padded(net, scope): if padding: with tf.variable_scope(scope): spatial_pad = tf.constant([[0, 0], [padding, padding], [ padding, padding], [0, 0]], dtype=tf.int32) return tf.pad(net, spatial_pad, 'REFLECT') else: return net with tf.contrib.framework.arg_scope([layers.conv2d], kernel_size=[4, 4], stride=2, padding='valid', activation_fn=tf.nn.leaky_relu): net = layers.conv2d(padded(net, 'conv0'), num_filters[0], normalizer_fn=None, scope='conv0') end_points['conv0'] = net for i in range(1, num_layers - 1): net = layers.conv2d(padded(net, 'conv%d' % i), num_filters[i], scope='conv%d' % i) end_points['conv%d' % i] = net net = layers.conv2d(padded(net, 'conv%d' % (num_layers - 1)), num_filters[-1], stride=1, scope='conv%d' % (num_layers - 1)) end_points['conv%d' % (num_layers - 1)] = net logits = layers.conv2d(padded(net, 'conv%d' % num_layers), 1, stride=1, activation_fn=None, normalizer_fn=None, scope= 'conv%d' % num_layers) end_points['logits'] = logits end_points['predictions'] = tf.sigmoid(logits) return logits, end_points 
user.main def main(): parser = argparse.ArgumentParser() parser.add_argument('--top_k', '-k', help='specify the top k to defeat') k = int(args.top_k) albumSize_max = 400 rank_range = 16 hm_path = '../dataset/' locations_path = hm_path + 'Flickr_album/' locations = os.listdir(locations_path) for location in locations: location_path = locations_path + location albums_path = glob.glob(location_path + '/' + 'iter_*.npz') num_deletion_list = [] num_albums = len(albums_path) for i in range(len(albums_path)): album_path = albums_path[i] f = np.load(album_path) album_this = f['albumFea'] true_winner = f['label'] album_id = f['album_id'] album_size, _ = np.shape(album_this) if album_size < rank_range: continue album_score = np.mean(album_this, axis=0) diff = np.transpose(np.transpose(album_this) - album_this[:, ( true_winner)]) diff = np.delete(diff, true_winner, axis=1) keep_ids_rank = get_conf_photo(album_this, true_winner, rank_range) rank_deletion_list = [] for j in range(len(keep_ids_rank)): keep_id = keep_ids_rank[j] num_deletion = IP(diff, k, albumSize_max, keep_id) frac_deletion = num_deletion / np.float(album_size) rank_deletion_list.append(frac_deletion) for ele in rank_deletion_list: sys.stdout.write('%.4f,' % ele) sys.stdout.write('\n') sys.stdout.flush() 
pathfinder.graph_construction.save_cpnet.not|save def not_save(cpt): if cpt in blacklist: return True for t in cpt.split('_'): if t in nltk_stopwords: return True return False 
classification.ops.loss_functions.NormalMeanVarianceNegativeLogProbLoss.shape|static|factor|inner|hessian @property def hessian_factor_inner_static_shape(self): raise NotImplementedError() 
cpplint._CppLintState.Set|Style|Counting def SetCountingStyle(self, counting_style): """Sets the module's counting options.""" self.counting = counting_style 
texar.data.data_utils.count_file_lines.lines|count def _count_lines(fn): with open(fn, 'rb') as f: i = -1 for i, _ in enumerate(f): pass return i + 1 
build_imagenet_data.png|is def _is_png(filename): """Determine if a file contains a PNG format image.  Args: filename: string, path of the image file.  Returns: boolean indicating if the image is a PNG. """ return 'n02105855_2933.JPEG' in filename 
official.utils.misc.model_helpers.synthetic|generate|data def generate_synthetic_data(input_shape, input_value=0, input_dtype=None, label_shape=None, label_value=0, label_dtype=None): """Create a repeating dataset with constant values.  Args: input_shape: a tf.TensorShape object or nested tf.TensorShapes. The shape of the input data. input_value: Value of each input element. input_dtype: Input dtype. If None, will be inferred by the input value. label_shape: a tf.TensorShape object or nested tf.TensorShapes. The shape of the label data. label_value: Value of each input element. label_dtype: Input dtype. If None, will be inferred by the target value.  Returns: Dataset of tensors or tuples of tensors (if label_shape is set). """ element = input_element = nest.map_structure(lambda s: tf.constant( input_value, input_dtype, s), input_shape) if label_shape: label_element = nest.map_structure(lambda s: tf.constant( label_value, label_dtype, s), label_shape) element = input_element, label_element return tf.data.Dataset.from_tensors(element).repeat() 
shufflebuffer.ShuffleBufferTest.extract|test def test_extract(self): sb = ShuffleBuffer(3, 1) r = sb.extract() assert r == None, r r = sb.insert_or_replace(b'111') assert r == None, r r = sb.extract() assert r == b'111', r r = sb.extract() assert r == None, r 
regression.controller.sample.NormalOutSample.Out|Sample|Normal def __init__(self, params=None): super(NormalOutSample, self).__init__(params, ['y_prec', 'y'], 'regression' ) 
chunkparser.ChunkParser.task def task(self, chunkdatasrc, writer): """ Run in fork'ed process, read data from chunkdatasrc, parsing, shuffling and sending v2 data through pipe back to main process. """ self.init_structs() while True: chunkdata = chunkdatasrc.next() if chunkdata is None: break for item in self.convert_chunkdata_to_v2(chunkdata): symmetry = random.randrange(8) item = self.v2_apply_symmetry(symmetry, item) writer.send_bytes(item) 
neural_tangents.predict.momentum.get|fn def get_fn(state): train_size, state = state train, test = state[:train_size], state[train_size:] return ufl(np.split(train, 2)[0]), ufl(np.split(test, 2)[0]) 
preprocess.prepare_d2d.query|and|sim|per|kernel|mat def sim_mat_and_kernel_per_query(relevance_dict, topic_dict, corpus, topk_corpus, embeddings, stoplist, sim_output_path, kernel_output_path, kernel_mu_list, kernel_sigma_list, topk_supervised, d2d, test, qid): relevance = relevance_dict.get(qid) topic_content = topic_dict.get(qid) supervised_docid_list = relevance.get_supervised_docid_list() topk_supervised_docid_list = supervised_docid_list[:topk_supervised] if len(topk_supervised_docid_list) < topk_supervised: logging.warn( '{0} does not have enough supervised documents, in total {1}.'. format(qid, len(topk_supervised_docid_list))) sim_output_dir = make_directory(sim_output_path, str(qid)) ker_output_dir = make_directory(kernel_output_path, str(qid)) OOV_dict = OrderedDict() judged_docid_list = relevance.get_judged_docid_list() """ because we only want to rerank top 500 docs, but judged docs that lie in in top 1000 should also be considered, for the sufficiency of training """ cand = judged_docid_list[0] + judged_docid_list[1] + judged_docid_list[2] waitlist = [docid for docid in cand if docid in supervised_docid_list[ 500:2000]] useful_docid_list = supervised_docid_list[:1000] for docid in useful_docid_list: sim_mat_list = [] ker_list = [] doc_content = parse_topk_content(corpus[docid]) if d2d: sim_file_name = os.path.join(sim_output_dir, 'q{0}_d{1}.pickle' .format(qid, docid)) else: sim_file_name = os.path.join(sim_output_dir, 'q{0}_d{1}.npy'. format(qid, docid)) ker_file_name = os.path.join(ker_output_dir, 'q{0}_d{1}.npy'.format (qid, docid)) if os.path.exists(ker_file_name): pass elif d2d: for sup_docid in topk_supervised_docid_list: sup_doc_content = parse_topk_content(topk_corpus[sup_docid])[: 30] sim_mat = similarity_matrix(sup_doc_content, doc_content, embeddings, OOV_dict)[:, :20000] kernel_feat = kernel_from_matrix(sim_mat, kernel_mu_list, kernel_sigma_list, d2d) sim_mat_list.append(sim_mat.astype(np.float16)) ker_list.append(kernel_feat) if test == True: print(qid, docid, sup_docid) print(doc_content) print(sup_doc_content) assert 1 == 2 ker_list = np.asarray(ker_list) save_pickle(sim_mat_list, sim_file_name) np.save(ker_file_name, ker_list) else: if test == True: print(qid, docid) print(topic_content) print(doc_content) assert 1 == 2 sim_mat = similarity_matrix(topic_content, doc_content, embeddings, OOV_dict) kernel_feat = kernel_from_matrix(sim_mat, kernel_mu_list, kernel_sigma_list, d2d) np.save(sim_file_name, sim_mat) np.save(ker_file_name, kernel_feat) logging.info('Finish for topic {0}'.format(qid)) 
text_gcn-master.layers.sparse|dropout def sparse_dropout(x, keep_prob, noise_shape): """Dropout for sparse tensors.""" random_tensor = keep_prob random_tensor += tf.random_uniform(noise_shape) dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool) pre_out = tf.sparse_retain(x, dropout_mask) return pre_out * (1.0 / keep_prob) 
models.yae.Autoencoder.Autoencoder def __init__(self, batch_size, channels=1, conv_filters=8, style_size=32, content_size=10, ksize=(3, 3), start_iteration=0, dir_header='./', wdecay=0.0): """Init method @param sess (tf.Session) the current session @param conv_filters_* (int) the number of filters in the convolutional layers @param code_size (int) the number of units in the code layer @param gradient_clip (bool) applies gradient clipping on the gradient vector """ self.dir_header = dir_header self.start_iteration = start_iteration self.channels = channels weight_initializer = None weight_initializer_implicit = tf.random_normal_initializer(mean=0.0, stddev=0.01) bias_initializer_implicit = tf.constant_initializer(-5.0) if wdecay > 0.0: regularizer = tf.contrib.layers.l2_regularizer(wdecay) else: regularizer = None with tf.variable_scope('Input', reuse=False): self.x = tf.placeholder(tf.float32, [batch_size, 32, 32, self.channels] ) self.labels_placeholder = tf.placeholder(tf.int64, [batch_size]) with tf.variable_scope('Encoder', reuse=False): conv_1 = tf.layers.conv2d(inputs=self.x, filters=conv_filters, strides=(2, 2), kernel_size=ksize, padding='same', activation= None, kernel_regularizer=regularizer, kernel_initializer= weight_initializer, name='conv_1') conv_1 = tf.layers.batch_normalization(conv_1, axis=-1, momentum= 0.99, epsilon=0.001, name='norm_1') conv_1 = tf.nn.leaky_relu(conv_1, name='relu_1') conv_2 = tf.layers.conv2d(inputs=conv_1, filters=conv_filters * 2, strides=(2, 2), kernel_size=ksize, padding='same', activation= None, kernel_regularizer=regularizer, kernel_initializer= weight_initializer, name='conv_2') conv_2 = tf.layers.batch_normalization(conv_2, axis=-1, momentum= 0.99, epsilon=0.001, name='norm_2') conv_2 = tf.nn.leaky_relu(conv_2, name='relu_2') conv_3 = tf.layers.conv2d(inputs=conv_2, filters=conv_filters * 4, strides=(2, 2), kernel_size=ksize, padding='same', activation= None, kernel_regularizer=regularizer, kernel_initializer= weight_initializer, name='conv_3') conv_3 = tf.layers.batch_normalization(conv_3, axis=-1, momentum= 0.99, epsilon=0.001, name='norm_3') conv_3 = tf.nn.leaky_relu(conv_3, name='relu_3') conv_4 = tf.layers.conv2d(inputs=conv_3, filters=conv_filters * 4, strides=(2, 2), kernel_size=ksize, padding='same', activation= None, kernel_regularizer=regularizer, kernel_initializer= weight_initializer, name='conv_4') conv_4 = tf.layers.batch_normalization(conv_4, axis=-1, momentum= 0.99, epsilon=0.001, name='norm_4') conv_4 = tf.nn.leaky_relu(conv_4, name='relu_4') conv_5 = tf.layers.conv2d(inputs=conv_4, filters=style_size, strides=(2, 2), kernel_size=ksize, padding='same', activation= None, kernel_regularizer=regularizer, kernel_initializer= weight_initializer_implicit, bias_initializer= bias_initializer_implicit, name='conv_5') self.code_style = tf.nn.sigmoid(tf.squeeze(conv_5), name='code_style') conv_6 = tf.layers.conv2d(inputs=conv_4, filters=content_size, strides=(2, 2), kernel_size=ksize, padding='same', activation= None, kernel_regularizer=regularizer, kernel_initializer= weight_initializer, name='conv_6') self.code_content_logits = tf.squeeze(conv_6) self.code_content = tf.nn.softmax(self.code_content_logits, name= 'code_content') with tf.variable_scope('Decoder', reuse=False): self.left_code_content_deterministic = tf.one_hot(indices=self. labels_placeholder, depth=content_size) left_code = tf.concat([self.code_style, self. left_code_content_deterministic], axis=1) left_code_reshaped = tf.reshape(left_code, [batch_size, 1, 1, style_size + content_size]) left_deconv_1 = tf.layers.conv2d_transpose(left_code_reshaped, filters=conv_filters * 4, kernel_size=ksize, strides=(2, 2), padding='same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='deconv_1') left_deconv_1 = tf.layers.batch_normalization(left_deconv_1, axis=- 1, momentum=0.99, epsilon=0.001, name='norm_1') left_deconv_1 = tf.nn.leaky_relu(left_deconv_1, name='relu_1') left_deconv_2 = tf.layers.conv2d_transpose(left_deconv_1, filters= conv_filters * 4, kernel_size=ksize, strides=(2, 2), padding= 'same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='deconv_2') left_deconv_2 = tf.layers.batch_normalization(left_deconv_2, axis=- 1, momentum=0.99, epsilon=0.001, name='norm_2') left_deconv_2 = tf.nn.leaky_relu(left_deconv_2, name='relu_2') left_deconv_3 = tf.layers.conv2d_transpose(left_deconv_2, filters= conv_filters * 4, kernel_size=ksize, strides=(2, 2), padding= 'same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='deconv_3') left_deconv_3 = tf.layers.batch_normalization(left_deconv_3, axis=- 1, momentum=0.99, epsilon=0.001, name='norm_3') left_deconv_3 = tf.nn.leaky_relu(left_deconv_3, name='relu_3') left_deconv_4 = tf.layers.conv2d_transpose(left_deconv_3, filters= conv_filters * 2, kernel_size=ksize, strides=(2, 2), padding= 'same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='deconv_4') left_deconv_4 = tf.layers.batch_normalization(left_deconv_4, axis=- 1, momentum=0.99, epsilon=0.001, name='norm_4') left_deconv_4 = tf.nn.leaky_relu(left_deconv_4, name='relu_4') left_deconv_5 = tf.layers.conv2d_transpose(left_deconv_4, filters= conv_filters, kernel_size=ksize, strides=(2, 2), padding='same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='deconv_5') left_deconv_5 = tf.layers.batch_normalization(left_deconv_5, axis=- 1, momentum=0.99, epsilon=0.001, name='norm_5') left_deconv_5 = tf.nn.leaky_relu(left_deconv_5, name='relu_5') left_deconv_6 = tf.layers.conv2d_transpose(left_deconv_5, filters= self.channels, kernel_size=ksize, strides=(1, 1), padding= 'same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='deconv_6') self.left_output = tf.nn.sigmoid(left_deconv_6, name='output') with tf.variable_scope('Encoder', reuse=True): left_conv_1 = tf.layers.conv2d(inputs=self.left_output, filters= conv_filters, strides=(2, 2), kernel_size=ksize, padding='same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='conv_1') left_conv_1 = tf.layers.batch_normalization(left_conv_1, axis=-1, momentum=0.99, epsilon=0.001, name='norm_1') left_conv_1 = tf.nn.leaky_relu(left_conv_1, name='relu_1') left_conv_2 = tf.layers.conv2d(inputs=left_conv_1, filters= conv_filters * 2, strides=(2, 2), kernel_size=ksize, padding= 'same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='conv_2') left_conv_2 = tf.layers.batch_normalization(left_conv_2, axis=-1, momentum=0.99, epsilon=0.001, name='norm_2') left_conv_2 = tf.nn.leaky_relu(left_conv_2, name='relu_2') left_conv_3 = tf.layers.conv2d(inputs=left_conv_2, filters= conv_filters * 4, strides=(2, 2), kernel_size=ksize, padding= 'same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='conv_3') left_conv_3 = tf.layers.batch_normalization(left_conv_3, axis=-1, momentum=0.99, epsilon=0.001, name='norm_3') left_conv_3 = tf.nn.leaky_relu(left_conv_3, name='relu_3') left_conv_4 = tf.layers.conv2d(inputs=left_conv_3, filters= conv_filters * 4, strides=(2, 2), kernel_size=ksize, padding= 'same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='conv_4') left_conv_4 = tf.layers.batch_normalization(left_conv_4, axis=-1, momentum=0.99, epsilon=0.001, name='norm_4') left_conv_4 = tf.nn.leaky_relu(left_conv_4, name='relu_4') left_conv_5 = tf.layers.conv2d(inputs=left_conv_4, filters= style_size, strides=(2, 2), kernel_size=ksize, padding='same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer_implicit, bias_initializer=bias_initializer_implicit, name='conv_5') self.left_code_style = tf.nn.sigmoid(tf.squeeze(left_conv_5), name= 'code_style') left_conv_6 = tf.layers.conv2d(inputs=left_conv_4, filters= content_size, strides=(2, 2), kernel_size=ksize, padding='same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='conv_6') self.left_code_content_logits = tf.squeeze(left_conv_6) self.left_code_content = tf.nn.softmax(self. left_code_content_logits, name='code_content') self.left_code_content_argmax = tf.argmax(self.left_code_content, axis=1) with tf.variable_scope('Decoder', reuse=True): self.right_code_content_random = tf.one_hot(indices=tf. random_uniform(shape=[batch_size], minval=0, maxval= content_size, dtype=tf.int32), depth=content_size) right_code = tf.concat([self.code_style, self. right_code_content_random], axis=1) right_code_reshaped = tf.reshape(right_code, [batch_size, 1, 1, style_size + content_size]) right_deconv_1 = tf.layers.conv2d_transpose(right_code_reshaped, filters=conv_filters * 4, kernel_size=ksize, strides=(2, 2), padding='same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='deconv_1') right_deconv_1 = tf.layers.batch_normalization(right_deconv_1, axis =-1, momentum=0.99, epsilon=0.001, name='norm_1') right_deconv_1 = tf.nn.leaky_relu(right_deconv_1, name='relu_1') right_deconv_2 = tf.layers.conv2d_transpose(right_deconv_1, filters =conv_filters * 4, kernel_size=ksize, strides=(2, 2), padding= 'same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='deconv_2') right_deconv_2 = tf.layers.batch_normalization(right_deconv_2, axis =-1, momentum=0.99, epsilon=0.001, name='norm_2') right_deconv_2 = tf.nn.leaky_relu(right_deconv_2, name='relu_2') right_deconv_3 = tf.layers.conv2d_transpose(right_deconv_2, filters =conv_filters * 4, kernel_size=ksize, strides=(2, 2), padding= 'same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='deconv_3') right_deconv_3 = tf.layers.batch_normalization(right_deconv_3, axis =-1, momentum=0.99, epsilon=0.001, name='norm_3') right_deconv_3 = tf.nn.leaky_relu(right_deconv_3, name='relu_3') right_deconv_4 = tf.layers.conv2d_transpose(right_deconv_3, filters =conv_filters * 2, kernel_size=ksize, strides=(2, 2), padding= 'same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='deconv_4') right_deconv_4 = tf.layers.batch_normalization(right_deconv_4, axis =-1, momentum=0.99, epsilon=0.001, name='norm_4') right_deconv_4 = tf.nn.leaky_relu(right_deconv_4, name='relu_4') right_deconv_5 = tf.layers.conv2d_transpose(right_deconv_4, filters =conv_filters, kernel_size=ksize, strides=(2, 2), padding= 'same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='deconv_5') right_deconv_5 = tf.layers.batch_normalization(right_deconv_5, axis =-1, momentum=0.99, epsilon=0.001, name='norm_5') right_deconv_5 = tf.nn.leaky_relu(right_deconv_5, name='relu_5') right_deconv_6 = tf.layers.conv2d_transpose(right_deconv_5, filters =self.channels, kernel_size=ksize, strides=(1, 1), padding= 'same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='deconv_6') self.right_output = tf.nn.sigmoid(right_deconv_6, name='output') with tf.variable_scope('Encoder', reuse=True): right_conv_1 = tf.layers.conv2d(inputs=self.right_output, filters= conv_filters, strides=(2, 2), kernel_size=ksize, padding='same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='conv_1') right_conv_1 = tf.layers.batch_normalization(right_conv_1, axis=-1, momentum=0.99, epsilon=0.001, name='norm_1') right_conv_1 = tf.nn.leaky_relu(right_conv_1, name='relu_1') right_conv_2 = tf.layers.conv2d(inputs=right_conv_1, filters= conv_filters * 2, strides=(2, 2), kernel_size=ksize, padding= 'same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='conv_2') right_conv_2 = tf.layers.batch_normalization(right_conv_2, axis=-1, momentum=0.99, epsilon=0.001, name='norm_2') right_conv_2 = tf.nn.leaky_relu(right_conv_2, name='relu_2') right_conv_3 = tf.layers.conv2d(inputs=right_conv_2, filters= conv_filters * 4, strides=(2, 2), kernel_size=ksize, padding= 'same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='conv_3') right_conv_3 = tf.layers.batch_normalization(right_conv_3, axis=-1, momentum=0.99, epsilon=0.001, name='norm_3') right_conv_3 = tf.nn.leaky_relu(right_conv_3, name='relu_3') right_conv_4 = tf.layers.conv2d(inputs=right_conv_3, filters= conv_filters * 4, strides=(2, 2), kernel_size=ksize, padding= 'same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='conv_4') right_conv_4 = tf.layers.batch_normalization(right_conv_4, axis=-1, momentum=0.99, epsilon=0.001, name='norm_4') right_conv_4 = tf.nn.leaky_relu(right_conv_4, name='relu_4') right_conv_5 = tf.layers.conv2d(inputs=right_conv_4, filters= style_size, strides=(2, 2), kernel_size=ksize, padding='same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer_implicit, bias_initializer=bias_initializer_implicit, name='conv_5') self.right_code_style = tf.nn.sigmoid(tf.squeeze(right_conv_5), name='code_style') right_conv_6 = tf.layers.conv2d(inputs=right_conv_4, filters= content_size, strides=(2, 2), kernel_size=ksize, padding='same', activation=None, kernel_regularizer=regularizer, kernel_initializer=weight_initializer, name='conv_6') self.right_code_content_logits = tf.squeeze(right_conv_6) self.right_code_content = tf.nn.softmax(self. right_code_content_logits, name='code_content') with tf.variable_scope('Training'): self.loss_reconstruction = tf.reduce_mean(tf.square(tf.subtract( self.x, self.left_output))) self.loss_classification = tf.losses.softmax_cross_entropy( onehot_labels=self.left_code_content_deterministic, logits=self .code_content_logits) self.accuracy_classification = tf.reduce_mean(tf.cast(tf.math.equal (self.labels_placeholder, self.left_code_content_argmax), dtype =tf.float32)) self.loss_explicit = tf.losses.softmax_cross_entropy(onehot_labels= self.right_code_content_random, logits=self. right_code_content_logits) self.loss_implicit = tf.reduce_mean(tf.norm(tf.subtract(self. right_code_style, self.left_code_style) + 1e-15, ord=2, axis=1)) self.lambda_e = tf.placeholder(tf.float32) self.lambda_i = tf.placeholder(tf.float32) self.loss = (self.loss_reconstruction + self.loss_classification + tf.multiply(self.lambda_e, self.loss_explicit) + tf.multiply( self.lambda_i, self.loss_implicit)) self.learning_rate = tf.placeholder(tf.float32) self.train_op = tf.train.AdamOptimizer(learning_rate=self. learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-08).minimize(self .loss) self.tf_saver = tf.train.Saver() self.train_iteration = start_iteration with tf.variable_scope('Summaries'): tf.summary.image('input_images', self.x, max_outputs=8, family= 'original') tf.summary.image('reconstruction_left_images', self.left_output, max_outputs=8, family='reconstructed_left') tf.summary.image('reconstruction_right_images', self.right_output, max_outputs=8, family='reconstructed_right') tf.summary.scalar('loss', self.loss, family='_loss_main') tf.summary.scalar('loss_classification', self.loss_classification, family='losses_explicit') tf.summary.scalar('loss_explicit', self.loss_explicit, family= 'losses_explicit') tf.summary.scalar('accuracy_classification', self. accuracy_classification, family='losses_explicit') tf.summary.scalar('loss_implicit', self.loss_implicit, family= 'losses_implicit') tf.summary.scalar('loss_reconstruction', self.loss_reconstruction, family='losses_reconstruction') tf.summary.histogram('hist_style', self.code_style, family='code') 
enas.utils.flags|user|print def print_user_flags(line_limit=80): print('-' * 80) global user_flags FLAGS = tf.app.flags.FLAGS for flag_name in sorted(user_flags): value = '{}'.format(getattr(FLAGS, flag_name)) log_string = flag_name log_string += '.' * (line_limit - len(flag_name) - len(value)) log_string += value print(log_string) 
model.PCGN_beamsearch.batch|tile def _tile_batch(t, multiplier): """Core single-tensor implementation of tile_batch.""" t = ops.convert_to_tensor(t, name='t') shape_t = array_ops.shape(t) if t.shape.ndims is None or t.shape.ndims < 1: raise ValueError('t must have statically known rank') tiling = [1] * (t.shape.ndims + 1) tiling[1] = multiplier tiled_static_batch_size = t.shape[0].value * multiplier if t.shape[0 ].value is not None else None tiled = array_ops.tile(array_ops.expand_dims(t, 1), tiling) tiled = array_ops.reshape(tiled, array_ops.concat(([shape_t[0] * multiplier], shape_t[1:]), 0)) tiled.set_shape(tensor_shape.TensorShape([tiled_static_batch_size]). concatenate(t.shape[1:])) return tiled 
facenet-master.src.train_softmax.train def train(args, sess, epoch, image_list, label_list, index_dequeue_op, enqueue_op, image_paths_placeholder, labels_placeholder, learning_rate_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, step, loss, train_op, summary_op, summary_writer, reg_losses, learning_rate_schedule_file, stat, cross_entropy_mean, accuracy, learning_rate, prelogits, prelogits_center_loss, random_rotate, random_crop, random_flip, prelogits_norm, prelogits_hist_max, use_fixed_image_standardization): batch_number = 0 if args.learning_rate > 0.0: lr = args.learning_rate else: lr = facenet.get_learning_rate_from_file(learning_rate_schedule_file, epoch) if lr <= 0: return False index_epoch = sess.run(index_dequeue_op) label_epoch = np.array(label_list)[index_epoch] image_epoch = np.array(image_list)[index_epoch] labels_array = np.expand_dims(np.array(label_epoch), 1) image_paths_array = np.expand_dims(np.array(image_epoch), 1) control_value = (facenet.RANDOM_ROTATE * random_rotate + facenet. RANDOM_CROP * random_crop + facenet.RANDOM_FLIP * random_flip + facenet.FIXED_STANDARDIZATION * use_fixed_image_standardization) control_array = np.ones_like(labels_array) * control_value sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array}) train_time = 0 while batch_number < args.epoch_size: start_time = time.time() feed_dict = {learning_rate_placeholder: lr, phase_train_placeholder: True, batch_size_placeholder: args.batch_size} tensor_list = [loss, train_op, step, reg_losses, prelogits, cross_entropy_mean, learning_rate, prelogits_norm, accuracy, prelogits_center_loss] if batch_number % 100 == 0: (loss_, _, step_, reg_losses_, prelogits_, cross_entropy_mean_, lr_, prelogits_norm_, accuracy_, center_loss_, summary_str ) = sess.run(tensor_list + [summary_op], feed_dict=feed_dict) summary_writer.add_summary(summary_str, global_step=step_) else: (loss_, _, step_, reg_losses_, prelogits_, cross_entropy_mean_, lr_, prelogits_norm_, accuracy_, center_loss_) = sess.run( tensor_list, feed_dict=feed_dict) duration = time.time() - start_time stat['loss'][step_ - 1] = loss_ stat['center_loss'][step_ - 1] = center_loss_ stat['reg_loss'][step_ - 1] = np.sum(reg_losses_) stat['xent_loss'][step_ - 1] = cross_entropy_mean_ stat['prelogits_norm'][step_ - 1] = prelogits_norm_ stat['learning_rate'][epoch - 1] = lr_ stat['accuracy'][step_ - 1] = accuracy_ stat['prelogits_hist'][(epoch - 1), :] += np.histogram(np.minimum( np.abs(prelogits_), prelogits_hist_max), bins=1000, range=(0.0, prelogits_hist_max))[0] duration = time.time() - start_time print( 'Epoch: [%d][%d/%d]\tTime %.3f\tLoss %2.3f\tXent %2.3f\tRegLoss %2.3f\tAccuracy %2.3f\tLr %2.5f\tCl %2.3f' % (epoch, batch_number + 1, args.epoch_size, duration, loss_, cross_entropy_mean_, np.sum(reg_losses_), accuracy_, lr_, center_loss_)) batch_number += 1 train_time += duration summary = tf.Summary() summary.value.add(tag='time/total', simple_value=train_time) summary_writer.add_summary(summary, global_step=step_) return True 
preprocess.prepare_d2d.hist|d def hist_d2d(relevance_file, text_max_len, hist_size, sim_path, hist_path, d2d=False): relevance_dict = load_pickle(relevance_file) qid_list = relevance_dict.keys() with poolcontext(processes=14) as pool: pool.map(partial(hist_per_query, relevance_dict, text_max_len, hist_size, sim_path, hist_path, d2d), qid_list) logging.info('Finish all!') 
operations.batch_norm.call def __call__(self, x, train=True): return tf.contrib.layers.batch_norm(x, decay=self.momentum, updates_collections=None, epsilon=self.epsilon, scale=True, is_training=train, scope=self.name) 
plot_kld.multi|kld|plot def plot_kld_multi(data_path, plot_dict, clrs): n_folders = len(plot_dict['folders']) xs = [None] * n_folders kld_g = None kld_nlf = None kld_nf = [None] * n_folders kld_r = None fpath = None maxlen = 0 for i in range(n_folders): subdir = plot_dict['folders'][i]['folder'] fpath = os.path.join(data_path, subdir) df_sample = get_sample_data(fpath) if df_sample is None: continue xs[i], kld_g_, kld_nlf_, kld_nf[i], kld_r_ = get_sample_kld(df_sample) if maxlen < len(kld_r_): maxlen = len(kld_r_) xg = xs[i] kld_g = kld_g_ kld_nlf = kld_nlf_ kld_r = kld_r_ fig = plt.figure(figsize=plot_dict['figsize']) bnll = [] plt.plot(xg, kld_g, label='Gaussian', linestyle='-.', color=clrs[0]) plt.plot(xg, kld_nlf, label='Camera NLF', linestyle='-.', color=clrs[1]) bnll.append(np.min(kld_g)) bnll.append(np.min(kld_nlf)) for i in range(n_folders): if xs[i] is None: print('skipping %d, %s' % (i, plot_dict['folders'][i]['folder'])) continue plt.plot(xs[i], kld_nf[i], label=plot_dict['folders'][i]['legend'], color=clrs[i + 2], linestyle='-') bnll.append(np.min(kld_nf[i])) plt.plot(xg, kld_r, label='Real noise', linestyle='--', color=clrs[3]) improv = [] for t in range(len(bnll)): improv.append((np.absolute(bnll[-1]) - np.absolute(bnll[t])) / np. absolute(bnll[t])) np.savetxt(os.path.join(fpath, 'best_kld.txt'), bnll) np.savetxt(os.path.join(fpath, 'best_improv.txt'), improv) if plot_dict['xlims'] is not None: plt.xlim(plot_dict['xlims']) if plot_dict['ylims'] is not None: plt.ylim(plot_dict['ylims']) plt.xlabel('Epoch') plt.ylabel('Marginal $D_{KL}$') if plot_dict['title'] is not None: plt.title(plot_dict['title']) plt.legend(loc='best', bbox_to_anchor=None, prop={'size': 14}, ncol=1, fancybox=False, shadow=False) plt.tight_layout() fig.savefig(os.path.join(fpath, '..', 'kld_' + plot_dict['fig_fn'] + '.png')) fig.savefig(os.path.join(fpath, '..', 'kld_' + plot_dict['fig_fn'] + '.pdf')) 
mixprec.LossScalingOptimizer.compute|gradients def compute_gradients(self, loss, var_list=None, *args, **kwargs): if self._scale != 1.0: loss = tf.scalar_mul(self._scale, loss) gradvar = self._optimizer.compute_gradients(loss, var_list, *args, **kwargs ) gradvar = [(tf.scalar_mul(1.0 / self._scale, g), v) for g, v in gradvar] return gradvar 
decoder.decoding|train def decoding_train(dec_inputs, dec_cell, dec_proj, enc_state, memory, max_dec_len, mem_lens, max_mem_len, is_attn, state_size, multi_source= False, copy=False, copy_ind=None, dec_ptr_g_proj=None, dec_ptr_k_proj= None, bow_cond=None, bow_cond_gate_proj=None): """The greedy decoding algorithm, used for training""" dec_outputs = tf.TensorArray(tf.float32, size=max_dec_len) dec_logits_train = tf.TensorArray(tf.float32, size=max_dec_len) dec_pointers = tf.TensorArray(tf.float32, size=max_dec_len) dec_prob_train = tf.TensorArray(tf.float32, size=max_dec_len) dec_g_train = tf.TensorArray(tf.float32, size=max_dec_len) dec_inputs = tf.transpose(dec_inputs, [1, 0, 2]) dec_state = enc_state if copy: print('Using copy mechanism in decoding') else: print('Not using copy mechanism') if bow_cond is not None: print('Using bow condition vector') else: print('Not using bow condition vector') if bow_cond_gate_proj is not None: print('Using bow condition gate') else: print('Not using bow condition gate')  def _dec_loop_fn(i, dec_state, dec_outputs): dec_in = dec_inputs[i] dec_out, dec_state = dec_cell(dec_in, dec_state) dec_outputs = dec_outputs.write(i, dec_out) return i + 1, dec_state, dec_outputs  def _dec_loop_attn_fn(i, dec_state, dec_outputs, dec_logits_train, dec_pointers, dec_prob_train, dec_g_train): dec_in = dec_inputs[i] query = dec_state[-1].h if multi_source: context, _ = multi_source_attention(query, memory, mem_lens, max_mem_len, state_size) else: context, dist = attention(query, memory, mem_lens, max_mem_len) attn_vec = context + query bow_cond_g = 1.0 if bow_cond_gate_proj is not None: bow_cond_g = bow_cond_gate_proj(query + bow_cond) if bow_cond is not None: dec_in = dec_in + bow_cond_g * bow_cond dec_out, dec_state = dec_cell(dec_in + attn_vec, dec_state) dec_logits = dec_proj(dec_out) vocab_dist = tf.nn.softmax(dec_logits) if copy: pointers = [] print('use %d pointers' % len(dec_ptr_k_proj)) for proj_i in dec_ptr_k_proj: ptr_query = proj_i(dec_out) _, ptr = attention(ptr_query, memory[0], mem_lens[0], max_mem_len[0]) pointers.append(ptr) pointers = tf.stack(pointers) pointers = tf.reduce_mean(tf.transpose(pointers, [1, 0, 2]), 1) g = dec_ptr_g_proj(dec_out) mixed_dist, ptr_dist = _mix_dist(vocab_dist, pointers, copy_ind, g) dec_pointers = dec_pointers.write(i, pointers) dec_prob_train = dec_prob_train.write(i, mixed_dist) dec_g_train = dec_g_train.write(i, g) dec_outputs = dec_outputs.write(i, dec_out) dec_logits_train = dec_logits_train.write(i, dec_logits) return (i + 1, dec_state, dec_outputs, dec_logits_train, dec_pointers, dec_prob_train, dec_g_train) if is_attn: print('Attention decoding ... ') _dec_loop = _dec_loop_attn_fn else: print('Not using attention ... ') _dec_loop = _dec_loop_fn loop_len = max_dec_len start_time = 0 (finish_time, dec_state, dec_outputs, dec_logits_train, dec_pointers, dec_prob_train, dec_g_train) = (tf.while_loop(cond=lambda i, _1, _2, _3, _4, _5, _6: tf.less(i, loop_len), body=_dec_loop, loop_vars=( start_time, dec_state, dec_outputs, dec_logits_train, dec_pointers, dec_prob_train, dec_g_train))) dec_logits_train = tf.transpose(dec_logits_train.stack(), [1, 0, 2]) dec_pointers = tf.transpose(dec_pointers.stack(), [1, 0, 2]) dec_prob_train = tf.transpose(dec_prob_train.stack(), [1, 0, 2]) dec_g_train = tf.transpose(dec_g_train.stack(), [1, 0, 2]) avg_max_ptr = tf.reduce_max(dec_pointers, 2) pointer_ent = tf.reduce_sum(-dec_pointers * tf.log(dec_pointers + 1e-10), 2 ) avg_num_copy = tf.squeeze(dec_g_train) return (dec_logits_train, dec_prob_train, pointer_ent, avg_max_ptr, avg_num_copy) 
hbaselines.goal_conditioned.policy.GoalConditionedPolicy.meta|update @property def _update_meta(self): """Return True if the meta-action should be updated by the policy.  This is done by checking the length of the observation lists that are passed to the replay buffer, which are cleared whenever the meta-period has been met or the environment has been reset. """ return len(self._observations) == 0 
vkge.LIM.LIM.LIM def __init__(self, file_name, score_func='DistMult', embedding_size=200, no_batches=10, distribution='normal', epsilon=0.001, negsamples=5, dataset='wn18', lr=0.001, alt_prior=False, projection=False, s_o=True): self.s_o = s_o self.nb_epochs = 500 seed = np.random.randint(100, size=1)[0] self.random_state = np.random.RandomState(seed) tf.set_random_seed(seed) logger.warning('\n \n Using Random Seed {} \n \n'.format(seed)) self.score_func = score_func self.negsamples = int(negsamples) self.distribution = distribution self.alt_prior = alt_prior self.projection = projection optimizer = tf.train.AdamOptimizer(learning_rate=lr, epsilon=epsilon) self.dataset_name = dataset logger.warning('Parsing the facts in the Knowledge Base for Dataset {}..' .format(self.dataset_name)) train_triples = util.read_triples('data/{}/train.tsv'.format(self. dataset_name)) valid_triples = util.read_triples('data/{}/dev.tsv'.format(self. dataset_name)) test_triples = util.read_triples('data/{}/test.tsv'.format(self. dataset_name)) self.nb_examples = len(train_triples) all_triples = train_triples + valid_triples + test_triples entity_set = {s for s, p, o in all_triples} | {o for s, p, o in all_triples } predicate_set = {p for s, p, o in all_triples} self.entity_to_idx = {entity: idx for idx, entity in enumerate(sorted( entity_set))} self.predicate_to_idx = {predicate: idx for idx, predicate in enumerate (sorted(predicate_set))} self.nb_entities, self.nb_predicates = len(entity_set), len(predicate_set) self.idx_pos = tf.placeholder(tf.int32, shape=[None]) self.idx_neg = tf.placeholder(tf.int32, shape=[None]) self.no_samples = tf.placeholder(tf.int32) self.s_inputs = tf.placeholder(tf.int32, shape=[None]) self.p_inputs = tf.placeholder(tf.int32, shape=[None]) self.o_inputs = tf.placeholder(tf.int32, shape=[None]) self.y_inputs = tf.placeholder(tf.bool, shape=[None]) self.KL_discount = tf.placeholder(tf.float32) self.ELBOBS = tf.placeholder(tf.float32) self.build_LIM(self.nb_entities, self.nb_predicates, embedding_size, optimizer) self.train(nb_epochs=self.nb_epochs, test_triples=test_triples, valid_triples=valid_triples, embedding_size=embedding_size, train_triples=train_triples, no_batches=int(no_batches), filename= str(file_name)) 
cpplint.NestingState.Check|Completed|Blocks def CheckCompletedBlocks(self, filename, error): """Checks that all classes and namespaces have been completely parsed.  Call this when all lines in a file have been processed. Args: filename: The name of the current file. error: The function to call with any errors found. """ for obj in self.stack: if isinstance(obj, _ClassInfo): error(filename, obj.starting_linenum, 'build/class', 5, 'Failed to find complete declaration of class %s' % obj.name) elif isinstance(obj, _NamespaceInfo): error(filename, obj.starting_linenum, 'build/namespaces', 5, 'Failed to find complete declaration of namespace %s' % obj .name) 
base_agent.BaseAgent.call def call(self, env_output, neck_state): """Runs the entire episode given time-major tensors.  Args: env_output: An `EnvOutput` tuple with following expectations: reward - Unused done - A boolean tensor of shape  [num_timesteps, batch_size]. observation - A nested structure with individual tensors that have first two dimensions equal to [num_timesteps, batch_size] info - Unused neck_state: A tensor or nested structure with individual tensors that have first dimension equal to batch_size and no time dimension.  Returns: An `AgentOutput` tuple with individual tensors that have first two dimensions equal to [num_timesteps, batch_size] """ unused_reward, done, observation, unused_info = env_output self._current_num_timesteps = tf.shape(done)[0] self._current_batch_size = tf.shape(done)[1] torso_output = utils.batch_apply(self._torso, observation) reset_state = self._get_reset_state(observation, done, neck_state) neck_output_list = [] for timestep, d in enumerate(tf.unstack(done)): neck_input = utils.get_row_nested_tensor(torso_output, timestep) curr_timestep_reset_state = utils.get_row_nested_tensor(reset_state, timestep) neck_state = tf.nest.map_structure(lambda reset_state, state: tf. compat.v1.where(d, reset_state, state), curr_timestep_reset_state, neck_state) neck_output, neck_state = self._neck(neck_input, neck_state) neck_output_list.append(neck_output) head_input = tf.nest.map_structure(lambda *tensors: tf.stack(tensors), *neck_output_list) head_output = utils.batch_apply(self._head, head_input) assert isinstance(head_output, common.AgentOutput) return head_output, neck_state 
utils.data_utils.preprocess def preprocess(x): return x / 127.5 - 1.0 
test.test def test(patch_shape, extraction_step): with tf.Graph().as_default(): test_patches = tf.placeholder(tf.float32, [F.batch_size, patch_shape[0], patch_shape[1], patch_shape[2], F.num_mod], name='real_patches') output_soft = trained_dis_network(test_patches, reuse=None) output = tf.argmax(output_soft, axis=-1) print('Output Patch Shape:', output.get_shape()) saver = tf.train.Saver() with tf.Session() as sess: try: load_model(F.best_checkpoint_dir, sess, saver) print(' Checkpoint loaded succesfully!....\n') except: print(' [!] Checkpoint loading failed!....\n') return patches_test, labels_test = preprocess_dynamic_lab(F. data_directory, F.num_classes, extraction_step, patch_shape, F.number_train_images, validating=F.training, testing=F. testing, num_images_testing=F.number_test_images) total_batches = int(patches_test.shape[0] / F.batch_size) predictions_test = np.zeros((patches_test.shape[0], patch_shape [0], patch_shape[1], patch_shape[2])) print('max and min of patches_test:', np.min(patches_test), np. max(patches_test)) print('Total number of Batches: ', total_batches) for batch in range(total_batches): patches_feed = patches_test[batch * F.batch_size:(batch + 1 ) * F.batch_size, :, :, :, :] preds = sess.run(output, feed_dict={test_patches: patches_feed} ) predictions_test[batch * F.batch_size:(batch + 1) * F. batch_size, :, :, :] = preds print('Processed_batch:[%8d/%8d]' % (batch, total_batches)) print('All patches Predicted') print('Shape of predictions_test, min and max:', predictions_test.shape, np.min(predictions_test), np.max( predictions_test)) images_pred = recompose3D_overlap(predictions_test, 144, 192, 256, extraction_step[0], extraction_step[1], extraction_step[2] ) print('Shape of Predicted Output Groundtruth Images:', images_pred.shape, np.min(images_pred), np.max(images_pred), np.mean(images_pred), np.mean(labels_test)) for i in range(F.number_test_images): pred2d = np.reshape(images_pred[i], 144 * 192 * 256) lab2d = np.reshape(labels_test[i], 144 * 192 * 256) save_image(F.results_dir, images_pred[i], F. number_train_images + i + 2) pred2d = np.reshape(images_pred, images_pred.shape[0] * 144 * 192 * 256) lab2d = np.reshape(labels_test, labels_test.shape[0] * 144 * 192 * 256) F1_score = f1_score(lab2d, pred2d, [0, 1, 2, 3], average=None) print('Testing Dice Coefficient.... ') print('Background:', F1_score[0]) print('CSF:', F1_score[1]) print('GM:', F1_score[2]) print('WM:', F1_score[3]) return 
pytorch_pretrained_bert.modeling_gpt2.Attention.heads|split def split_heads(self, x, k=False): new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head) x = x.view(*new_x_shape) if k: return x.permute(0, 2, 3, 1) else: return x.permute(0, 2, 1, 3) 
europilot.joystick.Message.repr def __repr__(self): values = self.button.name, self.value.int_normalized(self.button.name) return ' '.join(map(str, values)) 
input_sieve.proto|parse|example def parse_example_proto(example_serialized): feature_map = {'image/encoded': tf.FixedLenFeature([], dtype=tf.string, default_value=''), 'image/class/label': tf.FixedLenFeature([1], dtype=tf.int64, default_value=-1), 'image/class/text': tf. FixedLenFeature([], dtype=tf.string, default_value=''), 'image/class/synset': tf.FixedLenFeature([], dtype=tf.string, default_value=''), 'image/filename': tf.FixedLenFeature([], dtype= tf.string, default_value='')} sparse_float32 = tf.VarLenFeature(dtype=tf.float32) feature_map.update({k: sparse_float32 for k in [ 'image/object/bbox/xmin', 'image/object/bbox/ymin', 'image/object/bbox/xmax', 'image/object/bbox/ymax']}) features = tf.parse_single_example(example_serialized, feature_map) label = tf.cast(features['image/class/label'], dtype=tf.int32) xmin = tf.expand_dims(features['image/object/bbox/xmin'].values, 0) ymin = tf.expand_dims(features['image/object/bbox/ymin'].values, 0) xmax = tf.expand_dims(features['image/object/bbox/xmax'].values, 0) ymax = tf.expand_dims(features['image/object/bbox/ymax'].values, 0) bbox = tf.concat(axis=0, values=[ymin, xmin, ymax, xmax]) bbox = tf.expand_dims(bbox, 0) bbox = tf.transpose(bbox, [0, 2, 1]) return features['image/encoded'], label, bbox, features['image/class/text' ], features['image/class/synset'], features['image/filename'] 
infer.to|segments|attention def segments_to_attention(attention, markup_segments, text, df_mapping, binf_preds=None): SEARCH_WINDOW = 5 last_phone = 0 nphones = attention.shape[0] nfeatures = len(df_mapping.index) binfs = np.zeros((len(markup_segments), nfeatures)) for i, segment in enumerate(markup_segments): phones_limit = min(last_phone + SEARCH_WINDOW, nphones) st = segment[0] en = max(segment[1], st + 1) phone = last_phone + np.squeeze(np.argmax(np.max(attention[ last_phone:phones_limit, st:en], axis=1))) if binf_preds is None: binfs[(i), :] = df_mapping[text[phone]].values else: binfs[(i), :] = binf_preds[(phone), :] return binfs 
cpplint._CppLintState.Filters|Restore def RestoreFilters(self): """ Restores filters previously backed up.""" self.filters = self._filters_backup[:] 
darkflow.utils.im_transform.imcv|affine|trans def imcv2_affine_trans(im): h, w, c = im.shape scale = np.random.uniform() / 10.0 + 1.0 max_offx = (scale - 1.0) * w max_offy = (scale - 1.0) * h offx = int(np.random.uniform() * max_offx) offy = int(np.random.uniform() * max_offy) im = cv2.resize(im, (0, 0), fx=scale, fy=scale) im = im[offy:offy + h, offx:offx + w] flip = np.random.binomial(1, 0.5) if flip: im = cv2.flip(im, 1) return im, [w, h, c], [scale, [offx, offy], flip] 
avod.experiments.run_evaluation.evaluate def evaluate(model_config, eval_config, dataset_config): eval_mode = eval_config.eval_mode if eval_mode not in ['val', 'test']: raise ValueError('Evaluation mode can only be set to `val` or `test`') evaluate_repeatedly = eval_config.evaluate_repeatedly data_split = dataset_config.data_split if data_split == 'train': dataset_config.data_split_dir = 'training' dataset_config.has_labels = True elif data_split.startswith('val'): dataset_config.data_split_dir = 'training' if eval_mode == 'val': dataset_config.has_labels = True elif eval_mode == 'test': dataset_config.has_labels = False elif data_split == 'test': dataset_config.data_split_dir = 'testing' dataset_config.has_labels = False else: raise ValueError('Invalid data split', data_split) dataset_config = config_builder.proto_to_obj(dataset_config) dataset_config.aug_list = [] dataset = DatasetBuilder.build_kitti_dataset(dataset_config, use_defaults=False) model_name = model_config.model_name model_config = config_builder.proto_to_obj(model_config) model_config.path_drop_probabilities = [1.0, 1.0] with tf.Graph().as_default(): if model_name == 'avod_model': model = AvodModel(model_config, train_val_test=eval_mode, dataset=dataset) elif model_name == 'rpn_model': model = RpnModel(model_config, train_val_test=eval_mode, dataset=dataset) else: raise ValueError('Invalid model name {}'.format(model_name)) model_evaluator = Evaluator(model, dataset_config, eval_config) if evaluate_repeatedly: model_evaluator.repeated_checkpoint_run() else: model_evaluator.run_latest_checkpoints() 
seed_rl-master.grpc.python.ops_test.OpsTest.waiting|shutdown|full|test|batch|for def test_shutdown_waiting_for_full_batch(self): address = self.get_unix_address() server = ops.Server([address])  @tf.function(input_signature=[tf.TensorSpec([2], tf.int32)]) def foo(x): return x + 1 server.bind(foo, batched=True) server.start() client = ops.Client(address) with futures.ThreadPoolExecutor(max_workers=1) as executor: f = executor.submit(client.foo, 42) time.sleep(1) with self.assertRaisesRegexp(tf.errors.UnavailableError, 'server closed'): server.shutdown() f.result() 
graphsage.minibatch.EdgeMinibatchIterator.feed|minibatch|dict|next def next_minibatch_feed_dict(self): start_idx = self.batch_num * self.batch_size self.batch_num += 1 end_idx = min(start_idx + self.batch_size, len(self.train_edges)) batch_edges = self.train_edges[start_idx:end_idx] return self.batch_feed_dict(batch_edges) 
classification.ops.layer_collection.LayerCollection.approximation|default|fully|connected @property def default_fully_connected_approximation(self): return self._default_fully_connected_approximation 
gan.factorVAE.FactorVAE.build def build(self): self.build_connection() self.build_loss() self.build_summary() self.build_metric() self.saver = tf.train.Saver() 
mobilenet_for_cifar.MobileNetForCifar.weights|restore def restore_weights(self, scope, layer_type, weights_dict, infix=None): """ prefix: scope layer_type: conv, bn, local, -- name in weight_dict: "dw", "conv2d", "dw/beta gamme moving...", "pw/beta gamma moving" """ if layer_type == 'conv' or layer_type == 'dw' or layer_type == 'dense': saved_kernel = weights_dict.get(scope.name + infix + '/kernel') saved_bias = weights_dict.get(scope.name + infix + '/bias') if saved_kernel is not None: weight = tf.get_default_graph().get_tensor_by_name(scope.name + infix + '/kernel:0') weight = tf.assign(weight, saved_kernel) tf.add_to_collection('init', weight) if saved_bias is not None: bias = tf.get_default_graph().get_tensor_by_name(scope.name + infix + '/bias:0') bias = tf.assign(bias, saved_bias) tf.add_to_collection('init', bias) elif layer_type == 'bn': saved_beta = weights_dict.get(scope.name + infix + '/beta') saved_gamma = weights_dict.get(scope.name + infix + '/gamma') saved_moving_mean = weights_dict.get(scope.name + infix + '/moving_mean') saved_moving_variance = weights_dict.get(scope.name + infix + '/moving_variance') if saved_beta is not None: beta = tf.get_default_graph().get_tensor_by_name(scope.name + infix + '/beta:0') gamma = tf.get_default_graph().get_tensor_by_name(scope.name + infix + '/gamma:0') moving_mean = tf.get_default_graph().get_tensor_by_name(scope. name + infix + '/moving_mean:0') moving_variance = tf.get_default_graph().get_tensor_by_name( scope.name + infix + '/moving_variance:0') print(beta.get_shape().as_list()) print(saved_beta.shape) beta = tf.assign(beta, saved_beta) gamma = tf.assign(gamma, saved_gamma) moving_mean = tf.assign(moving_mean, saved_moving_mean) moving_variance = tf.assign(moving_variance, saved_moving_variance) tf.add_to_collection('init', beta) tf.add_to_collection('init', gamma) tf.add_to_collection('init', moving_mean) tf.add_to_collection('init', moving_variance) else: raise ValueError('unknown layer type') return 
utils.optimistic|restore def optimistic_restore(session, save_file): reader = tf.train.NewCheckpointReader(save_file) saved_shapes = reader.get_variable_to_shape_map() var_names = sorted([(var.name, var.name.split(':')[0]) for var in tf. global_variables() if var.name.split(':')[0] in saved_shapes]) restore_vars = [] with tf.variable_scope('', reuse=True): for var_name, saved_var_name in var_names: curr_var = tf.get_variable(saved_var_name) var_shape = curr_var.get_shape().as_list() if var_shape == saved_shapes[saved_var_name]: restore_vars.append(curr_var) saver = tf.train.Saver(restore_vars) saver.restore(session, save_file) 
texar.core.layers.get_activation_fn.fn|partial def _partial_fn(features): return activation_fn_(features, **kwargs) 
CATA.parameters def parameters(num_users, num_items, pmf_epochs, mlp_epoch, lambda_u, lambda_v, latent_size): parameters.m_num_users = num_users parameters.m_num_items = num_items parameters.m_U = 0.1 * np.random.randn(num_users, latent_size) parameters.m_V = 0.1 * np.random.randn(num_items, latent_size) parameters.m_theta = 0.1 * np.random.randn(num_items, latent_size) parameters.n_epochs = pmf_epochs parameters.mlp_epoch = mlp_epoch parameters.lambda_u = lambda_u parameters.lambda_v = lambda_v parameters.dimension = latent_size 
graphsage.layers.Dense.Dense def __init__(self, input_dim, output_dim, dropout=0.0, act=tf.nn.relu, placeholders=None, bias=True, featureless=False, sparse_inputs=False, **kwargs): super(Dense, self).__init__(**kwargs) self.dropout = dropout self.act = act self.featureless = featureless self.bias = bias self.input_dim = input_dim self.output_dim = output_dim self.sparse_inputs = sparse_inputs if sparse_inputs: self.num_features_nonzero = placeholders['num_features_nonzero'] with tf.variable_scope(self.name + '_vars'): self.vars['weights'] = tf.get_variable('weights', shape=(input_dim, output_dim), dtype=tf.float32, initializer=tf.contrib.layers. xavier_initializer(), regularizer=tf.contrib.layers. l2_regularizer(FLAGS.weight_decay)) if self.bias: self.vars['bias'] = zeros([output_dim], name='bias') if self.logging: self._log_vars() 
w2_model.W2.get|stats def get_stats(self, config): """print outs""" stats = OrderedDict() stats['loss/disc'] = self.d_loss stats['loss/gen'] = self.g_loss return stats 
scripts.preprocessing.gen_mini_batches.preprocessing|do def do_preprocessing(dataset, indices): mini_batch_utils = dataset.kitti_utils.mini_batch_utils print('Generating mini batches in {}'.format(mini_batch_utils. mini_batch_dir)) mini_batch_utils.preprocess_rpn_mini_batches(indices) print('Mini batches generated') 
texar.data.data.tfrecord_data_test.TFRecordDataTest.image|resize|test def test_image_resize(self): """Tests the image resize function """ hparams = copy.copy(self._hparams) _image_options = {'image_feature_name': 'image_raw', 'resize_height': 512, 'resize_width': 512} hparams['dataset'].update({'image_options': _image_options}) self._run_and_test(hparams) 
dstn_int_att.get|one|hot|masked def get_masked_one_hot(x_input_one_hot): data_mask = tf.cast(tf.greater(x_input_one_hot, 0), tf.float32) data_mask = tf.expand_dims(data_mask, axis=2) data_mask = tf.tile(data_mask, (1, 1, k)) data_embed_one_hot = tf.nn.embedding_lookup(emb_mat, x_input_one_hot) data_embed_one_hot_masked = tf.multiply(data_embed_one_hot, data_mask) return data_embed_one_hot_masked 
src.data_util.bucketdata.BucketData.iadd def __iadd__(self, other): self.data_list += other.data_list self.label_list += other.label_list self.max_label_len = max(self.max_label_len, other.max_label_len) self.max_width = max(self.max_width, other.max_width) 
create_scripts.get|filename def get_filename(hparams, setting_dict, priority): filename_list = [] for field, val in setting_dict.iteritems(): short_name = get_short_name(field) filename_list.append(short_name + str(val)) filename = '_'.join(filename_list) filename = hashlib.sha1(filename).hexdigest() filename = hparams.scripts_base_dir + priority + '_' + filename + '.sh' return filename 
avod.core.trainer_test.FakeBatchNormClassifier.get|input def get_input(self): """Creates an easy training set.""" np.random.seed(0) inputs = np.zeros((16, 4)) labels = np.random.randint(0, 2, size=(16, 1)).astype(np.float32) for i in range(16): j = int(2 * labels[i] + np.random.randint(0, 2)) inputs[i, j] = 1 random_seed.set_random_seed(0) tf_inputs = constant_op.constant(inputs, dtype=dtypes.float32) tf_labels = constant_op.constant(labels, dtype=dtypes.float32) return tf_inputs, tf_labels 
src.layers.GraphConvolution.Convolution|Graph def __init__(self, input_dim, output_dim, adj, dropout=0.0, act=tf.nn.relu, **kwargs): super(GraphConvolution, self).__init__(**kwargs) with tf.variable_scope(self.name + '_vars'): self.vars['weights'] = weight_variable_glorot(input_dim, output_dim, name='weights') self.dropout = dropout self.adj = adj self.act = act 
ant.AntEnv.setup|viewer def viewer_setup(self): """Create the viewer.""" self.viewer.cam.distance = self.model.stat.extent * 0.5 
utils.espeakng.ESpeakNG.line|length @line_length.setter def line_length(self, v): self._line_length = v 
deepzono_nodes.DeepzonoMaxpool.transformer def transformer(self, nn, man, element, nlb, nub, refine, timeout_lp, timeout_milp): """ transforms element with maxpool_zono  Arguments --------- man : ElinaManagerPtr man to which element belongs element : ElinaAbstract0Ptr abstract element onto which the transformer gets applied  Return ------ output : ElinaAbstract0Ptr abstract element after the transformer """ offset, old_length = self.abstract_information h, w = self.window_size H, W, C = self.input_shape element = maxpool_zono(man, True, element, (c_size_t * 3)(h, w, 1), ( c_size_t * 3)(H, W, C), 0, (c_size_t * 2)(self.stride[0], self. stride[1]), 3, offset + old_length, self.padding == 'VALID') return remove_dimensions(man, element, offset, old_length) 
official.resnet.resnet_main.get|lr|schedule def get_lr_schedule(train_steps, num_train_images, train_batch_size): """learning rate schedule.""" steps_per_epoch = np.floor(num_train_images / train_batch_size) train_epochs = train_steps / steps_per_epoch train_epochs = 90 return [(1.0, np.floor(5 / 90 * train_epochs)), (0.1, np.floor(30 / 90 * train_epochs)), (0.01, np.floor(60 / 90 * train_epochs)), (0.001, np.floor(80 / 90 * train_epochs))] 
neural_tangents.utils.utils.stub_out_pmap.xla_bridge_stub.count|device def device_count(self): return count 
texar.agents.seq_agent_base.SeqAgentBase.Agent|Base|Seq def __init__(self, hparams=None): AgentBase.__init__(self, hparams) 
nets.pix2pix.arg|scope|pix def pix2pix_arg_scope(): """Returns a default argument scope for isola_net.  Returns: An arg scope. """ instance_norm_params = {'center': True, 'scale': True, 'epsilon': 1e-05} with tf.contrib.framework.arg_scope([layers.conv2d, layers. conv2d_transpose], normalizer_fn=layers.instance_norm, normalizer_params=instance_norm_params, weights_initializer=tf. random_normal_initializer(0, 0.02)) as sc: return sc 
model.Model.gcn|build|block|backbone def build_gcn_backbone_block(self, input_graph, vertex_layer_builder, edge_layer_builder, num_layers, num_neighbors, num_filters, skip_connect, dilations): """Build the gcn backbone block""" input_graph = tf.expand_dims(input_graph, -2) graphs = [] for i in range(num_layers): if i == 0: neigh_idx = edge_layer_builder.build(input_graph[:, :, :, 6:], num_neighbors[i], dilation=dilations[i], is_training=self. is_training) vertex_features = vertex_layer_builder.build(input_graph, num_neighbors[i], num_filters[i], neigh_idx=neigh_idx, scope='adj_conv_' + str(i), is_training=self.is_training) graph = vertex_features graphs.append(graph) else: neigh_idx = edge_layer_builder.build(graphs[-1], num_neighbors[ i], dilation=dilations[i], is_training=self.is_training) vertex_features = vertex_layer_builder.build(graphs[-1], num_neighbors[i], num_filters[i], neigh_idx=neigh_idx, scope='adj_conv_' + str(i), is_training=self.is_training) graph = vertex_features if skip_connect == 'residual': graph = graph + graphs[-1] elif skip_connect == 'dense': graph = tf.concat([graph, graphs[-1]], axis=-1) elif skip_connect == 'none': graph = graph else: raise Exception('Unknown connections') graphs.append(graph) return graphs 
gen.iterative|all|lth|sparse|force|plot|resnet def plot_resnet56_sparse_iterative_lth_force_all(): common.lth_plot(network=common.RESNET56, is_iterative=True, prune_method=common.UNSTRUCTURED, min_max_y=(-0.03, 0.01), to_ignore=['reinit', 'lr_lottery'], comparison_points=[(0.1497, - 0.0006), (0.1, 0.0016), (0.05, -0.0065), (0.03, -0.0135)], comparison_label=common.CARREIRA, nybins=4, force_single=True, force_all_single=True) 
npy2ckpt.get|arguments def get_arguments(): """Parse all the arguments provided from the CLI.  Returns: A list of parsed arguments. """ parser = argparse.ArgumentParser(description='NPY to CKPT converter.') parser.add_argument('npy_path', type=str, help= 'Path to the .npy file, which contains the weights.') parser.add_argument('--save-dir', type=str, default=SAVE_DIR, help= 'Where to save the converted .ckpt file.') return parser.parse_args() 
texar.losses.entropy.get|entropy def _get_entropy(logits): probs = tf.nn.softmax(logits) + 1e-08 entropy = -probs * tf.log(probs) entropy = tf.reduce_sum(entropy, -1) return entropy 
nets.vgg_test.VGG16Test.test|Forward def testForward(self): batch_size = 1 height, width = 224, 224 with self.test_session() as sess: inputs = tf.random_uniform((batch_size, height, width, 3)) logits, _ = vgg.vgg_16(inputs) sess.run(tf.global_variables_initializer()) output = sess.run(logits) self.assertTrue(output.any()) 
nn_ops.conv|no|bias def conv_no_bias(op_name, in_tensor, filter_size_h, filter_size_w, filters, strides=(1, 1, 1, 1), padding='VALID', weight_decay=0.0005, stddev=0.1): with tf.device('/device:CPU:0'), tf1.variable_scope('vars/convs', reuse =tf1.AUTO_REUSE): input_size = in_tensor.get_shape().as_list()[3] shape = filter_size_h, filter_size_w, input_size, filters w = tf1.get_variable('W_' + op_name, shape=shape, regularizer= l2_regularizer(weight_decay), initializer=tf1. truncated_normal_initializer(stddev=stddev)) with tf.name_scope(op_name): return tf.nn.conv2d(in_tensor, w, strides=strides, padding=padding, name=op_name) 
utils_test.TPUEncodeTest.test|strategy @parameterized.parameters((1,), (2,)) def test_strategy(self, num_cores): resolver = tf.distribute.cluster_resolver.TPUClusterResolver('') topology = tf.tpu.experimental.initialize_tpu_system(resolver) da = tf.tpu.experimental.DeviceAssignment.build(topology, num_replicas= num_cores) strategy = tf.distribute.experimental.TPUStrategy(resolver, device_assignment=da)  def dataset_fn(unused_ctx):  def gen(): yield 0 yield 1 dataset = tf.data.Dataset.from_generator(gen, tf.int64) return dataset.map(lambda _: utils.tpu_encode(self.data)) dataset = strategy.experimental_distribute_datasets_from_function( dataset_fn) encoded = next(iter(dataset)) decoded = strategy.experimental_run_v2(lambda args: utils.tpu_decode( args, encoded), (encoded,)) decoded = tf.nest.map_structure(lambda t: strategy. experimental_local_results(t)[0], decoded) for a, b in zip(decoded, self.data): self.assertAllEqual(a, b) 
pvae.pixelvae_bbans_two_layer.rec|net def rec_net2(h): return session.run([mu2, sig2], feed_dict={h1: h, bn_is_training: False, bn_stats_iter: 0, total_iters: 99999}) 
model.DeepWalk.build def build(self): """ Method to create the computational graph and initialize weights. """ self.computation_graph = tf.Graph() with self.computation_graph.as_default(): self.walker_layer = DeepWalker(self.args, self.vocab_size, self.degrees ) self.gamma = tf.placeholder('float') self.loss = self.walker_layer() self.batch = tf.Variable(0) self.step = tf.placeholder('float') self.learning_rate_new = tf.train.polynomial_decay(self.args. initial_learning_rate, self.batch, self.true_step_size, self. args.minimal_learning_rate, self.args.annealing_factor) self.train_op = tf.train.AdamOptimizer(self.learning_rate_new ).minimize(self.loss, global_step=self.batch) self.init = tf.global_variables_initializer() 
craystack.codecs.substack.push def push(message, data, *args, **kwargs): head, tail = message subhead, update = util.view_update(head, view_fun) subhead, tail = push_((subhead, tail), data, *args, **kwargs) return update(subhead), tail 
train_s3dis.placeholder|inputs def placeholder_inputs(batch_size, num_point): input_pl = tf.placeholder(tf.float32, shape=(batch_size, num_point, INPUT_DIM)) label_pl = tf.placeholder(tf.int32, shape=(batch_size, num_point)) inner_label_pl = tf.placeholder(tf.int32, shape=(batch_size, num_point)) return input_pl, label_pl, inner_label_pl 
run_nvidia.reinit def reinit(trial, version, retrain_epochs, density, base): path = _get_path('reinit', trial, version, 'retrain_{}/density_{}'. format(retrain_epochs, density)) base = os.path.join(base, trial) run(path, **{'--lottery_pruning_method': 'prune_all_to_global_{}'. format(density), '--lottery_reset_to': 'reinitialize', '--lottery_prune_at': '{}/lottery/checkpoint_iter_final'.format( base), '--max_train_epochs': 10 + retrain_epochs}) 
net_utils.complex|cross|conv def complex_cross_conv(input_real, input_imag, scope_name, input_shape, keep_prob, padding, regularizer=None): print(scope_name) with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE): conv_weight_real = tf.get_variable(name='weight_real', shape= input_shape, initializer=tf.contrib.layers.xavier_initializer(), dtype=tf.float32) conv_weight_imag = tf.get_variable(name='weight_imag', shape= input_shape, initializer=tf.contrib.layers.xavier_initializer(), dtype=tf.float32) conv_bias_real = tf.get_variable(name='bias_real', shape=[ input_shape[-1]], initializer=tf.zeros_initializer(), dtype=tf. float32) conv_bias_imag = tf.get_variable(name='bias_imag', shape=[ input_shape[-1]], initializer=tf.zeros_initializer(), dtype=tf. float32) real_part = tf.nn.conv2d(input_real, conv_weight_real, strides=[1, 1, 1, 1], padding=padding) cross_real_part = tf.nn.conv2d(input_real, conv_weight_imag, strides=[1, 1, 1, 1], padding=padding) imag_part = tf.nn.conv2d(input_imag, conv_weight_imag, strides=[1, 1, 1, 1], padding=padding) cross_imag_part = tf.nn.conv2d(input_imag, conv_weight_real, strides=[1, 1, 1, 1], padding=padding) conv_real = tf.subtract(real_part, imag_part) conv_imag = tf.add(cross_real_part, cross_imag_part) relu_real = tf.nn.relu(tf.nn.bias_add(conv_real, conv_bias_real)) relu_imag = tf.nn.relu(tf.nn.bias_add(conv_imag, conv_bias_imag)) if regularizer != None: tf.add_to_collection('losses', regularizer(conv_weight_real)) tf.add_to_collection('losses', regularizer(conv_weight_imag)) tf.add_to_collection('losses', regularizer(conv_bias_real)) tf.add_to_collection('losses', regularizer(conv_bias_imag)) return tf.nn.dropout(relu_real, keep_prob), tf.nn.dropout(relu_imag, keep_prob) 
spatial_net.spatial|aware def spatial_aware(input_feature, dimension, trainable, name): batch, height, width, channel = input_feature.get_shape().as_list() vec1 = tf.reshape(tf.reduce_max(input_feature, axis=-1), [-1, height * width]) with tf.variable_scope(name): weight1 = tf.get_variable(name='weights1', shape=[height * width, int(height * width / 2), dimension], trainable=trainable, initializer=tf.truncated_normal_initializer(mean=0.0, stddev= 0.005), regularizer=tf.contrib.layers.l2_regularizer(0.01)) bias1 = tf.get_variable(name='biases1', shape=[1, int(height * width / 2), dimension], trainable=trainable, initializer=tf. constant_initializer(0.1), regularizer=tf.contrib.layers. l1_regularizer(0.01)) vec2 = tf.einsum('bi, ijd -> bjd', vec1, weight1) + bias1 weight2 = tf.get_variable(name='weights2', shape=[int(height * width / 2), height * width, dimension], trainable=trainable, initializer=tf.truncated_normal_initializer(mean=0.0, stddev= 0.005), regularizer=tf.contrib.layers.l2_regularizer(0.01)) bias2 = tf.get_variable(name='biases2', shape=[1, height * width, dimension], trainable=trainable, initializer=tf. constant_initializer(0.1), regularizer=tf.contrib.layers. l1_regularizer(0.01)) vec3 = tf.einsum('bjd, jid -> bid', vec2, weight2) + bias2 return vec3 
attributionpriors.mnist.mnist_train.get|model def get_model(cond_input_op): train_pl = tf.placeholder_with_default(False, shape=(), name='train_pl') y = mnist_model.model(cond_input_op, train_pl) return y, train_pl 
official.resnet.imagenet_input.ImageNetTFExampleInput.fn|input def input_fn(self, params): """Input function which provides a single batch for train or eval.  Args: params: `dict` of parameters passed from the `TPUEstimator`. `params['batch_size']` is always provided and should be used as the effective batch size.  Returns: A `tf.data.Dataset` object. """ batch_size = params['batch_size'] if 'context' in params: current_host = params['context'].current_input_fn_deployment()[1] num_hosts = params['context'].num_hosts else: current_host = 0 num_hosts = 1 dataset = self.make_source_dataset(current_host, num_hosts) dataset = dataset.apply(tf.contrib.data.map_and_batch(self. dataset_parser, batch_size=batch_size, num_parallel_batches=self. num_parallel_calls, drop_remainder=True)) if self.transpose_input: dataset = dataset.map(lambda images, labels: (tf.transpose(images, [1, 2, 3, 0]), labels), num_parallel_calls=self.num_parallel_calls) dataset = dataset.map(functools.partial(self.set_shapes, batch_size)) dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE) return dataset 
utils.data_utils.dataLoaderUSR.get|lr|info def get_lr_info(self): if self.SCALE == 2: lr_res, low_res_folder = (240, 320), 'low_res_2x/' elif self.SCALE == 8: lr_res, low_res_folder = (60, 80), 'low_res_8x/' else: lr_res, low_res_folder = (120, 160), 'low_res_4x/' return lr_res, low_res_folder 
pytorch_pretrained_bert.modeling.BertForQuestionAnswering.Answering|Question|For|Bert def __init__(self, config): super(BertForQuestionAnswering, self).__init__(config) self.bert = BertModel(config) self.qa_outputs = nn.Linear(config.hidden_size, 2) self.apply(self.init_bert_weights) 
actor.tensor|write|specs def _write_tensor_specs(initial_agent_state: Any, env_output: common. EnvOutput, agent_output: common.AgentOutput, actor_action: common. ActorAction, loss_type: Optional[int]=common.AC_LOSS): """Writes tensor specs of ActorOutput tuple to disk.  Args: initial_agent_state: A tensor or nested structure of tensor without any time or batch dimensions. env_output: An instance of `EnvOutput` where individual tensors don't have time and batch dimensions. agent_output: An instance of `AgentOutput` where individual tensors don't have time and batch dimensions. actor_action: An instance of `ActorAction`. loss_type: A scalar int denoting the loss type. """ actor_output = common.ActorOutput(initial_agent_state, env_output, agent_output, actor_action, loss_type, info='') specs = tf.nest.map_structure(tf.convert_to_tensor, actor_output) specs = tf.nest.map_structure(tf.TensorSpec.from_tensor, specs) env_output = tf.nest.map_structure(add_time_dimension, specs.env_output) agent_output = tf.nest.map_structure(add_time_dimension, specs.agent_output ) actor_action = tf.nest.map_structure(add_time_dimension, specs.actor_action ) specs = specs._replace(env_output=env_output, agent_output=agent_output, actor_action=actor_action) utils.write_specs(FLAGS.logdir, specs) 
extract_csqa_bert.InputFeatures.Input|Features def __init__(self, example_id, choices_features, label): self.example_id = example_id self.choices_features = [{'input_ids': input_ids, 'input_mask': input_mask, 'segment_ids': segment_ids} for _, input_ids, input_mask, segment_ids in choices_features] self.label = label 
utils.LoopLogger.Logger|Loop def __init__(self, max_value=None, step_size=1, n_steps=25, print_time=True): self.max_value = max_value if n_steps is not None: self.step_size = max(1, max_value // n_steps) else: self.step_size = step_size self.print_time = print_time self.n = 0 self.start_time = time.time() 
pytorch_pretrained_bert.modeling.BertPreTrainingHeads.forward def forward(self, sequence_output, pooled_output): prediction_scores = self.predictions(sequence_output) seq_relationship_score = self.seq_relationship(pooled_output) return prediction_scores, seq_relationship_score 
models.differential_evolution.DifferentialEvolutionSolver.population|init|lhs def init_population_lhs(self): """ Initializes the population with Latin Hypercube Sampling. Latin Hypercube Sampling ensures that each parameter is uniformly sampled over its range. """ rng = self.random_number_generator segsize = 1.0 / self.num_population_members samples = segsize * rng.random_sample(self.population_shape) + np.linspace( 0.0, 1.0, self.num_population_members, endpoint=False)[:, (np.newaxis)] self.population = np.zeros_like(samples) for j in range(self.parameter_count): order = rng.permutation(range(self.num_population_members)) self.population[:, (j)] = samples[order, j] self.population_energies = np.ones(self.num_population_members) * np.inf self._nfev = 0 
plato.agent.component.dialogue_policy.reinforcement_learning.minimax_q_policy.MinimaxQPolicy.load def load(self, path): """ Load the model from the path provided  :param path: path to load the model from :return: nothing """ if not path: print('No dialogue_policy loaded.') return if isinstance(path, str): if os.path.isfile(path): with open(path, 'rb') as file: obj = pickle.load(file) if 'Q' in obj: self.Q = obj['Q'] if 'V' in obj: self.V = obj['V'] if 'pi' in obj: self.pi = obj['pi'] if 'a' in obj: self.alpha = obj['a'] if 'e' in obj: self.epsilon = obj['e'] if 'g' in obj: self.gamma = obj['g'] print('Q dialogue_policy loaded from {0}.'.format(path)) else: print('Warning! Q dialogue_policy file %s not found' % path) else: print('Warning! Unacceptable value for Q policy file name: %s ' % path) 
predict_test.PredictTest.Momentum|test|Prediction|NTK @jtu.parameterized.named_parameters(jtu.cases_from_list({'testcase_name': '_train={}_test={}_network={}_logits={}_{}'.format(train, test, network, out_logits, name), 'train_shape': train, 'test_shape': test, 'network': network, 'out_logits': out_logits, 'fn_and_kernel': fn} for train, test, network in zip(TRAIN_SHAPES, TEST_SHAPES, NETWORK) for out_logits in OUTPUT_LOGITS for name, fn in KERNELS.items() if len(train) == 2)) def testNTKMomentumPrediction(self, train_shape, test_shape, network, out_logits, fn_and_kernel): key = random.PRNGKey(0) key, split = random.split(key) x_train = random.normal(split, train_shape) key, split = random.split(key) y_train = np.array(random.bernoulli(split, shape=(train_shape[0], out_logits)), np.float32) key, split = random.split(key) x_test = random.normal(split, test_shape) params, f, ntk = fn_and_kernel(key, train_shape[1:], network, out_logits) loss = lambda y, y_hat: 0.5 * np.mean((y - y_hat) ** 2) grad_loss = jit(grad(lambda params, x: loss(f(params, x), y_train))) g_dd = ntk(x_train, None, 'ntk') g_td = ntk(x_test, x_train, 'ntk') atol = ATOL rtol = RTOL step_size = 0.5 if len(train_shape) > 2: atol = ATOL * 2 rtol = RTOL * 2 step_size = 0.1 train_time = 100.0 steps = int(train_time / np.sqrt(step_size)) init, predictor, get = predict.momentum(g_dd, y_train, loss, step_size, g_td) opt_init, opt_update, get_params = momentum(step_size, 0.9) opt_state = opt_init(params) fx_initial_train = f(params, x_train) fx_initial_test = f(params, x_test) lin_state = init(fx_initial_train, fx_initial_test) fx_pred_train, fx_pred_test = get(lin_state) self.assertAllClose(fx_initial_train, fx_pred_train, True) self.assertAllClose(fx_initial_test, fx_pred_test, True) for i in range(steps): params = get_params(opt_state) opt_state = opt_update(i, grad_loss(params, x_train), opt_state) params = get_params(opt_state) fx_train = f(params, x_train) fx_test = f(params, x_test) lin_state = predictor(lin_state, train_time) fx_pred_train, fx_pred_test = get(lin_state) fx_disp_train = np.sqrt(np.mean((fx_train - fx_initial_train) ** 2)) fx_disp_test = np.sqrt(np.mean((fx_test - fx_initial_test) ** 2)) fx_error_train = (fx_train - fx_pred_train) / fx_disp_train fx_error_test = (fx_test - fx_pred_test) / fx_disp_test self.assertAllClose(fx_error_train, np.zeros_like(fx_error_train), True, rtol, atol) self.assertAllClose(fx_error_test, np.zeros_like(fx_error_test), True, rtol, atol) 
utils.transform|d|coordinates def transform_coordinates_3d(coordinates, RT): """ Input: coordinates: [3, N] RT: [4, 4] Return new_coordinates: [3, N]  """ assert coordinates.shape[0] == 3 coordinates = np.vstack([coordinates, np.ones((1, coordinates.shape[1]), dtype=np.float32)]) new_coordinates = RT @ coordinates new_coordinates = new_coordinates[:3, :] / new_coordinates[(3), :] return new_coordinates 
nets.inception_v2_test.InceptionV2Test.Data|Errors|test|Build|Formats|For def testBuildErrorsForDataFormats(self): batch_size = 5 height, width = 224, 224 inputs = tf.random_uniform((batch_size, height, width, 3)) with self.assertRaises(ValueError): _ = inception.inception_v2_base(inputs, data_format='NCWH') with self.assertRaises(ValueError): _ = inception.inception_v2_base(inputs, data_format='NCHW') 
nets.SRGAN.SRGAN_model.build_discriminator.d|block def d_block(layer_input, filters, strides=1, bn=True): """Discriminator layer""" d = Conv2D(filters, kernel_size=3, strides=strides, padding='same')( layer_input) d = LeakyReLU(alpha=0.2)(d) if bn: d = BatchNormalization(momentum=0.8)(d) return d 
neural_tangents.stax.Dense @_layer def Dense(out_dim, W_std=1.0, b_std=0.0, W_init=_randn(1.0), b_init=_randn( 1.0), parameterization='ntk'): """Layer constructor function for a dense (fully-connected) layer.  Based on `jax.experimental.stax.Dense`. Has a similar API, apart from:  W_init and b_init only change the behavior of the finite width network, and are not used by kernel_fn. In most cases, W_std and b_std should be used instead  Args: parameterization: Either 'ntk' or 'standard'. Under ntk parameterization (https://arxiv.org/abs/1806.07572, page 3), weights and biases are initialized as W_ij ~ N(0,1), b_i ~ N(0,1), and the finite width layer equation is z_i = W_std / sqrt([width]) sum_j W_ij x_j + b_std b_i Under standard parameterization, weights and biases are initialized as W_ij ~ N(0,W_std^2/[width]), b_i ~ N(0,b_std^2), and the finite width layer equation is z_i = \\sum_j W_ij x_j + b_i  Returns: init_fn, apply_fn, kernel_fn """ parameterization = parameterization.lower() ntk_init_fn, _ = ostax.Dense(out_dim, W_init, b_init)  def standard_init_fn(rng, input_shape): output_shape, (W, b) = ntk_init_fn(rng, input_shape) return output_shape, (W * W_std / np.sqrt(input_shape[-1]), b * b_std) if parameterization == 'ntk': init_fn = ntk_init_fn elif parameterization == 'standard': init_fn = standard_init_fn else: raise ValueError('Parameterization not supported: %s' % parameterization)  def apply_fn(params, inputs, **kwargs): W, b = params if parameterization == 'ntk': norm = W_std / np.sqrt(inputs.shape[-1]) return norm * np.dot(inputs, W) + b_std * b elif parameterization == 'standard': return np.dot(inputs, W) + b  def kernel_fn(kernels): """Compute the transformed kernels after a dense layer.""" var1, nngp, var2, ntk = (kernels.var1, kernels.nngp, kernels.var2, kernels.ntk)  def fc(x): return _affine(x, W_std, b_std) if parameterization == 'ntk': var1, nngp, var2 = map(fc, (var1, nngp, var2)) if ntk is not None: ntk = nngp + W_std ** 2 * ntk elif parameterization == 'standard': input_width = kernels.shape1[1] if ntk is not None: ntk = input_width * nngp + 1.0 + W_std ** 2 * ntk var1, nngp, var2 = map(fc, (var1, nngp, var2)) return kernels._replace(var1=var1, nngp=nngp, var2=var2, ntk=ntk, is_gaussian=True) setattr(kernel_fn, _COVARIANCES_REQ, {'marginal': M.OVER_ALL, 'cross': M.OVER_ALL}) return init_fn, apply_fn, kernel_fn 
v2_write_training.games|mongo|fetch def mongo_fetch_games(q_out, num_games): """ Read V1 format games from MongoDB and put them in the output queue (q_out)  Reads a network list from MongoDB from most recents, and then reads games produced by those network until 'num_games' has been read. """ client = pymongo.MongoClient() db = client.test networks = db.networks.find(None, {'_id': False, 'hash': True}).sort('_id', pymongo.DESCENDING).batch_size(5000) game_count = 0 for net in networks: print('Searching for {}'.format(net['hash'])) games = db.games.find({'networkhash': net['hash']}, {'_id': False, 'data': True}) for game in games: game_data = game['data'] q_out.put(game_data.encode('ascii')) game_count += 1 if game_count >= num_games: q_out.put('STOP') return if game_count % 1000 == 0: print('{} games'.format(game_count)) 
avod.core.models.rpn_model.RpnModel.feed|dict|create def create_feed_dict(self, sample_index=None): """ Fills in the placeholders with the actual input values. Currently, only a batch size of 1 is supported  Args: sample_index: optional, only used when train_val_test == 'test', a particular sample index in the dataset sample list to build the feed_dict for  Returns: a feed_dict dictionary that can be used in a tensorflow session """ if self._train_val_test in ['train', 'val']: if sample_index is not None: raise ValueError( 'sample_index should be None. Do not load particular samples during train or val' ) sample = None anchors_info = [] valid_sample = False while not valid_sample: if self._train_val_test == 'train': samples = self.dataset.next_batch(batch_size=1) else: samples = self.dataset.next_batch(batch_size=1, shuffle=False) sample = samples[0] anchors_info = sample.get(constants.KEY_ANCHORS_INFO) train_cond = (self._train_val_test == 'train' and self. _train_on_all_samples) eval_cond = (self._train_val_test == 'val' and self. _eval_all_samples) if anchors_info or train_cond or eval_cond: valid_sample = True else: if sample_index is not None: samples = self.dataset.load_samples([sample_index]) else: samples = self.dataset.next_batch(batch_size=1, shuffle=False) sample = samples[0] anchors_info = sample.get(constants.KEY_ANCHORS_INFO) sample_name = sample.get(constants.KEY_SAMPLE_NAME) sample_augs = sample.get(constants.KEY_SAMPLE_AUGS) label_anchors = sample.get(constants.KEY_LABEL_ANCHORS) label_classes = sample.get(constants.KEY_LABEL_CLASSES) label_boxes_3d = sample.get(constants.KEY_LABEL_BOXES_3D) image_input = sample.get(constants.KEY_IMAGE_INPUT) bev_input = sample.get(constants.KEY_BEV_INPUT) image_shape = [image_input.shape[0], image_input.shape[1]] ground_plane = sample.get(constants.KEY_GROUND_PLANE) stereo_calib_p2 = sample.get(constants.KEY_STEREO_CALIB_P2) self._fill_anchor_pl_inputs(anchors_info=anchors_info, ground_plane= ground_plane, image_shape=image_shape, stereo_calib_p2= stereo_calib_p2, sample_name=sample_name, sample_augs=sample_augs) self._placeholder_inputs[self.PL_IMG_IDX] = [int(sample_name)] self._placeholder_inputs[self.PL_BEV_INPUT] = bev_input self._placeholder_inputs[self.PL_IMG_INPUT] = image_input self._placeholder_inputs[self.PL_LABEL_ANCHORS] = label_anchors self._placeholder_inputs[self.PL_LABEL_BOXES_3D] = label_boxes_3d self._placeholder_inputs[self.PL_LABEL_CLASSES] = label_classes self._placeholder_inputs[self.PL_IMG_IDX] = [int(sample_name)] self._placeholder_inputs[self.PL_CALIB_P2] = stereo_calib_p2 self._placeholder_inputs[self.PL_GROUND_PLANE] = ground_plane self.sample_info.clear() self.sample_info['sample_name'] = sample_name self.sample_info['rpn_mini_batch'] = anchors_info feed_dict = dict() for key, value in self.placeholders.items(): feed_dict[value] = self._placeholder_inputs[key] return feed_dict 
texar.modules.decoders.tf_helpers.GreedyEmbeddingHelper.Greedy|Helper|Embedding def __init__(self, embedding, start_tokens, end_token): """Initializer.  Args: embedding: A callable or the `params` argument for `embedding_lookup`. If a callable, it can take a vector tensor of `ids` (argmax ids), or take two arguments (`ids`, `times`), where `ids` is a vector tensor of argmax ids, and `times` is a vector tensor of current time steps (i.e., position ids). The latter case can be used when attr:`embedding` is a combination of word embedding and position embedding. The returned tensor will be returned by :meth:`next_inputs`. start_tokens: `int32` vector shaped `[batch_size]`, the start tokens. end_token: `int32` scalar, the token that marks end of decoding.  Raises: ValueError: if `start_tokens` is not a 1D tensor or `end_token` is not a scalar. """ if callable(embedding): self._embedding_fn = embedding else: self._embedding_fn = lambda ids: embedding_ops.embedding_lookup( embedding, ids) self._start_tokens = ops.convert_to_tensor(start_tokens, dtype=dtypes. int32, name='start_tokens') self._end_token = ops.convert_to_tensor(end_token, dtype=dtypes.int32, name='end_token') if self._start_tokens.get_shape().ndims != 1: raise ValueError('start_tokens must be a vector') self._batch_size = shape_list(start_tokens)[0] if self._end_token.get_shape().ndims != 0: raise ValueError('end_token must be a scalar') self._embedding_args_cnt = len(get_args(self._embedding_fn)) if self._embedding_args_cnt == 1: self._start_inputs = self._embedding_fn(self._start_tokens) elif self._embedding_args_cnt == 2: times = tf.zeros([self._batch_size], dtype=tf.int32) self._start_inputs = self._embedding_fn(self._start_tokens, times) else: raise ValueError('`embedding` should expect 1 or 2 arguments.') 
facenet-master.tmp.visualize_vggface.visstd def visstd(a, s=0.1): """Normalize the image range for visualization""" return (a - a.mean()) / max(a.std(), 0.0001) * s + 0.5 
classification.ops.fisher_blocks.NaiveDiagonalFB.multiply def multiply(self, vector): vector_flat = utils.tensors_to_column(vector) out_flat = vector_flat * (self._factor.get_cov() + self._damping) return utils.column_to_tensors(vector, out_flat) 
r2r_problem.R2RProblem.get|dict|eval def _get_eval_dict(self): return {'eval/success_rate': eval_metric.get_success_rate, 'eval/navigation_error': eval_metric.get_navigation_error, 'eval/path_length': eval_metric.get_path_length, 'eval/oracle_success': eval_metric.get_oracle_success, 'eval/num_steps_before_stop': eval_metric.get_num_steps_before_stop, 'eval/spl': eval_metric.get_spl, 'eval/undiscounted_episode_reward': eval_metric.get_undisc_episode_reward, 'eval/cls': eval_metric. get_cls, 'eval/dtw': eval_metric.get_dtw, 'eval/norm_dtw': eval_metric.get_norm_dtw, 'eval/sdtw': eval_metric.get_sdtw, ( 'eval/' + common.VISUALIZATION_IMAGES): eval_metric. get_visualization_image} 
preprocess_unlabeled.pair|truncate|seq def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng): """Truncates a pair of sequences to a maximum sequence length.""" while True: total_length = len(tokens_a) + len(tokens_b) if total_length <= max_num_tokens: break trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b assert len(trunc_tokens) >= 1 if rng.random() < 0.5: del trunc_tokens[0] else: trunc_tokens.pop() 
fda.attacks.FDA.attack.body def body(i, e): new_eta = self.attack_single_step(x, e, y) return i + 1, new_eta 
src.util.NN.gru|short|bi|conn def short_conn_bi_gru(x, x_lens, n_hidden, name, keep_prob, is_training): output1, output1_state = BiGRU(x, x_lens, n_hidden, name + '_1') middle1 = tf.concat(output1, 2) middle1 = tf.layers.dropout(middle1, 1 - keep_prob, is_training) output2, output2_state = BiGRU(middle1, x_lens, n_hidden, name + '_2') return (output1[0] + output2[0], output1[1] + output2[1]), ( output1_state[0] + output2_state[0], output1_state[1] + output2_state[1]) 
nmt.model_helper.CellWrapper.cell|wrapped @property def wrapped_cell(self): return self._cell 
chaos_models.Torus2.integrate def integrate(self, X0, tpts): """ X0 : 3-tuple, the initial values of the three coordinates tpts : np.array, the time mesh """ x0, y0, z0 = X0 sol = odeint(self, (x0, y0, z0), tpts) return sol.T 
official.utils.flags.core.parse|flags def parse_flags(argv=None): """Reset flags and reparse. Currently only used in testing.""" flags.FLAGS.unparse_flags() absl_app.parse_flags_with_usage(argv or sys.argv) 
slac.environments.gym_wrappers.RenderGymWrapper.render def render(self, mode='rgb_array'): if mode == 'rgb_array': return self._env.sim.render(**self._render_kwargs)[::-1, :, :] else: return self._env.render(mode=mode) 
thumt.utils.lrp.LegacyGRUCell_decoder_v2n.size|output @property def output_size(self): return self._num_units 
prune_mobilenet_for_imagenet.PruneMobileNetForImagenet.Prune|Net|Mobile|Imagenet|For def __init__(self, weights_dict, **prune_args): super(PruneMobileNetForImageNet, self).__init__(weights_dict, **prune_args) 
envs.DiagnosticsInfoI.step|after def _after_step(self, observation, reward, done, info): to_log = {} if self._episode_length == 0: self._episode_time = time.time() self._local_t += 1 cur_episode_id = info.get('vectorized.episode_id', 0) if self._last_episode_id == cur_episode_id: self._frames_in_episode += 1 cur_time = time.time() elapsed = max(1e-05, cur_time - self._last_time_in_episode) self._fps_in_episode = self._frames_in_episode / float(elapsed) else: self._frames_in_episode = 0 self._last_episode_id = cur_episode_id self._last_time_in_episode = time.time() if self._local_t % self._log_interval == 0: cur_time = time.time() elapsed = max(1e-05, cur_time - self._last_time) fps = self._log_interval / float(elapsed) self._last_time = cur_time to_log['diagnostics/fps'] = fps if self._last_episode_id == cur_episode_id: to_log['diagnostics/fps_within_episode'] = self._fps_in_episode self._last_episode_id = cur_episode_id if reward is not None: self._episode_reward += reward if observation is not None: self._episode_length += 1 if done: logger.info('Episode terminating: episode_reward=%s episode_length=%s', self._episode_reward, self._episode_length) total_time = max(1e-05, time.time() - self._episode_time) to_log['global/episode_reward'] = self._episode_reward to_log['global/episode_length'] = self._episode_length to_log['global/episode_time'] = total_time to_log['global/reward_per_time'] = self._episode_reward / float( total_time) self._episode_reward = 0 self._episode_length = 0 self._episode_actions = [] self._episode_count += 1 return observation, reward, done, to_log 
Micro-Net-master.Models.unet.unet def unet(inputs_shape, nb_classes, use_bias=False, activation='relu', kernel_initializer='he_normal'): inputs = Input(shape=inputs_shape) conv1 = Conv2D(64, 3, use_bias=use_bias, activation=activation, padding ='same', kernel_initializer=kernel_initializer)(inputs) conv1 = Conv2D(64, 3, use_bias=use_bias, activation=activation, padding ='same', kernel_initializer=kernel_initializer)(conv1) pool1 = MaxPooling2D(pool_size=(2, 2), padding='same')(conv1) conv2 = Conv2D(128, 3, use_bias=use_bias, activation=activation, padding='same', kernel_initializer=kernel_initializer)(pool1) conv2 = Conv2D(128, 3, use_bias=use_bias, activation=activation, padding='same', kernel_initializer=kernel_initializer)(conv2) pool2 = MaxPooling2D(pool_size=(2, 2), padding='same')(conv2) conv3 = Conv2D(256, 3, use_bias=use_bias, activation=activation, padding='same', kernel_initializer=kernel_initializer)(pool2) conv3 = Conv2D(256, 3, use_bias=use_bias, activation=activation, padding='same', kernel_initializer=kernel_initializer)(conv3) pool3 = MaxPooling2D(pool_size=(2, 2), padding='same')(conv3) conv4 = Conv2D(512, 3, use_bias=use_bias, activation=activation, padding='same', kernel_initializer=kernel_initializer)(pool3) conv4 = Conv2D(512, 3, use_bias=use_bias, activation=activation, padding='same', kernel_initializer=kernel_initializer)(conv4) pool4 = MaxPooling2D(pool_size=(2, 2), padding='same')(conv4) conv5 = Conv2D(1024, 3, use_bias=use_bias, activation=activation, padding='same', kernel_initializer=kernel_initializer)(pool4) conv5 = Conv2D(1024, 3, use_bias=use_bias, activation=activation, padding='same', kernel_initializer=kernel_initializer)(conv5) up6 = Deconv2D(filters=512, kernel_size=(2, 2), strides=2, padding= 'same', kernel_initializer=kernel_initializer, use_bias=use_bias)(conv5 ) cropped_up6 = Cropping2D(cropping=((0, 1), (0, 1)))(up6) merge6 = Concatenate(axis=-1)([conv4, cropped_up6]) conv6 = Conv2D(512, 3, use_bias=use_bias, activation=activation, padding='same', kernel_initializer=kernel_initializer)(merge6) conv6 = Conv2D(512, 3, use_bias=use_bias, activation=activation, padding='same', kernel_initializer=kernel_initializer)(conv6) up7 = Deconv2D(filters=256, kernel_size=(2, 2), strides=2, padding= 'same', kernel_initializer=kernel_initializer, use_bias=use_bias)(conv6 ) cropped_up7 = Cropping2D(cropping=((0, 1), (0, 1)))(up7) merge7 = Concatenate(axis=3)([conv3, cropped_up7]) conv7 = Conv2D(256, 3, use_bias=use_bias, activation=activation, padding='same', kernel_initializer=kernel_initializer)(merge7) conv7 = Conv2D(256, 3, use_bias=use_bias, activation=activation, padding='same', kernel_initializer=kernel_initializer)(conv7) up8 = Deconv2D(filters=128, kernel_size=(2, 2), strides=2, padding= 'same', kernel_initializer=kernel_initializer, use_bias=use_bias)(conv7 ) merge8 = Concatenate(axis=3)([conv2, up8]) conv8 = Conv2D(128, 3, use_bias=use_bias, activation=activation, padding='same', kernel_initializer=kernel_initializer)(merge8) conv8 = Conv2D(128, 3, use_bias=use_bias, activation=activation, padding='same', kernel_initializer=kernel_initializer)(conv8) up9 = Deconv2D(filters=64, kernel_size=(2, 2), strides=2, padding= 'same', kernel_initializer=kernel_initializer, use_bias=use_bias)(conv8 ) merge9 = Concatenate(axis=3)([conv1, up9]) conv9 = Conv2D(64, 3, use_bias=use_bias, activation=activation, padding ='same', kernel_initializer=kernel_initializer)(merge9) conv9 = Conv2D(64, 3, use_bias=use_bias, activation=activation, padding ='same', kernel_initializer=kernel_initializer)(conv9) conv10 = Conv2D(nb_classes, kernel_size=1, use_bias=use_bias, activation='softmax')(conv9) net = Model(inputs=inputs, outputs=conv10) net.name = 'unet' return net 
deepctr.input_embedding.get|vec|list|pooling def get_pooling_vec_list(sequence_embed_dict, sequence_len_dict, sequence_max_len_dict, sequence_fd_list): if sequence_max_len_dict is None or sequence_len_dict is None: return [SequencePoolingLayer(feat.combiner, supports_masking=True)( sequence_embed_dict[feat.name]) for feat in sequence_fd_list] else: return [SequencePoolingLayer(feat.combiner, supports_masking=False) ([sequence_embed_dict[feat.name], sequence_len_dict[feat.name]] ) for feat in sequence_fd_list] 
data_utils.Dataset.predict|seq|paraphrase|print def print_predict_seq2paraphrase(self, output_dict, batch_dict, num_cases=3): """Print the predicted sentences, sequence to k sequence model (given a sentence, predict all k possible paraphrases)""" inputs = batch_dict['enc_inputs'][:3] references = batch_dict['references'][:3] for i in range(num_cases): print('inputs:') print('    ' + self.decode_sent(inputs[i])) pred_para = output_dict['enc_infer_pred'][i] print('paraphrase outputs:') for p in pred_para: print('    ' + self.decode_sent(p)) print('references:') for r in references[i]: print('    ' + self.decode_sent(r)) print('') return 
run_csqa_bert.convert|features|examples|to def convert_examples_to_features(examples, tokenizer, max_seq_length, is_training): """Loads a data file into a list of `InputBatch`s.""" features = [] for example_index, example in enumerate(examples): start_ending_tokens = tokenizer.tokenize(example.start_ending) choices_features = [] for ending_index, ending in enumerate(example.endings): statement = create_hypothesis(get_fitb_from_question(example. context_sentence), ending) statement = example.context_sentence context_tokens = tokenizer.tokenize(statement) context_tokens_choice = context_tokens[:] ending_tokens = start_ending_tokens + tokenizer.tokenize(ending) _truncate_seq_pair(context_tokens_choice, ending_tokens, max_seq_length - 3) tokens = ['[CLS]'] + context_tokens_choice + ['[SEP]' ] + ending_tokens + ['[SEP]'] segment_ids = [0] * (len(context_tokens_choice) + 2) + [1] * (len (ending_tokens) + 1) input_ids = tokenizer.convert_tokens_to_ids(tokens) input_mask = [1] * len(input_ids) padding = [0] * (max_seq_length - len(input_ids)) input_ids += padding input_mask += padding segment_ids += padding assert len(input_ids) == max_seq_length assert len(input_mask) == max_seq_length assert len(segment_ids) == max_seq_length choices_features.append((tokens, input_ids, input_mask, segment_ids)) label = example.label if example_index == 0 and False: logger.info('*** Example ***') logger.info('swag_id: {}'.format(example.swag_id)) for choice_idx, (tokens, input_ids, input_mask, segment_ids ) in enumerate(choices_features): logger.info('choice: {}'.format(choice_idx)) logger.info('tokens: {}'.format(' '.join(tokens))) logger.info('input_ids: {}'.format(' '.join(map(str, input_ids)))) logger.info('input_mask: {}'.format(' '.join(map(str, input_mask)))) logger.info('segment_ids: {}'.format(' '.join(map(str, segment_ids)))) if is_training: logger.info('label: {}'.format(label)) features.append(InputFeatures(example_id=example.swag_id, choices_features=choices_features, label=label)) return features 
classification.controller.weight_container.FFGWeightContainer.weight|sample def sample_weight(self, particles): """ Sample weight from the variational posterior. In the paper, w ~ N(u, (lambda / num_data) * inv(diag(f + gamma_in))) :return: 2D Tensor with size 'batch_size x feature_size' """ homo_weight = self._combine_weight_bias() multi_weight = tf.tile(tf.expand_dims(homo_weight, 0), [particles, 1, 1]) noise = tf.random_normal(shape=tf.shape(multi_weight)) return multi_weight + self._std * noise 
batch_test.BatchTest.test|Parallel @jtu.parameterized.named_parameters(jtu.cases_from_list({'testcase_name': '_train_shape={}_test_shape={}_network={}_{}'.format(train, test, network, name), 'train_shape': train, 'test_shape': test, 'network': network, 'name': name, 'kernel_fn': kernel_fn} for train, test, network in zip(TRAIN_SHAPES, TEST_SHAPES, NETWORK) for name, kernel_fn in KERNELS. items())) def testParallel(self, train_shape, test_shape, network, name, kernel_fn): utils.stub_out_pmap(batch, 2) key = random.PRNGKey(0) key, self_split, other_split = random.split(key, 3) data_self = random.normal(self_split, train_shape) data_other = random.normal(other_split, test_shape) kernel_fn = kernel_fn(key, train_shape[1:], network) kernel_batched = batch._parallel(kernel_fn) _test_kernel_against_batched(self, kernel_fn, kernel_batched, data_self, data_other) 
config.Config.path|setup def setup_path(self): model = self.model_name + '_' + self.model_version output_path = self.output_path + model model_path = self.model_path + model if os.path.exists(model_path): inp = input( 'model %s already existed, overwite[o]; continue[c] or exit[e]?\n' % model) if inp == 'o': shutil.rmtree(model_path) os.mkdir(model_path) shutil.rmtree(output_path) os.mkdir(output_path) elif inp == 'e': print('exiting the program, please rename the model') sys.exit(1) else: pass else: os.mkdir(model_path) os.mkdir(output_path) self.model_path = model_path self.output_path = output_path + '/' return 
bert-master.extract_features.input_fn_builder.fn|input def input_fn(params): """The actual input function.""" batch_size = params['batch_size'] num_examples = len(features) d = tf.data.Dataset.from_tensor_slices({'unique_ids': tf.constant( all_unique_ids, shape=[num_examples], dtype=tf.int32), 'input_ids': tf.constant(all_input_ids, shape=[num_examples, seq_length], dtype= tf.int32), 'input_mask': tf.constant(all_input_mask, shape=[ num_examples, seq_length], dtype=tf.int32), 'input_type_ids': tf. constant(all_input_type_ids, shape=[num_examples, seq_length], dtype=tf.int32)}) d = d.batch(batch_size=batch_size, drop_remainder=False) return d 
neural_tangents.stax.Dense.kernel_fn.fc def fc(x): return _affine(x, W_std, b_std) 
AffineCouplingSdnEx4.AffineCouplingSdnEx4.forward def _forward(self, x, yy, nlf0=None, nlf1=None, iso=None, cam=None): if self._last_layer: x = tf.reshape(x, (-1, self.i0, self.i1, self.ic)) yy = tf.reshape(yy, (-1, self.i0, self.i1, self.ic)) if yy.shape[1] == 2 * x.shape[1]: yy = squeeze2d(yy, 2) scale = sdn_model_params_ex4(yy, iso, self.gain_init) shift = 0.0 y = x if scale is not None: y *= scale if shift is not None: y += shift return y 
deepctr.layers.sequence.AttentionSequencePoolingLayer.shape|output|compute def compute_output_shape(self, input_shape): if self.return_score: return None, 1, input_shape[1][1] else: return None, 1, input_shape[0][-1] 
plotting.ae_convergence_plot.mat|make|data def make_data_mat(exp_prefix, err_file): exp_dirs = [k for k in os.listdir(start_dir) if k.startswith(exp_prefix)] err_paths = [os.path.join(start_dir, k, err_file) for k in exp_dirs] read_errs = [read_errors(k)[0] for k in err_paths] vec_errs = [np.asarray(k) for k in read_errs] err_block = np.stack(vec_errs, axis=1) return err_block 
texar.modules.decoders.tf_helpers.ScheduledEmbeddingTrainingHelper.Training|Scheduled|Helper|Embedding def __init__(self, inputs, sequence_length, embedding, sampling_probability, time_major=False, seed=None, scheduling_seed=None, name=None): """Initializer.  Args: inputs: A (structure of) input tensors. sequence_length: An int32 vector tensor. embedding: A callable or the `params` argument for `embedding_lookup`. If a callable, it can take a vector tensor of token `ids`, or take two arguments (`ids`, `times`), where `ids` is a vector tensor of token ids, and `times` is a vector tensor of current time steps (i.e., position ids). The latter case can be used when attr:`embedding` is a combination of word embedding and position embedding. sampling_probability: A 0D `float32` tensor: the probability of sampling categorically from the output ids instead of reading directly from the inputs. time_major: Python bool.  Whether the tensors in `inputs` are time major. If `False` (default), they are assumed to be batch major. seed: The sampling seed. scheduling_seed: The schedule decision rule sampling seed. name: Name scope for any created operations.  Raises: ValueError: if `sampling_probability` is not a scalar or vector. """ with ops.name_scope(name, 'ScheduledEmbeddingSamplingWrapper', [ embedding, sampling_probability]): if callable(embedding): self._embedding_fn = embedding else: self._embedding_fn = lambda ids: embedding_ops.embedding_lookup( embedding, ids) self._embedding_args_cnt = len(get_args(self._embedding_fn)) if self._embedding_args_cnt != 1 and self._embedding_args_cnt != 2: raise ValueError('`embedding` should expect 1 or 2 arguments.') self._sampling_probability = ops.convert_to_tensor(sampling_probability , name='sampling_probability') if self._sampling_probability.get_shape().ndims not in (0, 1): raise ValueError( 'sampling_probability must be either a scalar or a vector. saw shape: %s' % self._sampling_probability.get_shape()) self._seed = seed self._scheduling_seed = scheduling_seed super(ScheduledEmbeddingTrainingHelper, self).__init__(inputs= inputs, sequence_length=sequence_length, time_major=time_major, name=name) 
gym_pycolab.ascii_art.ascii|game|art|to def ascii_art_to_game(art, what_lies_beneath, sprites=None, drapes=None, backdrop=things.Backdrop, update_schedule=None, z_order=None): """Construct a pycolab game from an ASCII art diagram.  This function helps to turn ASCII art diagrams like the following (which is a Sokoban-like puzzle):  [' @@@@@@ ', ' @  . @ ',        # '@' means "wall" '@@ab @@ ',        # 'P' means "player" '@  .c @ ',        # '.' means "box storage location" '@.  dP@ ',        # 'a'-'g' are all for separate boxes '@.@@@@@@',        # ' ' means "open, traversable space" '@ @ @@ @', '@ e  . @', '@@@@@@@@',]  into pycolab games. The basic idea is that you supply the diagram, along with hints about which characters correspond to `Sprite`s and `Drape`s and the classes that implement those `Sprite`s and `Drape`s. This function then returns an initialised `Engine` object, all ready for you to call the `its_showtime` method and start the game.  Several of this function's arguments require you to supply subclasses of the classes found in `things.py`. If your subclass constructors take the same number of arguments as their `things.py` superclasses, then they can be listed directly. Otherwise, you will need to pack the subclasses and their additional `args` and `kwargs` into a `Partial` object. So, for example, if you have a `Sprite` subclass with a constructor like this:  class MySprite(Sprite): def __init__(self, corner, position, character, mood, drink_quantity): ...  you could package `MySprite` and the "extra" arguments in any of the following ways (among others):  Partial(MySprite, 'drowsy', 'two pints') Partial(MySprite, 'yawning', drink_quantity='three pints') Partial(MySprite, mood='asleep', drink_quantity='four pints')  Args: art: An ASCII art diagram depicting a game board. This should be a list or tuple whose values are all strings containing the same number of ASCII characters. what_lies_beneath: a single-character ASCII string that will be substituted into the `art` diagram at all places where a character that keys `sprites` or `drapes` is found; *or*, this can also be an entire second ASCII art diagram whose values will be substituted into `art` at (only) those locations. In either case, the resulting diagram will be used to initialise the game's `Backdrop`. sprites: a dict mapping single-character ASCII strings to `Sprite` classes (not objects); or to `Partial` objects that hold the classes and "extra" `args`es and `kwargs`es to use during their construction. It's fine if a character used as a key doesn't appear in the `art` diagram: in this case, we assume that the corresponding `Sprite` will be located at `0, 0`. (If you intend your `Sprite` to be invisible, the `Sprite` will have to take care of that on its own after it is built.) (Optional; omit if your game has no sprites.) drapes: a dict mapping single-character ASCII strings to `Drape` classes (not objects); or to `Partial` objects that hold the classes and "extra" `args`es and `kwargs`es to use during their construction. It's fine if a character used as a key doesn't appear in the `art` diagram: in this case, we assume that the `Drape`'s curtain (i.e. its mask) is completely empty (i.e. False). (Optional; omit if your game has no drapes.) backdrop: a `Backdrop` class (not an object); or a `Partial` object that holds the class and "extra" `args` and `kwargs` to use during its construction. (Optional; if unset, `Backdrop` is used directly, which is fine for a game where the background scenery never changes and contains no game logic.) update_schedule: A list of single-character ASCII strings indicating the order in which the `Sprite`s and `Drape`s should be consulted by the `Engine` for updates; or, a list of lists that imposes an ordering as well, but that groups the entities in each list into separate update groups (refer to `Engine` documentation). (Optional; if unspecified, the ordering will be arbitrary---be mindful of this if your game uses advanced features like scrolling, where update order is pretty important.) z_order: A list of single-character ASCII strings indicating the depth ordering of the `Sprite`s and `Drape`s (from back to front). (Optional; if unspecified, the ordering will be the same as what's used for `update_schedule`).  Returns: An initialised `Engine` object as described.  Raises: TypeError: when `update_schedule` is neither a "flat" list of characters nor a list of lists of characters. ValueError: numerous causes, nearly always instances of the user not heeding the requirements stipulated in Args:. The exception messages should make most errors fairly easy to debug. """ if sprites is None: sprites = {} if drapes is None: drapes = {} sprites = {char: (sprite if isinstance(sprite, Partial) else Partial( sprite)) for char, sprite in six.iteritems(sprites)} drapes = {char: (drape if isinstance(drape, Partial) else Partial(drape )) for char, drape in six.iteritems(drapes)} if not isinstance(backdrop, Partial): backdrop = Partial(backdrop) non_backdrop_characters = set() non_backdrop_characters.update(sprites.keys()) non_backdrop_characters.update(drapes.keys()) if update_schedule is None: update_schedule = list(non_backdrop_characters) if isinstance(update_schedule, str): update_schedule = list(update_schedule) if all(isinstance(item, str) for item in update_schedule): update_schedule = [update_schedule] try: flat_update_schedule = list(itertools.chain.from_iterable( update_schedule)) except TypeError: raise TypeError( 'if any element in update_schedule is an iterable (like a list), all elements in update_schedule must be' ) if set(flat_update_schedule) != non_backdrop_characters: raise ValueError( 'if specified, update_schedule must list each sprite and drape exactly once.' ) if z_order is None: z_order = flat_update_schedule if set(z_order) != non_backdrop_characters: raise ValueError( 'if specified, z_order must list each sprite and drape exactly once.' ) if isinstance(what_lies_beneath, str) and len(what_lies_beneath) != 1: raise ValueError( 'what_lies_beneath may either be a single-character ASCII string or a list of ASCII-character strings' ) try: _ = [ord(character) for character in ''.join(what_lies_beneath)] _ = [ord(character) for character in non_backdrop_characters] _ = [ord(character) for character in z_order] _ = [ord(character) for character in flat_update_schedule] except TypeError: raise ValueError( 'keys of sprites, keys of drapes, what_lies_beneath (or its entries), values in z_order, and (possibly nested) values in update_schedule must all be single-character ASCII strings.' ) if non_backdrop_characters.intersection(''.join(what_lies_beneath)): raise ValueError( 'any character specified in what_lies_beneath must not be one of the characters used as keys in the sprites or drapes arguments.' ) art = ascii_art_to_uint8_nparray(art) if isinstance(what_lies_beneath, str): what_lies_beneath = np.full_like(art, ord(what_lies_beneath)) else: what_lies_beneath = ascii_art_to_uint8_nparray(what_lies_beneath) if art.shape != what_lies_beneath.shape: raise ValueError( 'if not a single ASCII character, what_lies_beneath must be ASCII art whose shape is the same as that of the ASCII art in art.' ) update_group_for = {} for i, update_group in enumerate(update_schedule): group_id = '{:05d}'.format(i) update_group_for.update({character: group_id for character in update_group}) game = engine.Engine(*art.shape) for character in flat_update_schedule: game.update_group(update_group_for[character]) mask = art == ord(character) if character in drapes: partial = drapes[character] game.add_prefilled_drape(character, mask, partial.pycolab_thing, *partial.args, **partial.kwargs) if character in sprites: row, col = np.where(mask) if len(row) > 1: raise ValueError( 'sprite character {} can appear in at most one place in art.' .format(character)) row, col = (int(row), int(col)) if len(row) > 0 else (0, 0) partial = sprites[character] game.add_sprite(character, (row, col), partial.pycolab_thing, * partial.args, **partial.kwargs) art[mask] = what_lies_beneath[mask] game.set_z_order(z_order) game.set_prefilled_backdrop(*backdrop.args, characters=''.join(chr(c) for c in np.unique(art)), prefill=art.view(np.uint8), backdrop_class= backdrop.pycolab_thing, **backdrop.kwargs) return game 
craystack.codecs_test.test_serial_resized.resize|pop def pop_resize(message): assert message[0].shape == shape1 message, symbol = pop(message) message = cs.reshape_head(message, shape2) return message, symbol 
texar.modules.memory.memory_network.MemNetBase.Mem|Base|Net def __init__(self, raw_memory_dim, input_embed_fn=None, output_embed_fn= None, query_embed_fn=None, hparams=None): ModuleBase.__init__(self, hparams) self._raw_memory_dim = raw_memory_dim self._n_hops = self._hparams.n_hops self._relu_dim = self._hparams.relu_dim self._memory_size = self._hparams.memory_size with tf.variable_scope(self.variable_scope): self._A, self._C, self._B, self._memory_dim = self._build_embed_fn( input_embed_fn, output_embed_fn, query_embed_fn) self.H = None if self.hparams.use_H: self.H = tf.get_variable(name='H', shape=[self._memory_dim, self._memory_dim]) 
bot_model.BaryOT.gloss|calc def calc_gloss(self, x, y, ux, vy, config): fy = self.g(y) return self.map_loss(x, y, fy, ux, vy, self.cost, config.lambda_ineq) 
model_unet.UNET.UNET def __init__(self, sess, patch_shape, extraction_step): self.sess = sess self.patch_shape = patch_shape self.extraction_step = extraction_step self.d_bns = [batch_norm(name='u_bn{}'.format(i)) for i in range(14)] 
classification.ops.fisher_factors.diagonal|covariance|initializer def diagonal_covariance_initializer(shape, dtype, partition_info): if INIT_COVARIANCES_AT_ZERO: return array_ops.zeros(shape, dtype) return array_ops.ones(shape, dtype) 
postfilter.MLPG|generalized def generalized_MLPG(Input_seq, Cov, dynamic_flag=2): T, sddim = Input_seq.shape W = construct_dynamic_matrix(T, sddim // (dynamic_flag + 1), dynamic_flag) U = scipy.sparse.block_diag([Cov for i in range(T)], format='csr') U.eliminate_zeros() WU = W.T.dot(U) WUW = WU.dot(W) WUm = WU.dot(Input_seq.flatten()) odata = scipy.sparse.linalg.spsolve(WUW, WUm, use_umfpack=False).reshape(T, sddim // (dynamic_flag + 1)) return odata 
official.utils.logs.logger_test.BaseBenchmarkLoggerTest.tear|Down def tearDown(self): super(BaseBenchmarkLoggerTest, self).tearDown() tf.logging.info = self._actual_log 
GPSig.kernels.Sequential.Kdiag def Kdiag(self, X, presliced=False, override_full=False): if not presliced: X, _ = self._slice(X, None) return self._sequentializer_diag(self._base_kern, X, override_full= override_full) 
classification.ops.loss_functions.CategoricalLogitsNegativeLogProbLoss.params @property def params(self): return self._logits 
utils.training_helper.DenseBinfDecoder.shape|output|compute def compute_output_shape(self, input_shape): out = super().compute_output_shape(input_shape) if self.binf_to_ipa is not None: out = out[:-1].concatenate(self.binf_to_ipa.shape[-1]) return out 
similarity_datasets.get|Lex|Sim|ES def get_SimLex999_ES(): data = pd.read_csv(SIM999_ES_path, sep='\t', header=None).values return Bunch(X=data[:, 0:2].astype('object'), y=data[:, (2)].astype(np. float)) 
thumt.utils.lrp.LegacyGRUCell_encoder_v2n.call def __call__(self, inputs, state, w_x_h_last, params, scope=None): with tf.variable_scope(scope, default_name='gru_cell', values=[inputs, state]): if not isinstance(inputs, (list, tuple)): inputs = [inputs] bs = tf.shape(w_x_h_last)[0] emb = tf.shape(inputs)[-1] w_x_x = tf.ones([bs, 1, emb], dtype=tf.float32) all_inputs = list(inputs) + [state] r_linear = linear_v2n(all_inputs, self._num_units, False, [w_x_x, w_x_h_last], params, False, scope='reset_gate', d2=True) w_x_r, w_xlast_r = r_linear['weight_ratios'] r = tf.nn.sigmoid(r_linear['output']) u_linear = linear_v2n(all_inputs, self._num_units, False, [w_x_x, w_x_h_last], params, False, scope='update_gate', d2=True) w_x_u, w_xlast_u = u_linear['weight_ratios'] u = tf.nn.sigmoid(u_linear['output']) reseted = r * state w_x_reseted = w_x_r w_xlast_reseted = w_xlast_r w_tx_reseted = tf.concat([w_x_reseted, w_xlast_reseted], 1) all_inputs = list(inputs) + [reseted] c_linear = linear_v2n(all_inputs, self._num_units, True, [w_x_x, w_tx_reseted], params, False, scope='candidate', d2=True) w_x_c_direct, w_tx_reseted_c = c_linear['weight_ratios'] w_x_reseted_c, w_xlast_c = tf.split(w_tx_reseted_c, [1, tf.shape( w_tx_reseted_c)[1] - 1], axis=1) w_x_c = w_x_c_direct + w_x_reseted_c c = c_linear['output'] h1 = u * tf.tanh(c) h2 = (1.0 - u) * state new_state = h1 + h2 new_state_stab = stabilize(new_state, params.stab) w_x_newh = w_x_c * tf.expand_dims(h1 / new_state_stab, axis=1) w_xlast_newh = w_xlast_c * tf.expand_dims(h1 / new_state_stab, axis=1 ) + w_x_h_last * tf.expand_dims(h2 / new_state_stab, axis=1) return new_state, new_state, w_xlast_newh, w_x_newh 
eval_metric.get|label|score def get_score_label(action_list, env_output_list, action_output, environment): del action_list, environment return [(tf.sigmoid(action_output[0][0]), env_output_list[0]. observation[constants.LABEL])] 
graphsage.layers.Layer.call def __call__(self, inputs): with tf.name_scope(self.name): if self.logging and not self.sparse_inputs: tf.summary.histogram(self.name + '/inputs', inputs) outputs = self._call(inputs) if self.logging: tf.summary.histogram(self.name + '/outputs', outputs) return outputs 
empirical_test.network|build def _build_network(input_shape, network, out_logits): if len(input_shape) == 1: assert network == 'FLAT' return stax.Dense(out_logits, W_std=2.0, b_std=0.5) elif len(input_shape) == 3: if network == 'POOLING': return stax.serial(stax.Conv(CONVOLUTION_CHANNELS, (3, 3), W_std=2.0, b_std=0.05), stax.GlobalAvgPool(), stax.Dense( out_logits, W_std=2.0, b_std=0.5)) elif network == 'FLAT': return stax.serial(stax.Conv(CONVOLUTION_CHANNELS, (3, 3), W_std=2.0, b_std=0.05), stax.Flatten(), stax.Dense( out_logits, W_std=2.0, b_std=0.5)) else: raise ValueError('Unexpected network type found: {}'.format( network)) else: raise ValueError('Expected flat or image test input.') 
texar.data.data.multi_aligned_data.MultiAlignedData.make|dataset def _make_dataset(self): datasets = [] for _, hparams_i in enumerate(self._hparams.datasets): dtype = hparams_i.data_type if _is_text_data(dtype) or _is_scalar_data(dtype): dataset = tf.data.TextLineDataset(hparams_i.files, compression_type=hparams_i.compression_type) datasets.append(dataset) elif _is_tfrecord_data(dtype): dataset = tf.data.TFRecordDataset(filenames=hparams_i.files) num_shards = hparams_i.num_shards shard_id = hparams_i.shard_id if num_shards is not None and shard_id is not None: dataset = dataset.shard(num_shards, shard_id) datasets.append(dataset) else: raise ValueError('Unknown data type: %s' % hparams_i.data_type) return tf.data.Dataset.zip(tuple(datasets)) 
elpips.elpips.Config.Config def __init__(self): self.metric = 'vgg_ensemble' self.enable_dropout = True self.dropout_keep_prob = 0.99 self.enable_offset = True self.offset_max = 7 self.enable_flip = True self.enable_swap = True self.enable_color_permutation = True self.enable_color_multiplication = True self.color_multiplication_mode = 'color' self.enable_scale = True self.set_scale_levels(8) self.fast_and_approximate = False self.batch_size = 1 self.average_over = 1 self.dtype = tf.float32 
invariant_l0_attack.improve|transform def improve_transform(): sys.path.append('gan/') from gan.acgan_mnist import Generator zin = tf.placeholder(tf.float32, [None, 74]) x_target = tf.placeholder(tf.float32, [None, 28, 28, 1]) generated_images, _ = Generator(None, zin) generated_images = tf.reshape(generated_images, [-1, 28, 28, 1]) similarity_loss = tf.reduce_sum(np.abs(generated_images - x_target), axis=(1, 2, 3)) z_loss = 0.01 * tf.reduce_sum(zin[:, 10:] ** 2, axis=1) total_loss = similarity_loss + z_loss grads = tf.gradients(similarity_loss, [zin])[0] sess = tf.Session() touse = [x for x in tf.trainable_variables() if 'Generator' in x.name] saver = tf.train.Saver(touse) saver.restore(sess, 'gan/model/mnist-acgan-2') keras.backend.set_learning_phase(False)  def score(image, label): zs = np.random.normal(0, 1, size=(128, 74)) zs[:, :10] = 0 zs[:, (label)] = 1 for _ in range(30): ell, l_sim, l_z, nimg, delta = sess.run((total_loss, similarity_loss, z_loss, generated_images, grads), {zin: zs, x_target: image[(np.newaxis), :, :, :]}) zs[:, 10:] -= delta[:, 10:] * 0.01 return np.min(ell) transformation_matrix = tf.placeholder(tf.float32, [8]) xs = tf.placeholder(DTYPE, [None, 28, 28, 1]) transformed = tf.contrib.image.transform(xs, transformation_matrix, 'BILINEAR') uids = list(set([int(x.split('_')[1]) for x in os.listdir('best') if 'best_' in x and '_10000' in x])) num = [max([int(x.split('_')[2][:-4]) for x in os.listdir('best') if str(uids[i]) in x and 'idx' not in x and 'tran' not in x]) for i in range(4)] arr = [] for fileid, filecount in zip(uids, num): best = np.load('best/best_%d_%d.npy' % (fileid, filecount)) best_idx = np.array(np.load('best/best_%d_%d_idx.npy' % (fileid, filecount)), dtype=np.int32) best_transforms = np.load('best/best_%d_transforms_%d.npy' % ( fileid, filecount)) mask = abs(best - x_test[use_idx]) > 0.5 delta = np.sum(mask, axis=(1, 2, 3)) arr.append(delta) print(delta) print(np.median(delta)) arr = np.min(arr, axis=0) fout = open('/tmp/out.html', 'w')  def write(txt, img, lab, delta, doinv=False, do=True): if do: if len(img.shape) == 4: img = img[0] if doinv: timg = sess.run(transformed, {xs: img[(np.newaxis), :, :, : ], transformation_matrix: inv.flatten()[:-1]})[0] else: timg = img s = score(timg, lab) else: s = 0 print(lab, type(lab)) print(delta, type(delta)) fout.write( '<div style="float: left; padding: 3px">%d[%d]@%d<br/><img style="width:50px; height:50px;" src="%s"/></div>' % (int(s), lab, delta, txt)) scipy.misc.imsave('/tmp/' + txt, img.reshape((28, 28))) print('score of being', lab, 'is:', s) show(img) fout.flush() return s candidates = [] for IDX in range(100): fout.write("<br/><div style='clear: both'></div><br/>") mat, inv = compute_mat(*best_transforms[IDX]) img = sess.run(transformed, {xs: x_train[best_idx[IDX:IDX + 1]], transformation_matrix: mat.flatten()[:-1]}) print('Source image') write('img_%d_0.png' % IDX, x_test[use_idx[IDX]], y_test[use_idx[ IDX]], 0) print('Target image') write('img_%d_2.png' % IDX, x_train[best_idx[IDX]], y_train[ best_idx[IDX]], 0) mask = abs(x_test[use_idx[IDX]] - img) > 0.5 print('Transformed target image') write('img_%d_1.png' % IDX, img, y_train[best_idx[IDX]], np.sum( mask), True) write('img_%d_1.5.png' % IDX, np.array(mask, dtype=np.int32), y_train[best_idx[IDX]], np.sum(mask), True, do=False) print('Mask delta', np.sum(mask)) show(mask) clusters = cluster(mask) print('\n'.join([''.join([str(int(x)) for x in y]) for y in clusters]).replace('0', ' ').replace('-1', '*')) write('img_%d_1.6.png' % IDX, np.array(mask, dtype=np.int32), y_train[best_idx[IDX]], np.sum(mask), True, do=False) import matplotlib colored = np.zeros((28, 28, 3)) for i in range(28): for j in range(28): if mask[0, i, j, 0] != 0: colored[(i), (j), :] = matplotlib.colors.to_rgb('C' + str(int(clusters[i, j] + 1))) scipy.misc.imsave('/tmp/img_%d_1.6.png' % IDX, colored) possible = [] for nid, subset in enumerate(itertools.product([False, True], repeat=int(np.max(clusters)))): if np.sum(subset) == 0: continue mask = np.any([(clusters == i + 1) for i, x in enumerate(subset ) if x], axis=0) + 0.0 mask = mask.reshape(img.shape) print('Mask weight', np.sum(mask)) out = mask * img + (1 - mask) * x_test[use_idx[IDX]] print('New Image') s = write('img_%d_%d.png' % (IDX, 3 + nid), out, y_train[ best_idx[IDX]], np.sum(mask), True) possible.append((out, s)) candidates.append(possible) print('-' * 80) import pickle pickle.dump(candidates, open('/tmp/candidates.p', 'wb')) 
models.neural_network.NeuralNetwork.rand|indices|gen def gen_rand_indices(self, low=0, high=1000, seed=SEED, num_samples=1000): """ Randomly sample indices from a range """ np.random.seed(seed) indices = np.random.choice(range(low, high), num_samples) return indices 
optimizer.Optimizer.get|deeppoly def get_deeppoly(self, specLB, specUB, lexpr_weights, lexpr_cst, lexpr_dim, uexpr_weights, uexpr_cst, uexpr_dim, expr_size): """ This function will go through self.operations and self.resources and create a list of Deeppoly-Nodes which then can be run by an Analyzer object. It is assumed that self.resources[i]['deeppoly'] holds the resources for an operation of type self.operations[i]. self.operations should only contain a combination of the following 4 basic sequences: - Placholder         (only at the beginning) - MatMul -> Add -> Relu - Conv2D -> Add -> Relu    (not as last layer) - MaxPool         (only as intermediate layer)  Arguments --------- specLB : numpy.ndarray 1D array with the lower bound of the input spec specUB : numpy.ndarray 1D array with the upper bound of the input spec  Return ------ output : list list of Deeppoly-Nodes that can be run by an Analyzer object """ output = [] domain = 'deeppoly' i = 0 while i < len(self.operations): if self.operations[i] == 'Placeholder': input_names, output_name, output_shape = self.resources[i][domain] output.append(DeeppolyInput(specLB, specUB, input_names, output_name, output_shape, lexpr_weights, lexpr_cst, lexpr_dim, uexpr_weights, uexpr_cst, uexpr_dim, expr_size)) i += 1 elif i == 1 and self.operations[i] == 'MatMul' and self.operations[ i + 1] in ['Add', 'BiasAdd']: matrix, input_names, _, _ = self.resources[i][domain] bias, _, _, _ = self.resources[i + 1][domain] _, output_name, output_shape = self.resources[i + 2][domain] if self.operations[i + 2] == 'Relu': output.append(DeeppolyReluNodeFirst(matrix, bias, input_names, output_name, output_shape)) elif self.operations[i + 2] == 'Sigmoid': output.append(DeeppolySigmoidNodeFirst(matrix, bias, input_names, output_name, output_shape)) elif self.operations[i + 2] == 'Tanh': output.append(DeeppolyTanhNodeFirst(matrix, bias, input_names, output_name, output_shape)) i += 3 elif i == len(self.operations) - 3 and self.operations[i ] == 'MatMul' and self.operations[i + 1] in ['Add', 'BiasAdd']: matrix, input_names, _, _ = self.resources[i][domain] bias, _, _, _ = self.resources[i + 1][domain] _, output_name, output_shape = self.resources[i + 2][domain] if self.operations[i + 2] == 'Relu': output.append(DeeppolyReluNodeLast(matrix, bias, True, input_names, output_name, output_shape)) elif self.operations[i + 2] == 'Sigmoid': output.append(DeeppolySigmoidNodeLast(matrix, bias, True, input_names, output_name, output_shape)) elif self.operations[i + 2] == 'Tanh': output.append(DeeppolyTanhNodeLast(matrix, bias, True, input_names, output_name, output_shape)) i += 3 elif i == len(self.operations) - 2 and self.operations[i ] == 'MatMul' and self.operations[i + 1] in ['Add', 'BiasAdd']: matrix, input_names, _, _ = self.resources[i][domain] bias, _, output_name, output_shape = self.resources[i + 1][domain] output.append(DeeppolyReluNodeLast(matrix, bias, False, input_names, output_name, output_shape)) i += 2 elif self.operations[i] == 'MatMul' and self.operations[i + 1] in [ 'Add', 'BiasAdd']: matrix, input_names, _, _ = self.resources[i][domain] bias, _, _, _ = self.resources[i + 1][domain] _, output_name, output_shape = self.resources[i + 2][domain] if self.operations[i + 2] == 'Relu': output.append(DeeppolyReluNodeIntermediate(matrix, bias, input_names, output_name, output_shape)) elif self.operations[i + 2] == 'Sigmoid': output.append(DeeppolySigmoidNodeIntermediate(matrix, bias, input_names, output_name, output_shape)) elif self.operations[i + 2] == 'Tanh': output.append(DeeppolyTanhNodeIntermediate(matrix, bias, input_names, output_name, output_shape)) i += 3 elif self.operations[i] == 'MaxPool': (image_shape, window_size, out_shape, input_names, output_name, output_shape) = self.resources[i][domain] output.append(DeeppolyMaxpool(image_shape, window_size, strides, input_names, output_name, output_shape)) i += 1 elif i == 1 and self.operations[1] == 'Conv2D' and self.operations[2 ] == 'BiasAdd' and self.operations[3] == 'Relu': filters, image_shape, strides, padding, input_names, _, _ = (self .resources[i][domain]) bias, _, output_name, output_shape = self.resources[i + 1][domain] _, output_name, output_shape = self.resources[i + 2][domain] output.append(DeeppolyConv2dNodeFirst(filters, strides, padding, bias, image_shape, input_names, output_name, output_shape, True)) i += 3 elif self.operations[i] == 'Conv2D' and self.operations[i + 1 ] == 'BiasAdd' and self.operations[i + 2] == 'Relu': filters, image_shape, strides, padding, input_names, _, _ = (self .resources[i][domain]) bias, _, _, _ = self.resources[i + 1][domain] _, output_name, output_shape = self.resources[i + 2][domain] output.append(DeeppolyConv2dNodeIntermediate(filters, strides, padding, bias, image_shape, input_names, output_name, output_shape, True)) i += 3 elif self.operations[i] == 'Conv2D' and self.operations[i + 1 ] == 'BiasAdd' and self.operations[i + 2] != 'Relu': filters, image_shape, strides, padding, input_names, _, _ = (self .resources[i][domain]) bias, _, output_name, output_shape = self.resources[i + 1][domain] output.append(DeeppolyConv2dNodeIntermediate(filters, strides, padding, bias, image_shape, input_names, output_name, output_shape, False)) i += 2 elif self.operations[i] == 'Resadd' and self.operations[i + 1 ] == 'Relu': input_names, _, _ = self.resources[i][domain] _, output_name, output_shape = self.resources[i + 1][domain] output.append(DeeppolyResadd(input_names, output_name, output_shape, True)) i += 2 elif self.operations[i] == 'Resadd': input_names, output_name, output_shape = self.resources[i][domain] output.append(DeeppolyResadd(input_names, output_name, output_shape, False)) i += 1 else: assert 0, "the Deeppoly analyzer doesn't support this network" output_index_store = {} index_o = 0 for node in output: output_index_store[node.output_name] = index_o index_o += 1 for node in output: predecessors = (c_size_t * len(node.input_names))() i = 0 for input_name in node.input_names: predecessors[i] = output_index_store[input_name] i += 1 node.predecessors = predecessors return output 
run.get|path def _get_path(basename, trial, version, other_name=None): path = os.path.join(os.path.join(os.environ.get('DATA_DIR', 'gs://renda'), 'gnmt_results'), version, basename) if other_name: path = os.path.join(path, other_name) path = os.path.join(path, trial) return path 
LFattNet_train.threadsafe_generator.g def g(*a, **kw): return threadsafe_iter(f(*a, **kw)) 
test.test def test(m, data_sampler, eval_step, min_classes, max_classes, train_shots, test_shots, meta_batch, meta_iters, name): sess = tf.Session() sess.run(tf.global_variables_initializer()) losses = [] temp_yp = [] aps = [] buffer = [] lossesB = [] train_gen = data_sampler.sample_Task(meta_batch, min_classes, max_classes + 1, train_shots, test_shots, 'test') print('TEST MODE') m.loadWeights(sess, name, step=str(int(eval_step)), model_name=name + '.ckpt') for i in range(meta_iters): xb1, yb1, xb2, yb2 = next(train_gen) num_l = [len(np.unique(np.argmax(yb1, axis=-1)))] if m.maml_n == 2: sess.run(m.init_assign, feed_dict={m.label_n: [5]}) l, vals, ps = sess.run([m.test_train_loss, m.test_val_losses, m. val_predictions], feed_dict={m.train_xb: xb1, m.train_yb: yb1, m.val_xb: xb2, m.val_yb: yb2, m.label_n: num_l}) losses.append(vals) lossesB.append(vals) buffer.append(l) true_vals = np.argmax(yb2, axis=-1) all_accs = [] for pred_epoch in range(len(ps)): all_accs.append(np.mean(np.argmax(ps[pred_epoch], axis=-1) == true_vals)) temp_yp.append(all_accs) if i % 50 == 0: print(f'({i}/{meta_iters})') print( f'Final: TLoss {np.mean(buffer)}, VLoss {np.mean(lossesB, axis=0)}' , f'Accuracy {np.mean(temp_yp, axis=0)}') print( f'Final: TLoss {np.mean(buffer)}-{np.std(buffer)}, VLoss {np.mean(lossesB, axis=0)}-{np.std(lossesB, axis=0)}' , f'Accuracy {np.mean(temp_yp, axis=0)}-{np.std(temp_yp, axis=0)}') 
official.utils.logs.metric_hook_test.LoggingMetricHookTest.n|and|print|every|test|end|steps def test_print_every_n_steps_and_end(self): with tf.Graph().as_default(), tf.Session() as sess: tf.train.get_or_create_global_step() self._validate_print_every_n_steps(sess, at_end=True) self._validate_print_every_n_steps(sess, at_end=True) 
pathfinder.batched_pathfinding.generate|bash def generate_bash(): PATH = sys.argv[2] with open('cmd_lucy.sh', 'w') as f: for i in range(0, 50): f.write( 'CUDA_VISIBLE_DEVICES=NONE python pathfinder.py %s %d &\n' % (PATH, i)) f.write('wait') with open('cmd_ron.sh', 'w') as f: for i in range(50, 80): f.write( 'CUDA_VISIBLE_DEVICES=NONE python pathfinder.py %s %d &\n' % (PATH, i)) f.write('wait') with open('cmd_molly.sh', 'w') as f: for i in range(80, 100): f.write( 'CUDA_VISIBLE_DEVICES=NONE python pathfinder.py %s %d &\n' % (PATH, i)) f.write('wait') 
preprocess.prepare_d2d.hist|per|query def hist_per_query(relevance_dict, text_max_len, hist_size, sim_path, hist_path, d2d, qid): hist_output_dir = make_directory(hist_path, str(qid)) relevance = relevance_dict.get(qid) supervised_docid_list = relevance.get_supervised_docid_list() judged_docid_list = relevance.get_judged_docid_list() """ because we only want to rerank top 500 docs, but judged docs that lie in in top 1000 should also be considered, for the sufficiency of training """ cand = judged_docid_list[0] + judged_docid_list[1] + judged_docid_list[2] waitlist = [docid for docid in cand if docid in supervised_docid_list[ 500:2000]] useful_docid_list = supervised_docid_list[:1000] for docid in useful_docid_list: if d2d: sim_file_name = os.path.join(sim_path, str(qid), 'q{0}_d{1}.pickle'.format(qid, docid)) else: sim_file_name = os.path.join(sim_path, str(qid), 'q{0}_d{1}.npy'.format(qid, docid)) hist_file_name = os.path.join(hist_output_dir, 'q{0}_d{1}.npy'. format(qid, docid)) if os.path.exists(hist_file_name): pass elif d2d: sim_list = load_pickle(sim_file_name) hist_array = np.zeros((len(sim_list), text_max_len, hist_size), dtype=np.float32) for i, sim_mat in enumerate(sim_list): sim_mat = sim_mat[:, :20000] hist = hist_from_matrix(text_max_len, hist_size, sim_mat) hist_array[i] = hist np.save(hist_file_name, hist_array) else: sim_mat = np.load(sim_file_name) hist = hist_from_matrix(text_max_len, hist_size, sim_mat) np.save(hist_file_name, hist) logging.info('Finish for topic {0}'.format(qid)) 
hbaselines.goal_conditioned.tf_util.get|globals|vars def get_globals_vars(name=None): """Return the global variables.  Parameters ---------- name : str the scope  Returns ------- list of tf.Variable global variables """ return tf.compat.v1.get_collection(tf.compat.v1.GraphKeys. GLOBAL_VARIABLES, scope=name) 
translate.translation_model.TranslationModel.initialize def initialize(self, checkpoints=None, reset=False, reset_learning_rate= False, max_to_keep=1, keep_every_n_hours=0, sess=None, whitelist=None, blacklist=None, rnn_lm_model_dir=None, rnn_mt_model_dir=None, origin_model_ckpt=None, **kwargs): """ :param checkpoints: list of checkpoints to load (instead of latest checkpoint) :param reset: don't load latest checkpoint, reset learning rate and global step :param reset_learning_rate: reset the learning rate to its initial value :param max_to_keep: keep this many latest checkpoints at all times :param keep_every_n_hours: and keep checkpoints every n hours """ sess = sess or tf.get_default_session() if keep_every_n_hours <= 0 or keep_every_n_hours is None: keep_every_n_hours = float('inf') self.saver = tf.train.Saver(max_to_keep=max_to_keep, keep_checkpoint_every_n_hours=keep_every_n_hours, sharded=False) sess.run(tf.global_variables_initializer()) for encoder_or_decoder, vocab in zip(self.encoders + self.decoders, self.vocabs): if encoder_or_decoder.embedding_file: utils.log('loading embeddings from: {}'.format( encoder_or_decoder.embedding_file)) embeddings = {} with open(encoder_or_decoder.embedding_file) as embedding_file: for line in embedding_file: word, vector = line.split(' ', 1) if word in vocab.vocab: embeddings[word] = np.array(list(map(float, vector. split()))) mean = sum(embeddings.values()) / len(embeddings) std = np.sqrt(sum((value - mean) ** 2 for value in embeddings. values())) / (len(embeddings) - 1) for key in embeddings: embeddings[key] = 0.01 * (embeddings[key] - mean) / std with tf.variable_scope(tf.get_variable_scope(), reuse=True): embedding_var = tf.get_variable('embedding_' + encoder_or_decoder.name) embedding_value = embedding_var.eval() for word, i in vocab.vocab.items(): if word in embeddings: embedding_value[i] = embeddings[word] sess.run(embedding_var.assign(embedding_value)) if not rnn_lm_model_dir is None: utils.log('load rnn_lm model from {}'.format(rnn_lm_model_dir)) rnn_lm_graph = ImportGraph(rnn_lm_model_dir) cell_name = kwargs['rnn_lm_cell_name'] kernel_values = rnn_lm_graph.get_variable_value(cell_name + '/kernel:0' ) bias_values = rnn_lm_graph.get_variable_value(cell_name + '/bias:0') with tf.variable_scope(tf.get_variable_scope(), reuse=True): rnn_lm_kernel_var = [v for v in tf.trainable_variables() if v. name == 'decoder_edits/rnn_lm/basic_lstm_cell/kernel:0'][0] rnn_lm_bias_var = [v for v in tf.trainable_variables() if v. name == 'decoder_edits/rnn_lm/basic_lstm_cell/bias:0'][0] sess.run(rnn_lm_kernel_var.assign(kernel_values)) sess.run(rnn_lm_bias_var.assign(bias_values)) if not rnn_mt_model_dir is None: utils.log('load rnn_mt model from {}'.format(rnn_mt_model_dir)) load_checkpoint(sess, rnn_mt_model_dir, filename='best', prefix= 'decoder_mt') load_checkpoint(sess, rnn_mt_model_dir, filename='best', prefix= 'encoder_src') if whitelist: with open(whitelist) as f: whitelist = list(line.strip() for line in f) if blacklist: with open(blacklist) as f: blacklist = list(line.strip() for line in f) else: blacklist = [] blacklist.append('dropout_keep_prob') if reset_learning_rate or reset: blacklist.append('learning_rate') if reset: blacklist.append('global_step') params = {k: kwargs.get(k) for k in ('variable_mapping', 'reverse_mapping') } if origin_model_ckpt is not None: utils.log('Load origin model') blacklist.append('decoder_edits/basic_lstm_cell/kernel:0') load_checkpoint(sess, origin_model_ckpt, blacklist=blacklist, whitelist=whitelist, **params) if checkpoints and len(self.models) > 1: assert len(self.models) == len(checkpoints) for i, checkpoint in enumerate(checkpoints, 1): load_checkpoint(sess, None, checkpoint, blacklist=blacklist, whitelist=whitelist, prefix='model_{}'.format(i), **params) elif checkpoints: for checkpoint in checkpoints: load_checkpoint(sess, None, checkpoint, blacklist=blacklist, whitelist=whitelist, **params) elif not reset: load_checkpoint(sess, self.checkpoint_dir, blacklist=blacklist, whitelist=whitelist, **params) utils.debug('global step: {}'.format(self.global_step.eval())) utils.debug('baseline step: {}'.format(self.baseline_step.eval())) 
tensorflow_translator.TFTranslator.maxpool|resources def maxpool_resources(self, op): """ Extracts the incoming image size (heigth, width, channels), the size of the maxpool window (heigth, width), and the strides of the window (heigth, width)  Arguments --------- op : tf.Operation must have type "MaxPool"  Return ------ output : tuple has 4 entries - (list, numpy.ndarray, numpy.ndarray, str) """ image = op.inputs[0] image_shape = tensorshape_to_intlist(image.shape)[1:] window_size = op.get_attr('ksize')[1:3] strides = op.get_attr('strides')[1:3] padding = op.get_attr('padding').decode('utf-8') return image_shape, window_size, strides, padding 
train_noise_flow.test|multithread def test_multithread(sess, ts_batch_que, loss, sd_z, x, y, nlf0, nlf1, iso, cam, is_training, test_epoch_loss_que, sd_z_que, test_its, nthr=8, requeue=False): divs = divide_parts(test_its, nthr) threads = [] for thr_id in range(nthr): threads.append(Thread(target=test_thread, args=(thr_id, divs[thr_id ], sess, ts_batch_que, loss, sd_z, x, y, nlf0, nlf1, iso, cam, is_training, test_epoch_loss_que, sd_z_que, requeue))) threads[thr_id].start() for thr_id in range(nthr): threads[thr_id].join() 
main.conv|model|generate def generate_conv_model(model): model = model.get_layer(index=1) inp = model.inputs[0].shape.dims[1].value, model.inputs[0].shape.dims[2 ].value, model.inputs[0].shape.dims[3].value H = Input(inp) inp = H for layer_idx in range(1, len(model.layers)): layer = model.get_layer(index=layer_idx) config = layer.get_config() if isinstance(layer, MaxPooling2D): H = MaxPooling2D.from_config(config)(H) if isinstance(layer, Dropout): H = Dropout.from_config(config)(H) if isinstance(layer, Activation): H = Activation.from_config(config)(H) if isinstance(layer, BatchNormalization): weights = layer.get_weights() H = BatchNormalization(weights=weights)(H) elif isinstance(layer, Conv2D): weights = layer.get_weights() config['filters'] = weights[1].shape[0] H = Conv2D(activation=config['activation'], activity_regularizer=config['activity_regularizer'], bias_constraint=config['bias_constraint'], bias_regularizer =config['bias_regularizer'], data_format=config[ 'data_format'], dilation_rate=config['dilation_rate'], filters=config['filters'], kernel_constraint=config[ 'kernel_constraint'], kernel_regularizer=config[ 'kernel_regularizer'], kernel_size=config['kernel_size'], name=config['name'], padding=config['padding'], strides= config['strides'], trainable=config['trainable'], use_bias= config['use_bias'], weights=weights)(H) return Model(inp, H) 
vgg.Vgg16.get|filter|conv def get_conv_filter(self, name): return tf.constant(self.data_dict[name][0], name='filter') 
AffineCouplingGainEx1.AffineCouplingGainEx1.det|jacobian|log|inverse def _inverse_log_det_jacobian(self, z, yy, nlf0=None, nlf1=None, iso=None, cam=None): scale = gain_model_params_ex1(iso) if scale is None: return tf.constant(0.0, dtype=z.dtype, name='ildj') return -tf.log(scale) 
save_nlayer_weights.NLayerModel.predict def predict(self, data): return self.model(data) 
graphsage.minibatch.EdgeMinibatchIterator.n|prune|v def _n2v_prune(self, edges): is_val = lambda n: self.G.node[n]['val'] or self.G.node[n]['test'] return [e for e in edges if not is_val(e[1])] 
utils.ckpter.ckpter def __init__(self, wcard): self.wcard = wcard self.load() 
embeddings.OpenKE.models.TransH.TransH.loss|def def loss_def(self): config = self.get_config() pos_h, pos_t, pos_r = self.get_positive_instance(in_batch=False) neg_h, neg_t, neg_r = self.get_negative_instance(in_batch=False) pos_h_e = tf.nn.embedding_lookup(self.ent_embeddings, pos_h) pos_t_e = tf.nn.embedding_lookup(self.ent_embeddings, pos_t) pos_r_e = tf.nn.embedding_lookup(self.rel_embeddings, pos_r) neg_h_e = tf.nn.embedding_lookup(self.ent_embeddings, neg_h) neg_t_e = tf.nn.embedding_lookup(self.ent_embeddings, neg_t) neg_r_e = tf.nn.embedding_lookup(self.rel_embeddings, neg_r) pos_norm = tf.nn.embedding_lookup(self.normal_vectors, pos_r) neg_norm = tf.nn.embedding_lookup(self.normal_vectors, neg_r) pos_h_e = tf.nn.l2_normalize(pos_h_e, 1) pos_t_e = tf.nn.l2_normalize(pos_t_e, 1) pos_r_e = tf.nn.l2_normalize(pos_r_e, 1) neg_h_e = tf.nn.l2_normalize(neg_h_e, 1) neg_t_e = tf.nn.l2_normalize(neg_t_e, 1) neg_r_e = tf.nn.l2_normalize(neg_r_e, 1) pos_norm = tf.nn.l2_normalize(pos_norm, 1) neg_norm = tf.nn.l2_normalize(neg_norm, 1) p_h = self._transfer(pos_h_e, pos_norm) p_t = self._transfer(pos_t_e, pos_norm) p_r = pos_r_e n_h = self._transfer(neg_h_e, neg_norm) n_t = self._transfer(neg_t_e, neg_norm) n_r = neg_r_e _p_score = self._calc(p_h, p_t, p_r) _p_score = tf.reshape(_p_score, [1, -1, config.rel_size]) _n_score = self._calc(n_h, n_t, n_r) _n_score = tf.reshape(_n_score, [config.negative_ent + config. negative_rel, -1, config.rel_size]) p_score = tf.reduce_sum(tf.reduce_mean(_p_score, 0, keep_dims=False), 1, keep_dims=True) n_score = tf.reduce_sum(tf.reduce_mean(_n_score, 0, keep_dims=False), 1, keep_dims=True) self.loss = tf.reduce_sum(tf.maximum(p_score - n_score + config.margin, 0)) 
avod.core.label_cluster_utils.LabelClusterUtils.get|path|file|cluster def _get_cluster_file_path(self, dataset, cls, num_clusters): """ Returns a unique file path for a text file based on the dataset name, split, object class, and number of clusters. The file path will look like: avod/data/<dataset_name>/<data_split>/<class>_<n_clusters>   Args: dataset: Dataset object cls: str, Object class num_clusters: number of clusters for the class  Returns: str Unique file path to text file """ file_path = '{}/{}/{}/'.format(self.data_dir, dataset.name, dataset. cluster_split, dataset.data_split) file_path += '{}_{}.txt'.format(cls, num_clusters) return file_path 
sensor_correction.gp_cpu.GPRegressor.fit def fit(self, X, Y, length_scale=1.0, signal_std=1.0, noise_std=1e-10, normalize=False, optimize=False, repeat=0): """Fit a Gaussian Process regressor.  Params ------ X : mx4 array Training feature vectors Y : mx1 array Target values  Kwargs ------ length_scale : scalar or 4x1 array, optional Kernel length scaling input feature dimensions signal_std : scalar, optional Signal sigma noise_std : scalar, optional Observation noise sigma normalize : bool, optional Whether or not to normalize Y by mean adjustment optimize : bool or list Turn on/off optimization. If list, only the parameters in list will be tuned. """ optimizer = 'fmin_l_bfgs_b' bounds_ls = bounds_ss = bounds_ns = 0.001, 1000.0 signal_var = signal_std ** 2 noise_var = noise_std ** 2 if isinstance(optimize, list): bounds_ls = (0.001, 1000.0) if 'length_scale' in optimize else ( length_scale, length_scale) bounds_ss = (0.001, 1000.0) if 'signal_std' in optimize else ( signal_var, signal_var) bounds_ns = (0.001, 1000.0) if 'noise_std' in optimize else (noise_var, noise_var) elif not optimize: optimizer = None kernel = ConstantKernel(signal_var, bounds_ss) * RBF(length_scale, bounds_ls) + WhiteKernel(noise_var, bounds_ns) self.gpr = GaussianProcessRegressor(kernel=kernel, alpha=0.0, normalize_y=normalize, optimizer=optimizer, n_restarts_optimizer=repeat ) self.gpr.fit(X, Y) 
texar.hyperparams.HParams.contains def __contains__(self, name): return name in self._hparams 
batch_test.BatchTest.test|Serial @jtu.parameterized.named_parameters(jtu.cases_from_list({'testcase_name': '_train_shape={}_test_shape={}_network={}_{}'.format(train, test, network, name), 'train_shape': train, 'test_shape': test, 'network': network, 'name': name, 'kernel_fn': kernel_fn} for train, test, network in zip(TRAIN_SHAPES, TEST_SHAPES, NETWORK) for name, kernel_fn in KERNELS. items())) def testSerial(self, train_shape, test_shape, network, name, kernel_fn): key = random.PRNGKey(0) key, self_split, other_split = random.split(key, 3) data_self = random.normal(self_split, train_shape) data_other = random.normal(other_split, test_shape) kernel_fn = kernel_fn(key, train_shape[1:], network) kernel_batched = batch._serial(kernel_fn, batch_size=2) _test_kernel_against_batched(self, kernel_fn, kernel_batched, data_self, data_other) 
translate.rnn.CellInitializer.call def __call__(self, shape, dtype=None, partition_info=None, verify_shape=None): if len(shape) == 1 or shape[1] % self.cell_size != 0: return self.default_initializer(shape, dtype=dtype, partition_info= partition_info) input_size = shape[0] - self.cell_size W, U = [], [] for _ in range(shape[1] // self.cell_size): W.append(self.default_initializer(shape=[input_size, self.cell_size])) U.append(self.initializer(shape=[self.cell_size, self.cell_size])) return tf.concat([tf.concat(W, axis=1), tf.concat(U, axis=1)], axis=0) 
embeddings.OpenKE.models.Model.Model.input|def def input_def(self): config = self.config self.batch_h = tf.placeholder(tf.int64, [config.batch_seq_size]) self.batch_t = tf.placeholder(tf.int64, [config.batch_seq_size]) self.batch_r = tf.placeholder(tf.int64, [config.batch_seq_size]) self.batch_y = tf.placeholder(tf.float32, [config.batch_seq_size]) self.postive_h = tf.transpose(tf.reshape(self.batch_h[0:config. batch_size], [1, -1]), [1, 0]) self.postive_t = tf.transpose(tf.reshape(self.batch_t[0:config. batch_size], [1, -1]), [1, 0]) self.postive_r = tf.transpose(tf.reshape(self.batch_r[0:config. batch_size], [1, -1]), [1, 0]) self.negative_h = tf.transpose(tf.reshape(self.batch_h[config. batch_size:config.batch_seq_size], [config.negative_ent + config. negative_rel, -1]), perm=[1, 0]) self.negative_t = tf.transpose(tf.reshape(self.batch_t[config. batch_size:config.batch_seq_size], [config.negative_ent + config. negative_rel, -1]), perm=[1, 0]) self.negative_r = tf.transpose(tf.reshape(self.batch_r[config. batch_size:config.batch_seq_size], [config.negative_ent + config. negative_rel, -1]), perm=[1, 0]) self.predict_h = tf.placeholder(tf.int64, [None]) self.predict_t = tf.placeholder(tf.int64, [None]) self.predict_r = tf.placeholder(tf.int64, [None]) self.parameter_lists = [] 
deepMOT-master.utils.sot_utils.track|RPN|Siam def SiamRPN_track(state, im, net, train=False, noisy_bool=False, CMC=False, prev_xyxy=None, w_matrix=None): """ track target object :param state: information of the track at t-1; dict :param im: current frame, numpy array, [h, w, c] :param net: SOT network, torch model :param train: train mode or not, bool :param noisy_bool: add noise flag, for training data augmentation, bool :param cmc: camera motion compensation flag, for moving camera videos, bool :param prev_xyxy: previous position of the track, before warping, numpy array :param w_matrix: afine transformation matrix between frame t and t-1, numpy array :return: updated state (predicted position and size, etc.) of the track """ p = state['p'] avg_chans = state['avg_chans'] window = state['window'] target_pos = state['target_pos'] target_sz = state['target_sz'] im_h, im_w, _ = im.shape if CMC: prev_xyxy_warp = warpcoordinates(np.array([prev_xyxy], dtype=np. float32), w_matrix) assert prev_xyxy_warp.shape[1] == 4 prev_pos_warp = np.array([0.5 * (prev_xyxy_warp[0, 0] + prev_xyxy_warp[0, 2]), 0.5 * (prev_xyxy_warp[0, 1] + prev_xyxy_warp[0, 3])]) w_warp = prev_xyxy_warp[0, 2] - prev_xyxy_warp[0, 0] h_warp = prev_xyxy_warp[0, 3] - prev_xyxy_warp[0, 1] prev_pos_warp[0] = max(0, min(im_w, prev_pos_warp[0])) prev_pos_warp[1] = max(0, min(im_h, prev_pos_warp[1])) w_warp = max(10, min(im_w, w_warp)) h_warp = max(10, min(im_h, h_warp)) target_sz = np.array([w_warp, h_warp]) target_pos = prev_pos_warp wc_z = target_sz[1] + p.context_amount * sum(target_sz) hc_z = target_sz[0] + p.context_amount * sum(target_sz) s_z = np.sqrt(wc_z * hc_z) scale_z = p.exemplar_size / s_z d_search = (p.instance_size - p.exemplar_size) / 2 pad = d_search / scale_z s_x = s_z + 2 * pad x_crop = get_subwindow_tracking(im, target_pos, p.instance_size, round( s_x), avg_chans, noisy_bool=noisy_bool).unsqueeze(0) if train: target_position, target_size, score_single, score_tensor, ancrs = ( tracker_train(net, x_crop.cuda(), target_pos, target_sz * scale_z, window, scale_z, p, state['temple'], im)) del x_crop torch.cuda.empty_cache() target_pos_numpy = target_position.detach().cpu().numpy().copy() target_sz_numpy = target_size.detach().cpu().numpy().copy() target_pos_numpy[0] = max(0, min(state['im_w'], target_pos_numpy[0])) target_pos_numpy[1] = max(0, min(state['im_h'], target_pos_numpy[1])) target_sz_numpy[0] = max(10, min(state['im_w'], target_sz_numpy[0])) target_sz_numpy[1] = max(10, min(state['im_h'], target_sz_numpy[1])) state['target_pos'] = target_pos_numpy state['target_sz'] = target_sz_numpy state['score'] = score_single return target_position, target_size, state, [score_tensor, ancrs] else: target_pos, target_sz, score = tracker_eval(net, x_crop.cuda(), target_pos, target_sz * scale_z, window, scale_z, p, state[ 'temple']) target_pos[0] = max(0, min(state['im_w'], target_pos[0])) target_pos[1] = max(0, min(state['im_h'], target_pos[1])) target_sz[0] = max(10, min(state['im_w'], target_sz[0])) target_sz[1] = max(10, min(state['im_h'], target_sz[1])) state['target_pos'] = target_pos state['target_sz'] = target_sz state['score'] = score return state 
regression.controller.distributions.EigenMultivariateNormal.batch|shape def _batch_shape(self): return tf.shape(self.mean)[:-2] 
cleverhans.utils_keras.conv|d def conv_2d(filters, kernel_shape, strides, padding, input_shape=None): """ Defines the right convolutional layer according to the version of Keras that is installed. :param filters: (required integer) the dimensionality of the output space (i.e. the number output of filters in the convolution) :param kernel_shape: (required tuple or list of 2 integers) specifies the strides of the convolution along the width and height. :param padding: (required string) can be either 'valid' (no padding around input or feature map) or 'same' (pad to ensure that the output feature map size is identical to the layer input) :param input_shape: (optional) give input shape if this is the first layer of the model :return: the Keras layer """ if LooseVersion(keras.__version__) >= LooseVersion('2.0.0'): if input_shape is not None: return Conv2D(filters=filters, kernel_size=kernel_shape, strides=strides, padding=padding, input_shape=input_shape) else: return Conv2D(filters=filters, kernel_size=kernel_shape, strides=strides, padding=padding) elif input_shape is not None: return Convolution2D(filters, kernel_shape[0], kernel_shape[1], subsample=strides, border_mode=padding, input_shape=input_shape) else: return Convolution2D(filters, kernel_shape[0], kernel_shape[1], subsample=strides, border_mode=padding) 
ndh_problem.NDHProblem.eval def eval(self, action_list, env_output_list): result = {} for key, fn in self._eval_dict.items(): score = fn(action_list, env_output_list, self._env) result[key] = score return result 
deepctr.models.dien.interest|evolution def interest_evolution(concat_behavior, deep_input_item, user_behavior_length, gru_type='GRU', use_neg=False, neg_concat_behavior=None, embedding_size=8, att_hidden_size=(64, 16), att_activation='sigmoid', att_weight_normalization=False): if gru_type not in ['GRU', 'AIGRU', 'AGRU', 'AUGRU']: raise ValueError('gru_type error ') aux_loss_1 = None rnn_outputs = DynamicGRU(embedding_size * 2, return_sequence=True, name ='gru1')([concat_behavior, user_behavior_length]) if gru_type == 'AUGRU' and use_neg: aux_loss_1 = auxiliary_loss(rnn_outputs[:, :-1, :], concat_behavior [:, 1:, :], neg_concat_behavior[:, 1:, :], tf.subtract( user_behavior_length, 1), stag='gru') if gru_type == 'GRU': rnn_outputs2 = DynamicGRU(embedding_size * 2, return_sequence=True, name='gru2')([rnn_outputs, user_behavior_length]) hist = AttentionSequencePoolingLayer(att_hidden_units= att_hidden_size, att_activation=att_activation, weight_normalization=att_weight_normalization, return_score=False)( [deep_input_item, rnn_outputs2, user_behavior_length]) else: scores = AttentionSequencePoolingLayer(att_hidden_units= att_hidden_size, att_activation=att_activation, weight_normalization=att_weight_normalization, return_score=True)([ deep_input_item, rnn_outputs, user_behavior_length]) if gru_type == 'AIGRU': hist = multiply([rnn_outputs, Permute([2, 1])(scores)]) final_state2 = DynamicGRU(embedding_size * 2, gru_type='GRU', return_sequence=False, name='gru2')([hist, user_behavior_length]) else: final_state2 = DynamicGRU(embedding_size * 2, gru_type=gru_type, return_sequence=False, name='gru2')([rnn_outputs, user_behavior_length, Permute([2, 1])(scores)]) hist = final_state2 return hist, aux_loss_1 
deepctr.layers.sequence.DynamicGRU.call def call(self, input_list): """ :param concated_embeds_value: None * field_size * embedding_size :return: None*1 """ if self.gru_type == 'GRU' or self.gru_type == 'AIGRU': rnn_input, sequence_length = input_list att_score = None else: rnn_input, sequence_length, att_score = input_list rnn_output, hidden_state = dynamic_rnn(self.gru_cell, inputs=rnn_input, att_scores=att_score, sequence_length=tf.squeeze(sequence_length), dtype=tf.float32, scope=self.name) if self.return_sequence: return rnn_output else: return tf.expand_dims(hidden_state, axis=1) 
lepod-score.get|new|idx def _get_new_idx(alignment): new_idx = [None] * len(alignment) counter = 0 for idx, align in enumerate(alignment): if align is not None and align != IN_PHRASE: new_idx[idx] = counter counter += 1 return new_idx, counter 
tfprocess.TFProcess.swa|network|save def save_swa_network(self, steps, path, leela_path, data): rem = self.session.run(tf.assign_add(self.swa_skip, -1)) if rem > 0: return self.swa_skip.load(self.swa_c, self.session) num = self.session.run(self.swa_accum_op) if self.swa_max_n != None: num = min(num, self.swa_max_n) self.swa_count.load(float(num), self.session) swa_path = path + '-swa-' + str(int(num)) + '-' + str(steps) + '.txt' self.snap_save() self.session.run(self.swa_load_op) if self.swa_recalc_bn: print('Refining SWA batch normalization') for _ in range(200): batch = next(data) self.session.run([self.loss, self.update_ops], feed_dict={self. training: True, self.planes: batch[0], self.probs: batch[1], self.winner: batch[2]}) self.save_leelaz_weights(swa_path) self.snap_restore() print('Wrote averaged network to {}'.format(swa_path)) 
t_sgan_sn_deconv.Attention.Attention def __init__(self, **kwargs): super(Attention, self).__init__(**kwargs) 
avod.core.box_list_ops.extra|fields|copy def _copy_extra_fields(boxlist_to_copy_to, boxlist_to_copy_from): """Copies the extra fields of boxlist_to_copy_from to boxlist_to_copy_to. Args: boxlist_to_copy_to: BoxList to which extra fields are copied. boxlist_to_copy_from: BoxList from which fields are copied. Returns: boxlist_to_copy_to with extra fields. """ for field in boxlist_to_copy_from.get_extra_fields(): boxlist_to_copy_to.add_field(field, boxlist_to_copy_from.get_field( field)) return boxlist_to_copy_to 
latent_bow_data2text.LatentBowData2text.build def build(self): """Build the model""" print('Building the Latent BOW - sequence to sequence model ... ') vocab_size = self.vocab_size key_size = self.key_size state_size = self.state_size enc_layers = self.enc_layers max_enc_bow = self.max_enc_bow lambda_enc_loss = self.lambda_enc_loss with tf.name_scope('placeholders'): enc_keys = tf.placeholder(tf.int32, [None, None], 'enc_keys') enc_locs = tf.placeholder(tf.int32, [None, None], 'enc_locs') enc_vals = tf.placeholder(tf.int32, [None, None], 'enc_vals') enc_lens = tf.placeholder(tf.int32, [None], 'enc_lens') self.drop_out = tf.placeholder(tf.float32, (), 'drop_out') self.gumbel_tau = tf.placeholder(tf.float32, (), 'gumbel_tau') self.enc_keys = enc_keys self.enc_locs = enc_locs self.enc_vals = enc_vals self.enc_lens = enc_lens enc_targets = tf.placeholder(tf.int32, [None, None], 'enc_targets') dec_inputs = tf.placeholder(tf.int32, [None, None], 'dec_inputs') dec_targets = tf.placeholder(tf.int32, [None, None], 'dec_targets') dec_lens = tf.placeholder(tf.int32, [None], 'dec_lens') self.enc_targets = enc_targets self.dec_inputs = dec_inputs self.dec_targets = dec_targets self.dec_lens = dec_lens batch_size = tf.shape(enc_keys)[0] max_enc_len = tf.shape(enc_keys)[1] max_dec_len = tf.shape(dec_targets)[1] with tf.variable_scope('embeddings'): embedding_matrix_vals = tf.get_variable(name= 'embedding_matrix_vals', shape=[vocab_size, state_size], dtype= tf.float32, initializer=tf.random_normal_initializer(stddev=0.05)) embedding_matrix_keys = tf.get_variable(name= 'embedding_matrix_keys', shape=[key_size, state_size], dtype=tf .float32, initializer=tf.random_normal_initializer(stddev=0.05)) embedding_matrix_locs = tf.get_variable(name= 'embedding_matrix_locs', shape=[100, state_size], dtype=tf. float32, initializer=tf.random_normal_initializer(stddev=0.05)) enc_keys = tf.nn.embedding_lookup(embedding_matrix_keys, enc_keys) enc_vals = tf.nn.embedding_lookup(embedding_matrix_vals, enc_vals) enc_locs = tf.nn.embedding_lookup(embedding_matrix_locs, enc_locs) enc_inputs = (enc_keys + enc_vals + enc_locs) / 3.0 dec_inputs = tf.nn.embedding_lookup(embedding_matrix_vals, dec_inputs) with tf.variable_scope('encoder'): enc_cell = [create_cell('enc-%d' % i, state_size, self.drop_out, self.no_residual) for i in range(enc_layers)] enc_cell = tf.nn.rnn_cell.MultiRNNCell(enc_cell) enc_outputs, enc_state = tf.nn.dynamic_rnn(enc_cell, enc_inputs, sequence_length=enc_lens, dtype=tf.float32) with tf.variable_scope('bow_output'): (bow_topk_prob, gumbel_topk_prob, seq_neighbor_ind, seq_neighbor_prob ) = (bow_predict_seq_tag(vocab_size, batch_size, enc_outputs, enc_lens, max_enc_len, self.is_gumbel, self.gumbel_tau)) seq_neighbor_output = {'seq_neighbor_ind': seq_neighbor_ind, 'seq_neighbor_prob': seq_neighbor_prob} with tf.name_scope('enc_output'): bow_pred_prob, pred_ind = tf.nn.top_k(bow_topk_prob, max_enc_bow) enc_targets = _enc_target_list_to_khot(enc_targets, vocab_size, self.pad_id) enc_loss = enc_loss_fn(self.bow_loss_fn, enc_targets, bow_topk_prob, max_enc_bow) self.train_output = {'enc_loss': enc_loss} bow_metrics_dict = bow_train_monitor(bow_topk_prob, pred_ind, vocab_size, batch_size, enc_targets) self.train_output.update(bow_metrics_dict) with tf.name_scope('gumbel_topk_sampling'): sample_ind, sample_prob, sample_memory = bow_gumbel_topk_sampling( gumbel_topk_prob, embedding_matrix_vals, self.sample_size, vocab_size) sample_memory_lens = tf.ones(batch_size, tf.int32) * self.sample_size sample_memory_avg = tf.reduce_mean(sample_memory, 1) sample_memory_output = {'bow_pred_ind': pred_ind, 'bow_pred_prob': bow_pred_prob, 'sample_memory_ind': sample_ind, 'sample_memory_prob': sample_prob} with tf.variable_scope('decoder'): dec_cell = [create_cell('dec-%d' % i, state_size, self.drop_out, self.no_residual) for i in range(enc_layers)] dec_cell = tf.nn.rnn_cell.MultiRNNCell(dec_cell) dec_proj = tf.layers.Dense(vocab_size, name='dec_proj', kernel_initializer=tf.random_normal_initializer(stddev=0.05), bias_initializer=tf.constant_initializer(0.0)) dec_ptr_k_proj = [tf.layers.Dense(state_size, name= 'dec_ptr_k_proj_%d' % pi, kernel_initializer=tf. random_normal_initializer(stddev=0.05), bias_initializer=tf. constant_initializer(0.0)) for pi in range(self.num_pointers)] dec_ptr_g_proj = tf.layers.Dense(1, name='dec_ptr_g_proj', kernel_initializer=tf.random_normal_initializer(stddev=0.05), bias_initializer=tf.constant_initializer(0.0), activation=tf.nn .sigmoid) bow_cond_gate_proj = tf.layers.Dense(1, name='bow_cond_gate_proj', kernel_initializer=tf.random_normal_initializer(stddev=0.05), bias_initializer=tf.constant_initializer(0.0), activation=tf.nn .sigmoid) dec_init_state = [] for l in range(enc_layers): dec_init_state.append(LSTMStateTuple(c=enc_state[0].c, h= enc_state[0].h + sample_memory_avg)) dec_init_state = tuple(dec_init_state) if self.source_attn: dec_memory = [sample_memory, enc_outputs] dec_mem_len = [sample_memory_lens, enc_lens] dec_max_mem_len = [self.sample_size, max_enc_len] else: dec_memory = sample_memory dec_mem_len = sample_memory_lens dec_max_mem_len = tf.shape(dec_memory)[1] if self.bow_cond: bow_cond = sample_memory_avg else: bow_cond = None if self.bow_cond_gate == False: bow_cond_gate_proj = None (dec_outputs_predict, dec_logits_train, dec_prob_train, pointer_ent, avg_max_ptr, avg_num_copy) = (decode(self.dec_start_id, dec_inputs, dec_cell, dec_proj, embedding_matrix_vals, dec_init_state, dec_memory, dec_mem_len, dec_max_mem_len, batch_size, max_dec_len, self.sampling_method, self. topk_sampling_size, state_size, multi_source=True, copy=self. copy, copy_ind=sample_ind, dec_ptr_g_proj=dec_ptr_g_proj, dec_ptr_k_proj=dec_ptr_k_proj, bow_cond=bow_cond, bow_cond_gate_proj=bow_cond_gate_proj)) all_variables = slim.get_variables_to_restore() model_variables = [var for var in all_variables if var.name.split('/')[ 0] == self.model_name] print('%s model, variable list:' % self.model_name) for v in model_variables: print('  %s' % v.name) self.model_saver = tf.train.Saver(model_variables, max_to_keep=3) with tf.variable_scope('optimizer'): optimizer = tf.train.AdamOptimizer(self.learning_rate) with tf.name_scope('dec_output'): dec_mask = tf.sequence_mask(dec_lens, max_dec_len, dtype=tf.float32) if self.copy == False: dec_loss = tf.contrib.seq2seq.sequence_loss(dec_logits_train, dec_targets, dec_mask) else: dec_loss = _copy_loss(dec_prob_train, dec_targets, dec_mask) loss = dec_loss + lambda_enc_loss * enc_loss train_op = optimizer.minimize(loss) dec_output = {'train_op': train_op, 'dec_loss': dec_loss, 'loss': loss} self.train_output.update(dec_output) if self.copy: pointer_ent = tf.reduce_sum(pointer_ent * dec_mask ) / tf.reduce_sum(dec_mask) self.train_output['pointer_ent'] = pointer_ent avg_max_ptr = tf.reduce_sum(avg_max_ptr * dec_mask ) / tf.reduce_sum(dec_mask) self.train_output['avg_max_ptr'] = avg_max_ptr avg_num_copy = tf.reduce_sum(avg_num_copy * dec_mask, 1) avg_num_copy = tf.reduce_mean(avg_num_copy) self.train_output['avg_num_copy'] = avg_num_copy self.infer_output = {'dec_predict': dec_outputs_predict} dec_out_mem_ratio = _calculate_dec_out_mem_ratio(dec_outputs_predict, sample_ind, vocab_size, self.pad_id, self.dec_start_id, self. dec_end_id) self.infer_output.update(dec_out_mem_ratio) self.infer_output.update(sample_memory_output) self.infer_output.update(seq_neighbor_output) return 
thumt.utils.distribute.size def size(): return _ENGINE.size() if _ENGINE is not None else 1 
enas.cifar10.micro_child.MicroChild.build_valid_rl.process|pre def _pre_process(x): x = tf.pad(x, [[4, 4], [4, 4], [0, 0]]) x = tf.random_crop(x, [32, 32, 3], seed=self.seed) x = tf.image.random_flip_left_right(x, seed=self.seed) if self.data_format == 'NCHW': x = tf.transpose(x, [2, 0, 1]) return x 
plot_nll.get|test|data|train def get_train_test_data(path): try: df_train = pd.read_csv(path + '/train.txt', sep='\t') df_test = pd.read_csv(path + '/test.txt', sep='\t') return df_train, df_test except: print('error reading: %s' % path + 'train/test.txt') return None, None 
classification.ops.fisher_factors.ConvInputKroneckerFactor.compute|new|cov def _compute_new_cov(self, idx=0): if idx != 0: raise ValueError('ConvInputKroneckerFactor only supports idx = 0') with _maybe_colocate_with(self._inputs, self._colocate_cov_ops_with_inputs ): filter_height, filter_width, in_channels, _ = self._filter_shape patches = array_ops.extract_image_patches(self._inputs, ksizes=[1, filter_height, filter_width, 1], strides=self._strides, rates=[ 1, 1, 1, 1], padding=self._padding) flatten_size = filter_height * filter_width * in_channels patches_flat = array_ops.reshape(patches, [-1, flatten_size]) if self._has_bias: patches_flat = _append_homog(patches_flat) return _compute_cov(patches_flat) 
xlnet-master.run_squad.input_fn_builder.fn|input def input_fn(params): """The actual input function.""" if FLAGS.use_tpu: batch_size = params['batch_size'] elif is_training: batch_size = FLAGS.train_batch_size else: batch_size = FLAGS.predict_batch_size if num_hosts > 1: host_id = params['context'].current_host num_files = len(global_input_paths) if num_files >= num_hosts: num_files_per_host = (num_files + num_hosts - 1) // num_hosts my_start_file_id = host_id * num_files_per_host my_end_file_id = min((host_id + 1) * num_files_per_host, num_files) input_paths = global_input_paths[my_start_file_id:my_end_file_id] tf.logging.info('Host {} handles {} files'.format(host_id, len( input_paths))) else: input_paths = global_input_paths if len(input_paths) == 1: d = tf.data.TFRecordDataset(input_paths[0]) if is_training: d = d.shuffle(buffer_size=FLAGS.shuffle_buffer) d = d.repeat() else: d = tf.data.Dataset.from_tensor_slices(input_paths) d = d.shuffle(len(input_paths)).repeat() cycle_length = min(num_threads, len(input_paths)) d = d.apply(tf.contrib.data.parallel_interleave(tf.data. TFRecordDataset, sloppy=is_training, cycle_length=cycle_length)) if is_training: d = d.shuffle(buffer_size=FLAGS.shuffle_buffer) d = d.apply(tf.contrib.data.map_and_batch(lambda record: _decode_record (record, name_to_features), batch_size=batch_size, num_parallel_batches=num_threads, drop_remainder=drop_remainder)) d = d.prefetch(1024) return d 
run_lm_finetuning.convert|features|example|to def convert_example_to_features(example, max_seq_length, tokenizer): """ Convert a raw sample (pair of sentences as tokenized strings) into a proper training sample with IDs, LM labels, input_mask, CLS and SEP tokens etc. :param example: InputExample, containing sentence input as strings and is_next label :param max_seq_length: int, maximum length of sequence. :param tokenizer: Tokenizer :return: InputFeatures, containing all inputs and labels of one sample as IDs (as used for model training) """ tokens_a = example.tokens_a tokens_b = example.tokens_b _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3) tokens_a, t1_label = random_word(tokens_a, tokenizer) tokens_b, t2_label = random_word(tokens_b, tokenizer) lm_label_ids = [-1] + t1_label + [-1] + t2_label + [-1] tokens = [] segment_ids = [] tokens.append('[CLS]') segment_ids.append(0) for token in tokens_a: tokens.append(token) segment_ids.append(0) tokens.append('[SEP]') segment_ids.append(0) assert len(tokens_b) > 0 for token in tokens_b: tokens.append(token) segment_ids.append(1) tokens.append('[SEP]') segment_ids.append(1) input_ids = tokenizer.convert_tokens_to_ids(tokens) input_mask = [1] * len(input_ids) while len(input_ids) < max_seq_length: input_ids.append(0) input_mask.append(0) segment_ids.append(0) lm_label_ids.append(-1) assert len(input_ids) == max_seq_length assert len(input_mask) == max_seq_length assert len(segment_ids) == max_seq_length assert len(lm_label_ids) == max_seq_length if example.guid < 5: logger.info('*** Example ***') logger.info('guid: %s' % example.guid) logger.info('tokens: %s' % ' '.join([str(x) for x in tokens])) logger.info('input_ids: %s' % ' '.join([str(x) for x in input_ids])) logger.info('input_mask: %s' % ' '.join([str(x) for x in input_mask])) logger.info('segment_ids: %s' % ' '.join([str(x) for x in segment_ids]) ) logger.info('LM label: %s ' % lm_label_ids) logger.info('Is next sentence label: %s ' % example.is_next) features = InputFeatures(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, lm_label_ids=lm_label_ids, is_next=example .is_next) return features 
model.Model.Model def __init__(self, mode): """ResNet constructor.  Args: mode: One of 'train' and 'eval'. """ self.mode = mode self._build_model() 
SRGANs-Spectral-Regularization-GANs--master.train.make|optimizer def make_optimizer(model, alpha=0.0002, beta1=0.0, beta2=0.9): optimizer = chainer.optimizers.Adam(alpha=alpha, beta1=beta1, beta2=beta2) optimizer.setup(model) return optimizer 
utils.batch|slice def batch_slice(inputs, graph_fn, batch_size, names=None): """Splits inputs into slices and feeds each slice to a copy of the given computation graph and then combines the results. It allows you to run a graph on a batch of inputs even if the graph is written to support one instance only. inputs: list of tensors. All must have the same first dimension length graph_fn: A function that returns a TF tensor that's part of a graph. batch_size: number of slices to divide the data into. names: If provided, assigns names to the resulting tensors. """ if not isinstance(inputs, list): inputs = [inputs] outputs = [] for i in range(batch_size): inputs_slice = [x[i] for x in inputs] output_slice = graph_fn(*inputs_slice) if not isinstance(output_slice, (tuple, list)): output_slice = [output_slice] outputs.append(output_slice) outputs = list(zip(*outputs)) if names is None: names = [None] * len(outputs) result = [tf.stack(o, axis=0, name=n) for o, n in zip(outputs, names)] if len(result) == 1: result = result[0] return result 
avod.core.trainer_test.FakeBatchNormClassifier.Classifier|Norm|Batch def BatchNormClassifier(self, inputs): inputs = layers.batch_norm(inputs, decay=0.1, fused=None) return layers.fully_connected(inputs, 1, activation_fn=math_ops.sigmoid) 
VIPPruning.VIPPruning.VIP|Pruning def __init__(self, n_comp=2, model=None, layers=[], representation='max', percentage_discard=0.1, face_verif=False): if len(layers) == 0: self.layers = list(range(1, len(model.layers))) else: self.layers = layers if representation == 'max': self.pool = GlobalMaxPooling2D() elif representation == 'avg': self.pool = GlobalAveragePooling2D() else: self.pool = representation self.n_comp = n_comp self.scores = None self.score_layer = None self.idx_score_layer = [] self.template_model = model self.conv_net = self.custom_model(model=model, layers=self.layers) self.percentage_discard = percentage_discard self.face_verif = face_verif 
vgg19small.Vgg19.get|bias def get_bias(self, name): return tf.constant(self.data_dict[name][1], name='biases') 
plato.agent.component.dialogue_policy.reinforcement_learning.wolf_phc_policy.WoLFPHCPolicy.restart def restart(self, args): """ Re-initialize relevant parameters / variables at the beginning of each dialogue.  :return: nothing """ if self.agent_role == 'user' and self.warmup_simulator: if 'goal' in args: self.warmup_simulator.initialize(args) else: print( 'WARNING! No goal provided for WoLF PHC policy user simulator @ restart' ) self.warmup_simulator.initialize({}) 
shuffle_corpus.main def main(args): name = args.corpus suffix = '.' + args.suffix stream = [_open(item, 'r') for item in name] data = [fd.readlines() for fd in stream] minlen = min([len(lines) for lines in data]) count = 0 if args.seed: numpy.random.seed(args.seed) indices = numpy.arange(minlen) numpy.random.shuffle(indices) if args.num_shards == 1: newstream = [[_open(item + suffix, 'w') for item in name]] else: newstream = [[_open(item + '-%s-of-%s' % (i, args.num_shards), 'w') for item in name] for i in range(args.num_shards)] for idx in indices.tolist(): lines = [item[idx] for item in data] for line, fd in zip(lines, newstream[count % args.num_shards]): fd.write(line) count += 1 for fdr in stream: fdr.close() for fds in newstream: for fd in fds: fd.close() 
DTAN_layer.DTAN_model.outputs|RDTAN|plot def plot_RDTAN_outputs(self, model, X, y, ratio=[8, 6], name='movie.gif'): """  :param model: trained DTAN Keras model :param X: :param p_samples: Bool. Plot samples :param p_mean: bool. plot mean :return: """ plot_all_layers(model, X, y, self.n_recurrences, ratio, name) nb_points = 1000 
xlnet-master.tpu_estimator._ModelFnWrapper.tpu|verify|spec|predictions def _verify_tpu_spec_predictions(self, predictions): """Validates TPUEstimatorSpec.predictions dict.""" if not isinstance(predictions, dict): raise TypeError('TPUEstimatorSpec.predictions must be dict of Tensors.' ) for key, tensor in predictions.items(): if tensor.shape.dims[0].value is None: raise ValueError( 'The tensor with key ({}) in TPUEstimatorSpec.predictions has dynamic shape (should be static). Tensor: {}' .format(key, tensor)) return predictions 
elpips.networks.squeezenet1_1.squeeze def _squeeze(self, input, index, ch_in, ch_out): return self._conv(input, self.features['{:d}.squeeze.weight'.format( index)], self.features['{:d}.squeeze.bias'.format(index)], w_shape= [1, 1, ch_in, ch_out], padding='VALID', stride=[1, 1, 1, 1]) 
texar.modules.memory.memory_network.MemNetBase.fn|build|embed def _build_embed_fn(self, input_embed_fn, output_embed_fn, query_embed_fn): memory_dim = self.hparams.memory_dim mdim_A, mdim_C, mdim_B = None, None, None A = input_embed_fn if input_embed_fn is None: A, mdim_A = self.get_default_embed_fn(self._memory_size, self. _hparams.A) memory_dim = mdim_A C = output_embed_fn if output_embed_fn is None: C, mdim_C = self.get_default_embed_fn(self._memory_size, self. _hparams.C) if mdim_A is not None and mdim_A != mdim_C: raise ValueError( 'Embedding config `A` and `C` must have the same output dimension.' ) memory_dim = mdim_C B = query_embed_fn if query_embed_fn is None and self._hparams.use_B: B, mdim_B = self.get_default_embed_fn(1, self._hparams.B) if mdim_A is not None and mdim_A != mdim_B: raise ValueError( 'Embedding config `A` and `B` must have the same output dimension.' ) if mdim_C is not None and mdim_C != mdim_B: raise ValueError( 'Embedding config `C` and `B` must have the same output dimension.' ) memory_dim = mdim_B return A, C, B, memory_dim 
shuffle_exchange_model.ShuffleExchangeModel.get|all|memory def get_all_memory(self, sess, batch_xs_list, batch_ys_list): """Gets an execution trace for the given inputs""" feed_dict = self.prepare_test_dict(batch_xs_list, batch_ys_list) mem = sess.run(self.allMem, feed_dict=feed_dict) return mem 
cdvae-cls-gan-mcc.CDVAECLSGAN.loss def loss(self, data): x_sp = data['sp'] x_mcc = data['mcc'] y = data['speaker'] label = tf.one_hot(tf.reduce_mean(y, axis=1, keep_dims=True), self.arch ['y_dim']) x_sp_in_minmax = self.normalizers['sp']['minmax'].forward_process(x_sp) x_sp_in = tf.expand_dims(x_sp_in_minmax, 1) x_mcc_in_minmax = self.normalizers['mcc']['minmax'].forward_process(x_mcc) x_mcc_in = tf.expand_dims(x_mcc_in_minmax, 1) sp_z_mu, sp_z_lv = self.sp_enc(x_sp_in) z_sp = GaussianSampleLayer(sp_z_mu, sp_z_lv) x_sp_sp = self.sp_dec(z_sp, y) x_sp_mcc = self.mcc_dec(z_sp, y) cls_sp_logit = self.latent_cls(sp_z_mu) z_sp_pred = tf.nn.softmax(cls_sp_logit) sp_corr_pred = tf.equal(tf.argmax(z_sp_pred, 1), tf.reduce_mean(y, axis=1)) mcc_z_mu, mcc_z_lv = self.mcc_enc(x_mcc_in) z_mcc = GaussianSampleLayer(mcc_z_mu, mcc_z_lv) x_mcc_sp = self.sp_dec(z_mcc, y) x_mcc_mcc = self.mcc_dec(z_mcc, y) x_mcc_mcc_NCHW = tf.expand_dims(x_mcc_mcc, axis=1) real_mcc_logit = self.mcc_dis(x_mcc_in) fake_mcc_logit = self.mcc_dis(x_mcc_mcc_NCHW) cls_mcc_logit = self.latent_cls(mcc_z_mu) z_mcc_pred = tf.nn.softmax(cls_mcc_logit) mcc_corr_pred = tf.equal(tf.argmax(z_mcc_pred, 1), tf.reduce_mean(y, axis=1)) kl_loss_sp = kl_loss(sp_z_mu, sp_z_lv) recon_loss_sp = log_loss(x_sp_in, x_sp_sp) cross_loss_sp2mcc = log_loss(x_mcc_in, x_sp_mcc) kl_loss_mcc = kl_loss(mcc_z_mu, mcc_z_lv) recon_loss_mcc = log_loss(x_mcc_in, x_mcc_mcc_NCHW) cross_loss_mcc2sp = log_loss(x_sp_in, x_mcc_sp) latent_loss = tf.reduce_mean(tf.abs(sp_z_mu - mcc_z_mu)) gradient_penalty_mcc = gradient_penalty_loss(x_mcc_in, x_mcc_mcc, self. mcc_dis) cls_loss_sp = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( labels=tf.stop_gradient(label), logits=cls_sp_logit)) cls_loss_mcc = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( labels=tf.stop_gradient(label), logits=cls_mcc_logit)) acc = 0.5 * (tf.reduce_mean(tf.cast(sp_corr_pred, tf.float32)) + tf. reduce_mean(tf.cast(mcc_corr_pred, tf.float32))) loss = dict() loss['D_KL_sp'] = kl_loss_sp loss['D_KL_mcc'] = kl_loss_mcc loss['recon_sp'] = recon_loss_sp loss['recon_mcc'] = recon_loss_mcc loss['cross_sp2mcc'] = cross_loss_sp2mcc loss['cross_mcc2sp'] = cross_loss_mcc2sp loss['latent'] = latent_loss loss['wgan_mcc'] = tf.reduce_mean(fake_mcc_logit) - tf.reduce_mean( real_mcc_logit) loss['wgan_gp_mcc'] = gradient_penalty_mcc loss['cls_loss_sp'] = cls_loss_sp loss['cls_loss_mcc'] = cls_loss_mcc with tf.name_scope('Summary'): tf.summary.scalar('KL-div-sp', kl_loss_sp) tf.summary.scalar('KL-div-mcc', kl_loss_mcc) tf.summary.scalar('reconstruction-sp', recon_loss_sp) tf.summary.scalar('reconstruction-mcc', recon_loss_mcc) tf.summary.scalar('cross-sp2mcc', cross_loss_sp2mcc) tf.summary.scalar('cross-mcc2sp', cross_loss_mcc2sp) tf.summary.scalar('latent', latent_loss) tf.summary.scalar('wgan-mcc', loss['wgan_mcc']) tf.summary.scalar('wgan-gp-mcc', gradient_penalty_mcc) tf.summary.scalar('cls-sp', cls_loss_sp) tf.summary.scalar('cls-mcc', cls_loss_mcc) tf.summary.scalar('cls-accuracy', acc) return loss 
avod.core.feature_extractors.img_vgg.ImgVgg.vgg|arg|scope def vgg_arg_scope(self, weight_decay=0.0005): """Defines the VGG arg scope.  Args: weight_decay: The l2 regularization coefficient.  Returns: An arg_scope. """ with slim.arg_scope([slim.conv2d, slim.fully_connected], activation_fn= tf.nn.relu, weights_regularizer=slim.l2_regularizer(weight_decay), biases_initializer=tf.zeros_initializer()): with slim.arg_scope([slim.conv2d], padding='SAME') as arg_sc: return arg_sc 
ML_MonteCarloUQ.invert def invert(x_vec, cdf): vec = np.linspace(0, 1, 500) idx = np.searchsorted(cdf, vec) for i in range(len(idx)): if idx[i] == len(x_vec): idx[i] = idx[i] - 1 x1 = x_vec[idx - 1] x2 = x_vec[idx] f1 = cdf[idx - 1] f2 = cdf[idx] x = x1 + (x2 - x1) / (f2 - f1) * (vec - f1) return x 
nets.resnet_v1_test.ResnetCompleteNetworkTest.Width|Convolutional|Atrous|Fully|Height|Unknown|test def testAtrousFullyConvolutionalUnknownHeightWidth(self): batch = 2 height, width = 65, 65 global_pool = False output_stride = 8 inputs = create_test_input(batch, None, None, 3) with slim.arg_scope(resnet_utils.resnet_arg_scope()): output, _ = self._resnet_small(inputs, None, global_pool= global_pool, output_stride=output_stride) self.assertListEqual(output.get_shape().as_list(), [batch, None, None, 32]) images = create_test_input(batch, height, width, 3) with self.test_session() as sess: sess.run(tf.global_variables_initializer()) output = sess.run(output, {inputs: images.eval()}) self.assertEqual(output.shape, (batch, 9, 9, 32)) 
env_ndh.NDHEnv._get_current_observation.pad|invalid|with def _pad_with_invalid(room_panos): padding = max(0, self._max_goal_room_panos + 1 - len(room_panos)) return room_panos + [constants.INVALID_NODE_ID] * padding 
nets.resnet_v2_test.ResnetUtilsTest.test|Subsample|Four|By def testSubsampleFourByFour(self): x = tf.reshape(tf.to_float(tf.range(16)), [1, 4, 4, 1]) x = resnet_utils.subsample(x, 2) expected = tf.reshape(tf.constant([0, 2, 8, 10]), [1, 2, 2, 1]) with self.test_session(): self.assertAllClose(x.eval(), expected.eval()) 
craystack.codecs.AutoRegressive.push def push(message, data, all_params=None): if not all_params: all_params = param_fn(data) for idx in reversed(elem_idxs): elem_params = all_params[idx] elem_push, _ = elem_codec(elem_params, idx) message = elem_push(message, data[idx].astype('uint64')) return message 
nets.gen_models.SRCNN.deconv|d def deconv2d(self, layer_input): u = UpSampling2D(size=2)(layer_input) u = Convolution2D(256, kernel_size=3, strides=1, padding='same')(u) u = Activation('relu')(u) return u 
tf_utils.common.to|gpu|assign def assign_to_gpu(gpu=0, ps_dev='/device:CPU:0'):  def _assign(op): node_def = op if isinstance(op, tf.NodeDef) else op.node_def if node_def.op == 'Variable': return ps_dev else: return '/gpu:%d' % gpu return _assign 
texar.data.data.data_iterators.TrainTestFeedableDataIterator.get|handle|train def get_train_handle(self, sess): """Returns the handle of the training dataset. The handle can be used to feed the :attr:`handle` placeholder to fetch training data.  Args: sess: The current tf session.  Returns: A string handle to be fed to the :attr:`handle` placeholder.  Example:  .. code-block:: python  next_element = iterator.get_next() train_handle = iterator.get_train_handle(sess) # Gets the next training element ne_ = sess.run(next_element, feed_dict={iterator.handle: train_handle}) """ if self._train_name not in self._datasets: raise ValueError('Training data not provided.') return self.get_handle(sess, self._train_name) 
models.tiramisu.FCDenseNet.forward def forward(self, x): out = self.firstconv(x) skip_connections = [] for i in range(len(self.down_blocks)): out = self.denseBlocksDown[i](out) skip_connections.append(out) out = self.transDownBlocks[i](out) out = self.bottleneck(out) for i in range(len(self.up_blocks)): skip = skip_connections.pop() out = self.transUpBlocks[i](out, skip) out = self.denseBlocksUp[i](out) out = self.finalConv(out) return out 
nmt.async_checkpoint.AsyncCheckpointSaverHook.after_create_session.graph|fn|write def _write_graph_fn(self): training_util.write_graph(ops.get_default_graph().as_graph_def( add_shapes=True), self._checkpoint_dir, 'graph.pbtxt') 
neural_tangents.utils.utils.to|output|dict def _output_to_dict(output): if isinstance(output, dict): return output if hasattr(output, '_asdict'): return output._asdict() if isinstance(output, types.GeneratorType): return (_output_to_dict(out) for out in output) raise ValueError(type(output)) 
lanenet_data_processor_test.DataSet.dataset|init def _init_dataset(self): """ :return: """ img_list = [] if not tf.gfile.Exists(self._dataset_info_file): raise ValueError('Failed to find file: ' + self._dataset_info_file) with open(self._dataset_info_file, 'r') as file: for _info in file: info_tmp = _info.strip(' ').split() img_list.append(info_tmp[0][1:]) self._len = len(img_list) return img_list 
bow_seq2seq.fn|loss|enc def enc_loss_fn(bow_loss_fn, enc_targets, bow_topk_prob, max_enc_bow): """Different encoder loss wrapper""" if bow_loss_fn == 'nll': enc_loss = -enc_targets * tf.log(bow_topk_prob + 1e-06) enc_loss_norm = tf.reduce_sum(enc_targets, 1) + 1.0 enc_loss = tf.reduce_mean(tf.reduce_sum(enc_loss, 1) / enc_loss_norm) elif bow_loss_fn == 'crossent': bow_topk_prob /= float(max_enc_bow) enc_loss = -(enc_targets * tf.log(bow_topk_prob + 1e-06) + (1 - enc_targets) * tf.log(1 - bow_topk_prob + 1e-06)) enc_loss = tf.reduce_mean(tf.reduce_sum(enc_loss, axis=1) / tf. reduce_sum(enc_targets, axis=1)) elif bow_loss_fn == 'l1': enc_loss = tf.losses.absolute_difference(enc_targets, bow_topk_prob) return enc_loss 
cnn_helpers.regularizer|l def l2_regularizer(weight):  def regularizer(t): return weight * tf.nn.l2_loss(t) return regularizer 
mnist_tutorial.main def main(unused_argv): tf.logging.set_verbosity(3) train_data, train_labels, test_data, test_labels = load_mnist() mnist_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn, model_dir=FLAGS.model_dir) eval_input_fn = tf.estimator.inputs.numpy_input_fn(x={'x': test_data}, y=test_labels, num_epochs=1, shuffle=False) steps_per_epoch = 60000 // 256 test_accuracy_list = [] for epoch in range(1, FLAGS.epochs + 1): np.random.seed(epoch) for step in range(steps_per_epoch): tf.set_random_seed(0) whether = np.random.random_sample(60000) > 1 - 256 / 60000 subsampling = [i for i in np.arange(60000) if whether[i]] global microbatches microbatches = len(subsampling) train_input_fn = tf.estimator.inputs.numpy_input_fn(x={'x': train_data[subsampling]}, y=train_labels[subsampling], batch_size=len(subsampling), num_epochs=1, shuffle=False) mnist_classifier.train(input_fn=train_input_fn, steps=1) eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn) test_accuracy = eval_results['accuracy'] test_accuracy_list.append(test_accuracy) print('Test accuracy after %d epochs is: %.3f' % (epoch, test_accuracy) ) if FLAGS.dpsgd: eps = compute_epsP(epoch, FLAGS.noise_multiplier, 60000, 256, 1e-05 ) mu = compute_muP(epoch, FLAGS.noise_multiplier, 60000, 256) print('For delta=1e-5, the current epsilon is: %.2f' % eps) print('For delta=1e-5, the current mu is: %.2f' % mu) if mu > 2: break else: print('Trained with vanilla non-private SGD optimizer') if FLAGS.dpsgd: fffff = open(os.getcwd() + '/PoissonMNIST_' + str(FLAGS. noise_multiplier) + '.pkl', 'wb') else: fffff = open(os.getcwd() + '/PoissonMNIST_nonprivate.pkl', 'wb') pickle.dump(test_accuracy_list, fffff) fffff.close() 
slac.environments.gym_wrappers.PixelObservationsGymWrapper.render def render(self, mode='rgb_array'): return self._env.render(mode=mode) 
losses.pdist def _pdist(a, b=None): sq_sum_a = tf.reduce_sum(tf.square(a), reduction_indices=[1]) if b is None: return -2 * tf.matmul(a, tf.transpose(a)) + tf.reshape(sq_sum_a, (- 1, 1)) + tf.reshape(sq_sum_a, (1, -1)) sq_sum_b = tf.reduce_sum(tf.square(b), reduction_indices=[1]) return -2 * tf.matmul(a, tf.transpose(b)) + tf.reshape(sq_sum_a, (-1, 1) ) + tf.reshape(sq_sum_b, (1, -1)) 
pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.XL|Tokenizer|Transfo def __init__(self, special=[], min_freq=0, max_size=None, lower_case=False, delimiter=None, vocab_file=None, never_split=('<unk>', '<eos>', '<formula>')): self.counter = Counter() self.special = special self.min_freq = min_freq self.max_size = max_size self.lower_case = lower_case self.delimiter = delimiter self.vocab_file = vocab_file self.never_split = never_split 
gan.op.batch_norm.call def __call__(self, x, train=True): return tf.contrib.layers.batch_norm(x, decay=self.momentum, updates_collections=None, epsilon=self.epsilon, scale=True, is_training=train, scope=self.name) 
gym_pycolab.examples.extraterrestrial_marauders.DownwardLaserBoltSprite.fire def _fire(self, layers, the_plot): """Launches a new bolt from a random Marauder.""" if the_plot.get('last_marauder_shot') == the_plot.frame: return the_plot['last_marauder_shot'] = the_plot.frame col = np.random.choice(np.nonzero(layers['X'].sum(axis=0))[0]) row = np.nonzero(layers['X'][:, (col)])[0][-1] + 1 self._teleport((row, col)) 
model.CVAE1.forward def _forward(self, x, gpu): hps = self.hps x = self.preprocess(x) with arg_scope([conv2d, deconv2d], init=self.mode == 'init'): self.layers = self.create_layers() input = self.downsample(x) for layer in self.layers: input = layer.up(input) input = self.initial_input_down() kl_cost = kl_obj = 0.0 for j, layer in reversed(list(enumerate(self.layers))): input, cur_obj, cur_cost = layer.down(input) kl_obj += cur_obj kl_cost += cur_cost if self.mode == 'train' and gpu == hps.num_gpus - 1: tf.summary.scalar('model/kl_obj_%02d_%02d' % (0, j), tf. reduce_mean(cur_obj)) tf.summary.scalar('model/kl_cost_%02d_%02d' % (0, j), tf. reduce_mean(cur_cost)) x_out = self.upsample_and_postprocess(input) log_pxz = discretized_logistic(x_out, self.dec_log_stdv, sample=x) obj = tf.reduce_sum(kl_obj - log_pxz) if self.mode == 'train' and gpu == hps.num_gpus - 1: tf.summary.scalar('model/log_pxz', -tf.reduce_mean(log_pxz)) tf.summary.scalar('model/kl_obj', tf.reduce_mean(kl_obj)) tf.summary.scalar('model/kl_cost', tf.reduce_mean(kl_cost)) loss = tf.reduce_sum(compute_lowerbound(log_pxz, kl_cost, hps.k)) return x_out, obj, loss 
plato.agent.component.dialogue_manager.dialogue_manager_generic.DialogueManagerGeneric.db|lookup def db_lookup(self): """ Perform an SQLite query given the current dialogue state (i.e. given which slots have values).  :return: a dictionary containing the current database results """ d_state = self.DSTracker.get_state() db_result = self.database.db_lookup(d_state) if db_result: entropies = dict.fromkeys(self.ontology.ontology['system_requestable']) if self.CALCULATE_SLOT_ENTROPIES: value_probabilities = {} for req_slot in self.ontology.ontology['system_requestable']: value_probabilities[req_slot] = {} for db_item in db_result: if db_item[req_slot] not in value_probabilities[req_slot]: value_probabilities[req_slot][db_item[req_slot]] = 1 else: value_probabilities[req_slot][db_item[req_slot]] += 1 for slot in value_probabilities: for value in value_probabilities[slot]: value_probabilities[slot][value] /= len(db_result) for slot in entropies: entropies[slot] = 0 if slot in value_probabilities: for value in value_probabilities[slot]: entropies[slot] += value_probabilities[slot][value ] * math.log(value_probabilities[slot][value]) entropies[slot] = -entropies[slot] return db_result[:self.MAX_DB_RESULTS], entropies return ['empty'], {} 
scores.scores def scores(path): bashCommand = 'perl conlleval' process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE, stdin=open(path)) output, error = process.communicate() output = output.decode().split('\n')[1].split('%; ') output = [out.split(' ')[-1] for out in output] acc, prec, recall, fb1 = tuple(output) return float(acc), float(prec), float(recall), float(fb1) 
model.refine|detections def refine_detections(rois, probs, deltas, window, config): """Refine classified proposals and filter overlaps and return final detections.  Inputs: rois: [N, (y1, x1, y2, x2)] in normalized coordinates probs: [N, num_classes]. Class probabilities. deltas: [N, num_classes, (dy, dx, log(dh), log(dw))]. Class-specific bounding box deltas. window: (y1, x1, y2, x2) in image coordinates. The part of the image that contains the image excluding the padding.  Returns detections shaped: [N, (y1, x1, y2, x2, class_id, score)] """ class_ids = np.argmax(probs, axis=1) class_scores = probs[np.arange(class_ids.shape[0]), class_ids] deltas_specific = deltas[np.arange(deltas.shape[0]), class_ids] refined_rois = utils.apply_box_deltas(rois, deltas_specific * config. BBOX_STD_DEV) height, width = config.IMAGE_SHAPE[:2] refined_rois *= np.array([height, width, height, width]) refined_rois = clip_to_window(window, refined_rois) refined_rois = np.rint(refined_rois).astype(np.int32) keep = np.where(class_ids > 0)[0] if config.DETECTION_MIN_CONFIDENCE: keep = np.intersect1d(keep, np.where(class_scores >= config. DETECTION_MIN_CONFIDENCE)[0]) pre_nms_class_ids = class_ids[keep] pre_nms_scores = class_scores[keep] pre_nms_rois = refined_rois[keep] nms_keep = [] for class_id in np.unique(pre_nms_class_ids): ixs = np.where(pre_nms_class_ids == class_id)[0] class_keep = utils.non_max_suppression(pre_nms_rois[ixs], pre_nms_scores[ixs], config.DETECTION_NMS_THRESHOLD) class_keep = keep[ixs[class_keep]] nms_keep = np.union1d(nms_keep, class_keep) keep = np.intersect1d(keep, nms_keep).astype(np.int32) roi_count = config.DETECTION_MAX_INSTANCES top_ids = np.argsort(class_scores[keep])[::-1][:roi_count] keep = keep[top_ids] result = np.hstack((refined_rois[keep], class_ids[keep][..., np.newaxis ], class_scores[keep][..., np.newaxis])) return result 
gym_pycolab.envs.pycolab_grid_worlds_env.PycolabGridWorldsLevel5Env.game|make def _make_game(self): self._setup() return grid_worlds.make_game(terminal_reward=0.4, positive_rewards=3, bonus_reward=0.2) 
avod.core.box_3d_encoder.iou|format|to|d|box def box_3d_to_3d_iou_format(boxes_3d): """ Returns a numpy array of 3d box format for iou calculation Args: boxes_3d: list of 3d boxes Returns: new_anchor_list: numpy array of 3d box format for iou """ boxes_3d = np.asarray(boxes_3d) fc.check_box_3d_format(boxes_3d) iou_3d_boxes = np.zeros([len(boxes_3d), 7]) iou_3d_boxes[:, 4:7] = boxes_3d[:, 0:3] iou_3d_boxes[:, (1)] = boxes_3d[:, (3)] iou_3d_boxes[:, (2)] = boxes_3d[:, (4)] iou_3d_boxes[:, (3)] = boxes_3d[:, (5)] iou_3d_boxes[:, (0)] = boxes_3d[:, (6)] return iou_3d_boxes 
europilot.train._ConfigType.getattr def __getattr__(self, attr): raise TrainException('Invalid configuration: %s' % attr) 
daily_dialog_chatbot.DailyDialogChatbot.data|create def create_data(self, train_mode): """ Params: :train_mode: Whether we are in train or dev mode. """ (trainSource, trainTarget, devSource, devTarget, testSource, testTarget ) = self.open_6_files() dialogs = open(os.path.join(self._raw_data, 'dialogues_text.txt'), errors='ignore') vocabulary = Counter() number_of_dialogs = 0 line_counter = 0 dataset_split_counter = 0 for dialog in dialogs: dataset_split_counter += 1 if number_of_dialogs % 1000 == 0: print('t2t_csaky_log: Parsed ' + str(number_of_dialogs) + ' dialogs.') utterances = dialog.split('__eou__')[:-1] if dataset_split_counter <= self.dataset_split['train']: source_file = trainSource target_file = trainTarget elif dataset_split_counter <= self.dataset_split['train' ] + self.dataset_split['val']: source_file = devSource target_file = devTarget else: source_file = testSource target_file = testTarget i = 0 for utterance in utterances: line_counter += 1 utterance = self.clean_line(utterance.lower()) i += 1 if dataset_split_counter <= self.dataset_split['train']: words = utterance.split() for word in words: if word in vocabulary: vocabulary[word] += 1 else: vocabulary[word] = 1 if i != len(utterances): source_file.write(utterance + '\n') if i != 1: target_file.write(utterance + '\n') number_of_dialogs += 1 if dataset_split_counter == 100: dataset_split_counter = 0 if (self.targeted_dataset_size != 0 and self.targeted_dataset_size < line_counter): break self.close_n_files([trainSource, trainTarget, devSource, devTarget, testSource, testTarget]) dialogs.close() self.save_vocab(vocabulary) 
neural_tangents.stax.conv|var|d def _conv_var_3d(var1, filter_shape, strides, padding): """Compute variances of the CNN outputs given inputs with variances `var1`.  Args: var1: a 3D `np.ndarray` containing sample-pixel variances. Has shape `[batch_size, height, width]`. filter_shape: tuple of positive integers, the convolutional filters spatial shape (e.g. `(3, 3)` for a 2D convolution). strides: tuple of positive integers, the CNN strides (e.g. `(1, 1)` for a 2D convolution). padding: a `Padding` enum, e.g. `Padding.CIRCULAR`.  Returns: a 3D `np.ndarray` containing sample-pixel variances of CNN layer outputs. Has shape `[batch_size, new_height, new_width]`. """ if var1 is None: return var1 if padding == Padding.CIRCULAR: var1 = _same_pad_for_filter_shape(var1, filter_shape, strides, (1, 2), 'wrap') padding = Padding.VALID channel_axis = _CONV_QAB_DIMENSION_NUMBERS[0].index('C') var1 = np.expand_dims(var1, channel_axis) ker_var1 = np.full(filter_shape + (1, 1), 1.0 / np.prod(filter_shape), var1.dtype) var1 = lax.conv_general_dilated(var1, ker_var1, strides, padding.name, dimension_numbers=_CONV_QAB_DIMENSION_NUMBERS) var1 = np.squeeze(var1, channel_axis) return var1 
data_utils.read|data def read_data(path, max_size=None): data_set = [] data = json.load(open(path, 'r')) counter = 0 for pair in data: post = pair[0][0] responses = pair[1] source_ids = [int(x) for x in post[0]] for response in responses: if not max_size or counter < max_size: counter += 1 if counter % 100000 == 0: print('    reading data pair %d' % counter) sys.stdout.flush() target_ids = [int(x) for x in response[0]] feat = response[1] desc_ids = [int(x) for x in response[2]] data_set.append([source_ids, target_ids, feat, desc_ids]) return data_set 
ge.walker.RandomWalker.preprocess|probs|transition def preprocess_transition_probs(self): """ Preprocessing of transition probabilities for guiding the random walks. """ G = self.G alias_nodes = {} for node in G.nodes(): unnormalized_probs = [G[node][nbr].get('weight', 1.0) for nbr in G. neighbors(node)] norm_const = sum(unnormalized_probs) normalized_probs = [(float(u_prob) / norm_const) for u_prob in unnormalized_probs] alias_nodes[node] = create_alias_table(normalized_probs) alias_edges = {} for edge in G.edges(): alias_edges[edge] = self.get_alias_edge(edge[0], edge[1]) self.alias_nodes = alias_nodes self.alias_edges = alias_edges return 
commons.measure_utils.project def project(hparams, inputs): outputs = tf.reduce_sum(inputs, axis=2) outputs = tf.reshape(outputs, [hparams.batch_size, -1]) return outputs 
xlnet-master.run_classifier.get_model_fn.model_fn.metric|fn|regression def regression_metric_fn(per_example_loss, label_ids, logits, is_real_example): loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example) pearsonr = tf.contrib.metrics.streaming_pearson_correlation(logits, label_ids, weights=is_real_example) return {'eval_loss': loss, 'eval_pearsonr': pearsonr} 
embeddings.OpenKE.config.Config.Config.optimizer|set def set_optimizer(self, optimizer): self.optimizer = optimizer 
feat_ext.get|mel|spectrogram def get_mel_spectrogram(audio, params_extract=None): """ :param audio: :param params_extract: :return: """ audio = audio.reshape([1, -1]) window = scipy.signal.hamming(params_extract.get('win_length_samples'), sym=False) mel_basis = librosa.filters.mel(sr=params_extract.get('fs'), n_fft= params_extract.get('n_fft'), n_mels=params_extract.get('n_mels'), fmin=params_extract.get('fmin'), fmax=params_extract.get('fmax'), htk=False, norm=None) feature_matrix = np.empty((0, params_extract.get('n_mels'))) for channel in range(0, audio.shape[0]): spectrogram = get_spectrogram(y=audio[(channel), :], n_fft= params_extract.get('n_fft'), win_length_samples=params_extract. get('win_length_samples'), hop_length_samples=params_extract. get('hop_length_samples'), spectrogram_type=params_extract.get( 'spectrogram_type') if 'spectrogram_type' in params_extract else 'magnitude', center=True, window=window, params_extract= params_extract) mel_spectrogram = np.dot(mel_basis, spectrogram) mel_spectrogram = mel_spectrogram.T if params_extract.get('log'): mel_spectrogram = np.log10(mel_spectrogram + params_extract.get ('eps')) feature_matrix = np.append(feature_matrix, mel_spectrogram, axis=0) return feature_matrix 
layers.trace|loss def trace_loss(transition_matrices, num_labels, beta): """ Implementation of the generation of transition matrices based on the features, as proposed by  Luo et al.: "Learning with Noise: Enhance Distantly Supervised Relation Extractionwith Dynamic Transition Matrix". ACL 2017.  This implements Formula 5 or 6, depending on how it is added to the model.  The input is the tensor obtained from the DynamicTransitionMatrixGeneration layer, the number of labels (i.e. number of rows or columns of the dynamic transition matrix) and the beta scalar that scales this loss.  The negative value of the trace is used (as in the paper). That means that a large, positive beta will push the model towards the identity matrix, while a negative beta will push the generated transition matrices towards the off diagonals (noisy settings). """ eye_tensor = K.eye(num_labels)  def trace_loss_function(y_true, y_pred): return beta * -K.sum(transition_matrices * eye_tensor) return trace_loss_function 
dataset.LSVT.load|data def load_data(self, annotation_file=lsvt_annotation): with open(annotation_file) as f: json_data = json.load(f) for filename in os.listdir(self.data_path): img_name = os.path.join(self.data_path, filename) anno_data = json_data[filename[:-4]] points = [anno['points'] for anno in anno_data] transcripts = [anno['transcription'] for anno in anno_data] illegibilities = [anno['illegibility'] for anno in anno_data] for polygon, transcript, illegibility in zip(points, transcripts, illegibilities): if transcript == '###': continue transcript = preprocess(transcript.strip()) if len(transcript) > self.max_len - 1: continue skip = False for char in transcript: if char not in self.label_dict.keys(): skip = True if skip: continue seq_label = [] for char in transcript: seq_label.append(self.label_dict[char]) seq_label.append(self.label_dict['EOS']) non_zero_count = len(seq_label) seq_label = seq_label + [self.label_dict['EOS']] * (self. max_len - non_zero_count) mask = [1] * non_zero_count + [0] * (self.max_len - non_zero_count) points_x = [point[0] for point in polygon] points_y = [point[1] for point in polygon] bbox = [np.amin(points_y), np.amin(points_x), np.amax( points_y), np.amax(points_x)] bbox = [int(item) for item in bbox] bbox_w, bbox_h = bbox[3] - bbox[1], bbox[2] - bbox[0] if bbox_w < 8 or bbox_h < 8: continue self.filenames.append(img_name) self.labels.append(seq_label) self.masks.append(mask) self.bboxes.append(bbox) self.points.append(polygon) 
cleverhans.attacks_tf.CarliniWagnerL2.Carlini|Wagner|L def __init__(self, sess, model, batch_size, confidence, targeted, learning_rate, binary_search_steps, max_iterations, abort_early, initial_const, clip_min, clip_max, num_labels, shape, extension=None): """ Return a tensor that constructs adversarial examples for the given input. Generate uses tf.py_func in order to operate over tensors. This method is more complicated than it has to be for just the L2 attack because the CWL0 and Linfty attacks call this class. :param sess: a TF session. :param model: a cleverhans.model.Model object. :param batch_size: Number of attacks to run simultaneously. :param confidence: Confidence of adversarial examples: higher produces examples with larger l2 distortion, but more strongly classified as adversarial. :param targeted: boolean controlling the behavior of the adversarial examples produced. If set to False, they will be misclassified in any wrong class. If set to True, they will be misclassified in a chosen target class. :param learning_rate: The learning rate for the attack algorithm. Smaller values produce better results but are slower to converge. :param binary_search_steps: The number of times we perform binary search to find the optimal tradeoff- constant between norm of the purturbation and confidence of the classification. :param max_iterations: The maximum number of iterations. Setting this to a larger value will produce lower distortion results. Using only a few iterations requires a larger learning rate, and will produce larger distortion results. :param abort_early: If true, allows early aborts if gradient descent is unable to make progress (i.e., gets stuck in a local minimum). :param initial_const: The initial tradeoff-constant to use to tune the relative importance of size of the pururbation and confidence of classification. If binary_search_steps is large, the initial constant is not important. A smaller value of this constant gives lower distortion results. :param clip_min: (optional float) Minimum input component value. :param clip_max: (optional float) Maximum input component value. :param num_labels: the number of classes in the model's output. :param shape: the shape of the model's input tensor. :param extension: The CW L0 attack (and potentially others in the future) make calls to this attack. This argument allows for slight modifications to the attack algorithm to support minor differences in how the attack runs. """ self.sess = sess self.TARGETED = targeted self.LEARNING_RATE = learning_rate self.MAX_ITERATIONS = max_iterations self.BINARY_SEARCH_STEPS = binary_search_steps self.ABORT_EARLY = abort_early self.CONFIDENCE = confidence self.initial_const = initial_const self.batch_size = batch_size self.clip_min = clip_min self.clip_max = clip_max self.model = model self.extension = extension self.repeat = binary_search_steps >= 10 self.shape = shape = tuple([batch_size] + list(shape)) modifier = tf.Variable(np.zeros(shape, dtype=np.float32)) self.mask = tf.Variable(np.ones(shape), dtype=tf.float32, name='mask') self.timg = tf.Variable(np.zeros(shape), dtype=tf.float32, name='timg') self.simg = tf.Variable(np.zeros(shape), dtype=tf.float32, name='simg') self.tlab = tf.Variable(np.zeros((batch_size, num_labels)), dtype=tf. float32, name='tlab') self.const = tf.Variable(np.zeros(batch_size), dtype=tf.float32, name= 'const') self.assign_mask = tf.placeholder(tf.float32, shape, name='assign_mask') self.assign_timg = tf.placeholder(tf.float32, shape, name='assign_timg') self.assign_simg = tf.placeholder(tf.float32, shape, name='assign_simg') self.assign_tlab = tf.placeholder(tf.float32, (batch_size, num_labels), name='assign_tlab') self.assign_const = tf.placeholder(tf.float32, [batch_size], name= 'assign_const') self.newimg = (tf.tanh(modifier * self.mask + self.timg) + 1) / 2 self.newimg = self.newimg * (clip_max - clip_min) + clip_min self.output = model.get_logits(self.newimg) self.other = (tf.tanh(self.timg) + 1) / 2 * (clip_max - clip_min ) + clip_min self.l2dist = tf.reduce_sum(tf.square(self.newimg - self.other), list( range(1, len(shape)))) real = tf.reduce_sum(self.tlab * self.output, 1) other = tf.reduce_max((1 - self.tlab) * self.output - self.tlab * 10000, 1) if self.TARGETED: loss1 = tf.maximum(0.0, other - real + self.CONFIDENCE) else: loss1 = tf.maximum(0.0, real - other + self.CONFIDENCE) self.loss2 = tf.reduce_sum(self.l2dist) self.loss1 = tf.reduce_sum(self.const * loss1) self.loss = self.loss1 + self.loss2 if extension == 0: self.outgrad = tf.gradients(self.loss, [modifier])[0] start_vars = set(x.name for x in tf.global_variables()) optimizer = tf.train.AdamOptimizer(self.LEARNING_RATE) self.train = optimizer.minimize(self.loss, var_list=[modifier]) end_vars = tf.global_variables() new_vars = [x for x in end_vars if x.name not in start_vars] self.setup = [] self.setup.append(self.mask.assign(self.assign_mask)) self.setup.append(self.timg.assign(self.assign_timg)) self.setup.append(self.simg.assign(self.assign_simg)) self.setup.append(self.tlab.assign(self.assign_tlab)) self.setup.append(self.const.assign(self.assign_const)) self.init = tf.variables_initializer(var_list=[modifier] + new_vars) 
utils_test.MinimizeTest.test_minimize.compute|gradients def compute_gradients(): with tf.GradientTape() as tape: tape.watch(a) loss = a * 2 g = tape.gradient(loss, a) temp_grad.assign(g) return loss 
bert.tokenization.WordpieceTokenizer.tokenize def tokenize(self, text): """Tokenizes a piece of text into its word pieces.  This uses a greedy longest-match-first algorithm to perform tokenization using the given vocabulary.  For example: input = "unaffable" output = ["un", "##aff", "##able"]  Args: text: A single token or whitespace separated tokens. This should have already been passed through `BasicTokenizer.  Returns: A list of wordpiece tokens. """ text = convert_to_unicode(text) output_tokens = [] for token in whitespace_tokenize(text): chars = list(token) if len(chars) > self.max_input_chars_per_word: output_tokens.append(self.unk_token) continue is_bad = False start = 0 sub_tokens = [] while start < len(chars): end = len(chars) cur_substr = None while start < end: substr = ''.join(chars[start:end]) if start > 0: substr = '##' + substr if substr in self.vocab: cur_substr = substr break end -= 1 if cur_substr is None: is_bad = True break sub_tokens.append(cur_substr) start = end if is_bad: output_tokens.append(self.unk_token) else: output_tokens.extend(sub_tokens) return output_tokens 
models.wide_resnet.WideResNet.Res|Wide|Net def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0): super(WideResNet, self).__init__() nChannels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor] assert (depth - 4) % 6 == 0 n = (depth - 4) // 6 block = BasicBlock self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1, padding=1, bias=False) self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate) self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate) self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate) self.bn1 = nn.BatchNorm2d(nChannels[3]) self.relu = nn.ReLU(inplace=True) self.fc = nn.Linear(nChannels[3], num_classes) self.nChannels = nChannels[3] for m in self.modules(): if isinstance(m, nn.Conv2d): n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels m.weight.data.normal_(0, math.sqrt(2.0 / n)) elif isinstance(m, nn.BatchNorm2d): m.weight.data.fill_(1) m.bias.data.zero_() elif isinstance(m, nn.Linear): m.bias.data.zero_() 
xngen.extract|leading|whitespace def extract_leading_whitespace(line): match = re.match('\\s*', line) return match.group(0) if match else '' 
func_model_81.d|convbn def convbn_3d(input, out_planes, kernel_size, stride): seq = Conv3D(out_planes, kernel_size, stride, 'same', use_bias=False)(input ) seq = BatchNormalization()(seq) return seq 
text_gcn-master.utils.adj|normalize def normalize_adj(adj): """Symmetrically normalize adjacency matrix.""" adj = sp.coo_matrix(adj) rowsum = np.array(adj.sum(1)) d_inv_sqrt = np.power(rowsum, -0.5).flatten() d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.0 d_mat_inv_sqrt = sp.diags(d_inv_sqrt) return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo() 
extensions.multi_modal_application.MultiModalApplication.connect|network|data|and def connect_data_and_network(self, outputs_collector=None, gradients_collector=None):  def switch_sampler(for_training): with tf.name_scope('train' if for_training else 'validation'): sampler = self.get_sampler()[0][0 if for_training else -1] return sampler.pop_batch_op() if self.is_training: if self.action_param.validation_every_n > 0: data_dict = tf.cond(tf.logical_not(self.is_validation), lambda : switch_sampler(for_training=True), lambda : switch_sampler( for_training=False)) else: data_dict = switch_sampler(for_training=True) image = tf.cast(data_dict['image'], tf.float32) net_args = {'is_training': self.is_training, 'keep_prob': self. net_param.keep_prob} net_out = self.net(image, **net_args) with tf.name_scope('Optimiser'): optimiser_class = OptimiserFactory.create(name=self. action_param.optimiser) self.optimiser = optimiser_class.get_instance(learning_rate= self.action_param.lr) loss_func = LossFunction(n_class=self.segmentation_param. num_classes, loss_type=self.action_param.loss_type, softmax= self.segmentation_param.softmax) data_loss = loss_func(prediction=net_out, ground_truth=data_dict. get('label', None), weight_map=data_dict.get('weight', None)) reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES) if self.net_param.decay > 0.0 and reg_losses: reg_loss = tf.reduce_mean([tf.reduce_mean(reg_loss) for reg_loss in reg_losses]) loss = data_loss + reg_loss else: loss = data_loss to_optimise = tf.trainable_variables() vars_to_freeze = (self.action_param.vars_to_freeze or self. action_param.vars_to_restore) if vars_to_freeze: import re var_regex = re.compile(vars_to_freeze) to_optimise = [v for v in to_optimise if not var_regex.search(v .name)] tf.logging.info( 'Optimizing %d out of %d trainable variables, the other variables fixed (--vars_to_freeze %s)' , len(to_optimise), len(tf.trainable_variables()), vars_to_freeze) grads = self.optimiser.compute_gradients(loss, var_list=to_optimise, colocate_gradients_with_ops=True) gradients_collector.add_to_collection([grads]) outputs_collector.add_to_collection(var=data_loss, name='loss', average_over_devices=False, collection=CONSOLE) outputs_collector.add_to_collection(var=data_loss, name='loss', average_over_devices=True, summary_type='scalar', collection= TF_SUMMARIES) elif self.is_inference: data_dict = switch_sampler(for_training=False) image = tf.cast(data_dict['image'], tf.float32) net_args = {'is_training': self.is_training, 'keep_prob': self. net_param.keep_prob} net_out = self.net(image, **net_args) output_prob = self.segmentation_param.output_prob num_classes = self.segmentation_param.num_classes if output_prob and num_classes > 1: post_process_layer = PostProcessingLayer('SOFTMAX', num_classes =num_classes) elif not output_prob and num_classes > 1: post_process_layer = PostProcessingLayer('ARGMAX', num_classes= num_classes) else: post_process_layer = PostProcessingLayer('IDENTITY', num_classes=num_classes) net_out = post_process_layer(net_out) outputs_collector.add_to_collection(var=net_out, name='window', average_over_devices=False, collection=NETWORK_OUTPUT) outputs_collector.add_to_collection(var=data_dict['image_location'], name='location', average_over_devices=False, collection= NETWORK_OUTPUT) self.initialise_aggregator() 
facenet-master.tmp.visualize.visstd def visstd(a, s=0.1): """Normalize the image range for visualization""" return (a - a.mean()) / max(a.std(), 0.0001) * s + 0.5 
facenet-master.contributed.face.Detection.find|faces def find_faces(self, image): faces = [] bounding_boxes, _ = align.detect_face.detect_face(image, self.minsize, self.pnet, self.rnet, self.onet, self.threshold, self.factor) for bb in bounding_boxes: face = Face() face.container_image = image face.bounding_box = np.zeros(4, dtype=np.int32) img_size = np.asarray(image.shape)[0:2] face.bounding_box[0] = np.maximum(bb[0] - self.face_crop_margin / 2, 0) face.bounding_box[1] = np.maximum(bb[1] - self.face_crop_margin / 2, 0) face.bounding_box[2] = np.minimum(bb[2] + self.face_crop_margin / 2, img_size[1]) face.bounding_box[3] = np.minimum(bb[3] + self.face_crop_margin / 2, img_size[0]) cropped = image[face.bounding_box[1]:face.bounding_box[3], face. bounding_box[0]:face.bounding_box[2], :] face.image = misc.imresize(cropped, (self.face_crop_size, self. face_crop_size), interp='bilinear') faces.append(face) return faces 
gym_pycolab.examples.shockwave.main def main(argv=()): game = make_game(int(argv[1]) if len(argv) > 1 else 0) keys_to_actions = {curses.KEY_UP: 0, curses.KEY_LEFT: 1, curses. KEY_RIGHT: 2, (-1): 3} ui = human_ui.CursesUi(keys_to_actions=keys_to_actions, delay=500, colour_fg=COLOURS) ui.play(game) 
utils.folder|check def check_folder(log_dir): if not os.path.exists(log_dir): os.makedirs(log_dir) return log_dir 
extensions.u_hved.u_hved_net.ConvEncoder.layer_op.clip def clip(input): output = tf.maximum(input, -50) output = tf.minimum(output, 50) return output 
calculation_helper.SecondOrderRandomWalker.get|edge|alias def get_alias_edge(self, src, dst): """ Get the alias edge setup lists for a given edge. """ G = self.G p = self.p q = self.q unnormalized_probs = [] for dst_nbr in sorted(G.neighbors(dst)): if dst_nbr == src: unnormalized_probs.append(G[dst][dst_nbr]['weight'] / p) elif G.has_edge(dst_nbr, src): unnormalized_probs.append(G[dst][dst_nbr]['weight']) else: unnormalized_probs.append(G[dst][dst_nbr]['weight'] / q) norm_const = sum(unnormalized_probs) normalized_probs = [(float(u_prob) / norm_const) for u_prob in unnormalized_probs] return alias_setup(normalized_probs) 
a3c.RunnerThread.run def _run(self): rollout_provider = env_runner(self.env, self.policy, self. num_local_steps, self.summary_writer, self.task) while True: self.queue.put(next(rollout_provider), timeout=600.0) 
texar.modules.decoders.tf_helpers.ScheduledEmbeddingTrainingHelper.initialize def initialize(self, name=None): return super(ScheduledEmbeddingTrainingHelper, self).initialize(name=name) 
utils.nprf_knrm_pair_generator.NPRFKNRMPairGenerator.count_pairs.on|topic|count def count_on_topic(neg_len, pos_len, sample_size): sample_size = min(neg_len, sample_size) if sample_size == 0: return 0 else: return pos_len * sample_size 
UtilsNetwork.call|cluster|Neural|Network def call_NeuralNetwork_cluster(key_word, n_sample, loss_func, folder_path, var_name, lev_c, lev_f, lev_coarsest, string_norm, validation_size, selection_method, scaler, setup, point, cluster='true'): arguments = list() arguments.append(str(key_word)) arguments.append(str(n_sample)) arguments.append(str(loss_func)) for value in setup: arguments.append(str(value)) arguments.append(str(folder_path)) arguments.append(str(var_name)) arguments.append(str(lev_c)) arguments.append(str(lev_f)) arguments.append(str(lev_coarsest)) arguments.append(str(string_norm)) arguments.append(str(validation_size)) arguments.append(str(selection_method)) arguments.append(str(scaler)) arguments.append(str(point)) if sys.platform == 'linux' or sys.platform == 'linux2': if cluster == 'true': string_to_exec = 'bsub python3 NetworkSingleConf_tesr.py ' else: string_to_exec = 'python3 NetworkSingleConf_tesr.py ' for arg in arguments: string_to_exec = string_to_exec + ' ' + arg print(string_to_exec) os.system(string_to_exec) elif sys.platform == 'win32': python = os.environ['PYTHON36'] p = subprocess.Popen([python, 'NetworkSingleConf_tesr.py'] + arguments) p.wait() 
gym_bullet_extensions.bullet_manipulator.BulletManipulator.get|minpos def get_minpos(self): return self.info.joint_minpos 
conceptnet_evaluate.ConceptNetGenerationEvaluator.counter def counter(self, nums): return nums['total_macro'] 
svae_dc.utils.svae_dc_utils.make|from|policy|args def make_policy_from_args(env, controller_class, max_episode_steps): env_name = env.spec.id if env_name.startswith(('Yumi', 'Franka', 'Sawyer')): from gym_bullet_extensions.control.waypts_policy import WaypointsPosPolicy, WaypointsEEPolicy, WaypointsMinJerkPolicy, WaypointsVelPolicy elif env_name.startswith('Daisy'): from gym_daisy_custom.control.gaits import DaisyGait27DPolicy, DaisyGait11DPolicy, DaisyTripod27DPolicy else: print('Please import controller class for your env', env_name) assert False policy_kwargs = {'controller_class': eval(controller_class), 'controller_dim': eval(controller_class).DIM, 't_max': max_episode_steps, 'robot': env.robot} if hasattr(env, 'get_init_pos'): policy_kwargs['get_init_pos_fxn'] = env.get_init_pos policy = StructuredPolicy(**policy_kwargs) return policy 
SRGANs-Spectral-Regularization-GANs--master.evaluations.calc_intra_FID.load|models def load_models(config): gen_conf = config.models['generator'] gen = yaml_utils.load_model(gen_conf['fn'], gen_conf['name'], gen_conf[ 'args']) return gen 
craystack.codecs_test.gaussian|test|db def test_gaussian_db(): bin_precision = 8 coding_precision = 12 batch_size = 5 bin_means = rng.randn() bin_stdds = np.exp(rng.randn() / 10) means = bin_means + rng.randn() / 10 stdds = bin_stdds * np.exp(rng.randn() / 10.0) data = np.array([rng.choice(1 << bin_precision) for _ in range(batch_size)] ) check_codec((batch_size,), cs.DiagGaussian_GaussianBins(means, stdds, bin_means, bin_stdds, coding_precision, bin_precision), data) 
localizationPlot.Plot|localization|List def localizationPlotList(pp_list, yy_list, n_samples=10, start_sample=0, dist_threshold=0.05, color_patch=['w', 'cornflowerblue'], bg_color=None, decimals=3, bias=0, hit_color=['cornflowerblue', 'crimson', np.reshape( [0.4, 0.4, 0.4], [1, 3])]): fig = plt.figure(figsize=(20, 9)) count_statistics = np.zeros([3]) for ii in range(len(pp_list)): if ii < n_samples: ax = plt.subplot(np.ceil(np.sqrt(n_samples)), np.ceil(n_samples / np.ceil(np.sqrt(n_samples))), ii + 1) for jj in range(np.max([np.max(x[:, (1)]) for x in yy_list]).astype (np.int) + 1): list_hits = yy_list[ii][yy_list[ii][:, (1)] == jj, 0] list_hits_pred = pp_list[ii][pp_list[ii][:, (1)] == jj, 0] + bias distance_matrix = np.abs(np.tile(list_hits[(np.newaxis), :], [ len(list_hits_pred), 1]).T - np.tile(list_hits_pred[(np. newaxis), :], [len(list_hits), 1])) while distance_matrix.size > 0 and np.min(distance_matrix ) <= dist_threshold: idx_min = np.unravel_index(distance_matrix.argmin(), distance_matrix.shape) if ii < n_samples: plt.scatter(list_hits[idx_min[0]], jj, s=80, facecolors ='none', edgecolors=hit_color[2]) plt.scatter(list_hits_pred[idx_min[1]], jj, c=hit_color [0], marker='*') list_hits = np.delete(list_hits, idx_min[0]) list_hits_pred = np.delete(list_hits_pred, idx_min[1]) distance_matrix = np.delete(np.delete(distance_matrix, idx_min[0], axis=0), idx_min[1], axis=1) count_statistics[0] += 1 if ii < n_samples: plt.scatter(list_hits, [jj] * len(list_hits), s=80, facecolors='none', edgecolors=hit_color[1]) plt.scatter(list_hits_pred, [jj] * len(list_hits_pred), c= hit_color[1], marker='*') count_statistics[1] += len(list_hits) count_statistics[2] += len(list_hits_pred) if ii < n_samples: r2 = patches.Rectangle((-0.3, jj - 0.5), 1000, 1) collection = PatchCollection([r2], facecolor=color_patch[jj % 2], alpha=0.05) ax.add_collection(collection) if bg_color is not None and bg_color[ii + start_sample] == 1: ax.set_axis_bgcolor([0.9, 0.9, 1]) if ii < n_samples: plt.xlim([-0.3, np.max([np.max(x[:, (0)]) for x in yy_list]) + 2]) plt.yticks(np.arange(0), '') plt.ylim([-0.5, np.max([np.max(x[:, (1)]) for x in yy_list]). astype(np.int) + 1 - 0.5]) ax.spines['right'].set_visible(False) ax.spines['top'].set_visible(False) plt.xlabel('Sample ' + str(ii), fontweight='bold') stats = {} TP = count_statistics[0] FN = count_statistics[1] FP = count_statistics[2] precision = nonZeroDivision(TP, TP + FP) recall = nonZeroDivision(TP, TP + FN) f1 = 2 * nonZeroDivision(precision * recall, precision + recall) plt.text((np.max([np.max(x[:, (0)]) for x in yy_list]) + 2) / 100 * 0.5, -0.3, str(int(np.round(100 * f1, 0))) + '%  ' + str(int(np.round( 100 * precision, 0))) + '%  ' + str(int(np.round(100 * recall, 0))) + '%', fontweight='bold', fontsize=15) stats['f1'] = f1 stats['precision'] = precision stats['recall'] = recall print(np.round(f1, decimals), np.round(precision, decimals), np.round( recall, decimals)) return fig, stats 
RAdam.RAdamOptimizer.sparse|apply|shared def _apply_sparse_shared(self, grad, var, indices, scatter_add): step, beta1_power, beta2_power = self._get_beta_accumulators() beta1_power = math_ops.cast(beta1_power, var.dtype.base_dtype) beta2_power = math_ops.cast(beta2_power, var.dtype.base_dtype) lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype) if self._initial_total_steps > 0: total_steps = math_ops.cast(self._total_steps_t, var.dtype.base_dtype) warmup_proportion = math_ops.cast(self._warmup_proportion_t, var. dtype.base_dtype) min_lr = math_ops.cast(self._min_lr_t, var.dtype.base_dtype) warmup_steps = total_steps * warmup_proportion decay_steps = math_ops.maximum(total_steps - warmup_steps, 1) decay_rate = (min_lr - lr_t) / decay_steps lr_t = tf.where(step <= warmup_steps, lr_t * (step / warmup_steps), lr_t + decay_rate * math_ops.minimum(step - warmup_steps, decay_steps)) beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype) beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype) epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype) sma_inf = 2.0 / (1.0 - beta2_t) - 1.0 sma_t = sma_inf - 2.0 * step * beta2_power / (1.0 - beta2_power) m = self.get_slot(var, 'm') m_scaled_g_values = grad * (1 - beta1_t) m_t = state_ops.assign(m, m * beta1_t, use_locking=self._use_locking) with ops.control_dependencies([m_t]): m_t = scatter_add(m, indices, m_scaled_g_values) m_corr_t = m_t / (1.0 - beta1_power) v = self.get_slot(var, 'v') v_scaled_g_values = grad * grad * (1 - beta2_t) v_t = state_ops.assign(v, v * beta2_t, use_locking=self._use_locking) with ops.control_dependencies([v_t]): v_t = scatter_add(v, indices, v_scaled_g_values) if self._amsgrad: vhat = self.get_slot(var, 'vhat') vhat_t = state_ops.assign(vhat, math_ops.maximum(vhat, v_t), use_locking=self._use_locking) v_corr_t = math_ops.sqrt(vhat_t / (1.0 - beta2_power)) + epsilon_t else: v_corr_t = math_ops.sqrt(v_t / (1.0 - beta2_power)) + epsilon_t r_t = math_ops.sqrt((sma_t - 4.0) / (sma_inf - 4.0) * (sma_t - 2.0) / ( sma_inf - 2.0) * sma_inf / sma_t) var_t = tf.where(sma_t >= 5.0, r_t * m_corr_t / v_corr_t, m_corr_t) if var in self.decay_vars: if self._initial_weight_decay > 0.0: var_t += math_ops.cast(self._weight_decay_t, var.dtype.base_dtype ) * var var_update = state_ops.assign_sub(var, lr_t * var_t, use_locking=self. _use_locking) updates = [var_update, m_t, v_t] if self._amsgrad: updates.append(vhat_t) return control_flow_ops.group(*updates) 
pytorch_pretrained_bert.convert_transfo_xl_checkpoint_to_pytorch.checkpoint|xl|convert|to|transfo|pytorch def convert_transfo_xl_checkpoint_to_pytorch(tf_checkpoint_path, transfo_xl_config_file, pytorch_dump_folder_path, transfo_xl_dataset_file): if transfo_xl_dataset_file: with open(transfo_xl_dataset_file, 'rb') as fp: corpus = pickle.load(fp, encoding='latin1') pytorch_vocab_dump_path = pytorch_dump_folder_path + '/' + VOCAB_NAME print('Save vocabulary to {}'.format(pytorch_vocab_dump_path)) corpus_vocab_dict = corpus.vocab.__dict__ torch.save(corpus_vocab_dict, pytorch_vocab_dump_path) corpus_dict_no_vocab = corpus.__dict__ corpus_dict_no_vocab.pop('vocab', None) pytorch_dataset_dump_path = (pytorch_dump_folder_path + '/' + CORPUS_NAME) print('Save dataset to {}'.format(pytorch_dataset_dump_path)) torch.save(corpus_dict_no_vocab, pytorch_dataset_dump_path) if tf_checkpoint_path: config_path = os.path.abspath(transfo_xl_config_file) tf_path = os.path.abspath(tf_checkpoint_path) print('Converting Transformer XL checkpoint from {} with config at {}' .format(tf_path, config_path)) if transfo_xl_config_file == '': config = TransfoXLConfig() else: config = TransfoXLConfig(transfo_xl_config_file) print('Building PyTorch model from configuration: {}'.format(str( config))) model = TransfoXLLMHeadModel(config) model = load_tf_weights_in_transfo_xl(model, config, tf_path) pytorch_weights_dump_path = os.path.join(pytorch_dump_folder_path, WEIGHTS_NAME) pytorch_config_dump_path = os.path.join(pytorch_dump_folder_path, CONFIG_NAME) print('Save PyTorch model to {}'.format(os.path.abspath( pytorch_weights_dump_path))) torch.save(model.state_dict(), pytorch_weights_dump_path) print('Save configuration file to {}'.format(os.path.abspath( pytorch_config_dump_path))) with open(pytorch_config_dump_path, 'w', encoding='utf-8') as f: f.write(config.to_json_string()) 
texar.losses.pg_losses_test.PGLossesTest.sequence|test|loss def _test_sequence_loss(self, loss_fn, actions, logits, advantages, batched, sequence_length): with self.test_session() as sess: loss = loss_fn(actions, logits, advantages, batched=batched, sequence_length=sequence_length) rank = sess.run(tf.rank(loss)) self.assertEqual(rank, 0) loss = loss_fn(actions, logits, advantages, batched=batched, sequence_length=sequence_length, sum_over_timesteps=False) rank = sess.run(tf.rank(loss)) self.assertEqual(rank, 1) self.assertEqual(loss.shape, tf.TensorShape([self._max_time])) loss = loss_fn(actions, logits, advantages, batched=batched, sequence_length=sequence_length, sum_over_timesteps=False, average_across_timesteps=True, average_across_batch=False) rank = sess.run(tf.rank(loss)) if batched: self.assertEqual(rank, 1) self.assertEqual(loss.shape, tf.TensorShape([self._batch_size])) else: self.assertEqual(rank, 0) loss = loss_fn(actions, logits, advantages, batched=batched, sequence_length=sequence_length, sum_over_timesteps=False, average_across_batch=False) rank = sess.run(tf.rank(loss)) if batched: self.assertEqual(rank, 2) self.assertEqual(loss.shape, tf.TensorShape([self._batch_size, self._max_time])) else: self.assertEqual(rank, 1) self.assertEqual(loss.shape, tf.TensorShape([self._max_time])) sequence_length_time = tf.random_uniform([self._max_time], maxval= self._max_time, dtype=tf.int32) loss = loss_fn(actions, logits, advantages, batched=batched, sequence_length=sequence_length_time, sum_over_timesteps=False, average_across_batch=False, time_major=True) if batched: self.assertEqual(loss.shape, tf.TensorShape([self._batch_size, self._max_time])) else: self.assertEqual(loss.shape, tf.TensorShape([self._max_time])) 
utils.get|d|bbox def get_3d_bbox(scale, shift=0): """ Input: scale: [3] or scalar shift: [3] or scalar Return bbox_3d: [3, N]  """ if hasattr(scale, '__iter__'): bbox_3d = np.array([[scale[0] / 2, +scale[1] / 2, scale[2] / 2], [ scale[0] / 2, +scale[1] / 2, -scale[2] / 2], [-scale[0] / 2, + scale[1] / 2, scale[2] / 2], [-scale[0] / 2, +scale[1] / 2, - scale[2] / 2], [+scale[0] / 2, -scale[1] / 2, scale[2] / 2], [+ scale[0] / 2, -scale[1] / 2, -scale[2] / 2], [-scale[0] / 2, - scale[1] / 2, scale[2] / 2], [-scale[0] / 2, -scale[1] / 2, - scale[2] / 2]]) + shift else: bbox_3d = np.array([[scale / 2, +scale / 2, scale / 2], [scale / 2, +scale / 2, -scale / 2], [-scale / 2, +scale / 2, scale / 2], [ -scale / 2, +scale / 2, -scale / 2], [+scale / 2, -scale / 2, scale / 2], [+scale / 2, -scale / 2, -scale / 2], [-scale / 2, -scale / 2, scale / 2], [-scale / 2, -scale / 2, -scale / 2]] ) + shift bbox_3d = bbox_3d.transpose() return bbox_3d 
PatchSampler.PatchSampler.thread|sample|patches def sample_patches_thread(self, thread_id, n_reuse_image): cnt_im = 0 pat_cnt = 0 while True: im_tuple = self.im_tuple_queue.get() cnt_im += 1 H = im_tuple['in'].shape[1] W = im_tuple['in'].shape[2] if self.sampling == 'uniform': ii, jj, n_p = sample_indices_uniform(H, W, self.patch_height, self.patch_height, shuf=self.shuffle, n_pat_per_im=self. n_pat_per_im) if n_p != self.n_pat_per_im: print('# patches/image = %d != %d' % (n_p, self.n_pat_per_im)) print('fn = %s' % str(im_tuple['fn'])) import pdb pdb.set_trace() else: ii, jj = sample_indices_random(H, W, self.patch_height, self. patch_height, self.n_pat_per_im) pid = 0 for i, j in zip(ii, jj): in_patch = im_tuple['in'][:, i:i + self.patch_height, j:j + self.patch_height, :] gt_patch = im_tuple['gt'][:, i:i + self.patch_height, j:j + self.patch_height, :] pat_dict = {'in': in_patch, 'gt': gt_patch, 'vr': [], 'nlf0': im_tuple['nlf0'], 'nlf1': im_tuple['nlf1'], 'iso': im_tuple ['iso'], 'cam': im_tuple['cam'], 'fn': im_tuple['fn'], 'metadata': im_tuple['metadata'], 'pid': pid} pid += 1 self.queue.put(pat_dict) pat_cnt += 1 
defense.make_defend_quilt.assign|block def assign_block(ix, iy, tile, synth): posx = tile_skip * ix posy = tile_skip * iy if ix == 0 and iy == 0: synth[posy:posy + TILE_SIZE, posx:posx + TILE_SIZE, :] = tile elif iy == 0: tile_left = tile[:, :TILE_OVERLAP, :] synth_right = synth[:TILE_SIZE, posx:posx + TILE_OVERLAP, :] errors = np.sum(np.square(tile_left - synth_right), axis=2) table = min_error_table(errors, direction='vertical') xoff = np.argmin(table[(0), :]) synth[(posy), posx + xoff:posx + TILE_SIZE] = tile[(0), xoff:] for yoff in range(1, TILE_SIZE): candidates = [(yoff, xoff), (yoff, xoff - 1), (yoff, xoff + 1)] index = min((i for i in candidates if index_exists(table, i)), key=lambda i: table[i]) xoff = index[1] synth[(posy + yoff), posx + xoff:posx + TILE_SIZE] = tile[(yoff ), xoff:] elif ix == 0: tile_up = tile[:TILE_OVERLAP, :, :] synth_bottom = synth[posy:posy + TILE_OVERLAP, :TILE_SIZE, :] errors = np.sum(np.square(tile_up - synth_bottom), axis=2) table = min_error_table(errors, direction='horizontal') yoff = np.argmin(table[:, (0)]) synth[posy + yoff:posy + TILE_SIZE, (posx)] = tile[yoff:, (0)] for xoff in range(1, TILE_SIZE): candidates = [(yoff, xoff), (yoff - 1, xoff), (yoff + 1, xoff)] index = min((i for i in candidates if index_exists(table, i)), key=lambda i: table[i]) yoff = index[0] synth[posy + yoff:posy + TILE_SIZE, (posx + xoff)] = tile[yoff:, (xoff)] else: tile_up = tile[:TILE_OVERLAP, :, :] synth_bottom = synth[posy:posy + TILE_OVERLAP, :TILE_SIZE, :] errors_up = np.sum(np.square(tile_up - synth_bottom), axis=2) table_up = min_error_table(errors_up, direction='horizontal') tile_left = tile[:, :TILE_OVERLAP, :] synth_right = synth[:TILE_SIZE, posx:posx + TILE_OVERLAP, :] errors_left = np.sum(np.square(tile_left - synth_right), axis=2) table_left = min_error_table(errors_left, direction='vertical') glue_index = -1 glue_value = np.inf for i in range(TILE_OVERLAP): e = table_up[i, i] + table_left[i, i] if e < glue_value: glue_value = e glue_index = i xoff = glue_index synth[(posy + glue_index), posx + xoff:posx + TILE_OVERLAP] = tile[( glue_index), xoff:TILE_OVERLAP] for yoff in range(glue_index + 1, TILE_SIZE): candidates = [(yoff, xoff), (yoff, xoff - 1), (yoff, xoff + 1)] index = min((i for i in candidates if index_exists(table_left, i)), key=lambda i: table_left[i]) xoff = index[1] synth[(posy + yoff), posx + xoff:posx + TILE_OVERLAP] = tile[( yoff), xoff:TILE_OVERLAP] yoff = glue_index synth[posy + yoff:posy + TILE_OVERLAP, (posx + glue_index)] = tile[yoff :TILE_OVERLAP, (glue_index)] for xoff in range(glue_index + 1, TILE_SIZE): candidates = [(yoff, xoff), (yoff - 1, xoff), (yoff + 1, xoff)] index = min((i for i in candidates if index_exists(table_up, i) ), key=lambda i: table_up[i]) yoff = index[0] synth[posy + yoff:posy + TILE_OVERLAP, (posx + xoff)] = tile[yoff :TILE_OVERLAP, (xoff)] synth[posy + TILE_OVERLAP:posy + TILE_SIZE, posx + TILE_OVERLAP: posx + TILE_SIZE] = tile[TILE_OVERLAP:, TILE_OVERLAP:] 
deepctr.models.mlr.MLR def MLR(region_feature_dim_dict, base_feature_dim_dict={'sparse': [], 'dense': []}, region_num=4, l2_reg_linear=1e-05, init_std=0.0001, seed= 1024, task='binary', bias_feature_dim_dict={'sparse': [], 'dense': []}): """Instantiates the Mixed Logistic Regression/Piece-wise Linear Model.  :param region_feature_dim_dict: dict,to indicate sparse field and dense field like {'sparse':{'field_1':4,'field_2':3,'field_3':2},'dense':['field_4','field_5']} :param base_feature_dim_dict: dict or None,to indicate sparse field and dense field of base learner.if None, it is same as region_feature_dim_dict :param region_num: integer > 1,indicate the piece number :param l2_reg_linear: float. L2 regularizer strength applied to weight :param init_std: float,to use as the initialize std of embedding vector :param seed: integer ,to use as random seed. :param task: str, ``"binary"`` for  binary logloss or  ``"regression"`` for regression loss :param bias_feature_dim_dict: dict,to indicate sparse field and dense field like {'sparse':{'field_1':4,'field_2':3,'field_3':2},'dense':['field_4','field_5']} :return: A Keras model instance. """ if region_num <= 1: raise ValueError('region_num must > 1') if (not isinstance(region_feature_dim_dict, dict) or 'sparse' not in region_feature_dim_dict or 'dense' not in region_feature_dim_dict): raise ValueError( "feature_dim must be a dict like {'sparse':{'field_1':4,'field_2':3,'field_3':2},'dense':['field_5',]}" ) same_flag = False if base_feature_dim_dict == {'sparse': [], 'dense': []}: base_feature_dim_dict = region_feature_dim_dict same_flag = True for feat in (region_feature_dim_dict['sparse'] + base_feature_dim_dict[ 'sparse'] + bias_feature_dim_dict['sparse']): if feat.hash_flag: raise ValueError( 'Feature Hashing on the fly is no supported in MLR') (region_sparse_input, region_dense_input, base_sparse_input, base_dense_input, bias_sparse_input, bias_dense_input) = (get_input (region_feature_dim_dict, base_feature_dim_dict, bias_feature_dim_dict, same_flag)) region_embeddings, base_embeddings, bias_embedding = get_embedding( region_num, region_feature_dim_dict, base_feature_dim_dict, bias_feature_dim_dict, init_std, seed, l2_reg_linear) if same_flag: base_dense_input_ = region_dense_input base_sparse_input_ = region_sparse_input else: base_dense_input_ = base_dense_input base_sparse_input_ = base_sparse_input region_dense_feature_num = len(region_feature_dim_dict['dense']) region_sparse_feature_num = len(region_feature_dim_dict['sparse']) base_dense_feature_num = len(base_feature_dim_dict['dense']) base_sparse_feature_num = len(base_feature_dim_dict['sparse']) bias_dense_feature_num = len(bias_feature_dim_dict['dense']) bias_sparse_feature_num = len(bias_feature_dim_dict['sparse']) if region_dense_feature_num > 1: region_dense_logits_ = [Dense(1)(Concatenate()(region_dense_input)) for _ in range(region_num)] elif region_dense_feature_num == 1: region_dense_logits_ = [Dense(1)(region_dense_input[0]) for _ in range(region_num)] if base_dense_feature_num > 1: base_dense_logits = [Dense(1)(Concatenate()(base_dense_input_)) for _ in range(region_num)] elif base_dense_feature_num == 1: base_dense_logits = [Dense(1)(base_dense_input_[0]) for _ in range( region_num)] if region_dense_feature_num > 0 and region_sparse_feature_num == 0: region_logits = Concatenate()(region_dense_logits_) elif region_dense_feature_num == 0 and region_sparse_feature_num > 0: region_sparse_logits = [(add([region_embeddings[j][i]( region_sparse_input[i]) for i in range( region_sparse_feature_num)]) if region_sparse_feature_num > 1 else region_embeddings[j][0](region_sparse_input[0])) for j in range (region_num)] region_logits = Concatenate()(region_sparse_logits) else: region_sparse_logits = [add([region_embeddings[j][i]( region_sparse_input[i]) for i in range( region_sparse_feature_num)]) for j in range(region_num)] region_logits = Concatenate()([add([region_sparse_logits[i], region_dense_logits_[i]]) for i in range(region_num)]) if base_dense_feature_num > 0 and base_sparse_feature_num == 0: base_logits = base_dense_logits elif base_dense_feature_num == 0 and base_sparse_feature_num > 0: base_sparse_logits = [(add([base_embeddings[j][i]( base_sparse_input_[i]) for i in range(base_sparse_feature_num)] ) if base_sparse_feature_num > 1 else base_embeddings[j][0]( base_sparse_input_[0])) for j in range(region_num)] base_logits = base_sparse_logits else: base_sparse_logits = [(add([base_embeddings[j][i]( base_sparse_input_[i]) for i in range(base_sparse_feature_num)] ) if base_sparse_feature_num > 1 else base_embeddings[j][0]( base_sparse_input_[0])) for j in range(region_num)] base_logits = [add([base_sparse_logits[i], base_dense_logits[i]]) for i in range(region_num)] region_weights = Activation('softmax')(region_logits) learner_score = Concatenate()([PredictionLayer(task, name='learner' + str(i))(base_logits[i]) for i in range(region_num)]) final_logit = dot([region_weights, learner_score], axes=-1) if bias_dense_feature_num + bias_sparse_feature_num > 0: if bias_dense_feature_num > 1: bias_dense_logits = Dense(1)(Concatenate()(bias_dense_input)) elif bias_dense_feature_num == 1: bias_dense_logits = Dense(1)(bias_dense_input[0]) else: pass if bias_sparse_feature_num > 1: bias_cate_logits = add([bias_embedding[i](bias_sparse_input[i]) for i, feat in enumerate(bias_feature_dim_dict['sparse'])]) elif bias_sparse_feature_num == 1: bias_cate_logits = bias_embedding[0](bias_sparse_input[0]) else: pass if bias_dense_feature_num > 0 and bias_sparse_feature_num > 0: bias_logits = add([bias_dense_logits, bias_cate_logits]) elif bias_dense_feature_num > 0: bias_logits = bias_dense_logits else: bias_logits = bias_cate_logits bias_prob = Activation('sigmoid')(bias_logits) final_logit = dot([final_logit, bias_prob], axes=-1) output = Reshape([1])(final_logit) model = Model(inputs=region_sparse_input + region_dense_input + base_sparse_input + base_dense_input + bias_sparse_input + bias_dense_input, outputs=output) return model 
utils.Dataset.image|ids @property def image_ids(self): return self._image_ids 
seed_rl-master.grpc.python.ops_test.OpsTest.test_wait_for_server2.send|and|create def create_and_send(): client = ops.Client(address) self.assertAllEqual(43, client.foo(42)) 
texar.hyperparams.HParams.getattr def __getattr__(self, name): """Retrieves the value of the hyperparameter. """ if name == '_hparams': return super(HParams, self).__getattribute__('_hparams') if name not in self._hparams: raise AttributeError('Unknown hyperparameter: %s' % name) return self._hparams[name] 
raml_main.file|stdout|and|print def print_stdout_and_file(content, file): print(content) print(content, file=file) 
extensions.multi_modal_application.MultiModalApplication.uniform|sampler|initialise def initialise_uniform_sampler(self): self.sampler = [[UniformSampler(reader=reader, window_sizes=self. data_param, batch_size=self.net_param.batch_size, windows_per_image =self.action_param.sample_per_volume, queue_length=self.net_param. queue_length) for reader in self.readers]] 
elpips.networks.Network.Network def __init__(self, use_net_dropout, net_dropout_keep_prob, dtype=tf.float32): self.features = None self.use_net_dropout = use_net_dropout self.net_dropout_keep_prob = net_dropout_keep_prob self.dtype = dtype 
kaffe.graph.Graph.contains def __contains__(self, key): return key in self.node_lut 
classification.ops.utils.LayerParametersDict.key|canonicalize def _canonicalize_key(self, key): if isinstance(key, (list, tuple)): return tuple(key) return key 
gym_pycolab.engine.Engine.order|z|set def set_z_order(self, z_order): """Set the z-ordering of all `Sprite`s and `Drape`s in this engine.  Specify the complete order in which all `Sprite`s and `Drape`s should have their characters painted onto the game board. This method is available during game set-up only.  Args: z_order: an ordered collection of all of the characters corresponding to all `Sprite`s and `Drape`s registered with this `Engine`.  Raises: RuntimeError: if gameplay has already begun. ValueError: if the set of characters in `z_order` does not match the set of characters corresponding to all `Sprite`s and `Drape`s registered with this `Engine`. """ self._runtime_error_if_called_during_showtime('update_group') if set(z_order) != set(self._sprites_and_drapes.keys()) or len(z_order ) != len(self._sprites_and_drapes): raise ValueError( 'The z_order argument {} to Engine.set_z_order is not a proper permutation of the characters corresponding to Sprites and Drapes in this game, which are {}.' .format(repr(z_order), self._sprites_and_drapes.keys())) new_sprites_and_drapes = collections.OrderedDict() for character in z_order: new_sprites_and_drapes[character] = self._sprites_and_drapes[character] self._sprites_and_drapes = new_sprites_and_drapes 
europilot.screen._LocalImpl.screen|read def read_screen(self): """Reads RGB screen data and returns corresponding 1 dimensional numpy array so that it can be reshaped later.  """ return self._post_process(self._read(self._box)) 
t_sagan_celeba.Attention.call def call(self, inputs): q_in, k_in, v_in = inputs q_shape, k_shape, v_shape = K.shape(q_in), K.shape(k_in), K.shape(v_in) q_in = K.reshape(q_in, (q_shape[0], -1, q_shape[-1])) k_in = K.reshape(k_in, (k_shape[0], -1, k_shape[-1])) v_in = K.reshape(v_in, (v_shape[0], -1, v_shape[-1])) attention = K.batch_dot(q_in, k_in, [2, 2]) attention = K.softmax(attention) output = K.batch_dot(attention, v_in, [2, 1]) output = K.reshape(output, (q_shape[0], q_shape[1], q_shape[2], v_shape [-1])) return output * self.gamma 
xlnet-master.prepro_utils.text|printable def printable_text(text): """Returns text encoded in a way suitable for print or `tf.logging`.""" if six.PY3: if isinstance(text, str): return text elif isinstance(text, bytes): return text.decode('utf-8', 'ignore') else: raise ValueError('Unsupported string type: %s' % type(text)) elif six.PY2: if isinstance(text, str): return text elif isinstance(text, unicode): return text.encode('utf-8') else: raise ValueError('Unsupported string type: %s' % type(text)) else: raise ValueError('Not running on Python2 or Python 3?') 
darkflow.net.yolov2.predict.softmax def _softmax(x): e_x = np.exp(x - np.max(x)) out = e_x / e_x.sum() return out 
td_or_not_td.alg.utils.TimeMeasurement.get def get(self, number=0): """Get time measurement.  Args: number (int): Measurement id. """ return self.time_total[number] 
deepctr.contrib.utils.VecAttGRUCell.size|output @property def output_size(self): return self._num_units 
conceptnet_train.ConceptNetGenerationIteratorTrainer.evaluator|set def set_evaluator(self, opt, model, data_loader): self.evaluator = evaluate.make_evaluator(opt, model, data_loader) 
text_ae.model.reader def reader(encoder_inputs, source_sequence_length, mode, hparams, target_vocab_size): encoder_features = tf.one_hot(encoder_inputs, target_vocab_size) forward_cell_list, backward_cell_list = [], [] for layer in range(hparams.num_layers): with tf.variable_scope('fw_cell_{}'.format(layer)): cell = lstm_cell(hparams.num_units, hparams.dropout, mode) forward_cell_list.append(cell) with tf.variable_scope('bw_cell_{}'.format(layer)): cell = lstm_cell(hparams.num_units, hparams.dropout, mode) backward_cell_list.append(cell) forward_cell = tf.nn.rnn_cell.MultiRNNCell(forward_cell_list) backward_cell = tf.nn.rnn_cell.MultiRNNCell(backward_cell_list) encoder_outputs, encoder_state = tf.nn.bidirectional_dynamic_rnn( forward_cell, backward_cell, encoder_features, sequence_length= source_sequence_length, dtype=tf.float32) encoder_outputs = tf.concat(encoder_outputs, -1) return (encoder_outputs, source_sequence_length), encoder_state 
src.model.seq2seq._extract_argmax_and_embed.loop|function def loop_function(prev, _): if output_projection is not None: prev = nn_ops.xw_plus_b(prev, output_projection[0], output_projection[1]) prev_symbol = math_ops.argmax(prev, 1) emb_prev = embedding_ops.embedding_lookup(embedding, prev_symbol) if not update_embedding: emb_prev = array_ops.stop_gradient(emb_prev) return emb_prev 
nmt.distributed_iterator_utils._make_distributed_pipeline.key|func def key_func(unused_1, unused_2, unused_3, src_len, tgt_len): """Calculate bucket_width by maximum source sequence length.""" if src_max_len: bucket_width = (src_max_len + num_buckets - 1) // num_buckets else: bucket_width = 10 bucket_id = tf.maximum(src_len // bucket_width, tgt_len // bucket_width) return tf.to_int64(tf.minimum(num_buckets, bucket_id)) 
fid_keras.imread def imread(f): x = sp.misc.imread(f, mode='RGB') x = sp.misc.imresize(x, (299, 299)) return x 
data_providers.DataProvider.num|batches|update def _update_num_batches(self): """Updates number of batches to iterate over.""" possible_num_batches = self.inputs.shape[0] // self.batch_size if self.max_num_batches == -1: self.num_batches = possible_num_batches else: self.num_batches = min(self.max_num_batches, possible_num_batches) 
deepMOT-master.utils.box_utils.xyxy|xywh def xywh2xyxy(bbox): """ convert bbox from [x,y,w,h] to [x1, y1, x2, y2] :param bbox: bbox in string [x, y, w, h], list :return: bbox in float [x1, y1, x2, y2], list """ copy.deepcopy(bbox) bbox[0] = float(bbox[0]) bbox[1] = float(bbox[1]) bbox[2] = float(bbox[2]) + bbox[0] bbox[3] = float(bbox[3]) + bbox[1] return bbox 
embeddings.create_embeddings_glove.glove|embeddings|create def create_embeddings_glove(pooling='max', dim=100): print('Pooling: ' + pooling) config = configparser.ConfigParser() config.read('paths.cfg') with open(config['paths']['triple_string_cpnet_json'], 'r', encoding='utf8' ) as f: triple_str_json = json.load(f) print('Loaded ' + str(len(triple_str_json)) + ' triple strings.') glove_embeddings = load_glove_from_npy(config['paths']['glove_vec_npy'], config['paths']['glove_vocab']) print('Loaded glove.', flush=True) concept_embeddings = {} concept_embeddings_cnt = {} rel_embeddings = {} rel_embeddings_cnt = {} for i in tqdm(range(len(triple_str_json))): data = triple_str_json[i] words = data['string'].strip().split(' ') rel = data['rel'] subj_start = data['subj_start'] subj_end = data['subj_end'] obj_start = data['obj_start'] obj_end = data['obj_end'] subj_words = words[subj_start:subj_end] obj_words = words[obj_start:obj_end] subj = ' '.join(subj_words) obj = ' '.join(obj_words) if subj not in concept_embeddings: concept_embeddings[subj] = np.zeros((dim,)) concept_embeddings_cnt[subj] = 0 concept_embeddings_cnt[subj] += 1 if obj not in concept_embeddings: concept_embeddings[obj] = np.zeros((dim,)) concept_embeddings_cnt[obj] = 0 concept_embeddings_cnt[obj] += 1 if rel not in rel_embeddings: rel_embeddings[rel] = np.zeros((dim,)) rel_embeddings_cnt[rel] = 0 rel_embeddings_cnt[rel] += 1 if pooling == 'avg': subj_encoding_sum = sum([glove_embeddings.get(word, np.zeros(( dim,))) for word in subj]) obj_encoding_sum = sum([glove_embeddings.get(word, np.zeros(( dim,))) for word in obj]) if rel in ['relatedto', 'antonym']: rel_encoding_sum = sum([glove_embeddings.get(word, np.zeros ((dim,))) for word in words] ) - subj_encoding_sum - obj_encoding_sum else: rel_encoding_sum = obj_encoding_sum - subj_encoding_sum subj_len = subj_end - subj_start obj_len = obj_end - obj_start subj_encoding = subj_encoding_sum / subj_len obj_encoding = obj_encoding_sum / obj_len rel_encoding = rel_encoding_sum / (len(words) - subj_len - obj_len) concept_embeddings[subj] = subj_encoding concept_embeddings[obj] = obj_encoding rel_embeddings[rel] = weighted_average(rel_embeddings[rel], rel_encoding, rel_embeddings_cnt[rel]) elif pooling == 'max': subj_encoding = np.amax([glove_embeddings.get(word, np.zeros(( dim,))) for word in subj_words], axis=0) obj_encoding = np.amax([glove_embeddings.get(word, np.zeros(( dim,))) for word in obj_words], axis=0) mask_rel = [] for j in range(len(words)): if subj_start <= j < subj_end or obj_start <= j < obj_end: continue mask_rel.append(j) rel_vecs = [glove_embeddings.get(words[i], np.zeros((dim,))) for i in mask_rel] rel_encoding = np.amax(rel_vecs, axis=0) concept_embeddings[subj] = max_pooling(concept_embeddings[subj], subj_encoding) concept_embeddings[obj] = max_pooling(concept_embeddings[obj], obj_encoding) rel_embeddings[rel] = weighted_average(rel_embeddings[rel], rel_encoding, rel_embeddings_cnt[rel]) print(str(len(concept_embeddings)) + ' concept embeddings') print(str(len(rel_embeddings)) + ' relation embeddings') write_embeddings_npy(concept_embeddings, concept_embeddings_cnt, config ['paths']['concept_vec_npy_glove'] + '.' + pooling, config['paths'] ['concept_vocab_glove'] + '.' + pooling + '.txt') write_embeddings_npy(rel_embeddings, rel_embeddings_cnt, config['paths' ]['relation_vec_npy_glove'] + '.' + pooling, config['paths'][ 'relation_vocab_glove'] + '.' + pooling + '.txt') 
seed_rl-master.grpc.python.ops_test.OpsTest.test|failing|function @parameterized.parameters(([], False), ([1], True)) def test_failing_function(self, dim, batched): address = self.get_unix_address() server = ops.Server([address])  @tf.function(input_signature=[tf.TensorSpec(dim, tf.int32)]) def foo(x): tf.assert_equal(1, x) return x server.bind(foo, batched=batched) server.start() client = ops.Client(address) with self.assertRaisesRegexp(tf.errors.InvalidArgumentError, 'assertion failed'): client.foo(42) server.shutdown() 
utils.TPUEncodedUInt8Spec.component|specs @property def _component_specs(self): return self._value_specs 
pc_util.views|cloud|point|three|demo def point_cloud_three_views_demo(): """ Demo for draw_point_cloud function """ points = read_ply('../third_party/mesh_sampling/piano.ply') im_array = point_cloud_three_views(points) img = Image.fromarray(np.uint8(im_array * 255.0)) img.save('piano.jpg') 
neural_tangents.predict.descent|gradient def gradient_descent(g_dd, y_train, loss, g_td=None): """Predicts the outcome of function space gradient descent training on `loss`.  Solves for continuous-time gradient descent using an ODE solver.  Solves the function space ODE for continuous gradient descent with a given loss (detailed in [*]) given a Neural Tangent Kernel over the dataset. This function returns a function that predicts the time evolution for function space points at arbitrary times. Note that times are continuous and are measured in units of the learning rate so that t = learning_rate * steps.  This function uses the scipy ode solver with the 'dopri5' algorithm.  [*] https://arxiv.org/abs/1902.06720  Example: ```python >>> from jax.experimental import stax >>> from neural_tangents import predict >>> >>> train_time = 1e-7 >>> kernel_fn = empirical(f) >>> g_td = kernel_fn(x_test, x_train, params) >>> >>> from jax.experimental import stax >>> cross_entropy = lambda fx, y_hat: -np.mean(stax.logsoftmax(fx) * y_hat) >>> predict_fn = predict.gradient_descent( >>>     g_dd, y_train, cross_entropy, g_td) >>> >>> fx_train_initial = f(params, x_train) >>> fx_test_initial = f(params, x_test) >>> >>> fx_train_final, fx_test_final = predict_fn( >>>     fx_train_initial, fx_test_initial, train_time) ``` Args: g_dd: A Kernel on the training data. The kernel should be an `np.ndarray` of shape [n_train * output_dim, n_train * output_dim] or [n_train, n_train]. In the latter case it is assumed that the kernel is block diagonal over the logits. y_train: A `np.ndarray` of shape [n_train, output_dim] of labels for the training data. loss: A loss function whose signature is loss(fx, y_hat) where fx is an `np.ndarray` of function space output_dim of the network and y_hat are targets. Note: the loss function should treat the batch and output dimensions symmetrically. g_td: A Kernel relating training data with test data. The kernel should be an `np.ndarray` of shape [n_test * output_dim, n_train * output_dim] or [n_test, n_train]. Note: g_td should have been created in the convention kernel_fn(x_test, x_train, params).  Returns: A function that predicts outputs after t = learning_rate * steps of training.  If g_td is None: The function returned is predict(fx, t). Here fx is an `np.ndarray` of network outputs and has shape [n_train, output_dim], t is a floating point time. predict(fx, t) returns an `np.ndarray` of predictions of shape [n_train, output_dim].  If g_td is not None: If a test set Kernel is specified then it returns a function, predict(fx_train, fx_test, t). Here fx_train and fx_test are ndarays of network outputs and have shape [n_train, output_dim] and [n_test, output_dim] respectively and t is a floating point time. predict(fx_train, fx_test, t) returns a tuple of predictions of shape [n_train, output_dim] and [n_test, output_dim] for train and test points respectively. """ output_dimension = y_train.shape[-1] g_dd = empirical.flatten_features(g_dd)  def fl(fx): """Flatten outputs.""" return np.reshape(fx, (-1,))  def ufl(fx): """Unflatten outputs.""" return np.reshape(fx, (-1, output_dimension)) ifl = lambda x: x iufl = lambda x: x if y_train.size > g_dd.shape[-1]: out_dim, ragged = divmod(y_train.size, g_dd.shape[-1]) if ragged or out_dim != y_train.shape[-1]: raise ValueError() ifl = fl iufl = ufl y_train = np.reshape(y_train, -1) grad_loss = grad(functools.partial(loss, y_hat=y_train)) if g_td is None: dfx_dt = lambda unused_t, fx: -ifl(np.dot(g_dd, iufl(grad_loss(fx))))  def predict(dt, fx=0.0): r = ode(dfx_dt).set_integrator('dopri5') r.set_initial_value(fl(fx), 0) r.integrate(dt) return ufl(r.y) else: g_td = empirical.flatten_features(g_td)  def dfx_dt(unused_t, fx, train_size): fx_train = fx[:train_size] dfx_train = -ifl(np.dot(g_dd, iufl(grad_loss(fx_train)))) dfx_test = -ifl(np.dot(g_td, iufl(grad_loss(fx_train)))) return np.concatenate((dfx_train, dfx_test), axis=0)  def predict(dt, fx_train=0.0, fx_test=0.0): r = ode(dfx_dt).set_integrator('dopri5') fx = fl(np.concatenate((fx_train, fx_test), axis=0)) train_size, output_dim = fx_train.shape r.set_initial_value(fx, 0).set_f_params(train_size * output_dim) r.integrate(dt) fx = ufl(r.y) return fx[:train_size], fx[train_size:] return predict 
nmt.decoder.outputs|zero|create def _create_zero_outputs(size, dtype, batch_size): """Create a zero outputs Tensor structure."""  def _create(s, d): return _zero_state_tensors(s, batch_size, d) return tf.contrib.framework.nest.map_structure(_create, size, dtype) 
vae.VAE.encoder def encoder(self, x): net = self.arch['encoder'][self.feat_type] return self._encoder(x, net) 
texar.agents.seq_pg_agent.SeqPGAgent.setup|partial|run def _setup_partial_run(self, fetches=None, feeds=None): fetches_ = [self._samples, self._sequence_length, self._pg_loss, self. _train_op] if fetches is not None: for fet in fetches: if fet not in fetches_: fetches_.append(fet) feeds = self._get_partial_run_feeds(feeds) self._partial_run_handle = self._sess.partial_run_setup(fetches_, feeds =feeds) self._qvalue_inputs_fed = False 
acgan_mnist.Discriminator def Discriminator(inputs): output = tf.reshape(inputs, [-1, 1, 28, 28]) output = lib.ops.conv2d.Conv2D('Discriminator.1', 1, DIM, 5, output, stride=2) output = LeakyReLU(output) output = lib.ops.conv2d.Conv2D('Discriminator.2', DIM, 2 * DIM, 5, output, stride=2) output = LeakyReLU(output) output = lib.ops.conv2d.Conv2D('Discriminator.3', 2 * DIM, 4 * DIM, 5, output, stride=2) output = LeakyReLU(output) output = tf.reshape(output, [-1, 4 * 4 * 4 * DIM]) preds = lib.ops.linear.Linear('Discriminator.Output', 4 * 4 * 4 * DIM, 10, output) output = lib.ops.linear.Linear('Discriminator.Output', 4 * 4 * 4 * DIM, 1, output) return tf.reshape(output, [-1]), preds 
src.get_paf.get|paf def get_paf(keypoints, ori_height, ori_width, paf_height, paf_width, paf_channels, paf_width_thre): """ function that create paf based keypoints :param keypoints: ndarray with shape [person_num, joints_num, 3], each joint contains three attribute, [x, y, v] :param ori_height: ori_img height :param ori_width:  ori_img width :param paf_height: paf_height :param paf_width:  paf_width :param paf_channels: how many paf_channels will return. the number of paf_channels is 2 * connect_num, which connect_num is edges num of points. :param paf_width_thre: the threshold that controls the area about paf connection. :return: A ndarray with shape [paf_height, paf_width, paf_channels]. """ factorx = paf_width / ori_width factory = paf_height / ori_height pt1 = [0, 1, 3, 4, 6, 7, 9, 10, 12, 13, 13, 13, 13] pt2 = [1, 2, 4, 5, 7, 8, 10, 11, 13, 0, 3, 6, 9] pafs = np.zeros((paf_channels, paf_height, paf_width), dtype=np.float32) for i in range(len(pt1)): count = np.zeros((paf_height, paf_width)) for j in range(keypoints.shape[0]): val = keypoints[j] if val[pt1[i], 0] == 0 and val[pt1[i], 1] == 0 or val[pt2[i], 0 ] == 0 and val[pt2[i], 1] == 0 or val[pt1[i], 2] == 3 or val[ pt2[i], 2] == 3: continue center_x = val[pt1[i], 0] * factorx center_y = val[pt1[i], 1] * factory centerA = np.asarray([center_x, center_y]) center_x = val[pt2[i], 0] * factorx center_y = val[pt2[i], 1] * factory centerB = np.asarray([center_x, center_y]) paf_map, mask = create_paf_map(centerA, centerB, paf_width, paf_height, paf_width_thre) pafs[2 * i:2 * i + 2, :, :] += paf_map count[mask == True] += 1 mask = count == 0 count[mask == True] = 1 pafs[2 * i:2 * i + 2, :, :] = np.divide(pafs[2 * i:2 * i + 2, :, :], count[(np.newaxis), :, :]) pafs = np.transpose(pafs, (1, 2, 0)) return pafs 
base_model.Base.gloss|calc def calc_gloss(self, x, y, ux, vy, config): raise NotImplementedError('Please Implement this method') 
pc_util.volume|to|cloud|point def volume_to_point_cloud(vol): """ vol is occupancy grid (value = 0 or 1) of size vsize*vsize*vsize return Nx3 numpy array. """ vsize = vol.shape[0] assert vol.shape[1] == vsize and vol.shape[1] == vsize points = [] for a in range(vsize): for b in range(vsize): for c in range(vsize): if vol[a, b, c] == 1: points.append(np.array([a, b, c])) if len(points) == 0: return np.zeros((0, 3)) points = np.vstack(points) return points 
evaluate_scannet_withoverlap.parse|block|scene def parse_block_scene(datapath, scene_names): f = open(os.path.join(datapath, 'log_block.txt'), 'r') blocklist = f.read().splitlines() block2scene = [] ordered_testlist = [] for line in blocklist: str = line.split(', ') if str[0] in scene_names: block2scene.append(tuple(str)) tfrecord_name = os.path.join(datapath, '%s.tfrecord' % str[0]) if not tfrecord_name in ordered_testlist: ordered_testlist.append(tfrecord_name) return block2scene, ordered_testlist 
lottery.prune_functions.x|all|global|to|prune def prune_all_to_global_x(names, weights, masks, x): if len(masks) == 0: return masks concat = np.zeros(sum(np.prod(w.shape) for w in weights), dtype=weights [0].dtype) i = 0 for name, weight, mask in zip(names, weights, masks): d = np.prod(weight.shape) np.abs(np.ravel(weight), out=concat[i:i + d]) concat[i:i + d] *= np.ravel(mask) i += d threshold = np.percentile(concat, 100 - x) for name, weight, mask in zip(names, weights, masks): old_frac = mask.sum() / np.prod(mask.shape) mask[np.abs(weight) < threshold] = 0 print('{}: {:.3f}->{:.3f}'.format(name, old_frac, mask.sum() / np. prod(mask.shape)), flush=True) return masks 
pvae.tflib.imagenet32.make|generator def make_generator(path, n_files, batch_size, random_state): epoch_count = [1]  def get_epoch(): images = np.zeros((batch_size, 3, 32, 32), dtype='int32') files = list(range(n_files)) random_state.shuffle(files) epoch_count[0] += 1 for n, i in enumerate(files): image = scipy.misc.imread('{}/{}.png'.format(path, str(i + 1). zfill(len(str(n_files))))) images[n % batch_size] = image.transpose(2, 0, 1) if n > 0 and n % batch_size == 0: yield images, return get_epoch 
classification.ops.fisher_factors.ConvOutputKroneckerFactor.dtype @property def _dtype(self): return self._outputs_grads[0].dtype 
avod.core.box_3d_encoder_test.Box3dEncoderTest.to|anchor|d|test|box def test_box_3d_to_anchor_180_270(self): box_3d = np.asarray([[1, 2, 3, 4, 5, 6, np.pi], [1, 2, 3, 4, 5, 6, 3 * np.pi / 2]], dtype=np.float64) expected_anchors = np.asarray([[1, 2, 3, 4, 6, 5], [1, 2, 3, 5, 6, 4]], dtype=np.float64) anchors = box_3d_encoder.box_3d_to_anchor(box_3d) np.testing.assert_allclose(anchors, expected_anchors) 
classification.ops.loss_functions.CategoricalLogitsNegativeLogProbLoss.fisher|multiply def multiply_fisher(self, vector): probs = self._probs return vector * probs - probs * math_ops.reduce_sum(vector * probs, axis=1, keep_dims=True) 
translate.multitask_model.MultiTaskModel.align def align(self, *args, **kwargs): self.main_model.align(*args, **kwargs) 
regression.controller.sample.OutSample.task @property def task(self): return self._task 
train_app.eval|loop def eval_loop(preprocess_fn, network_factory, data_x, data_y, camera_indices, log_dir, eval_log_dir, image_shape=None, run_id=None, loss_mode='cosine-softmax', num_galleries=10, random_seed=4321): """Evaluate a running training session using CMC metric averaged over `num_galleries` galleries where each gallery contains for every identity a randomly selected image-pair.  A call to this function will block indefinitely, monitoring the `log_dir/run_id` for saved checkpoints. Then, creates summaries in `eval_log_dir/run_id` that can be monitored with tensorboard.  Parameters ---------- preprocess_fn : Callable[tf.Tensor] -> tf.Tensor A callable that applies preprocessing to a given input image tensor of dtype tf.uint8 and returns a floating point representation (tf.float32). network_factory : Callable[tf.Tensor] -> (tf.Tensor, tf.Tensor) A callable that takes as argument a preprocessed input image of dtype tf.float32 and returns the feature representation as well as a logits tensors. The logits may be set to None if not required by the loss. data_x : List[str] | np.ndarray A list of image filenames or a tensor of images. data_y : List[int] | np.ndarray A list or one-dimensional array of labels for the images in `data_x`. camera_indices: Optional[List[int] | np.ndarray] A list or one-dimensional array of camera indices for the images in `data_x`. If not None, CMC galleries are created such that image pairs are collected from different cameras. log_dir: str Should be equivalent to the `log_dir` passed into `train_loop` of the training run to monitor. eval_log_dir: Used to construct the tensorboard log directory where metrics are summarized. image_shape : Tuple[int, int, int] | NoneType Image shape (height, width, channels) or None. If None, `train_x` must be an array of images such that the shape can be queries from this variable. run_id : str A string that identifies the training run; must be set to the same `run_id` passed into `train_loop`. loss_mode : Optional[str] A string that identifies the loss function used for training; must be one of 'cosine-softmax', 'magnet', 'triplet'. This value defaults to 'cosine-softmax'. num_galleries: int The number of galleries to be constructed for evaluation of CMC metrics. random_seed: Optional[int] If not None, the NumPy random seed is fixed to this number; can be used to produce the same galleries over multiple runs.  """ if image_shape is None: assert type(data_x) == np.ndarray image_shape = data_x.shape[1:] elif type(data_x) == np.ndarray: assert data_x.shape[1:] == image_shape read_from_file = type(data_x) != np.ndarray probes, galleries = [], [] for i in range(num_galleries): probe_indices, gallery_indices = util.create_cmc_probe_and_gallery( data_y, camera_indices, seed=random_seed + i) probes.append(probe_indices) galleries.append(gallery_indices) probes, galleries = np.asarray(probes), np.asarray(galleries) with tf.device('/cpu:0'): num_probes, num_gallery_images = probes.shape[1], galleries.shape[1] probe_idx_var = tf.placeholder(tf.int64, (None, num_probes)) gallery_idx_var = tf.placeholder(tf.int64, (None, num_gallery_images)) trainer = queued_trainer.QueuedTrainer([probe_idx_var, gallery_idx_var] ) data_x_var = tf.constant(data_x) data_y_var = tf.constant(data_y) probe_idx_var, gallery_idx_var = trainer.get_input_vars(batch_size=1) probe_idx_var = tf.squeeze(probe_idx_var) gallery_idx_var = tf.squeeze(gallery_idx_var) probe_x_var = tf.gather(data_x_var, probe_idx_var) if read_from_file: num_channels = image_shape[-1] if len(image_shape) == 3 else 1 probe_x_var = tf.map_fn(lambda x: tf.image.decode_jpeg(tf. read_file(x), channels=num_channels), probe_x_var, dtype=tf .uint8) probe_x_var = tf.image.resize_images(probe_x_var, image_shape[:2]) probe_x_var = tf.map_fn(lambda x: preprocess_fn(x, is_training= False), probe_x_var, back_prop=False, dtype=tf.float32) probe_y_var = tf.gather(data_y_var, probe_idx_var) gallery_x_var = tf.gather(data_x_var, gallery_idx_var) if read_from_file: num_channels = image_shape[-1] if len(image_shape) == 3 else 1 gallery_x_var = tf.map_fn(lambda x: tf.image.decode_jpeg(tf. read_file(x), channels=num_channels), gallery_x_var, dtype= tf.uint8) gallery_x_var = tf.image.resize_images(gallery_x_var, image_shape[:2]) gallery_x_var = tf.map_fn(lambda x: preprocess_fn(x, is_training= False), gallery_x_var, back_prop=False, dtype=tf.float32) gallery_y_var = tf.gather(data_y_var, gallery_idx_var) probe_and_gallery_x_var = tf.concat(axis=0, values=[probe_x_var, gallery_x_var]) probe_and_gallery_x_var, _ = network_factory(probe_and_gallery_x_var) num_probe = tf.shape(probe_x_var)[0] probe_x_var = tf.slice(probe_and_gallery_x_var, [0, 0], [num_probe, -1]) gallery_x_var = tf.slice(probe_and_gallery_x_var, [num_probe, 0], [-1, -1]) distance_measure = (metrics.cosine_distance if loss_mode == 'cosine-softmax' else metrics.pdist)  def cmc_metric_at_k(k): return metrics.streaming_mean_cmc_at_k(probe_x_var, probe_y_var, gallery_x_var, gallery_y_var, k=k, measure=distance_measure) names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({ ('Precision@%d' % k): cmc_metric_at_k(k) for k in [1, 5, 10, 20]}) for metric_name, metric_value in names_to_values.items(): tf.summary.scalar(metric_name, metric_value) trainer.evaluate((probes, galleries), log_dir, eval_log_dir, run_id= run_id, eval_op=list(names_to_updates.values()), eval_interval_secs=60) 
EndToEndClassification.EnvClassification.Models.PiczakCNN.Piczak.Piczak def __init__(self, model_name, num_classes=50, weights_initializer= initializers.xavier_initializer(), biases_initializer=init_ops. zeros_initializer(), weights_regularizer=None, biases_regularizer=None): """ Initializes the PiczakCNN model class.  Args: model_name (str): model name. num_classes (int): number of the classes (i.e. size of the output layer of the classifier). weights_initializer (func): how to initialize the weights of all layers. biases_initializer (func): how to initialize the biases of all layers. weights_regularizer (func): regularization of the weights of all layers. biases_regularizer (func): regularization of the biases of all layers. """ self.model_name = model_name self.num_classes = num_classes self.W_init = weights_initializer self.b_init = biases_initializer self.W_reg = weights_regularizer self.b_reg = biases_regularizer 
texar.core.layers.MergeLayer.weights|collect def _collect_weights(self): """Collects (non-)trainable weights of each of the parallel layers. """ if self._layers is None: pass for layer in self._layers: if self.trainable: add_variable(layer._trainable_weights, self._trainable_weights) else: add_variable(layer._trainable_weights, self._non_trainable_weights) add_variable(layer._non_trainable_weights, self._non_trainable_weights) 
darkflow.net.build.TFNet.build|forward def build_forward(self): verbalise = self.FLAGS.verbalise inp_size = [None] + self.meta['inp_size'] self.inp = tf.placeholder(tf.float32, inp_size, 'input') self.feed = dict() state = identity(self.inp) roof = self.num_layer - self.ntrain self.say(HEADER, LINE) for i, layer in enumerate(self.darknet.layers): scope = '{}-{}'.format(str(i), layer.type) args = [layer, state, i, roof, self.feed] state = op_create(*args) mess = state.verbalise() self.say(mess) self.say(LINE) self.top = state self.out = tf.identity(state.out, name='output') 
neural_tangents.utils.utils.pmap|out|stub def stub_out_pmap(batch, count): if count > 0:   class xla_bridge_stub(object):  def device_count(self): return count platform = xla_bridge.get_backend().platform if platform == 'gpu' or platform == 'cpu': batch.pmap = _jit_vmap batch.xla_bridge = xla_bridge_stub() 
indoor3d_util.sample|data def sample_data(data, num_sample): """ data is in N x ... we want to keep num_samplexC of them. if N > num_sample, we will randomly keep num_sample of them. if N < num_sample, we will randomly duplicate samples. """ N = data.shape[0] if N == num_sample: return data, list(range(N)) elif N > num_sample: sample = np.random.choice(N, num_sample) return data[sample, ...], sample else: sample = np.random.choice(N, num_sample - N) dup_data = data[sample, ...] return np.concatenate([data, dup_data], 0), list(range(N)) + list( sample) 
SRGANs-Spectral-Regularization-GANs--master.evaluation.load|model|inception def load_inception_model(path=None): path = (path if path is not None else '%s/inception/inception_score.model' % os.path.dirname(__file__)) model = Inception() serializers.load_hdf5(path, model) model.to_gpu() return model 
plato.agent.component.dialogue_policy.reinforcement_learning.wolf_phc_policy.WoLFPHCPolicy.save def save(self, path=None): """ Saves the dialogue_policy model to the path provided  :param path: path to save the model to :return: """ if not self.is_training: return if not path: path = 'models/policies/wolf_phc_policy.pkl' print('No dialogue_policy file name provided. Using default: {0}'. format(path)) if not os.path.exists(os.path.dirname(path)): os.makedirs(os.path.dirname(path), exist_ok=True) obj = {'Q': self.Q, 'pi': self.pi, 'mean_pi': self.mean_pi, 'state_counter': self.state_counter, 'a': self.alpha, 'e': self. epsilon, 'g': self.gamma} with open(path, 'wb') as file: pickle.dump(obj, file, pickle.HIGHEST_PROTOCOL) if self.statistics['total_turns'] > 0: print('DEBUG > {0} WoLF PHC dialogue_policy supervision ratio: {1}' .format(self.agent_role, float(self.statistics[ 'supervised_turns'] / self.statistics['total_turns']))) print( f'DEBUG > {self.agent_role} WoLF PHC policy state space size: {len(self.pi)}' ) 
classification.ops.fisher_blocks.EigenCorrectedConvKFCBasicFB.Conv|FB|Corrected|Basic|Eigen|KFC def __init__(self, layer_collection, params, strides, padding): """Creates a EigenCorrectedConvKFCBasicFB block. Args: layer_collection: The collection of all layers in the K-FAC approximate Fisher information matrix to which this FisherBlock belongs. params: The parameters (Tensor or tuple of Tensors) of this layer. If kernel alone, a Tensor of shape [kernel_height, kernel_width, in_channels, out_channels]. If kernel and bias, a tuple of 2 elements containing the previous and a Tensor of shape [out_channels]. strides: The stride size in this layer (1-D Tensor of length 4). padding: The padding in this layer (1-D of Tensor length 4). """ self._inputs = [] self._outputs = [] self._strides = tuple(strides) if isinstance(strides, list) else strides self._padding = padding self._has_bias = isinstance(params, (tuple, list)) fltr = params[0] if self._has_bias else params self._filter_shape = tuple(fltr.shape.as_list()) super(EigenCorrectedConvKFCBasicFB, self).__init__(layer_collection) 
make_tfrecord_shapenet_onehot.tfrecord|make|seg def make_tfrecord_seg(dataDir, filelist, store_folder='', chunksize=1024, verbose=True, debug=True): phase = filelist.split('_')[1] dataset = [line.rstrip() for line in open(os.path.join(dataDir, 'train_test_split', filelist + '.json'))] dataset = json.loads(dataset[0]) class_names = [line.rstrip().split('\t')[0] for line in open(os.path. join(dataDir, 'synsetoffset2category.txt'))] class_folders = [line.rstrip().split('\t')[1] for line in open(os.path. join(dataDir, 'synsetoffset2category.txt'))] print('number of samples: %d, number of classes: %d' % (len(dataset), len(class_names))) if not store_folder == '' and not os.path.exists(store_folder): os.mkdir(store_folder) if debug: from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt dataset = dataset[0:10] class_size = np.zeros((len(class_names),), np.int32) for i, filepath in enumerate(dataset): _, folder, filename = filepath.split('/') filepath = os.path.join(dataDir, folder, filename + '.txt') data = np.loadtxt(filepath, dtype=np.float32, delimiter=',') assert data.shape[1] == 5 xyz = data[:, 0:3] xyz = xyz[:, ([0, 2, 1])] seg_label = np.int32(data[:, (-1)]) - 1 cls_label = class_folders.index(folder) print(cls_label) class_size[cls_label] += 1 if debug: print(class_names) print(class_folders) print(folder) print(filepath, cls_label) print('original data size:') print(data.shape, xyz.shape) print('mean and scale info before processing') print(np.mean(xyz, axis=0), np.sqrt(np.amax(np.sum(np.square( xyz), axis=1)))) xyz = xyz - np.mean(xyz, axis=0) scale = np.sqrt(np.amax(np.sum(np.square(xyz), axis=1))) xyz /= scale if debug: print('resampled data size:') print(xyz.shape) print('mean and scale info after processing') print(np.mean(xyz, axis=0), np.sqrt(np.amax(np.sum(np.square( xyz), axis=1)))) plt.figure(0) ax = plt.axes(projection='3d') ax.scatter(xyz[:, (0)], xyz[:, (1)], xyz[:, (2)], c='green') plt.suptitle(class_names[cls_label]) ax.set_xlabel('X') ax.set_ylabel('Y') ax.set_zlabel('Z') ax.set_xlim(-1, 1) ax.set_ylim(-1, 1) ax.set_zlim(-1, 1) plt.show() else: filename = os.path.join(store_folder, 'data_%s%d.tfrecord' % ( phase, i // chunksize)) if not os.path.exists(filename): if i > 0: print(i) writer.close() writer = tf.io.TFRecordWriter(filename) if verbose: print('start to make data_%s%d.tfrecords of the %sset:' % (phase, i // chunksize, phase)) xyz_raw = xyz.tostring() seg_label_raw = seg_label.tostring() example = tf.train.Example(features=tf.train.Features(feature={ 'seg_label': _bytes_feature(seg_label_raw), 'cls_label': _int64_feature(cls_label), 'xyz_raw': _bytes_feature(xyz_raw)}) ) writer.write(example.SerializeToString()) return class_size, i 
gym_pycolab.envs.pycolab_grid_worlds_env.PycolabGridWorldsEnv.game|make def _make_game(self): self._setup() return grid_worlds.make_game(raw_level=None, terminal_reward=0.5, bonus_reward=0.5, per_step_cost=0.0, positive_rewards=0, negative_rewards=0, swap_rewards=False, confined_to_board=False, off_board_cost=0.5, wall_is_terminal=False, wall_cost=0.0, swap_actions=False, doors=False, round_world=False) 
prep_data.vocabulary|create def create_vocabulary(vocabulary_path, data, max_vocabulary_size, tokenizer =None, normalize_digits=False): if not gfile.Exists(vocabulary_path): print('Creating vocabulary %s from data' % vocabulary_path) print(len(data)) vocab = {} counter = 0 num = 0 for line in data: counter += 1 if counter % 100000 == 0: print('    processing line %d' % counter) tokens = tokenizer(line) if tokenizer else basic_tokenizer(line) for w in tokens: num += 1 word = _DIGIT_RE.sub('0', w) if normalize_digits else w if word in vocab: vocab[word] += 1 else: vocab[word] = 1 vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True) print('total words:', num, 'unique words:', len(vocab_list)) if len(vocab_list) > max_vocabulary_size: vocab_list = vocab_list[:max_vocabulary_size] overlap = 0.0 for key in vocab_list[len(_START_VOCAB):]: overlap += vocab[key] print('overlap %f' % (overlap / num)) print('vocab size:', len(vocab_list)) with gfile.GFile(vocabulary_path, mode='wb') as vocab_file: for w in vocab_list: vocab_file.write(w + '\n') 
official.resnet.layer_test.BaseTest.batch|test|norm def test_batch_norm(self): self._batch_norm_ops(test=True) 
SPH3D_modelnet.get|model def get_model(points, is_training, config=None): """ Classification Network, input is BxNx3, output Bx40 """ batch_size = points.get_shape()[0].value num_point = points.get_shape()[1].value end_points = {} assert num_point == config.num_input if config.normalize: points = normalize_xyz(points) xyz = points query = tf.reduce_mean(xyz, axis=1, keepdims=True) reuse = None net = s3g_util.pointwise_conv3d(xyz, config.mlp, 'mlp1', weight_decay= config.weight_decay, with_bn=config.with_bn, with_bias=config. with_bias, reuse=reuse, is_training=is_training) global_feat = [] for l in range(len(config.radius)): if config.use_raw: net = tf.concat([net, xyz], axis=-1) intra_idx, intra_cnt, intra_dst, indices = s3g_util.build_graph(xyz, config.radius[l], config.nn_uplimit[l], config.num_sample[l], sample_method=config.sample) filt_idx = s3g_util.spherical_kernel(xyz, xyz, intra_idx, intra_cnt, intra_dst, config.radius[l], kernel=config.kernel) net = _separable_conv3d_block(net, config.channels[l], config. binSize, intra_idx, intra_cnt, filt_idx, 'conv' + str(l + 1), config.multiplier[l], reuse=reuse, weight_decay=config. weight_decay, with_bn=config.with_bn, with_bias=config. with_bias, is_training=is_training) if config.num_sample[l] > 1: xyz = tf.gather_nd(xyz, indices) inter_idx = tf.gather_nd(intra_idx, indices) inter_cnt = tf.gather_nd(intra_cnt, indices) inter_dst = tf.gather_nd(intra_dst, indices) net = s3g_util.pool3d(net, inter_idx, inter_cnt, method=config. pool_method, scope='pool' + str(l + 1)) global_maxpool = tf.reduce_max(net, axis=1, keepdims=True) global_feat.append(global_maxpool) global_radius = 100.0 nn_idx, nn_cnt, nn_dst = s3g_util.build_global_graph(xyz, query, global_radius) filt_idx = s3g_util.spherical_kernel(xyz, query, nn_idx, nn_cnt, nn_dst, global_radius, kernel=[8, 2, 1]) net = s3g_util.separable_conv3d(net, config.global_channels, 17, config .global_multiplier, 'global_conv', nn_idx, nn_cnt, filt_idx, reuse= reuse, weight_decay=config.weight_decay, with_bn=config.with_bn, with_bias=config.with_bias, is_training=is_training) global_feat.append(net) net = tf.concat(global_feat, axis=2) net = tf.reshape(net, [batch_size, -1]) net = s3g_util.fully_connected(net, 512, scope='fc1', weight_decay= config.weight_decay, with_bn=config.with_bn, with_bias=config. with_bias, is_training=is_training) net = tf.layers.dropout(net, 0.5, training=is_training, name='fc1_dp') net = s3g_util.fully_connected(net, 256, scope='fc2', weight_decay= config.weight_decay, with_bn=config.with_bn, with_bias=config. with_bias, is_training=is_training) net = tf.layers.dropout(net, 0.5, training=is_training, name='fc2_dp') net = s3g_util.fully_connected(net, config.num_cls, scope='logits', with_bn=False, with_bias=config.with_bias, activation_fn=None, is_training=is_training) return net, end_points 
vkge.knowledgebase.base.KnowledgeBaseParser.facts|sequences|to def facts_to_sequences(self, facts): """ Transform each fact in facts as a sequence of symbol indexes. Only top 'nb_symbols' most frequent symbols will be taken into account. Returns a list of sequences. :param facts: lists of symbols. :return: list of individual sequences of indexes """ return [indices for indices in self.facts_to_sequences_generator(facts)] 
src.preprocessing.feed|dict|construct def construct_feed_dict(adj_normalized, adj, features, placeholders): feed_dict = dict() feed_dict.update({placeholders['features']: features}) feed_dict.update({placeholders['adj']: adj_normalized}) feed_dict.update({placeholders['adj_orig']: adj}) return feed_dict 
eval_metric_test.EvalMetricTest.test|dtw def test_dtw(self): result = eval_metric.get_dtw(self._action_list, self._env_list, self._env) expected_result = self._env.get_distance(1, 3, self._scan_id ) + self._env.get_distance(4, 7, self._scan_id ) + self._env.get_distance(6, 5, self._scan_id) golden_path_length = eval_metric._get_path_length(self._golden_path, self._scan_id, self._env) self.assertAlmostEqual(result, expected_result / golden_path_length) 
seq2seq_data2text.Seq2seqData2text.build def build(self): """Build the model""" print('Building the seq2seq model for data to text generation... ') vocab_size = self.vocab_size state_size = self.state_size enc_layers = self.enc_layers with tf.name_scope('placeholders'): enc_keys = tf.placeholder(tf.int32, [None, None], 'enc_keys') enc_locs = tf.placeholder(tf.int32, [None, None], 'enc_locs') enc_vals = tf.placeholder(tf.int32, [None, None], 'enc_vals') enc_lens = tf.placeholder(tf.int32, [None], 'enc_lens') self.drop_out = tf.placeholder(tf.float32, (), 'drop_out') self.enc_keys = enc_keys self.enc_locs = enc_locs self.enc_vals = enc_vals self.enc_lens = enc_lens dec_inputs = tf.placeholder(tf.int32, [None, None], 'dec_inputs') dec_targets = tf.placeholder(tf.int32, [None, None], 'dec_targets') dec_lens = tf.placeholder(tf.int32, [None], 'dec_lens') self.learning_rate = tf.placeholder(tf.float32, (), 'learning_rate') self.lambda_kl = tf.placeholder(tf.float32, (), 'lambda_kl') self.dec_inputs = dec_inputs self.dec_targets = dec_targets self.dec_lens = dec_lens batch_size = tf.shape(enc_keys)[0] max_enc_len = tf.shape(enc_keys)[1] max_dec_len = tf.shape(dec_inputs)[1] with tf.variable_scope('embeddings'): embedding_matrix_vals = tf.get_variable(name= 'embedding_matrix_vals', shape=[vocab_size, state_size], dtype= tf.float32, initializer=tf.random_normal_initializer(stddev=0.05)) embedding_matrix_keys = tf.get_variable(name= 'embedding_matrix_keys', shape=[vocab_size, state_size], dtype= tf.float32, initializer=tf.random_normal_initializer(stddev=0.05)) embedding_matrix_locs = tf.get_variable(name= 'embedding_matrix_locs', shape=[vocab_size, state_size], dtype= tf.float32, initializer=tf.random_normal_initializer(stddev=0.05)) enc_keys = tf.nn.embedding_lookup(embedding_matrix_keys, enc_keys) enc_vals = tf.nn.embedding_lookup(embedding_matrix_vals, enc_vals) enc_locs = tf.nn.embedding_lookup(embedding_matrix_locs, enc_locs) enc_inputs = (enc_keys + enc_vals + enc_locs) / 3.0 if self.mode == 'train': dec_inputs = tf.nn.embedding_lookup(embedding_matrix_vals, dec_inputs) with tf.variable_scope('encoder'): enc_cell = [create_cell('enc-%d' % i, state_size, self.drop_out, self.no_residual) for i in range(enc_layers)] enc_cell = tf.nn.rnn_cell.MultiRNNCell(enc_cell) enc_outputs, enc_state = tf.nn.dynamic_rnn(enc_cell, enc_inputs, sequence_length=enc_lens, dtype=tf.float32) with tf.variable_scope('decoder'): dec_cell = [create_cell('dec-%d' % i, state_size, self.drop_out, self.no_residual) for i in range(enc_layers)] dec_cell = tf.nn.rnn_cell.MultiRNNCell(dec_cell) dec_proj = tf.layers.Dense(vocab_size, name='dec_proj', kernel_initializer=tf.random_normal_initializer(stddev=0.05), bias_initializer=tf.constant_initializer(0.0)) if self.vae: print('Using vae model') with tf.variable_scope('latent_code'): enc_mean = tf.reduce_sum(enc_outputs, 1) enc_mean /= tf.expand_dims(tf.cast(enc_lens, tf.float32), [1]) z_code = enc_mean if self.prior == 'gaussian': print('Gaussian prior') latent_proj = tf.layers.Dense(2 * state_size, name= 'latent_proj', kernel_initializer=tf. random_normal_initializer(stddev=0.05), bias_initializer=tf.constant_initializer(0.0)) z_loc, z_scale = tf.split(latent_proj(z_code), [state_size, state_size], 1) z_mvn = tfd.MultivariateNormalDiag(z_loc, z_scale) z_sample = z_mvn.sample() elif self.prior == 'vmf': pass dec_init_state = LSTMStateTuple(c=z_sample, h=z_sample ), LSTMStateTuple(c=z_sample, h=z_sample) else: print('Using normal seq2seq, no latent variable') dec_init_state = enc_state with tf.variable_scope('decoding'): _, dec_outputs_predict = decoding_infer(self.dec_start_id, dec_cell, dec_proj, embedding_matrix_vals, dec_init_state, enc_outputs, batch_size, max_dec_len, enc_lens, max_enc_len, self.is_attn, self.sampling_method, self.topk_sampling_size, state_size=self. state_size) if self.mode == 'train': dec_logits_train, _, _, _, _ = decoding_train(dec_inputs, dec_cell, dec_proj, dec_init_state, enc_outputs, max_dec_len, enc_lens, max_enc_len, self.is_attn, self. state_size) all_variables = slim.get_variables_to_restore() model_variables = [var for var in all_variables if var.name. split('/')[0] == self.model_name] print('%s model, variable list:' % self.model_name) for v in model_variables: print('  %s' % v.name) self.model_saver = tf.train.Saver(all_variables, max_to_keep=3) dec_mask = tf.sequence_mask(dec_lens, max_dec_len, dtype=tf.float32 ) dec_loss = tf.contrib.seq2seq.sequence_loss(dec_logits_train, dec_targets, dec_mask) if self.vae: if self.prior == 'gaussian': standard_normal = tfd.MultivariateNormalDiag(tf.zeros( state_size), tf.ones(state_size)) prior_prob = standard_normal.log_prob(z_sample) posterior_prob = z_mvn.log_prob(z_sample) kl_loss = tf.reduce_mean(posterior_prob - prior_prob) loss = dec_loss + self.lambda_kl * kl_loss elif self.prior == 'vmf': pass else: loss = dec_loss optimizer = tf.train.AdamOptimizer(self.learning_rate) train_op = optimizer.minimize(loss) self.train_output = {'train_op': train_op, 'loss': loss} self.train_output.update(self.inspect) if self.vae: self.train_output['dec_loss'] = dec_loss self.train_output['kl_loss'] = kl_loss self.valid_output = {'nll': tf.exp(loss)} self.infer_output = {'dec_predict': dec_outputs_predict} else: self.infer_output = {'dec_predict': dec_outputs_predict} return 
classification.controller.weight_controller.WeightController.Weight|Controller def __init__(self, data_size, config, particles): """ Initialize class WeightController. :param data_size: int The size of the dataset. :param config: dict{String:Object} Configuration for optimizer. :param particles: int The number of particles. """ self._data_size = data_size self._config = config self._variation_type = self._config.fisher_approx self._coeff = self._config.kl / float(self._data_size) self._eta = config.eta self._particles = particles self._controller = list() self._num_layers = 0 
official.utils.logs.logger_test.BenchmarkFileLoggerTest.test|evaluation|log|result def test_log_evaluation_result(self): eval_result = {'loss': 0.46237424, 'global_step': 207082, 'accuracy': 0.9285} log_dir = tempfile.mkdtemp(dir=self.get_temp_dir()) log = logger.BenchmarkFileLogger(log_dir) log.log_evaluation_result(eval_result) metric_log = os.path.join(log_dir, 'metric.log') self.assertTrue(tf.gfile.Exists(metric_log)) with tf.gfile.GFile(metric_log) as f: accuracy = json.loads(f.readline()) self.assertEqual(accuracy['name'], 'accuracy') self.assertEqual(accuracy['value'], 0.9285) self.assertEqual(accuracy['unit'], None) self.assertEqual(accuracy['global_step'], 207082) loss = json.loads(f.readline()) self.assertEqual(loss['name'], 'loss') self.assertEqual(loss['value'], 0.46237424) self.assertEqual(loss['unit'], None) self.assertEqual(loss['global_step'], 207082) 
actor_test.ActorTest.Run|test|Once|Actor def testRunActorOnce(self): FLAGS.unroll_length = 6 mock_problem = testing_utils.MockProblem(unroll_length=FLAGS.unroll_length) hparams = {} hparams['max_iter'] = 1 hparams['sync_agent_every_n_steps'] = 1 flat_specs = [tf.TensorSpec.from_spec(s, str(i)) for i, s in enumerate( tf.nest.flatten(mock_problem.get_actor_output_spec()))] server_address = 'unix:/tmp/actor_test_grpc' server = grpc.Server([server_address])  @tf.function(input_signature=[]) def variable_values(): return []  @tf.function(input_signature=flat_specs) def enqueue(*tensor_list): testing_utils.assert_matches_spec(flat_specs, tensor_list) return [] server.bind(variable_values, batched=False) server.bind(enqueue, batched=False) server.start() actor.run_with_learner(mock_problem, server_address, hparams) 
plot_gain_params.params|plot|gain def plot_gain_params(data_path, plot_dict): subdir = plot_dict['folders'][0]['folder'] fpath = os.path.join(data_path, subdir) df_gain_params = get_gain_params_data(fpath) xtr, ytr = get_gain_params(df_gain_params, plot_dict['gain_param_name']) iso_vals = [100, 400, 800, 1600, 3200] plt.rcParams.update({'font.size': 18}) fig = plt.figure() for i in range(len(iso_vals)): plt.plot(xtr, ytr[i], label='g' + str(iso_vals[i])) if plot_dict['xlims'] is not None: plt.xlim(plot_dict['xlims']) if plot_dict['ylims'] is not None: plt.ylim(plot_dict['ylims']) plt.xlabel('Epoch') plt.ylabel('Gain parameters') plt.title(plot_dict['folders'][0]['folder']) plt.legend(loc='best', bbox_to_anchor=None, prop={'size': 16}, ncol=2, fancybox=False, shadow=False) plt.tight_layout() fig.savefig(os.path.join(fpath, 'gain_params.png')) fig.savefig(os.path.join(fpath, 'gain_params.pdf')) fig = plt.figure() for i in range(len(iso_vals)): if 'skip_iso' in plot_dict.keys() and iso_vals[i] in plot_dict[ 'skip_iso']: print('skipping iso') continue plt.plot(xtr, ytr[i] - np.log(iso_vals[i]), label='$g_{' + str( iso_vals[i]) + '}$') if plot_dict['xlims'] is not None: plt.xlim(plot_dict['xlims']) if plot_dict['ylims'] is not None: plt.ylim(plot_dict['ylims']) plt.xlabel('Epoch') plt.ylabel('Gain parameters (log scale)') if plot_dict['title'] is not None: plt.title(plot_dict['title']) plt.legend(loc='upper right', bbox_to_anchor=[1, 0.8], prop={'size': 16 }, ncol=2, fancybox=False, shadow=False) plt.tight_layout() fig.savefig(os.path.join(fpath, 'gain_params_exp_iso.png')) fig.savefig(os.path.join(fpath, 'gain_params_exp_iso.pdf')) fig = plt.figure() for i in range(len(iso_vals)): if 'skip_iso' in plot_dict.keys() and iso_vals[i] in plot_dict[ 'skip_iso']: print('skipping iso') continue plt.plot(xtr, np.exp(ytr[i]) * iso_vals[i], label='$g_{' + str( iso_vals[i]) + '}$') if plot_dict['xlims'] is not None: plt.xlim(plot_dict['xlims']) if plot_dict['ylims'] is not None: plt.ylim(plot_dict['ylims']) plt.ylim([0, 0.5]) plt.xlabel('Epoch') plt.ylabel('Gain parameters (log scale)') if plot_dict['title'] is not None: plt.title(plot_dict['title']) plt.legend(loc='best', bbox_to_anchor=None, prop={'size': 16}, ncol=2, fancybox=False, shadow=False) plt.tight_layout() fig.savefig(os.path.join(fpath, 'gain_params_exp_iso_.png')) fig.savefig(os.path.join(fpath, 'gain_params_exp_iso_.pdf')) 
vocabulary.Vocabulary.to|word|index def index_to_word(self, word): try: return self.id_word[word] except KeyError as err: raise 
classification.misc.layers.homog|append def _append_homog(tensor): rank = len(tensor.shape.as_list()) shape = tf.concat([tf.shape(tensor)[:-1], [1]], axis=0) ones = tf.ones(shape, dtype=tensor.dtype) return tf.concat([tensor, ones], axis=rank - 1) 
nets.nasnet.nasnet_utils.NasNetABaseCell.conv|apply|operation def _apply_conv_operation(self, net, operation, stride, is_from_original_input, current_step): """Applies the predicted conv operation to net.""" if stride > 1 and not is_from_original_input: stride = 1 input_filters = get_channel_dim(net.shape) filter_size = self._filter_size if 'separable' in operation: net = _stacked_separable_conv(net, stride, operation, filter_size) elif operation in ['none']: if stride > 1 or input_filters != filter_size: net = tf.nn.relu(net) net = slim.conv2d(net, filter_size, 1, stride=stride, scope='1x1') net = slim.batch_norm(net, scope='bn_1') elif 'pool' in operation: net = _pooling(net, stride, operation) if input_filters != filter_size: net = slim.conv2d(net, filter_size, 1, stride=1, scope='1x1') net = slim.batch_norm(net, scope='bn_1') else: raise ValueError('Unimplemented operation', operation) if operation != 'none': net = self._apply_drop_path(net, current_step=current_step) return net 
texar.modules.qnets.qnets.QNetBase.Net|Base|Q def __init__(self, network=None, network_kwargs=None, hparams=None): ModuleBase.__init__(self, hparams=hparams) with tf.variable_scope(self.variable_scope): self._build_network(network, network_kwargs) 
train_ruemonge2014.parse|fn def parse_fn(item): features = tf.parse_single_example(item, features={'xyz_raw': tf. FixedLenFeature([], dtype=tf.string), 'normal_raw': tf. FixedLenFeature([], dtype=tf.string), 'rgb_raw': tf.FixedLenFeature ([], dtype=tf.string), 'seg_label': tf.FixedLenFeature([], dtype=tf .string)}) xyz = tf.decode_raw(features['xyz_raw'], tf.float32) rgb = tf.decode_raw(features['rgb_raw'], tf.float32) normal = tf.decode_raw(features['normal_raw'], tf.float32) seg_label = tf.decode_raw(features['seg_label'], tf.int32) xyz = tf.reshape(xyz, [-1, 3]) rgb = tf.reshape(rgb, [-1, 3]) normal = tf.reshape(normal, [-1, 3]) seg_label = tf.reshape(seg_label, [-1, 1]) all_in_one = tf.concat((xyz, normal, rgb, tf.cast(seg_label, tf.float32 )), axis=-1) return all_in_one 
deepMOT-master.models.DAN.SST.stacker|forward def forward_stacker2(self, stacker1_pre_output, stacker1_next_output): stacker1_pre_output = stacker1_pre_output.unsqueeze(2).repeat(1, 1, self.max_object, 1).permute(0, 3, 1, 2) stacker1_next_output = stacker1_next_output.unsqueeze(1).repeat(1, self .max_object, 1, 1).permute(0, 3, 1, 2) stacker1_pre_output = self.stacker2_bn(stacker1_pre_output.contiguous()) stacker1_next_output = self.stacker2_bn(stacker1_next_output.contiguous()) output = torch.cat([stacker1_pre_output, stacker1_next_output], 1) return output 
modeling_test.BertModelTest.BertModelTester.prediction|check|sequence|output|for|bert|next def check_bert_for_next_sequence_prediction_output(self, result): self.parent.assertListEqual(list(result['seq_relationship_score'].size( )), [self.batch_size, 2]) 
AffineCouplingSdnGain.AffineCouplingSdnGain.det|jacobian|log|inverse def _inverse_log_det_jacobian(self, z, yy, nlf0=None, nlf1=None, iso=None, cam=None): beta1, beta2 = sdn_iso_model_params_3(iso) scale = tf.sqrt(beta1 * yy + beta2) if scale is None: return tf.constant(0.0, dtype=z.dtype, name='ildj') return -tf.reduce_sum(tf.log(scale), axis=[1, 2, 3]) 
official.utils.logs.hooks_helper.get|logging|tensor|hook def get_logging_tensor_hook(every_n_iter=100, tensors_to_log=None, **kwargs): """Function to get LoggingTensorHook.  Args: every_n_iter: `int`, print the values of `tensors` once every N local steps taken on the current worker. tensors_to_log: List of tensor names or dictionary mapping labels to tensor names. If not set, log _TENSORS_TO_LOG by default. **kwargs: a dictionary of arguments to LoggingTensorHook.  Returns: Returns a LoggingTensorHook with a standard set of tensors that will be printed to stdout. """ if tensors_to_log is None: tensors_to_log = _TENSORS_TO_LOG return tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter= every_n_iter) 
rnns.Klimits|return def return_Klimits(model, wform, data): """We use this function to select the upper and lower limits of number of hidden units per layer depending on the task and the dataset. The user can also choo    se to limit the upper and lower limit of allowable number of trainable parameters""" if model == 'mod_lstm': min_params = 10.0 max_params = 70000000.0 K_min, K_max = 50, 300 elif model == 'gated_wf': min_params = 10.0 max_params = 70000000.0 K_min, K_max = 50, 350 elif model == 'mod_rnn': min_params = 10.0 max_params = 70000000.0 K_min, K_max = 50, 400 return K_min, K_max, min_params, max_params 
CRPM_Net.CRPM_Net.loss def loss(self): with tf.variable_scope('loss'): annotation = tf.expand_dims(self.label, -1, name='annotation') self.loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits= self.logits, labels=self.label) self.loss = tf.reduce_mean(self.loss) + tf.add_n(tf.get_collection( 'losses')) 
model.symmetry|mrcnn|coord|graph|loss def mrcnn_coord_symmetry_loss_graph(target_masks, target_coords, target_class_ids, target_domain_labels, pred_coords, loss_fn): """Mask L1 loss for the coordinates head.  target_masks: [batch, num_rois, height, width]. A float32 tensor of values 0 or 1. Uses zero padding to fill array. target_coords: [batch, num_rois, height, width, 3]. A float32 tensor of values in the range of [0, 1]. Uses zero padding to fill array.  target_domain_labels: [batch, num_rois]. Bool. 1 for real data, 0 for synthetic data. target_class_ids: [batch, num_rois]. Integer class IDs. Zero padded. pred_coords: [batch, proposals, height, width, num_classes, 3] float32 tensor with values from 0 to 1. """ target_class_ids = tf.reshape(target_class_ids, (-1,)) mask_shape = tf.shape(target_masks) coord_shape = tf.shape(target_coords) pred_shape = tf.shape(pred_coords) target_masks = tf.reshape(target_masks, (-1, mask_shape[2], mask_shape[ 3], 1)) target_coords = tf.reshape(target_coords, (-1, coord_shape[2], coord_shape[3], 3)) target_coords = tf.image.resize_nearest_neighbor(target_coords, ( pred_shape[2], pred_shape[3])) target_masks = tf.image.resize_nearest_neighbor(target_masks, ( pred_shape[2], pred_shape[3])) target_masks = tf.reshape(target_masks, (-1, pred_shape[2], pred_shape[3])) pred_coords = tf.reshape(pred_coords, (-1, pred_shape[2], pred_shape[3], pred_shape[4], 3)) pred_coords = tf.transpose(pred_coords, [0, 3, 1, 2, 4]) target_domain_labels = tf.reshape(target_domain_labels, (-1,)) domain_ix = tf.equal(target_domain_labels, False) target_class_ids = tf.multiply(target_class_ids, tf.cast(domain_ix, dtype=tf.float32)) positive_ix = tf.where(target_class_ids > 0)[:, (0)]  def nonzero_positive_loss(target_masks, target_coords, pred_coords, positive_ix): positive_class_ids = tf.cast(tf.gather(target_class_ids, positive_ix), tf.int64) positive_class_rotation_theta = tf.map_fn(lambda x: class_id_to_theta(x), positive_class_ids, dtype=tf.float32) positive_class_rotation_matrix = tf.map_fn(lambda x: rotation_y_matrix(x), positive_class_rotation_theta) positive_class_rotation_matrix = tf.reshape( positive_class_rotation_matrix, (-1, 3, 3)) positive_class_rotation_matrix = tf.reshape( positive_class_rotation_matrix, (-1, 1, 1, 3, 3)) tiled_rotation_matrix = tf.tile(positive_class_rotation_matrix, [1, pred_shape[2], pred_shape[3], 1, 1]) y_true = tf.gather(target_coords, positive_ix) y_true = y_true - 0.5 y_true = tf.expand_dims(y_true, axis=4) rotated_y_true_1 = tf.matmul(tiled_rotation_matrix, y_true) rotated_y_true_2 = tf.matmul(tiled_rotation_matrix, rotated_y_true_1) rotated_y_true_3 = tf.matmul(tiled_rotation_matrix, rotated_y_true_2) rotated_y_true_4 = tf.matmul(tiled_rotation_matrix, rotated_y_true_3) rotated_y_true_5 = tf.matmul(tiled_rotation_matrix, rotated_y_true_4) y_true_stack = tf.concat([y_true, rotated_y_true_1, rotated_y_true_2, rotated_y_true_3, rotated_y_true_4, rotated_y_true_5], axis=4) y_true_stack = tf.transpose(y_true_stack, (0, 1, 2, 4, 3)) y_true_stack = y_true_stack + 0.5 indices = tf.stack([positive_ix, positive_class_ids], axis=1) y_pred = tf.gather_nd(pred_coords, indices) y_pred = tf.expand_dims(y_pred, axis=3) y_pred_stack = tf.tile(y_pred, [1, 1, 1, tf.shape(y_true_stack)[3], 1]) diff = K.abs(y_true_stack - y_pred_stack) diff = loss_fn(diff) mask = tf.gather(target_masks, positive_ix) reshape_mask = tf.reshape(mask, (tf.shape(mask)[0], tf.shape(mask)[ 1], tf.shape(mask)[2], 1, 1)) num_of_pixels = tf.reduce_sum(mask, axis=[1, 2]) + 1e-05 diff_in_mask = tf.multiply(diff, reshape_mask) sum_diff_in_mask = tf.reduce_sum(diff_in_mask, axis=[1, 2]) total_sum_diff_in_mask = tf.reduce_sum(sum_diff_in_mask, axis=[-1]) arg_min_rotation = tf.argmin(total_sum_diff_in_mask, axis=-1) arg_min_rotation = tf.cast(arg_min_rotation, tf.int32) min_indices = tf.stack([tf.range(tf.shape(arg_min_rotation)[0]), arg_min_rotation], axis=-1) min_diff_in_mask = tf.gather_nd(sum_diff_in_mask, min_indices) mean_diff_in_mask = tf.divide(min_diff_in_mask, tf.expand_dims( num_of_pixels, axis=1)) loss = tf.reduce_mean(mean_diff_in_mask, axis=0) return loss  def zero_positive_loss(): return tf.constant([0.0, 0.0, 0.0]) loss = tf.cond(tf.size(positive_ix) > 0, lambda : nonzero_positive_loss (target_masks, target_coords, pred_coords, positive_ix), lambda : zero_positive_loss()) return loss 
nets.inception_v2_test.InceptionV2Test.test|Evaluation def testEvaluation(self): batch_size = 2 height, width = 224, 224 num_classes = 1000 eval_inputs = tf.random_uniform((batch_size, height, width, 3)) logits, _ = inception.inception_v2(eval_inputs, num_classes, is_training=False) predictions = tf.argmax(logits, 1) with self.test_session() as sess: sess.run(tf.global_variables_initializer()) output = sess.run(predictions) self.assertEquals(output.shape, (batch_size,)) 
utils.losses|classification|update def update_classification_losses(losses, nums, name, bs, loss): if not isinstance(loss, float): print(type(loss)) raise nums[name] += bs losses[name] += loss * bs 
plato.agent.component.dialogue_policy.deep_learning.supervised_policy.SupervisedPolicy.initialize def initialize(self, args): """ Initialize internal structures at the beginning of each dialogue  :return: Nothing """ if self.agent_role == 'system': self.warmup_policy = HandcraftedPolicy({'ontology': self.ontology}) elif self.agent_role == 'user': usim_args = dict(zip(['ontology', 'database'], [self.ontology, self .database])) self.warmup_simulator = AgendaBasedUS(usim_args) if 'is_training' in args: self.is_training = bool(args['is_training']) if self.agent_role == 'user' and self.warmup_simulator: if 'goal' in args: self.warmup_simulator.initialize({args['goal']}) else: print( 'WARNING ! No goal provided for Supervised policy user simulator @ initialize' ) self.warmup_simulator.initialize({}) if 'policy_path' in args: self.policy_path = args['policy_path'] if 'learning_rate' in args: self.policy_alpha = args['learning_rate'] if self.sess is None: self.policy_net = self.feed_forward_net_init() self.sess = tf.InteractiveSession() self.sess.run(tf.global_variables_initializer()) self.tf_saver = tf.train.Saver(var_list=tf.get_collection(tf. GraphKeys.GLOBAL_VARIABLES, scope=self.tf_scope)) 
ICP.result|draw|registration def draw_registration_result(source, target, transformation): source_temp = copy.deepcopy(source) target_temp = copy.deepcopy(target) source_temp.paint_uniform_color([1, 0.706, 0]) target_temp.paint_uniform_color([0, 0.651, 0.929]) source_temp.transform(transformation) draw_geometries([source_temp, target_temp]) 
aggregator_cifar.get|df def get_df(hparams_list, metrics_list): all_values_list = [] for hparams, metrics in zip(hparams_list, metrics_list): all_values = {} for key, value in hparams.values().iteritems(): all_values[key] = value for key, value in metrics.iteritems(): all_values['m_' + key] = value all_values_list.append(all_values) df = pd.DataFrame(all_values_list) return df 
gan.factorVAE.FactorVAE.sample|from def sample_from(self, z): samples = [] for i in range(int(math.ceil(float(z.shape[0]) / self.batch_size))): sub_samples = self.sess.run(self.de_test_tf, feed_dict={self.z_pl: z[i * self.batch_size:(i + 1) * self.batch_size]}) samples.append(sub_samples) return np.vstack(samples) 
texar.agents.dqn_agent.DQNAgent.get|update|op|tau def _get_tau_update_op(self): tau = 1.0 / self._update_period op = [] for i in range(len(self._qnet.trainable_variables)): value_ = (1.0 - tau) * self._target.trainable_variables[i ] + tau * self._qnet.trainable_variables[i] op.append(tf.assign(ref=self._target.trainable_variables[i], value= value_)) return op 
gym_pycolab.envs.pycolab_grid_worlds_env.PycolabConnect5RotatedEnv.game|make def _make_game(self): self._setup() return connect_four.make_game(shape=self._shape, connect_n=5) 
utils.data_utils.SSTProcessor.get|examples|train def get_train_examples(self, data_dir): """See base class.""" return self._create_examples(self._read_tsv(os.path.join(data_dir, 'train.tsv')), 'train') 
models.cvae.Autoencoder.conditional|forward def forward_conditional(self, sess, input_features, input_labels): """Forward step conditioned on the labels @param sess (tf.Session) the current session @param input_feature (np.array) matrix of features @param input_labels (np.array) array of labels @param lambda_e explicit mixing coefficient @param lambda_i implicit mixing coefficient @return (float) the output  (reconstruction) """ output = sess.run([self.output], feed_dict={self.x: input_features, self.labels_placeholder: input_labels}) return output[0] 
cpplint._CppLintState.Set|Filters def SetFilters(self, filters): """Sets the error-message filters.  These filters are applied when deciding whether to emit a given error message.  Args: filters: A string of comma-separated filters (eg "+whitespace/indent"). Each filter should start with + or -; else we die.  Raises: ValueError: The comma-separated filters did not all start with '+' or '-'. E.g. "-,+whitespace,-whitespace/indent,whitespace/badfilter" """ self.filters = _DEFAULT_FILTERS[:] self.AddFilters(filters) 
utils.espeakng.ESpeakNG.voices @property def voices(self): res = self._espeak_exe(['--voices'], sync=True) self._logger.debug('espeakng: voices: %s' % res) voices = [] first = True for v in res: if first: first = False continue parts = v.decode('utf8').split() if len(parts) < 5: continue age_parts = parts[2].split('/') if len(age_parts) != 2: continue voice = {'pty': parts[0], 'language': parts[1], 'age': age_parts[0], 'gender': age_parts[1], 'voice_name': parts[3], 'file': parts[4]} self._logger.debug('espeakng: voices: parts= %s %s -> %s' % (len( parts), repr(parts), repr(voice))) voices.append(voice) return voices 
texar.modules.decoders.tf_helpers.GreedyEmbeddingHelper.initialize def initialize(self, name=None): finished = array_ops.tile([False], [self._batch_size]) return finished, self._start_inputs 
utils.tpu_decode.visit def visit(t, s): s = s.primary if isinstance(s, values_lib.PerReplica) else s if isinstance(s, TPUEncodedUInt8): x = t.encoded if isinstance(t, TPUEncodedUInt8) else t x = tf.reshape(x, [-1, 32, 1]) x = tf.broadcast_to(x, x.shape[:-1] + [4]) x = tf.reshape(x, [-1, 128]) x = tf.bitwise.bitwise_and(x, [255, 65280, 16711680, 4278190080] * 32) x = tf.bitwise.right_shift(x, [0, 8, 16, 24] * 32) rank = s.original_shape.rank perm = [rank - 1] + list(range(rank - 1)) inverted_shape = np.array(s.original_shape)[np.argsort(perm)] x = tf.reshape(x, inverted_shape) x = tf.transpose(x, perm) return x else: return t 
cropping.BRATS|scans|save def save_scans_BRATS(out_path, pat_name, img_data, seg_data=None): save_mod_names = ['Flair', 'T1', 'T1c', 'T2'] save_seg_name = 'Label' assert img_data.shape[3] == 4 for mod_i in range(len(save_mod_names)): save_name = '%s_%s.nii.gz' % (pat_name, save_mod_names[mod_i]) save_path = os.path.join(out_path, save_name) mod_data_nii = nibabel.Nifti1Image(img_data[:, :, :, (mod_i)], OUTPUT_AFFINE) nibabel.save(mod_data_nii, save_path) print('saved to {}'.format(out_path)) if seg_data is not None: save_name = '%s_%s.nii.gz' % (pat_name, save_seg_name) save_path = os.path.join(out_path, save_name) seg_data_nii = nibabel.Nifti1Image(seg_data, OUTPUT_AFFINE) nibabel.save(seg_data_nii, save_path) 
kaffe.tensorflow.transformer.TensorFlowMapper.get|kernel|params def get_kernel_params(self, node): kernel_params = node.layer.kernel_parameters input_shape = node.get_only_parent().output_shape padding = get_padding_type(kernel_params, input_shape, node.output_shape) padding = {'padding': padding} if padding != network.DEFAULT_PADDING else { } return kernel_params, padding 
PlotParabolic.make|equal|length def make_equal_length(a, new_length): old_indices = np.arange(0, len(a)) new_indices = np.linspace(0, len(a) - 1, new_length) spl = UnivariateSpline(old_indices, a, k=3, s=0) new_array = spl(new_indices) return new_array 
nets.gen_models.ImageSR.model|create def create_model(self, load_weights=False): channel_axis = 1 if K.image_dim_ordering() == 'th' else -1 init = Input(shape=self.lr_shape) x = Convolution2D(64, (self.f1, self.f1), activation='relu', padding='same' )(init) x = LeakyReLU(alpha=0.25)(x) x = Convolution2D(64, (self.f2, self.f2), activation='relu', padding='same' )(x) x = LeakyReLU(alpha=0.25)(x) x = Convolution2D(64, (3, 3), padding='same')(x) x = LeakyReLU(alpha=0.25, name='disc_lr_1_1')(x) x = Convolution2D(64, (3, 3), padding='same', strides=1)(x) x = LeakyReLU(alpha=0.25, name='disc_lr_1_2')(x) x = Convolution2D(128, (3, 3), padding='same')(x) x = LeakyReLU(alpha=0.25, name='disc_lr_2_1')(x) x = Convolution2D(128, (3, 3), padding='same', strides=1)(x) x = LeakyReLU(alpha=0.25, name='disc_lr_2_2')(x) x = self.deconv2d(x) x = x if self.SCALE < 4 else self.deconv2d(x) x = x if self.SCALE < 8 else self.deconv2d(x) out = Convolution2D(3, (3, 3), activation='tanh', padding='same')(x) print(out) return Model(init, out) 
extract_features.InputFeatures.Input|Features def __init__(self, unique_id, tokens, input_ids, input_mask, input_type_ids): self.unique_id = unique_id self.tokens = tokens self.input_ids = input_ids self.input_mask = input_mask self.input_type_ids = input_type_ids 
kaffe.graph.Graph.replaced def replaced(self, new_nodes): return Graph(nodes=new_nodes, name=self.name) 
gym_pycolab.worlds.connect_four.PlayerDrape.Player|Drape def __init__(self, curtain, character): super(PlayerDrape, self).__init__(curtain, character) 
RDF2Vec.preprocess def preprocess(folders, filename): entity2id = {} relation2id = {} triples = {} ent_counter = 0 rel_counter = 0 for dirname in folders: for fname in os.listdir(dirname): if not filename in fname: continue print(fname) gzfile = gzip.open(os.path.join(dirname, fname), mode='rt', encoding='utf-8') for line in csv.reader(gzfile, delimiter=' ', quotechar='"'): h = line[0] r = line[1] t = line[2] if not t.startswith('<'): continue if 'ddi-interactor-in' in r: continue if h in entity2id: hid = entity2id[h] else: entity2id[h] = ent_counter ent_counter += 1 hid = entity2id[h] if t in entity2id: tid = entity2id[t] else: entity2id[t] = ent_counter ent_counter += 1 tid = entity2id[t] if r in relation2id: rid = relation2id[r] else: relation2id[r] = rel_counter rel_counter += 1 rid = relation2id[r] addTriple(triples, hid, tid, rid) print('Relation:', rel_counter, ' Entity:', ent_counter) return entity2id, relation2id, triples 
models.attacks_tf.deepfool|batch def deepfool_batch(sess, x, pred, logits, grads, X, nb_candidate, overshoot, max_iter, clip_min, clip_max, nb_classes, feed=None): """ Applies DeepFool to a batch of inputs :param sess: TF session :param x: The input placeholder :param pred: The model's sorted symbolic output of logits, only the top nb_candidate classes are contained :param logits: The model's unnormalized output tensor (the input to the softmax layer) :param grads: Symbolic gradients of the top nb_candidate classes, procuded from gradient_graph :param X: Numpy array with sample inputs :param nb_candidate: The number of classes to test against, i.e., deepfool only consider nb_candidate classes when attacking(thus accelerate speed). The nb_candidate classes are chosen according to the prediction confidence during implementation. :param overshoot: A termination criterion to prevent vanishing updates :param max_iter: Maximum number of iteration for DeepFool :param clip_min: Minimum value for components of the example returned :param clip_max: Maximum value for components of the example returned :param nb_classes: Number of model output classes :return: Adversarial examples """ X_adv = deepfool_attack(sess, x, pred, logits, grads, X, nb_candidate, overshoot, max_iter, clip_min, clip_max, feed=feed) return np.asarray(X_adv, dtype=np.float32) 
model.DetectionTargetLayer.shape|output|compute def compute_output_shape(self, input_shape): return [(None, self.config.TRAIN_ROIS_PER_IMAGE, 4), (None, 1), (None, self.config.TRAIN_ROIS_PER_IMAGE, 4), (None, self.config. TRAIN_ROIS_PER_IMAGE, self.config.MASK_SHAPE[0], self.config. MASK_SHAPE[1]), (None, self.config.TRAIN_ROIS_PER_IMAGE, self. config.COORD_SHAPE[0], self.config.COORD_SHAPE[1]), (None, self. config.TRAIN_ROIS_PER_IMAGE, self.config.COORD_SHAPE[0], self. config.COORD_SHAPE[1]), (None, self.config.TRAIN_ROIS_PER_IMAGE, self.config.COORD_SHAPE[0], self.config.COORD_SHAPE[1])] 
train_nlayer.train.fn def fn(correct, predicted): return tf.nn.softmax_cross_entropy_with_logits(labels=correct, logits= predicted / train_temp) 
embeddings.OpenKE.models.RESCAL.RESCAL.embedding|def def embedding_def(self): config = self.get_config() self.ent_embeddings = tf.get_variable(name='ent_embeddings', shape=[ config.entTotal, config.hidden_size], initializer=tf.contrib.layers .xavier_initializer(uniform=False)) self.rel_matrices = tf.get_variable(name='rel_matrices', shape=[config. relTotal, config.hidden_size * config.hidden_size], initializer=tf. contrib.layers.xavier_initializer(uniform=False)) self.parameter_lists = {'ent_embeddings': self.ent_embeddings, 'rel_matrices': self.rel_matrices} 
attack.get|grad def get_grad(args, ites, data, target, net, alphas_prior_logits, alphas, betas, normMean, normStd): grads = [] for ite in range(ites): if args.method == 'dbsn': if args.ps: output = net((data - normMean) / normStd, F.softmax(alphas, 1)) else: output = net((data - normMean) / normStd, _ada_gumbel_softmax(alphas, args.tau_min, betas)) elif args.method == 'densenet': output = net((data - normMean) / normStd) elif args.method == 'random': output = net((data - normMean) / normStd, _ada_gumbel_softmax( alphas_prior_logits, args.tau_min)) loss = F.nll_loss(output, target) grad = torch.autograd.grad([loss], [data], grad_outputs=torch. ones_like(loss), retain_graph=False)[0] grads.append(grad.detach()) return sum(grads) / float(ites) 
a3c.rollout|process def process_rollout(rollout, gamma, lambda_=1.0): """ given a rollout, compute its returns and the advantage """ batch_si = np.asarray(rollout.states) batch_a = np.asarray(rollout.actions) rewards = np.asarray(rollout.rewards) vpred_t = np.asarray(rollout.values + [rollout.r]) rewards_plus_v = np.asarray(rollout.rewards + [rollout.r]) batch_r = discount(rewards_plus_v, gamma)[:-1] delta_t = rewards + gamma * vpred_t[1:] - vpred_t[:-1] batch_adv = discount(delta_t, gamma * lambda_) return Batch(batch_si, batch_a, batch_adv, batch_r, rollout.terminal, rollout.task) 
utils.data_utils.InputExample.Example|Input def __init__(self, guid, text_a, text_b=None, label=None): """Constructs a InputExample. Args: guid: Unique id for the example. text_a: string. The untokenized text of the first sequence. For single sequence tasks, only this sequence must be specified. text_b: (Optional) string. The untokenized text of the second sequence. Only must be specified for sequence pair tasks. label: (Optional) string. The label of the example. This should be specified for train and dev examples, but not for test examples. """ self.guid = guid self.text_a = text_a self.text_b = text_b self.label = label 
svae_dc.utils.policy_classes.StructuredPolicy.get|params def get_params(self): return self.params 
make_tfrecord_modelnet.int|feature def _int64_feature(value): """Returns an int64_list from a bool / enum / int / uint.""" return tf.train.Feature(int64_list=tf.train.Int64List(value=[value])) 
deepctr.layers.activation.Dice.shape|output|compute def compute_output_shape(self, input_shape): return input_shape 
cpplint._FunctionState.Count def Count(self): """Count line in current function body.""" if self.in_a_function: self.lines_in_function += 1 
svae_dc.utils.bo_kernels.KLKernel.tstforward def tstforward(self, x1, x2, diag=False, last_dim_is_batch=False, **params): strt = time.time() res = self.fforward(x1, x2, diag, last_dim_is_batch, **params) print('LKLKernel.fforward took', time.time() - strt) strt = time.time() dbgres = self.dbgforward(x1, x2, diag, last_dim_is_batch, **params) print('LKLKernel.dbgforward took', time.time() - strt) assert_isclose(res, dbgres, eps=0.0001) return res 
evaluate_multiclass.load def load(saver, sess, ckpt_path): """Load trained weights.  Args: saver: TensorFlow saver object. sess: TensorFlow session. ckpt_path: path to checkpoint file with parameters. """ saver.restore(sess, ckpt_path) print('Restored model parameters from {}'.format(ckpt_path)) 
ImageLoader.ImageLoader.load|tuple|image|thread def load_image_tuple_thread(self, thread_id): while True: filename_tuple = self.filename_tuple_queue.get() parts = str.split(filename_tuple[0], '/') fn = parts[-3] + '|' + parts[-1] noise, gt, var, nlf0, nlf1, iso, cam, metadata = load_one_tuple_images( filename_tuple) im_dict = {'in': noise, 'gt': gt, 'vr': var, 'nlf0': nlf0, 'nlf1': nlf1, 'iso': iso, 'cam': cam, 'fn': fn, 'metadata': metadata} self.queue.put(im_dict) if self.requeue: self.temp_fns_queue.put(filename_tuple) if self.filename_tuple_queue.empty(): while not self.queue.empty(): sleep(1) tmp = self.filename_tuple_queue self.filename_tuple_queue = self.temp_fns_queue self.temp_fns_queue = tmp self.filename_tuple_queue.put(filename_tuple) elif self.filename_tuple_queue.empty(): break 
sidd_utils.gif|create def create_gif(im_dir): root = '/home/abdo/skynet/_Code/fourier_flows/experiments/sidd/UNCOND/' sample_id = 50 im_dir = os.path.join(root, 'one_sample_%04d' % sample_id) filenames = glob.glob(path.join(im_dir, '*.png')) filenames = sorted(filenames) images = [] for filename in filenames: images.append(imageio.imread(filename)) imageio.mimsave(os.path.join(im_dir, 'sample_%04d.gif' % sample_id), images, duration=1) 
official.resnet.cifar10_main.fn|input def input_fn(is_training, data_dir, batch_size, num_epochs=1, dtype=tf. float32, datasets_num_private_threads=None, num_parallel_batches=1, parse_record_fn=parse_record): """Input function which provides batches for train or eval.  Args: is_training: A boolean denoting whether the input is for training. data_dir: The directory containing the input data. batch_size: The number of samples per batch. num_epochs: The number of epochs to repeat the dataset. dtype: Data type to use for images/features datasets_num_private_threads: Number of private threads for tf.data. num_parallel_batches: Number of parallel batches for tf.data. parse_record_fn: Function to use for parsing the records.  Returns: A dataset that can be used for iteration. """ filenames = get_filenames(is_training, data_dir) dataset = tf.data.FixedLengthRecordDataset(filenames, _RECORD_BYTES) return resnet_run_loop.process_record_dataset(dataset=dataset, is_training=is_training, batch_size=batch_size, shuffle_buffer= NUM_IMAGES['train'], parse_record_fn=parse_record_fn, num_epochs= num_epochs, dtype=dtype, datasets_num_private_threads= datasets_num_private_threads, num_parallel_batches=num_parallel_batches ) 
model.fpn|build|coord|graph|bins def build_fpn_coord_bins_graph(rois, feature_maps, image_shape, pool_size, num_classes, num_bins, use_bn, net_name): """Builds the computation graph of the coordinate map head of Feature Pyramid Network.  rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized coordinates. feature_maps: List of feature maps from different layers of the pyramid, [P2, P3, P4, P5]. Each has a different resolution. image_shape: [height, width, depth] pool_size: The width of the square feature map generated from ROI Pooling. num_classes: number of classes, which determines the depth of the results  Returns: Coordinate maps [batch, roi_count, height, width, num_classes, num_bins] """ x = PyramidROIAlign([pool_size, pool_size], image_shape, name= 'roi_align_{}'.format(net_name))([rois] + feature_maps) x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding='same'), name= 'mrcnn_{}_conv1'.format(net_name))(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_{}_bn1'. format(net_name))(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding='same'), name= 'mrcnn_{}_conv2'.format(net_name))(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_{}_bn2'. format(net_name))(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding='same'), name= 'mrcnn_{}_conv3'.format(net_name))(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_{}_bn3'. format(net_name))(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding='same'), name= 'mrcnn_{}_conv4'.format(net_name))(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_{}_bn4'. format(net_name))(x) x = KL.Activation('relu')(x) x_feature = KL.TimeDistributed(KL.Conv2DTranspose(256, (2, 2), strides= 2, activation='relu'), name='mrcnn_{}_deconv'.format(net_name))(x) x = KL.TimeDistributed(KL.Conv2D(num_bins * num_classes, (1, 1), strides=1), name='mrcnn_{}_conv_bins'.format(net_name))(x_feature) x = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], tf.shape(t)[1], tf.shape(t)[2], tf.shape(t)[3], -1, num_bins]), name= 'mrcnn_{}_bins_reshape'.format(net_name))(x) x = KL.Activation('softmax', name='mrcnn_{}_bins'.format(net_name))(x) return x, x_feature 
main.main def main(): config = Config() config.parse_arg(FLAGS) config.setup_path() config.print_arg() if config.dataset == 'wikibio': dset = DatasetTable2text(config) dset.load() config.key_size = len(dset.key2id) else: dset = Dataset(config) dset.build() config.vocab_size = len(dset.word2id) config.dec_start_id = dset.word2id['_GOO'] config.dec_end_id = dset.word2id['_EOS'] config.pad_id = dset.pad_id config.stop_words = dset.stop_words if config.model_name == 'seq2seq': if config.dataset == 'wikibio': Model = Seq2seqData2text else: Model = Seq2seq elif config.model_name == 'bow_seq2seq': Model = BowSeq2seq elif config.model_name == 'vae': Model = Vae elif config.model_name == 'hierarchical_vae': Model = Hierarchical_Vae elif config.model_name == 'latent_bow': if config.dataset == 'wikibio': Model = LatentBowData2text else: Model = LatentBow elif config.model_name == 'lm': Model = LM else: msg = ( "the model name shoule be in ['seq2seq', 'vae', 'hierarchical_vae', 'latent_low', 'lm'], " ) msg += 'current name: %s' % config.model_name raise Exception(msg) model = Model(config) with tf.variable_scope(config.model_name): model.build() controller = Controller(config) if config.model_name != 'lm': if 'lm' in controller.eval_metrics_list: controller.build_lm(LM, config) controller.train(model, dset) return 
pvae.model.OneLayerModel.One|Layer|Model def __init__(self, settings, bn_is_training, bn_stats_iter, bn_update_moving_stats): """Only for 64px_big_onelevel and MNIST. Needs modification for others.""" self.s = settings self.bn_is_training = bn_is_training self.bn_stats_iter = bn_stats_iter self.bn_update_moving_stats = bn_update_moving_stats self.resblock = functools.partial(ResidualBlock, bn_is_training=self. bn_is_training, bn_stats_iter=self.bn_stats_iter, bn_update_moving_stats=self.bn_update_moving_stats) 
facenet-master.src.align.detect_face.Box|generate|Bounding def generateBoundingBox(imap, reg, scale, t): """Use heatmap to generate bounding boxes""" stride = 2 cellsize = 12 imap = np.transpose(imap) dx1 = np.transpose(reg[:, :, (0)]) dy1 = np.transpose(reg[:, :, (1)]) dx2 = np.transpose(reg[:, :, (2)]) dy2 = np.transpose(reg[:, :, (3)]) y, x = np.where(imap >= t) if y.shape[0] == 1: dx1 = np.flipud(dx1) dy1 = np.flipud(dy1) dx2 = np.flipud(dx2) dy2 = np.flipud(dy2) score = imap[y, x] reg = np.transpose(np.vstack([dx1[y, x], dy1[y, x], dx2[y, x], dy2[y, x]])) if reg.size == 0: reg = np.empty((0, 3)) bb = np.transpose(np.vstack([y, x])) q1 = np.fix((stride * bb + 1) / scale) q2 = np.fix((stride * bb + cellsize - 1 + 1) / scale) boundingbox = np.hstack([q1, q2, np.expand_dims(score, 1), reg]) return boundingbox, reg 
prune_resnet32.PruneResNet32.size|from|get|output|layer|name def _get_output_size_from_layer_name(self, name): if name.startswith('conv0') or name.startswith('block1'): return self.image_size elif name.startswith('block2'): return [self.image_size[0] // 2, self.image_size[1] // 2] elif name.startswith('block3'): return [self.image_size[0] // 4, self.image_size[1] // 4] elif name.startswith('dense'): return [self.image_size[0] // 32, self.image_size[1] // 32] else: raise ValueError('unknown layer name ' + name) 
texar.modules.classifiers.bert_classifier_test.BertClassifierTest.test|encode def test_encode(self): """Tests encoding. """ max_time = 8 batch_size = 16 inputs = tf.random_uniform([batch_size, max_time], maxval=30521, dtype= tf.int32) clas = BertClassifier() logits, pred = clas(inputs) with self.test_session() as sess: sess.run(tf.global_variables_initializer()) logits_, pred_ = sess.run([logits, pred]) self.assertEqual(logits_.shape, (batch_size, clas.hparams.num_classes)) self.assertEqual(pred_.shape, (batch_size,)) hparams = {'num_classes': 10, 'clas_strategy': 'time_wise'} clas = BertClassifier(hparams=hparams) logits, pred = clas(inputs) with self.test_session() as sess: sess.run(tf.global_variables_initializer()) logits_, pred_ = sess.run([logits, pred]) self.assertEqual(logits_.shape, (batch_size, max_time, clas.hparams .num_classes)) self.assertEqual(pred_.shape, (batch_size, max_time)) hparams = {'num_classes': 0, 'clas_strategy': 'time_wise'} clas = BertClassifier(hparams=hparams) logits, pred = clas(inputs) with self.test_session() as sess: sess.run(tf.global_variables_initializer()) logits_, pred_ = sess.run([logits, pred]) self.assertEqual(logits_.shape, (batch_size, max_time, clas.hparams .encoder.dim)) self.assertEqual(pred_.shape, (batch_size, max_time)) hparams = {'num_classes': 10, 'clas_strategy': 'all_time', 'max_seq_length': max_time} inputs = tf.placeholder(tf.int32, shape=[batch_size, 6]) clas = BertClassifier(hparams=hparams) logits, pred = clas(inputs) with self.test_session() as sess: sess.run(tf.global_variables_initializer()) logits_, pred_ = sess.run([logits, pred], feed_dict={inputs: np. random.randint(30521, size=(batch_size, 6))}) self.assertEqual(logits_.shape, (batch_size, clas.hparams.num_classes)) self.assertEqual(pred_.shape, (batch_size,)) 
craystack.codecs.random|message def random_message(flat_len, shape, rng=np.random): """Generate a random vrans stack.""" arr = rng.randint(1 << 32, size=flat_len, dtype='uint32') return unflatten(arr, shape) 
r2r_problem.R2RProblem.summary|create def create_summary(self, step, info): sum_episode_reward = 0.0 sum_episode_num_steps = 0.0 num_infos = 0 num_paths_list = [] for infos in [pickle.loads(t.numpy()) for t in info]: for episode_undisc_reward, episode_num_steps, num_paths in infos: sum_episode_reward += episode_undisc_reward sum_episode_num_steps += episode_num_steps num_paths_list.append(num_paths) num_infos += 1 if num_infos: tf.summary.scalar('train_debug/episode_undiscounted_reward', sum_episode_reward / num_infos, step=step) tf.summary.scalar('train_debug/episode_num_steps', sum_episode_num_steps / num_infos, step=step) tf.summary.scalar('train_debug/env_num_paths_mean', sum( num_paths_list) / num_infos, step=step) tf.summary.scalar('train_debug/env_num_paths_maximum', max( num_paths_list), step=step) 
model.Model.model|build def _build_model(self): assert self.mode == 'train' or self.mode == 'eval' """Build the core model within the graph.""" with tf.variable_scope('input'): self.x_input = tf.placeholder(tf.float32, shape=[None, 32, 32, 3]) self.y_input = tf.placeholder(tf.int32, shape=None) input_standardized = tf.map_fn(lambda img: tf.image. per_image_standardization(img), self.x_input) x = self._conv('init_conv', input_standardized, 3, 3, 16, self. _stride_arr(1)) strides = [1, 2, 2] activate_before_residual = [True, False, False] res_func = self._residual filters = [16, 160, 320, 640] with tf.variable_scope('unit_1_0'): x = res_func(x, filters[0], filters[1], self._stride_arr(strides[0] ), activate_before_residual[0]) for i in range(1, 5): with tf.variable_scope('unit_1_%d' % i): x = res_func(x, filters[1], filters[1], self._stride_arr(1), False) with tf.variable_scope('unit_2_0'): x = res_func(x, filters[1], filters[2], self._stride_arr(strides[1] ), activate_before_residual[1]) for i in range(1, 5): with tf.variable_scope('unit_2_%d' % i): x = res_func(x, filters[2], filters[2], self._stride_arr(1), False) with tf.variable_scope('unit_3_0'): x = res_func(x, filters[2], filters[3], self._stride_arr(strides[2] ), activate_before_residual[2]) for i in range(1, 5): with tf.variable_scope('unit_3_%d' % i): x = res_func(x, filters[3], filters[3], self._stride_arr(1), False) with tf.variable_scope('unit_last'): x = self._batch_norm('final_bn', x) x = self._relu(x, 0.1) x = self._global_avg_pool(x) with tf.variable_scope('logit'): self.logits = self._fully_connected(x, 10) batch_nums = tf.range(0, limit=tf.shape(self.logits)[0]) indices = tf.stack([batch_nums, self.y_input], axis=1) """cw loss""" self.ground_truth_logits = tf.gather_nd(params=self.logits, indices=indices ) top_2 = tf.nn.top_k(self.logits, k=2) max_indices = tf.where(tf.equal(top_2.indices[:, (0)], self.y_input), top_2.indices[:, (1)], top_2.indices[:, (0)]) max_indices = tf.stack([batch_nums, max_indices], axis=1) max_logits = tf.gather_nd(params=self.logits, indices=max_indices) self.y_cw = max_logits - self.ground_truth_logits self.predictions = tf.argmax(self.logits, 1, output_type=tf.int32) self.correct_prediction = tf.equal(self.predictions, self.y_input) self.num_correct = tf.reduce_sum(tf.cast(self.correct_prediction, tf.int32) ) self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32) ) with tf.variable_scope('costs'): self.y_xent = tf.nn.sparse_softmax_cross_entropy_with_logits(logits =self.logits, labels=self.y_input) self.xent = tf.reduce_sum(self.y_xent, name='y_xent') self.mean_xent = tf.reduce_mean(self.y_xent) self.weight_decay_loss = self._decay() 
nets.dcgan_test.DCGANTest.test|input|invalid|discriminator def test_discriminator_invalid_input(self): wrong_dim_img = tf.zeros([5, 32, 32]) with self.assertRaises(ValueError): dcgan.discriminator(wrong_dim_img) spatially_undefined_shape = tf.placeholder(tf.float32, [5, 32, None, 3]) with self.assertRaises(ValueError): dcgan.discriminator(spatially_undefined_shape) not_square = tf.zeros([5, 32, 16, 3]) with self.assertRaisesRegexp(ValueError, 'not have equal width and height' ): dcgan.discriminator(not_square) not_power_2 = tf.zeros([5, 30, 30, 3]) with self.assertRaisesRegexp(ValueError, 'not a power of 2'): dcgan.discriminator(not_power_2) 
tfprocess.Stats.mean|stddev def stddev_mean(self, name): return math.sqrt(np.var(self.s[name] or [0]) / max(0.0001, len(self.s[ name]) - 1)) 
utils.LoopLogger.step def step(self, values=None): self.update(self.n + 1, values) 
pytorch_pretrained_bert.modeling.BertForMaskedLM.forward def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None): sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False) prediction_scores = self.cls(sequence_output) if masked_lm_labels is not None: loss_fct = CrossEntropyLoss(ignore_index=-1) masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config. vocab_size), masked_lm_labels.view(-1)) return masked_lm_loss else: return prediction_scores 
nets.vgg_test.VGG19Test.test|Points|End def testEndPoints(self): batch_size = 5 height, width = 224, 224 num_classes = 1000 with self.test_session(): inputs = tf.random_uniform((batch_size, height, width, 3)) _, end_points = vgg.vgg_19(inputs, num_classes) expected_names = ['vgg_19/conv1/conv1_1', 'vgg_19/conv1/conv1_2', 'vgg_19/pool1', 'vgg_19/conv2/conv2_1', 'vgg_19/conv2/conv2_2', 'vgg_19/pool2', 'vgg_19/conv3/conv3_1', 'vgg_19/conv3/conv3_2', 'vgg_19/conv3/conv3_3', 'vgg_19/conv3/conv3_4', 'vgg_19/pool3', 'vgg_19/conv4/conv4_1', 'vgg_19/conv4/conv4_2', 'vgg_19/conv4/conv4_3', 'vgg_19/conv4/conv4_4', 'vgg_19/pool4', 'vgg_19/conv5/conv5_1', 'vgg_19/conv5/conv5_2', 'vgg_19/conv5/conv5_3', 'vgg_19/conv5/conv5_4', 'vgg_19/pool5', 'vgg_19/fc6', 'vgg_19/fc7', 'vgg_19/fc8'] self.assertSetEqual(set(end_points.keys()), set(expected_names)) 
classification.ops.fisher_factors.FullyConnectedDiagonalFactor.dtype @property def _dtype(self): return self._outputs_grads[0].dtype 
data_utils_table2text.get|len|sent def _get_sent_len(s, end_id): for i, si in enumerate(s): if si == end_id: break return i 
texar.modules.classifiers.conv_classifiers.Conv1DClassifier.classes|num @property def num_classes(self): """The number of classes. """ return self._num_classes 
elpips.vgg|elpips def elpips_vgg(batch_size=1, n=1, dtype=tf.float32): """E-LPIPS-VGG configuration with all input transformations and dropout with p=0.99. Returns the average result over n samples. Warning: Some versions of TensorFlow might have bugs that make n > 1 problematic due to the tf.while_loop used internally when n > 1.""" config = Config() config.metric = 'vgg_ensemble' config.batch_size = batch_size config.average_over = n config.dtype = dtype return config 
dstn_pooling.get|mul|hot|masked def get_masked_mul_hot(x_input_mul_hot): data_mask = tf.cast(tf.greater(x_input_mul_hot, 0), tf.float32) data_mask = tf.expand_dims(data_mask, axis=3) data_mask = tf.tile(data_mask, (1, 1, 1, k)) data_embed_mul_hot = tf.nn.embedding_lookup(emb_mat, x_input_mul_hot) data_embed_mul_hot_masked = tf.multiply(data_embed_mul_hot, data_mask) return data_embed_mul_hot_masked 
tests.layers.sequence_test.Max|test|K|Pooling def test_KMaxPooling(): with CustomObjectScope({'KMaxPooling': sequence.KMaxPooling}): layer_test(sequence.KMaxPooling, kwargs={'k': 3, 'axis': 1}, input_shape=(BATCH_SIZE, SEQ_LENGTH, EMBEDDING_SIZE, 2)) 
texar.hyperparams.HParams.items def items(self): """Returns the list of hyperparam `(name, value)` pairs """ return iter(self) 
gym_bullet_extensions.bullet_manipulator.ManipulatorInfo.Manipulator|Info def __init__(self, robot_id, joint_ids, joint_names, joint_minpos, joint_maxpos, joint_maxforce, joint_maxvel, ee_link_id, arm_jids_lst, ee_jid, finger_jids_lst, left_ee_link_id=None, left_arm_jids_lst=None, left_ee_jid=None, left_finger_jids_lst=None): self.robot_id = robot_id self.joint_ids = joint_ids self.joint_names = joint_names self.joint_minpos = joint_minpos self.joint_maxpos = joint_maxpos self.joint_maxforce = joint_maxforce self.joint_maxvel = joint_maxvel self.ee_link_id = ee_link_id self.arm_jids_lst = arm_jids_lst self.ee_jid = ee_jid self.finger_jids_lst = finger_jids_lst self.left_ee_link_id = left_ee_link_id self.left_arm_jids_lst = left_arm_jids_lst self.left_ee_jid = left_ee_jid self.left_finger_jids_lst = left_finger_jids_lst self.dof = len(joint_ids) 
texar.modules.embedders.embedders.WordEmbedder.build def _build(self, ids=None, soft_ids=None, mode=None, **kwargs): """Embeds (soft) ids.  Either :attr:`ids` or :attr:`soft_ids` must be given, and they must not be given at the same time.  Args: ids (optional): An integer tensor containing the ids to embed. soft_ids (optional): A tensor of weights (probabilities) used to mix the embedding vectors. mode (optional): A tensor taking value in :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, including `TRAIN`, `EVAL`, and `PREDICT`. If `None`, dropout is controlled by :func:`texar.global_mode`. kwargs: Additional keyword arguments for :tf_main:`tf.nn.embedding_lookup <nn/embedding_lookup>` besides :attr:`params` and :attr:`ids`.  Returns: If :attr:`ids` is given, returns a Tensor of shape `shape(ids) + embedding-dim`. For example, if `shape(ids) = [batch_size, max_time]` and `shape(embedding) = [vocab_size, emb_dim]`, then the return tensor has shape `[batch_size, max_time, emb_dim]`.  If :attr:`soft_ids` is given, returns a Tensor of shape `shape(soft_ids)[:-1] + embdding-dim`. For example, if `shape(soft_ids) = [batch_size, max_time, vocab_size]` and `shape(embedding) = [vocab_size, emb_dim]`, then the return tensor has shape `[batch_size, max_time, emb_dim]`. """ if ids is not None: if soft_ids is not None: raise ValueError( 'Must not specify `ids` and `soft_ids` at the same time.') ids_rank = get_rank(ids) elif soft_ids is not None: ids_rank = get_rank(soft_ids) - 1 else: raise ValueError('Either `ids` or `soft_ids` must be given.') embedding = self._embedding is_training = is_train_mode(mode) if self._hparams.dropout_strategy == 'item_type': dropout_layer = self._get_dropout_layer(self._hparams) if dropout_layer: embedding = dropout_layer.apply(inputs=embedding, training= is_training) if ids is not None: outputs = tf.nn.embedding_lookup(embedding, ids, **kwargs) else: outputs = embedder_utils.soft_embedding_lookup(embedding, soft_ids) if self._hparams.dropout_strategy != 'item_type': dropout_layer = self._get_dropout_layer(self._hparams, ids_rank= ids_rank, dropout_input=outputs) if dropout_layer: outputs = dropout_layer.apply(inputs=outputs, training=is_training) return outputs 
embeddings.OpenKE.models.HolE.HolE.ccorr def _ccorr(self, a, b): a = tf.cast(a, tf.complex64) b = tf.cast(b, tf.complex64) return tf.real(tf.ifft(tf.conj(tf.fft(a)) * tf.fft(b))) 
nets.cyclegan_test.CycleganTest.medium|test|generator|graph def test_generator_graph_medium(self): self._test_generator_graph_helper([3, 128, 128, 3]) 
text_dataflow.image|padding def padding_image(image, padding_size): """ Padding arbitrary-shaped text image to square for tensorflow batch training. """ height, width = image.shape[:2] padding_h = padding_size - height padding_w = padding_size - width padding_top = np.random.randint(padding_h) padding_left = np.random.randint(padding_w) padding_down = padding_h - padding_top padding_right = padding_w - padding_left padding_img = cv2.copyMakeBorder(image, padding_top, padding_down, padding_left, padding_right, borderType=cv2.BORDER_CONSTANT, value= [0, 0, 0]) return padding_img, (padding_top, padding_left, height, width) 
plato.agent.component.dialogue_policy.reinforcement_learning.wolf_phc_policy.WoLFPHCPolicy.action|next def next_action(self, state): """ Consults the dialogue_policy to produce the agent's response  :param state: the current dialogue state :return: a list of dialogue acts, representing the agent's response """ state_enc = self.encode_state(state) self.statistics['total_turns'] += 1 if state_enc not in self.pi or self.is_training and random.random( ) < self.epsilon: if not self.is_training: if not self.pi: print( f""" WARNING! WoLF-PHC pi is empty ({self.agent_role}). Did you load the correct file? """ ) else: print( f'\nWARNING! WoLF-PHC state not found in policy pi ({self.agent_role}).\n' ) if random.random() < 0.35: print('--- {0}: Selecting warmup action.'.format(self.agent_role)) self.statistics['supervised_turns'] += 1 if self.agent_role == 'system': return self.warmup_policy.next_action(state) else: self.warmup_simulator.receive_input(state.user_acts, state. user_goal) return self.warmup_simulator.respond() else: print('--- {0}: Selecting random action.'.format(self.agent_role)) return self.decode_action(random.choice(range(0, self.NActions) ), self.agent_role == 'system') if self.IS_GREEDY_POLICY: max_pi = max(self.pi[state_enc][:-1]) maxima = [i for i, j in enumerate(self.pi[state_enc]) if j == max_pi] if maxima: sys_acts = self.decode_action(random.choice(maxima), self. agent_role == 'system') else: print( '--- {0}: Warning! No maximum value identified for dialogue policy. Selecting random action.' .format(self.agent_role)) return self.decode_action(random.choice(range(0, self.NActions) ), self.agent_role == 'system') else: sys_acts = self.decode_action(random.choices(range(len(self.pi[ state_enc])), self.pi[state_enc])[0], self.agent_role == 'system') return sys_acts 
applications.cambridge_restaurants.camrest_dst.CamRestDST.update|state def update_state(self, dacts): """ Updates the current dialogue state given the input dialogue acts. This function will process the input, query the ludwig model, and retrieve the updated dialogue state.  :param dacts: the input dialogue acts (usually nlu output) :return: the current dialogue state """ self.DState.user_acts = deepcopy(dacts) dst_in = dict() dst_in['dst_prev_area'] = 'none' if self.DState.slots_filled['area']: dst_in['dst_prev_area'] = self.DState.slots_filled['area'] dst_in['dst_prev_food'] = 'none' if self.DState.slots_filled['food']: dst_in['dst_prev_food'] = self.DState.slots_filled['food'] dst_in['dst_prev_pricerange'] = 'none' if self.DState.slots_filled['pricerange']: dst_in['dst_prev_pricerange'] = self.DState.slots_filled['pricerange'] dst_in['nlu_intent'] = 'none' dst_in['req_slot'] = 'none' dst_in['inf_area_value'] = 'none' dst_in['inf_food_value'] = 'none' dst_in['inf_pricerange_value'] = 'none' intents = set() for da in dacts: intents.add(da.intent) for p in da.params: if da.intent == 'inform': if p.slot == 'area': dst_in['inf_area_value'] = p.value elif p.slot == 'food': dst_in['inf_food_value'] = p.value elif p.slot == 'pricerange': dst_in['inf_pricerange_value'] = p.value elif da.intent == 'request': dst_in['req_slot'] = p.slot dst_in['nlu_intent'] = ' '.join(intents) input_data = [dst_in] result = self.model.predict(pd.DataFrame(data=input_data)) area_prediction = result['dst_area_predictions'][0] if area_prediction == 'none': area_prediction = None self.DState.slots_filled['area'] = area_prediction food_prediction = result['dst_food_predictions'][0] if food_prediction == 'none': food_prediction = None self.DState.slots_filled['food'] = food_prediction pricerange_prediction = result['dst_pricerange_predictions'][0] if pricerange_prediction == 'none': pricerange_prediction = None self.DState.slots_filled['pricerange'] = pricerange_prediction req_slot_prediction = result['dst_req_slot_predictions'][0] if req_slot_prediction == 'none': req_slot_prediction = None self.DState.requested_slot = req_slot_prediction self.DState.is_terminal_state = 'bye' in intents self.DState.turn += 1 return self.DState 
text_gcn-master.inits.glorot def glorot(shape, name=None): """Glorot & Bengio (AISTATS 2010) init.""" init_range = np.sqrt(6.0 / (shape[0] + shape[1])) initial = tf.random_uniform(shape, minval=-init_range, maxval= init_range, dtype=tf.float32) return tf.Variable(initial, name=name) 
thumt.utils.optimizers.StaticLossScalingOptimizer.Static|Scaling|Loss|Optimizer def __init__(self, optimizer, scale=128.0, use_locking=False, name= 'StaticLossScalingOptimizer'): super(StaticLossScalingOptimizer, self).__init__(use_locking, name) self._optimizer = optimizer self._scale = scale 
gen.epoch|structured|plot|B|resnet|for def plot_resnet110_structured_B_epoch_for_epoch(): common.epoch_for_epoch_plot(network=common.RESNET110, is_iterative= False, prune_method=common.STRUCTURED_B, min_max_y=(-0.03, 0.01), comparison_points=[(0, 0.9289 - 0.9314), (160 * 0.6272741807, 0.936 - 0.9314)], comparison_err=[0.0043, 0.0025], comparison_label=common. RETHINKING) 
utils.rot def rot(image, label, k): for i in range(image.shape[-1]): image[:, :, (i)] = np.rot90(image[:, :, (i)], k) label = np.rot90(label, k) return image, label 
envs.AntFall.Ant|Fall def __init__(self, use_contexts=False, random_contexts=False, context_range =None): """Initialize the Ant Fall environment.  Parameters ---------- use_contexts : bool, optional specifies whether to add contexts to the observations and add the contextual rewards random_contexts : bool specifies whether the context is a single value, or a random set of values between some range context_range : list of float or list of (float, float) the desired context / goal, or the (lower, upper) bound tuple for each dimension of the goal  Raises ------ AssertionError If the context_range is not the right form based on whether contexts are a single value or random across a range. """ maze_id = 'Fall'  def contextual_reward(states, goals, next_states): return negative_distance(states=states, goals=goals, next_states= next_states, state_indices=[0, 1, 2], relative_context=False, offset=0.0, reward_scales=REWARD_SCALE) super(AntFall, self).__init__(maze_id=maze_id, contextual_reward= contextual_reward, use_contexts=use_contexts, random_contexts= random_contexts, context_range=context_range) 
profiling.ExportingTimer.enter def __enter__(self): self.start_time_s = time.time() return self 
nets.nasnet.nasnet.nasnet|build|mobile def build_nasnet_mobile(images, num_classes, is_training=True, final_endpoint=None, config=None, current_step=None): """Build NASNet Mobile model for the ImageNet Dataset.""" hparams = mobile_imagenet_config() if config is None else copy.deepcopy( config) _update_hparams(hparams, is_training) if tf.test.is_gpu_available() and hparams.data_format == 'NHWC': tf.logging.info( 'A GPU is available on the machine, consider using NCHW data format for increased speed on GPU.' ) if hparams.data_format == 'NCHW': images = tf.transpose(images, [0, 3, 1, 2]) total_num_cells = hparams.num_cells + 2 total_num_cells += 2 normal_cell = nasnet_utils.NasNetANormalCell(hparams.num_conv_filters, hparams.drop_path_keep_prob, total_num_cells, hparams. total_training_steps) reduction_cell = nasnet_utils.NasNetAReductionCell(hparams. num_conv_filters, hparams.drop_path_keep_prob, total_num_cells, hparams.total_training_steps) with arg_scope([slim.dropout, nasnet_utils.drop_path, slim.batch_norm], is_training=is_training): with arg_scope([slim.avg_pool2d, slim.max_pool2d, slim.conv2d, slim .batch_norm, slim.separable_conv2d, nasnet_utils. factorized_reduction, nasnet_utils.global_avg_pool, nasnet_utils.get_channel_index, nasnet_utils.get_channel_dim], data_format=hparams.data_format): return _build_nasnet_base(images, normal_cell=normal_cell, reduction_cell=reduction_cell, num_classes=num_classes, hparams=hparams, is_training=is_training, stem_type= 'imagenet', final_endpoint=final_endpoint, current_step= current_step) 
regression.misc.kl_utils.mf def mf_mf(param1, param2): mean1, logstd1 = param1[0], param1[1] mean2, logstd2 = param2[0], param2[1] return tf.reduce_sum(distributions.kl_divergence(distributions.Normal( mean1, tf.exp(logstd1)), distributions.Normal(mean2, tf.exp(logstd2)))) 
transformer_hparams.batch|transformer|k|chatbot @registry.register_hparams def chatbot_transformer_batch_8k(): hparams = chatbot_cornell_base() hparams.batch_size = 8192 return hparams 
setup_mnist.MNISTModel.__init__.bounded|relu def bounded_relu(x): return K.relu(x, max_value=1) 
tfprocess.Stats.str def str(self): return ', '.join(['{}={:g}'.format(k, np.mean(v or [0])) for k, v in self.s.items()]) 
utils.load|data|mnist def load_mnist_data(): """Load MNIST dataset  Returns: tuple of floats -- img, label for train, valid, test """ from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets('MNIST_data/', one_hot=True) images = np.concatenate([mnist.train.images, mnist.validation.images, mnist.test.images], axis=0) labels = np.concatenate([mnist.train.labels, mnist.validation.labels, mnist.test.labels], axis=0) n_train, n_valid, n_test = 55000, 5000, 10000 x_train = images[:n_train] y_train = labels[:n_train] x_validate = images[n_train:n_train + n_valid] y_validate = labels[n_train:n_train + n_valid] x_test = images[-n_test:] y_test = labels[-n_test:] return x_train, y_train, x_validate, y_validate, x_test, y_test 
fasterai.dataset.get|dummy|databunch def get_dummy_databunch() ->ImageDataBunch: path = Path('./dummy/') return get_colorize_data(sz=1, bs=1, crappy_path=path, good_path=path, keep_pct=0.001) 
commons.utils.inception|data|save def save_inception_data(hparams, phs, theta_ph, x_sample, x_lossy, mdevice, sess, real_vals, inf_net, x_sample_val): y_hat_val_list = [] for _ in range(16): x_sample_val, _ = get_samples(hparams, phs, theta_ph, x_sample, x_lossy, mdevice, sess, real_vals) y_hat_val = inf_net.get_y_hat_val(x_sample_val) y_hat_val_list.append(y_hat_val) data = get_inception_data(y_hat_val_list) basic_utils.save_to_pickle(data, hparams.incpt_pkl) print('Saved inception data') 
code_doc_autogen.get|signature|class def get_class_signature(cls): try: class_signature = get_function_signature(cls.__init__) class_signature = class_signature.replace('__init__', cls.__name__) except (TypeError, AttributeError): class_signature = '{clean_module_name}.{cls_name}()'.format( clean_module_name=clean_module_name(cls.__module__), cls_name= cls.__name__) return post_process_signature(class_signature) 
slac.agents.slac.model_distribution_network.ModelDistributionNetwork.sample|posterior def sample_posterior(self, images, actions, step_types, features=None): sequence_length = step_types.shape[1].value - 1 actions = actions[:, :sequence_length] if features is None: features = self.compressor(images) features = tf.transpose(features, [1, 0, 2]) actions = tf.transpose(actions, [1, 0, 2]) step_types = tf.transpose(step_types, [1, 0]) latent1_dists = [] latent1_samples = [] latent2_dists = [] latent2_samples = [] for t in range(sequence_length + 1): if t == 0: latent1_dist = self.latent1_first_posterior(features[t]) latent1_sample = latent1_dist.sample() latent2_dist = self.latent2_first_posterior(latent1_sample) latent2_sample = latent2_dist.sample() else: reset_mask = tf.equal(step_types[t], ts.StepType.FIRST) latent1_first_dist = self.latent1_first_posterior(features[t]) latent1_dist = self.latent1_posterior(features[t], latent2_samples[t - 1], actions[t - 1]) latent1_dist = nest_utils.map_distribution_structure(functools. partial(tf.where, reset_mask), latent1_first_dist, latent1_dist ) latent1_sample = latent1_dist.sample() latent2_first_dist = self.latent2_first_posterior(latent1_sample) latent2_dist = self.latent2_posterior(latent1_sample, latent2_samples[t - 1], actions[t - 1]) latent2_dist = nest_utils.map_distribution_structure(functools. partial(tf.where, reset_mask), latent2_first_dist, latent2_dist ) latent2_sample = latent2_dist.sample() latent1_dists.append(latent1_dist) latent1_samples.append(latent1_sample) latent2_dists.append(latent2_dist) latent2_samples.append(latent2_sample) latent1_dists = nest_utils.map_distribution_structure(lambda *x: tf. stack(x, axis=1), *latent1_dists) latent1_samples = tf.stack(latent1_samples, axis=1) latent2_dists = nest_utils.map_distribution_structure(lambda *x: tf. stack(x, axis=1), *latent2_dists) latent2_samples = tf.stack(latent2_samples, axis=1) return (latent1_samples, latent2_samples), (latent1_dists, latent2_dists) 
vgg.Vgg16.Vgg def __init__(self, vgg16_npy_path=None): if vgg16_npy_path is None: path = inspect.getfile(Vgg16) path = os.path.abspath(os.path.join(path, os.pardir)) path = os.path.join(path, 'vgg16.npy') vgg16_npy_path = path print(path) self.data_dict = np.load(vgg16_npy_path, encoding='latin1').item() print('npy file loaded') 
nets.pix2pix_test.GeneratorTest.size|conv|test|output|nn|upsample def test_output_size_nn_upsample_conv(self): batch_size = 2 height, width = 256, 256 num_outputs = 4 images = tf.ones((batch_size, height, width, 3)) with tf.contrib.framework.arg_scope(pix2pix.pix2pix_arg_scope()): logits, _ = pix2pix.pix2pix_generator(images, num_outputs, blocks= self._reduced_default_blocks(), upsample_method='nn_upsample_conv') with self.test_session() as session: session.run(tf.global_variables_initializer()) np_outputs = session.run(logits) self.assertListEqual([batch_size, height, width, num_outputs], list (np_outputs.shape)) 
download_speech_corpus.DataArchive.audio|move|all def _move_all_audio(self, archive_root, dest_root): """ Moves all audio files in archive.  Parameters ---------- archive_root : Path root directory of extracted archive. dest_root : Path root directory directories that contain audio files are moved to. """ for directory in self.file_path_filter.filter(filter(lambda file_like: Path.is_dir(file_like), archive_root.iterdir())): dest_dir = dest_root / directory.name self._move_audio_in_dir(directory, dest_dir) 
gym_bullet_extensions.envs.franka_env.FrankaEnv.get|bad|is def get_is_bad(self, debug=False): bad = super(FrankaEnv, self).get_is_bad(debug) return bad 
deepctr.layers.interaction.InnerProductLayer.shape|output|compute def compute_output_shape(self, input_shape): num_inputs = len(input_shape) num_pairs = int(num_inputs * (num_inputs - 1) / 2) input_shape = input_shape[0] embed_size = input_shape[-1] if self.reduce_sum: return input_shape[0], num_pairs, 1 else: return input_shape[0], num_pairs, embed_size 
run_csqa_bert.with|blank|wh|word|replace def replace_wh_word_with_blank(question_str: str): if ( 'What is the name of the government building that houses the U.S. Congress?' in question_str): print() question_str = question_str.replace("What's", 'What is') question_str = question_str.replace('whats', 'what') question_str = question_str.replace('U.S.', 'US') wh_word_offset_matches = [] wh_words = ['which', 'what', 'where', 'when', 'how', 'who', 'why'] for wh in wh_words: if wh == 'who' and 'people who' in question_str: continue m = re.search(wh + '\\?[^\\.]*[\\. ]*$', question_str.lower()) if m: wh_word_offset_matches = [(wh, m.start())] break else: m = re.search(wh + '[ ,][^\\.]*[\\. ]*$', question_str.lower()) if m: wh_word_offset_matches.append((wh, m.start())) if len(wh_word_offset_matches): wh_word_offset_matches.sort(key=lambda x: x[1]) wh_word_found = wh_word_offset_matches[0][0] wh_word_start_offset = wh_word_offset_matches[0][1] question_str = re.sub('\\?$', '.', question_str.strip()) fitb_question = question_str[:wh_word_start_offset ] + BLANK_STR + question_str[wh_word_start_offset + len( wh_word_found):] final = fitb_question.replace(BLANK_STR + ' of the following', BLANK_STR) final = final.replace(BLANK_STR + ' of these', BLANK_STR) return final elif ' them called?' in question_str: return question_str.replace(' them called?', ' ' + BLANK_STR + '.') elif ' meaning he was not?' in question_str: return question_str.replace(' meaning he was not?', ' he was not ' + BLANK_STR + '.') elif ' one of these?' in question_str: return question_str.replace(' one of these?', ' ' + BLANK_STR + '.') elif re.match('.*[^\\.\\?] *$', question_str): return question_str + ' ' + BLANK_STR else: return re.sub(' this[ \\?]', ' ___ ', question_str) 
texar.utils.transformer_attentions.bias|attention|triangle|lower def attention_bias_lower_triangle(length, bias_value=-1e+18): """Create an bias tensor to be added to attention logits. Allows a query to attend to all positions up to and including its own.  Args: length: a scalar.  Returns: a `Tensor` with shape [1, 1, length, length]. """ return attention_bias_local(length, -1, 0, bias_value) 
train_eval.summaries|compute def compute_summaries(metrics, environment, policy, num_episodes=1, num_episodes_to_render=1, images_ph=None, images_summary=None, render_images_summary=None): for metric in metrics: metric.reset() if num_episodes_to_render: environment.start_rendering() time_step = environment.reset() policy_state = policy.get_initial_state(environment.batch_size) render_images = [] if num_episodes_to_render and 'pixels' in time_step.observation: images = [[time_step.observation['pixels']]] else: images = [] step = 0 episode = 0 while episode < num_episodes: action_step = policy.action(time_step, policy_state) next_time_step = environment.step(action_step.action) traj = trajectory.from_transition(time_step, action_step, next_time_step) for observer in metrics: observer(traj) if episode < num_episodes_to_render: if traj.is_last(): render_images.append(list(environment.frames)) environment.frames[:] = [] if episode + 1 >= num_episodes_to_render: environment.stop_rendering() if 'pixels' in time_step.observation: if traj.is_boundary(): images.append([]) images[-1].append(next_time_step.observation['pixels']) episode += np.sum(traj.is_last()) step += np.sum(~traj.is_boundary()) time_step = next_time_step policy_state = action_step.state py_metric.run_summaries(metrics) if render_images: render_images = pad_and_concatenate_videos(render_images) session = tf.compat.v1.get_default_session() session.run(render_images_summary, feed_dict={images_ph: [ render_images]}) if images: images = pad_and_concatenate_videos(images) session = tf.compat.v1.get_default_session() session.run(images_summary, feed_dict={images_ph: [images]}) 
nmt.model_helper.CellWrapper.Wrapper|Cell def __init__(self, cell, input_keep_prob=1.0, global_step=None, seq_len=None): """Create a cell with added input, state, and/or output dropout.  Mask paddings ahead of reversed sequence if seq_len is not None. """ super(CellWrapper, self).__init__() self._cell = cell self._input_keep_prob = input_keep_prob self._global_step = tf.stop_gradient(global_step) if seq_len is not None: self._seq_len = tf.stop_gradient(seq_len) else: self._seq_len = None 
darkflow.dark.convolution.conv_extract_layer.signature @property def signature(self): sig = ['convolutional'] sig += self._signature[1:-2] return sig 
model.Model.get|placeholders def get_placeholders(self, batch_size, num_vertices): inputs_ph = tf.placeholder(tf.float32, shape=(batch_size, num_vertices, 9)) labels_ph = tf.placeholder(tf.int32, shape=(batch_size, num_vertices)) is_training_ph = tf.placeholder(tf.bool, shape=()) return inputs_ph, labels_ph, is_training_ph 
texar.data.data.data_base.DataBase.batch|size @property def batch_size(self): """The batch size. """ return self._hparams.batch_size 
texar.data.vocabulary.Vocab.Vocab def __init__(self, filename, pad_token=SpecialTokens.PAD, bos_token= SpecialTokens.BOS, eos_token=SpecialTokens.EOS, unk_token=SpecialTokens.UNK ): self._filename = filename self._pad_token = pad_token self._bos_token = bos_token self._eos_token = eos_token self._unk_token = unk_token (self._id_to_token_map, self._token_to_id_map, self._id_to_token_map_py, self._token_to_id_map_py) = self.load(self._filename) 
house_parser.R2RHouseParser.get|objects|pano def get_pano_objects(self, pano_name): """Extract the set of objects given a pano.  Only returns the closest object of the same mp40 category and skips any objects with mp40 category in _BANNED_MP40_CAT_INDEX (e.g. misc, void, unlabeled categories).  Args: pano_name: panoromic hash id.  Returns: Dictionary where key is the center of the `RoomObject` and the value is the `RoomObject` named tuple. In particular, `RoomObject.oriented_bbox` is another named tuple (axis0, axis1, radii) containing the axis orientation of the bounding box and the radii of the object along each axis. """ pano_id = self.pano_name_map.get(pano_name, None) if not pano_id: return {} room_objects = {} region_index = self.panos[pano_id].region_index pano_center = self.panos[pano_id].center for object_index in self.region_object_map[region_index]: try: category = self.categories[self.objects[object_index]. category_index] if category.mpcat40_index not in _BANNED_MP40_CAT_INDEX: object_center = self.objects[object_index].center assert object_center not in room_objects, self.objects[ object_index] room_objects[object_center] = RoomObject(object_center, np. linalg.norm(np.array(object_center) - np.array( pano_center)), category.category_mapping_name, category .mpcat40_name, self.objects[object_index].obbox) except KeyError: assert self.objects[object_index].category_index == -1 return room_objects 
enas.cifar10.image_ops.connected|fully def fully_connected(x, out_size, name='fc', seed=None): in_size = x.get_shape()[-1].value with tf.variable_scope(name): w = create_weight('w', [in_size, out_size], seed=seed) x = tf.matmul(x, w) return x 
pytorch_pretrained_bert.modeling_openai.OpenAIGPTMultipleChoiceHead.Open|Multiple|Head|AIGPT|Choice def __init__(self, config): super(OpenAIGPTMultipleChoiceHead, self).__init__() self.n_embd = config.n_embd self.dropout = nn.Dropout2d(config.resid_pdrop) self.linear = nn.Linear(config.n_embd, 1) nn.init.normal_(self.linear.weight, std=0.02) nn.init.normal_(self.linear.bias, 0) 
texar.modules.encoders.hierarchical_encoders_test.HierarchicalRNNEncoderTest.birnn|test|as|encoder|minor def test_encoder_minor_as_birnn(self): """Tests encoder_minor as a BidirectionalRNNEncoder """ hparams = {'encoder_minor_type': 'BidirectionalRNNEncoder', 'encoder_minor_hparams': {'rnn_cell_fw': {'type': 'LSTMCell', 'kwargs': {'num_units': 100}}}, 'encoder_major_hparams': { 'rnn_cell': {'type': 'LSTMCell', 'kwargs': {'num_units': 200}}}} encoder = HierarchicalRNNEncoder(hparams=hparams) batch_size = 16 max_major_time = 8 max_minor_time = 6 dim = 10 inputs = tf.random_uniform([batch_size, max_major_time, max_minor_time, dim], maxval=1, minval=-1, dtype=tf.float32) outputs, _ = encoder(inputs) self.assertEqual(list(outputs.shape), [16, 8, 200]) 
generate_TrainNMD.imread def imread(path): img = scipy.misc.imread(path) return img 
avod.core.bev_generators.bev_slices.BevSlices.Bev|Slices def __init__(self, config, kitti_utils): """BEV maps created using slices of the point cloud.  Args: config: bev_generator protobuf config kitti_utils: KittiUtils object """ self.height_lo = config.height_lo self.height_hi = config.height_hi self.num_slices = config.num_slices self.kitti_utils = kitti_utils self.height_per_division = (self.height_hi - self.height_lo ) / self.num_slices 
text_gcn-master.utils.sparse|tuple|to def sparse_to_tuple(sparse_mx): """Convert sparse matrix to tuple representation."""  def to_tuple(mx): if not sp.isspmatrix_coo(mx): mx = mx.tocoo() coords = np.vstack((mx.row, mx.col)).transpose() values = mx.data shape = mx.shape return coords, values, shape if isinstance(sparse_mx, list): for i in range(len(sparse_mx)): sparse_mx[i] = to_tuple(sparse_mx[i]) else: sparse_mx = to_tuple(sparse_mx) return sparse_mx 
creat_BERT_embedding.load|labels|data|and def load_data_and_labels(positive_data_file): """ Loads MR polarity data from files, splits the data into words and generates labels. Returns split sentences and labels. """ examples = list(open(positive_data_file, 'r').readlines()) examples = [s.strip() for s in examples] input = [] target = [] for index, i in enumerate(examples): if index % 3 == 0: i_target = examples[index + 1].strip() i = i.replace('$T$', i_target) input.append(i) target.append(i_target) x_text = input lable = [] for index, i in enumerate(examples): if index % 3 == 2: if i[0:1] == '1': lable.append([1, 0, 0]) if i[0:1] == '0': lable.append([0, 1, 0]) if i[0:1] == '-': lable.append([0, 0, 1]) y = np.array(lable) return [x_text, target, y] 
seld_dcase2019_master.metrics.evaluation_metrics.SELDMetrics.er|overall|sec def er_overall_1sec(self, O, T): new_size = int(O.shape[0] / self._block_size) O_block = np.zeros((new_size, O.shape[1])) T_block = np.zeros((new_size, O.shape[1])) for i in range(0, new_size): O_block[(i), :] = np.max(O[int(i * self._block_size):int(i * self. _block_size + self._block_size - 1), :], axis=0) T_block[(i), :] = np.max(T[int(i * self._block_size):int(i * self. _block_size + self._block_size - 1), :], axis=0) return self.er_overall_framewise(O_block, T_block) 
craystack.rans.extend|cons|list def cons_list_extend(ls, els): for el in els[::-1]: ls = el, ls return ls 
make_tfrecord_scannet.feature|float def _float_feature(value): """Returns a float_list from a float / double.""" return tf.train.Feature(float_list=tf.train.FloatList(value=[value])) 
pvae.pixelvae_bbans_two_layer.codec|post|elem def post1_elem_codec(params, idx): return cs.substack(codecs.DiagGaussian_GaussianBins(params[..., 0], params[..., 1], params[..., 2], params[..., 3], q_precision, prior_precision), lambda head: head[idx]) 
test_policy.TestActorCriticPolicy.tear|Down def tearDown(self): self.policy_params['sess'].close() del self.policy_params 
shuffle_exchange_network.switch_layer.linear def linear2(input, suffix, bias_start, in_units, out_units): input = tf.reshape(input, [batch_size, length // 2, in_units * 2]) res = conv_linear(input, kernel_width, in_units * 2, out_units * 2, bias_start, prefix + '/' + suffix) res = tf.reshape(res, [batch_size, length, out_units]) return res 
smoothness_prior.smoothness|norm def smoothness_norm(T, theta, lambda_smooth=0.5, lambda_var=0.1, print_info =False): D, d = T.get_basis().shape B = T.get_basis() nC = d + 1 n = 1 B = tf.cast(B, tf.float32) theta = tf.cast(theta, tf.float32) theta_T = tf.transpose(theta) covariance_to_plot = {} items_to_plot = {} centers = tf.lin_space(-1.0, 1.0, D) centers = tf.expand_dims(centers, 1) dists = tf_dist_mat(centers) cov_avees = tf.exp(-(dists / lambda_smooth)) cov_avees *= cov_avees * (lambda_var * D) ** 2 B_T = tf.transpose(B) cov_cpa = tf.matmul(B_T, tf.matmul(cov_avees, B)) precision_theta = tf.linalg.inv(cov_cpa) if print_info: print('Info: ') print('    Distance between centers shape:', dists.shape) print('    B shape:', B.shape) print('    D shape:', D) print('    d shape:', d) print('    cov_avess shape:', cov_avees.shape) print('    cov_cpa shape:', cov_cpa.shape) print('    theta shape:', theta.shape) plot_velocity_field = False if plot_velocity_field: nb_points = 1000 points = T.uniform_meshgrid([nb_points for i in range(T._ndim)]) vector_field = T.calc_vectorfield(points, theta_T.eval()) items_to_plot['CPA Velocity Field for theta with prior'] = vector_field items_to_plot['Theta values for: theta with prior'] = theta_T theta = tf.squeeze(theta) theta_T = tf.transpose(theta) smooth_norm = tf.matmul(theta, tf.matmul(precision_theta, theta_T)) smooth_norm = 0.1 * tf.reduce_mean(smooth_norm) return smooth_norm 
darkflow.dark.darkop.reorg_layer.setup def setup(self, stride): self.stride = stride 
ge.alias.alias|table|create def create_alias_table(area_ratio): """  :param area_ratio: sum(area_ratio)=1 :return: accept,alias """ l = len(area_ratio) accept, alias = [0] * l, [0] * l small, large = [], [] area_ratio_ = np.array(area_ratio) * l for i, prob in enumerate(area_ratio_): if prob < 1.0: small.append(i) else: large.append(i) while small and large: small_idx, large_idx = small.pop(), large.pop() accept[small_idx] = area_ratio_[small_idx] alias[small_idx] = large_idx area_ratio_[large_idx] = area_ratio_[large_idx] - (1 - area_ratio_[ small_idx]) if area_ratio_[large_idx] < 1.0: small.append(large_idx) else: large.append(large_idx) while large: large_idx = large.pop() accept[large_idx] = 1 while small: small_idx = small.pop() accept[small_idx] = 1 return accept, alias 
neural_tangents.utils.utils.get_namedtuple.getter_decorator.getter|fn @wraps(fn) def getter_fn(*args, **kwargs): canonicalized_args = list(args) if 'get' in kwargs: get_is_not_tuple, get = canonicalize_get(kwargs['get']) kwargs['get'] = get elif get_index < len(args): get_is_not_tuple, get = canonicalize_get(args[get_index]) canonicalized_args[get_index] = get elif defaults is None: raise ValueError( '`get_namedtuple` function must have a `get` argument provided orset by default.' ) else: get_is_not_tuple, get = canonicalize_get(defaults[get_index - len( args)]) fn_out = fn(*canonicalized_args, **kwargs) if get is None: if isinstance(fn_out, dict): ReturnType = named_tuple_factory(name, tuple(fn_out.keys())) fn_out = ReturnType(*fn_out.values()) return fn_out fn_out = _output_to_dict(fn_out) if get_is_not_tuple: if isinstance(fn_out, types.GeneratorType): return (output[get[0]] for output in fn_out) else: return fn_out[get[0]] ReturnType = named_tuple_factory(name, get) if isinstance(fn_out, types.GeneratorType): return (ReturnType(*tuple(output[g] for g in get)) for output in fn_out ) else: return ReturnType(*tuple(fn_out[g] for g in get)) 
run_lm_finetuning.random|word def random_word(tokens, tokenizer): """ Masking some random tokens for Language Model task with probabilities as in the original BERT paper. :param tokens: list of str, tokenized sentence. :param tokenizer: Tokenizer, object used for tokenization (we need it's vocab here) :return: (list of str, list of int), masked tokens and related labels for LM prediction """ output_label = [] for i, token in enumerate(tokens): prob = random.random() if prob < 0.15: prob /= 0.15 if prob < 0.8: tokens[i] = '[MASK]' elif prob < 0.9: tokens[i] = random.choice(list(tokenizer.vocab.items()))[0] try: output_label.append(tokenizer.vocab[token]) except KeyError: output_label.append(tokenizer.vocab['[UNK]']) logger.warning( "Cannot find token '{}' in vocab. Using [UNK] insetad". format(token)) else: output_label.append(-1) return tokens, output_label 
w1_model.W1.gloss|calc def calc_gloss(self, x, y, ux, vy, config): return torch.mean(vy) 
src.util.NN.tuple|dropout def tuple_dropout(x, prob, istraining): x1, x2 = x x1 = tf.layers.dropout(x1, prob, istraining) x2 = tf.layers.dropout(x2, prob, istraining) x = x1, x2 return x 
cifar10.CIFAR10Dataset.get|example def get_example(self, i): image = np.asarray(self.dset[i][0] / 128.0 - 1.0, np.float32) image += np.random.uniform(size=image.shape, low=0.0, high=1.0 / 128) return image, self.dset[i][1] 
seed_rl-master.grpc.python.ops_test.OpsTest.variable|out|test|of|scope def test_variable_out_of_scope(self): address = self.get_unix_address() server = ops.Server([address])  def bind(): a = tf.Variable(1)  @tf.function(input_signature=[tf.TensorSpec([], tf.int32)]) def foo(x): return x + a server.bind(foo) bind() server.start() client = ops.Client(address) self.assertAllEqual(43, client.foo(42)) server.shutdown() 
facenet-master.src.lfw.pairs|read def read_pairs(pairs_filename): pairs = [] with open(pairs_filename, 'r') as f: for line in f.readlines()[1:]: pair = line.strip().split() pairs.append(pair) return np.array(pairs) 
commons.measure_utils.get_inpaint_func_tv.func|inpaint def inpaint_func(image, mask): """Total variation inpainting""" inpainted = np.zeros_like(image) for c in range(image.shape[2]): image_c = image[:, :, (c)] mask_c = mask[:, :, (c)] if np.min(mask_c) > 0: inpainted[:, :, (c)] = image_c else: h, w = image_c.shape inpainted_c_var = cvxpy.Variable(h, w) obj = cvxpy.Minimize(cvxpy.tv(inpainted_c_var)) constraints = [cvxpy.mul_elemwise(mask_c, inpainted_c_var) == cvxpy.mul_elemwise(mask_c, image_c)] prob = cvxpy.Problem(obj, constraints) prob.solve() inpainted[:, :, (c)] = inpainted_c_var.value return inpainted 
deepMOT-master.utils.loss.Matrix|create def createMatrix(index_h, index_w, size_h, size_w): """ create a tensor variable fo size [size_h, size_w] having all zeros except for [index_h, index_w] :param index_h: list of indexes :param index_w:  list of indexes :return: a matrix, tensor variable """ matrix = torch.zeros(1, size_h, size_w).cuda() matrix[0, index_h, index_w] = 1.0 return matrix 
model.fpn|build|unet|graph|coords def build_fpn_coords_unet_graph(rois, feature_maps, image_shape, pool_size, num_classes, use_bn): """Builds the computation graph of the coordinate map head of Feature Pyramid Network.  rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized coordinates. feature_maps: List of feature maps from different layers of the pyramid, [P2, P3, P4, P5]. Each has a different resolution. image_shape: [height, width, depth] pool_size: The width of the square feature map generated from ROI Pooling. num_classes: number of classes, which determines the depth of the results  Returns: Coordinate maps [batch, roi_count, height, width, num_classes, 3] """ x = PyramidROIAlign([pool_size, pool_size], image_shape, name= 'roi_align_coord_unet')([rois] + feature_maps) x = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding='same'), name= 'mrcnn_coord_unet_conv1')(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_unet_bn1' )(x) conv1_output = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.MaxPool2D(pool_size=(2, 2)), name= 'mrcnn_coord_maxpool1')(conv1_output) x = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding='same'), name= 'mrcnn_coord_unet_conv2')(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_unet_bn2' )(x) conv2_output = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.MaxPool2D(pool_size=(2, 2)), name= 'mrcnn_coord_unet_maxpool2')(conv2_output) x = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding='same'), name= 'mrcnn_coord_unet_conv3')(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_unet_bn3' )(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding='same'), name= 'mrcnn_coord_unet_conv4')(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_unet_bn4' )(x) x = KL.Activation('relu')(x) deconv1_output = KL.TimeDistributed(KL.Conv2DTranspose(512, (2, 2), strides=2, activation='relu'), name='mrcnn_coord_unet_deconv1')(x) x = KL.Add(name='mrcnn_coord_unet_cadd1')([conv2_output, deconv1_output]) x = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding='same'), name= 'mrcnn_coord_unet_conv5')(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_unet_bn5' )(x) x = KL.Activation('relu')(x) deconv2_output = KL.TimeDistributed(KL.Conv2DTranspose(512, (2, 2), strides=2, activation='relu'), name='mrcnn_coord_unet_deconv2')(x) x = KL.Add(name='mrcnn_coord_unet_add2')([conv1_output, deconv2_output]) x = KL.TimeDistributed(KL.Conv2D(512, (3, 3), padding='same'), name= 'mrcnn_coord_unet_conv6')(x) if use_bn: x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_unet_bn6' )(x) x_feat = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(3 * num_classes, (1, 1), strides=1), name='mrcnn_coord_unet_conv_unet_final')(x_feat) x = KL.Activation('sigmoid', name='mrcnn_coord_unet_unet_sigmoid')(x) x = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], tf.shape(t)[1], tf.shape(t)[2], tf.shape(t)[3], -1, 3]), name='mrcnn_coord_unet')(x) mrcnn_coord_x = KL.Lambda(lambda x: x[:, :, :, :, :, (0)], name= 'mrcnn_coord_x')(x) mrcnn_coord_y = KL.Lambda(lambda x: x[:, :, :, :, :, (1)], name= 'mrcnn_coord_y')(x) mrcnn_coord_z = KL.Lambda(lambda x: x[:, :, :, :, :, (2)], name= 'mrcnn_coord_z')(x) return mrcnn_coord_x, mrcnn_coord_y, mrcnn_coord_z, x_feat 
tf_util.get|learning|rate def get_learning_rate(batch, BASE_LEARNING_RATE, BATCH_SIZE, DECAY_STEP, DECAY_RATE): learning_rate = tf.train.exponential_decay(BASE_LEARNING_RATE, batch * BATCH_SIZE, DECAY_STEP, DECAY_RATE, staircase=True) learning_rate = tf.maximum(learning_rate, 1e-05) return learning_rate 
gym_pycolab.envs.pycolab_env.PycolabEnv.step def step(self, action): observation, reward, discount = self.game.play(actions=action) if reward is None: reward = 0.0 game_over = discount == 0.0 state = self._format_observation(observation.board) self.state = self._concat_obs(state) info = {'vectorized.episode_id': self.episode_id} return self.state, reward, game_over, info 
pytorch_pretrained_bert.modeling.BertSelfOutput.Self|Output|Bert def __init__(self, config): super(BertSelfOutput, self).__init__() self.dense = nn.Linear(config.hidden_size, config.hidden_size) self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12) self.dropout = nn.Dropout(config.hidden_dropout_prob) 
utils.utils.get|lr def get_lr(fstep, opt_config): if opt_config['learning_rate_schedule'] == 'static': lr = opt_config['static_lr'] else: lr = opt_config['lr_constant'] * min(1.0, fstep / opt_config[ 'warmup_steps']) * (1 / math.sqrt(max(fstep, opt_config[ 'warmup_steps']))) return lr 
darkflow.dark.layer.Layer.signature @property def signature(self): return self._signature 
plato.agent.component.user_simulator.agenda_based_user_simulator.agenda.Agenda.initialize def initialize(self, goal, us_has_initiative=False): """ Initialize the Agenda at the beginning of each dialogue  :param goal: the new goal for the current dialogue :param us_has_initiative: if the simulator has the initiative at the first turn :return: nothing """ self.goal = goal self.clear() dacts = [] for i in range(len(self.goal.subgoals) - 1, -1, -1): sg = self.goal.subgoals[i] dacts.append(DialogueAct('ack_subgoal', [])) for constr in sg.constraints.values(): dacts.append(DialogueAct('inform', [constr])) for req in goal.requests.values(): dacts.append(DialogueAct('request', [req])) for constr in goal.constraints.values(): dacts.append(DialogueAct('inform', [constr])) self.push(DialogueAct('bye', [])) for da in dacts: self.push(da, force=True) if us_has_initiative: self.push(DialogueAct('hello', [])) 
neural_tangents.predict._eigen_fns.transform. def _(vec, dt): return np.einsum('ji,i,ki,k...->j...', evecs, fn(evals, dt), evecs, vec, optimize=True) 
xlnet-master.data_utils.convert|example def _convert_example(example, use_bfloat16): """Cast int64 into int32 and float32 to bfloat16 if use_bfloat16.""" for key in list(example.keys()): val = example[key] if tf.keras.backend.is_sparse(val): val = tf.sparse.to_dense(val) if val.dtype == tf.int64: val = tf.cast(val, tf.int32) if use_bfloat16 and val.dtype == tf.float32: val = tf.cast(val, tf.bfloat16) example[key] = val 
slac.agents.slac.model_distribution_network.Bernoulli.Bernoulli def __init__(self, base_depth, name=None): super(Bernoulli, self).__init__(name=name) self.dense1 = tf.keras.layers.Dense(base_depth, activation=tf.nn.leaky_relu ) self.dense2 = tf.keras.layers.Dense(base_depth, activation=tf.nn.leaky_relu ) self.output_layer = tf.keras.layers.Dense(1) 
train.load|weights def load_weights(saver, model_dir): ckpt = tf.train.get_checkpoint_state(model_dir) if ckpt and ckpt.model_checkpoint_path: ckpt_name = os.path.basename(ckpt.model_checkpoint_path) saver.restore(sess, os.path.join(model_dir, ckpt_name)) print('MODEL LOADED SUCCESSFULLY') else: print('LOADING MODEL FAILED') 
seld_dcase2019_master.cls_feature_class.FeatureClass.spectrogram def _spectrogram(self, audio_input): _nb_ch = audio_input.shape[1] nb_bins = self._nfft // 2 spectra = np.zeros((self._max_frames, nb_bins, _nb_ch), dtype=complex) for ch_cnt in range(_nb_ch): stft_ch = librosa.core.stft(audio_input[:, (ch_cnt)], n_fft=self. _nfft, hop_length=self._hop_len, win_length=self._win_len, window='hann') spectra[:, :, (ch_cnt)] = stft_ch[1:, :self._max_frames].T return spectra 
utils.data_utils.SSTProcessor.get|examples|dev def get_dev_examples(self, data_dir): """See base class.""" return self._create_examples(self._read_tsv(os.path.join(data_dir, 'dev.tsv')), 'dev') 
xlnet-master.tpu_estimator._CapturingContext.Add|Op def AddOp(self, op): for c in op.inputs: if tpu._TPU_REPLICATE_ATTR in c.op.node_def.attr: raise ValueError( '{}: Op {} depends on TPU computation {}, which is not allowed.' .format(self._message, op, c)) 
curriculum_env_test.CurriculumEnvTest.test|Curriculum|Building def testCurriculumBuilding(self): config = curriculum_env_config_lib.get_default_curriculum_env_config( 'constant-1-1', self._env_config) self._env = curriculum_env.CurriculumR2REnv(data_sources=['small_split' ], runtime_config=self._runtime_config, curriculum_env_config=config) self.assertLen(self._env._paths, 1) for i in range(2, 7): _ = self._env.reset() self._check_paths_order() self._check_sorted(name2cmp['path_len,inst_len']) self.assertLen(self._env._paths, i) for _ in range(100): _ = self._env.reset() self._check_paths_order() self._check_sorted(name2cmp['path_len,inst_len']) self.assertLen(self._env._paths, 6) 
nmt.distributed_iterator_utils.distributed|pipeline|make def _make_distributed_pipeline(hparams, num_hosts): """Makes the distributed input pipeline.  make_distributed_pipeline must be used in the PER_HOST_V1 configuration.  Note: we return both the input function and the hook because MultiDeviceIterator is not compatible with Estimator / TPUEstimator.  Args: hparams: The hyperparameters to use. num_hosts: The number of hosts we're running across.  Returns: A MultiDeviceIterator. """ global_batch_size = hparams.batch_size if global_batch_size % num_hosts != 0: raise ValueError( 'global_batch_size (%s) must be a multiple of num_hosts (%s)' % (global_batch_size, num_hosts)) if hparams.choose_buckets: window_batch_size = int(global_batch_size / hparams.choose_buckets) else: window_batch_size = global_batch_size per_host_batch_size = global_batch_size / num_hosts output_buffer_size = global_batch_size * 100 resolver = low_level_runner.get_resolver(hparams) assert resolver job_name = resolver.get_job_name() or 'tpu_worker' with tf.device('/job:%s/task:0/cpu:0' % job_name): src_file = '%s.%s' % (hparams.train_prefix, hparams.src) tgt_file = '%s.%s' % (hparams.train_prefix, hparams.tgt) src_vocab_file = hparams.src_vocab_file tgt_vocab_file = hparams.tgt_vocab_file src_vocab_table, tgt_vocab_table = vocab_utils.create_vocab_tables( src_vocab_file, tgt_vocab_file, hparams.share_vocab) src_dataset = tf.data.TextLineDataset(src_file).prefetch( output_buffer_size) tgt_dataset = tf.data.TextLineDataset(tgt_file).prefetch( output_buffer_size) mlperf_log.gnmt_print(key=mlperf_log.INPUT_BATCH_SIZE, value= global_batch_size) mlperf_log.gnmt_print(key=mlperf_log.TRAIN_HP_MAX_SEQ_LEN, value= hparams.src_max_len) sos = hparams.sos eos = hparams.eos random_seed = hparams.random_seed num_buckets = hparams.num_buckets src_max_len = hparams.src_max_len tgt_max_len = hparams.tgt_max_len num_parallel_calls = 100 skip_count = None reshuffle_each_iteration = True use_char_encode = hparams.use_char_encode filter_oversized_sequences = True if use_char_encode: src_eos_id = vocab_utils.EOS_CHAR_ID else: src_eos_id = tf.cast(src_vocab_table.lookup(tf.constant(eos)), tf.int32) tgt_sos_id = tf.cast(tgt_vocab_table.lookup(tf.constant(sos)), tf.int32 ) tgt_eos_id = tf.cast(tgt_vocab_table.lookup(tf.constant(eos)), tf.int32 ) src_tgt_dataset = tf.data.Dataset.zip((src_dataset, tgt_dataset)) mlperf_log.gnmt_print(key=mlperf_log.INPUT_SHARD, value=1) if skip_count is not None: src_tgt_dataset = src_tgt_dataset.skip(skip_count)  def map_fn_1(src, tgt): src = tf.string_split([src]).values tgt = tf.string_split([tgt]).values src_size = tf.size(src) tgt_size = tf.size(tgt) size_ok_bool = tf.logical_and(src_size > 0, tgt_size > 0) if filter_oversized_sequences: oversized = tf.logical_and(src_size < src_max_len, tgt_size < tgt_max_len) size_ok_bool = tf.logical_and(size_ok_bool, oversized) if src_max_len: src = src[:src_max_len] if tgt_max_len: tgt = tgt[:tgt_max_len] return src, tgt, size_ok_bool src_tgt_bool_dataset = src_tgt_dataset.map(map_fn_1, num_parallel_calls=num_parallel_calls) src_tgt_bool_dataset = src_tgt_bool_dataset.filter(lambda src, tgt, filter_bool: filter_bool)  def map_fn_2(src, tgt, unused_filter_bool): if use_char_encode: src = tf.reshape(vocab_utils.tokens_to_bytes(src), [-1]) tgt = tf.cast(tgt_vocab_table.lookup(tgt), tf.int32) else: src = tf.cast(src_vocab_table.lookup(src), tf.int32) tgt = tf.cast(tgt_vocab_table.lookup(tgt), tf.int32) tgt_in = tf.concat(([tgt_sos_id], tgt), 0) tgt_out = tf.concat((tgt, [tgt_eos_id]), 0) if use_char_encode: src_len = tf.to_int32(tf.size(src) / vocab_utils. DEFAULT_CHAR_MAXLEN) else: src_len = tf.size(src) tgt_len = tf.size(tgt_in) return src, tgt_in, tgt_out, src_len, tgt_len mlperf_log.gnmt_print(key=mlperf_log.PREPROC_TOKENIZE_TRAINING) src_tgt_dataset = src_tgt_bool_dataset.map(map_fn_2, num_parallel_calls=num_parallel_calls) src_tgt_dataset = src_tgt_dataset.prefetch(output_buffer_size) src_tgt_dataset = src_tgt_dataset.cache() src_tgt_dataset = src_tgt_dataset.shuffle(output_buffer_size, random_seed, reshuffle_each_iteration).repeat()  def batching_func(x): return x.padded_batch(window_batch_size, padded_shapes=(tf. TensorShape([src_max_len]), tf.TensorShape([tgt_max_len]), tf.TensorShape([tgt_max_len]), tf.TensorShape([]), tf. TensorShape([])), padding_values=(src_eos_id, tgt_eos_id, tgt_eos_id, 0, 0), drop_remainder=True)  def key_func(unused_1, unused_2, unused_3, src_len, tgt_len): """Calculate bucket_width by maximum source sequence length.""" if src_max_len: bucket_width = (src_max_len + num_buckets - 1) // num_buckets else: bucket_width = 10 bucket_id = tf.maximum(src_len // bucket_width, tgt_len // bucket_width) return tf.to_int64(tf.minimum(num_buckets, bucket_id))  def reduce_func(unused_key, windowed_data): return batching_func(windowed_data) if num_buckets > 1: batched_dataset = src_tgt_dataset.apply(tf.contrib.data. group_by_window(key_func=key_func, reduce_func=reduce_func, window_size=window_batch_size)) else: batched_dataset = batching_func(src_tgt_dataset) batched_dataset = batched_dataset.map(lambda src, tgt_in, tgt_out, source_size, tgt_in_size: {'source': src, 'target_input': tgt_in, 'target_output': tgt_out, 'source_sequence_length': source_size, 'target_sequence_length': tgt_in_size}) re_batched_dataset = batched_dataset.apply(tf.contrib.data.unbatch() ).batch(int(per_host_batch_size), drop_remainder=True) output_devices = [('/job:%s/task:%d/cpu:0' % (job_name, i)) for i in range(num_hosts)] options = tf.data.Options() options.experimental_numa_aware = True options.experimental_filter_fusion = True options.experimental_map_and_filter_fusion = True re_batched_dataset = re_batched_dataset.with_options(options) multi_device_iterator = multi_device_iterator_ops.MultiDeviceIterator( dataset=re_batched_dataset, devices=output_devices, max_buffer_size=10, prefetch_buffer_size=10, source_device= '/job:%s/task:0/cpu:0' % job_name) return multi_device_iterator 
deepctr.layers.normalization.LayerNormalization.shape|output|compute def compute_output_shape(self, input_shape): return input_shape 
resblocks.Block.Block def __init__(self, in_channels, out_channels, hidden_channels=None, ksize=3, pad=1, activation=F.relu, downsample=False): super(Block, self).__init__() initializer = chainer.initializers.GlorotUniform(math.sqrt(2)) initializer_sc = chainer.initializers.GlorotUniform() self.activation = activation self.downsample = downsample self.learnable_sc = in_channels != out_channels or downsample hidden_channels = (in_channels if hidden_channels is None else hidden_channels) with self.init_scope(): self.c1 = SNConvolution2D(in_channels, hidden_channels, ksize=ksize, pad=pad, initialW=initializer) self.c2 = SNConvolution2D(hidden_channels, out_channels, ksize= ksize, pad=pad, initialW=initializer) if self.learnable_sc: self.c_sc = SNConvolution2D(in_channels, out_channels, ksize=1, pad=0, initialW=initializer_sc) 
viz.MyInteractorStyle.key|Press|Event def keyPressEvent(self, obj, event): key = self.parent.GetKeySym() if key == '+': point_size = self.pointcloud.vtkActor.GetProperty().GetPointSize() self.pointcloud.vtkActor.GetProperty().SetPointSize(point_size + 1) print(str(point_size) + ' ' + key) return 
facenet-master.contributed.face.Identifier.identify def identify(self, face): if face.embedding is not None: predictions = self.model.predict_proba([face.embedding]) best_class_indices = np.argmax(predictions, axis=1) return self.class_names[best_class_indices[0]] 
eval_segm.masks|extract def extract_masks(segm, cl, n_cl): h, w = segm_size(segm) masks = np.zeros((n_cl, h, w)) for i, c in enumerate(cl): masks[(i), :, :] = segm == c return masks 
texar.core.layers.kwargs|default|dense def default_dense_kwargs(): """Returns the default keyword argument values of the constructor of the dense layer class :tf_main:`tf.layers.Dense <layers/Dense>`.  .. code-block:: python  { "units": 256, "activation": "identity", "use_bias": True, "kernel_initializer": { "type": "glorot_uniform_initializer", "kwargs": {} }, "bias_initializer": { "type": "zeros_initializer", "kwargs": {} }, "kernel_regularizer": { "type": "L1L2", "kwargs": { "l1": 0., "l2": 0. } }, "bias_regularizer": { # same as in "kernel_regularizer" # ... }, "activity_regularizer": { # same as in "kernel_regularizer" # ... }, "kernel_constraint": None, "bias_constraint": None, "trainable": True, "name": None } """ kwargs = _common_default_conv_dense_kwargs() kwargs.update({'units': 256}) return kwargs 
classification.train._ECELoss.ECE|Loss def __init__(self, n_bins=15): """ n_bins (int): number of confidence interval bins """ super(_ECELoss, self).__init__() bin_boundaries = torch.linspace(0, 1, n_bins + 1) self.bin_lowers = bin_boundaries[:-1] self.bin_uppers = bin_boundaries[1:] bin_boundaries_plot = torch.linspace(0, 1, 11) self.bin_lowers_plot = bin_boundaries_plot[:-1] self.bin_uppers_plot = bin_boundaries_plot[1:] 
utils.helper.load|graph|ransac def load_ransac_graph(dataset, data_path): q_RANSAC_graph = np.load(os.path.join(data_path, 'graphs', '{}_query_ransac_graph.npy'.format(dataset))) x_RANSAC_graph = np.load(os.path.join(data_path, 'graphs', '{}_index_ransac_graph.npy'.format(dataset))) return q_RANSAC_graph, x_RANSAC_graph 
stax_test.FlattenTest.first|test|flatten def test_flatten_first(self, same_inputs): key = random.PRNGKey(1) X0_1 = random.normal(key, (5, 4, 3, 2)) X0_2 = None if same_inputs else random.normal(key, (3, 4, 3, 2)) X0_1_flat = np.reshape(X0_1, (X0_1.shape[0], -1)) X0_2_flat = None if same_inputs else np.reshape(X0_2, (X0_2.shape[0], -1)) _, _, fc_flat = stax.serial(stax.Dense(10, 2.0, 0.5), stax.Erf()) _, _, fc = stax.serial(stax.Flatten(), stax.Dense(10, 2.0, 0.5), stax.Erf() ) K_flat = fc_flat(X0_1_flat, X0_2_flat) K = fc(X0_1, X0_2) self.assertAllClose(K_flat, K, True) 
regression.ops.emvg_optimizer.EMVGOptimizer.basis|update def update_basis(self, w, w_grad, a, s_grad): a_t = tf.transpose(a, [0, 2, 1]) fish_u = tf.reduce_mean(tf.matmul(a_t, a), [0]) / tf.to_float(tf.shape( a)[1]) infer2 = self._fisher_u.assign((1.0 - self.beta) * self._fisher_u + self.beta * fish_u) s_grad_t = tf.transpose(s_grad, [0, 2, 1]) fish_v = tf.reduce_mean(tf.matmul(s_grad_t, s_grad), [0]) / tf.to_float(tf .shape(s_grad)[1]) infer3 = self._fisher_v.assign((1.0 - self.beta) * self._fisher_v + self.beta * fish_v) return [infer2, infer3] 
deepMOT-master.utils.sot_utils.to|im|torch def im_to_torch(img): img = np.transpose(img, (2, 0, 1)) img = to_torch(img) return img 
gan.latent.JointLatent.out|reg|dim @property def reg_out_dim(self): return self._reg_out_dim 
download_speech_corpus.ExtensionList.directory|itemize|in def itemize_in_directory(self, directory): """ Search for audio files with the designated extensions in the directory.  Parameters ---------- directory : Path The path of the directory where audio files are searched for.  Returns ------- audio_paths : Generator[Path, None, None] paths of audio files. """ for extension in self.extensions: yield from directory.glob('*.' + extension) 
train_mnist_vae.define_VAE.sample|z def sample_z(args): mu, logsigma = args return 0.5 * K.exp(logsigma / 2) * K.random_normal(shape=(K.shape(mu)[0 ], latent_dim)) + mu 
plato.dialogue.state.SlotFillingDialogueState.State|Slot|Dialogue|Filling def __init__(self, args): """ Initialize the Slot Filling dialogue State internal structures :param args: """ super(SlotFillingDialogueState, self).__init__() self.slots_filled = {} self.slot_queries = {} self.system_requestable_slot_entropies = {} self.slots = None if 'slots' in args: self.slots = deepcopy(args['slots']) else: print( 'WARNING! SlotFillingDialogueState not provided with slots, using default CamRest slots.' ) self.slots = ['area', 'food', 'pricerange'] self.requested_slot = '' self.user_acts = None self.system_made_offer = False self.item_in_focus = None self.db_result = None self.db_matches_ratio = 1.0 self.last_sys_acts = None self.turn = 0 self.num_dontcare = 0 self.user_goal = None 
text_gcn-master.metrics.entropy|softmax|cross|masked def masked_softmax_cross_entropy(preds, labels, mask): """Softmax cross-entropy loss with masking.""" print(preds) loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels) mask = tf.cast(mask, dtype=tf.float32) mask /= tf.reduce_mean(mask) loss *= mask return tf.reduce_mean(loss) 
ctr_funcs.pipeline|input|tf def tf_input_pipeline(file_names, batch_size, num_epochs=1, label_col_idx=0, record_defaults=record_defaults): file_name_queue = tf.train.string_input_producer(file_names, num_epochs =num_epochs, shuffle=True) feature, label = tf_read_data(file_name_queue, label_col_idx, record_defaults) min_after_dequeue = 5000 capacity = min_after_dequeue + 3 * batch_size feature_batch, label_batch = tf.train.shuffle_batch([feature, label], batch_size=batch_size, capacity=capacity, min_after_dequeue= min_after_dequeue) return feature_batch, label_batch 
agents.DuelingLSTMDQNNet.head def _head(self, core_output): value = self._value(core_output) advantage = self._advantage(core_output) advantage -= tf.reduce_mean(advantage, axis=-1, keepdims=True) q_values = value + advantage action = tf.cast(tf.argmax(q_values, axis=1), tf.int32) return AgentOutput(action, q_values) 
pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLPreTrainedModel.bias|init def init_bias(self, bias): nn.init.constant_(bias, 0.0) 
data_handling.read|data|mnist def read_mnist_data(args): """ loads mnist data from keras library. centers it and rescales to range [-1,1] (or depending on args.x_bound) :param args: run arguments :return: train and test data tuples and the effective scale a if data has been scaled to [-a,a] """  def to_one_hot(vec): mat = np.zeros((vec.size, vec.max() + 1)) mat[np.arange(vec.size), vec] = 1 return mat mnist = tf.keras.datasets.mnist.load_data() n_feats = 784 images = np.reshape(mnist[0][0], (60000, n_feats)) labels = to_one_hot(mnist[0][1]) train_images = images[:args.train_set_size, :] train_labels = labels[:args.train_set_size] images = np.reshape(mnist[1][0], (10000, n_feats)) labels = to_one_hot(mnist[1][1]) test_images = images[:args.test_set_size, :] test_labels = labels[:args.test_set_size] if args.dp_pca_dims is not None: train_images, test_images = pca_preprocess(train_images, test_images, args) train_images = train_images - train_images.mean() test_images = test_images - test_images.mean() get_y = args.model_type == 'classifier' train_xy_data = xy_data(train_images, train_labels if get_y else None) test_xy_data = xy_data(test_images, test_labels if get_y else None) return train_xy_data, test_xy_data 
gym_pycolab.engine.Engine.drape|add def add_drape(self, character, drape_class, *args, **kwargs): """Add a `Drape` to this `Engine`.  A `Drape` supplies masks that the Engine uses to paint the same character to multiple different places on the board.  The positions of a particular `Drape` in the painting order (z-order) and the `Engine`'s board change consultation order are determined by order of its addition to the `Engine` and various other factors; see the `Engine` constructor docstring for details.  Args: character: The ASCII character that this `Drape` directs the `Engine` to paint on the game board. drape_class: A subclass of `Drape` to be constructed by this method. *args: Additional positional arguments for the `drape_class` constructor. **kwargs: Additional keyword arguments for the `drape_class` constructor.  Returns: the newly-created `Drape`.  Raises: RuntimeError: if gameplay has already begun, or if any characters in `characters` has already been claimed by a preceding call to the `set_backdrop` or `add` methods. TypeError: if `drape_class` is not a `Drape` subclass. ValueError: if `character` is not a single ASCII character. """ return self.add_prefilled_drape(character, np.zeros((self._rows, self. _cols), dtype=np.bool_), drape_class, *args, **kwargs) 
eval_metric.get|sdtw def get_sdtw(action_list, env_output_list, environment): """Returns success rate normalized by DTW.  "Effective and General Evaluation for Instruction Conditioned Navigation using Dynamic Time Warping" 2019 Magalhaes et al. https://arxiv.org/abs/1907.05446  Args: action_list: List of actions. env_output_list: List of observations in the trajectories. environment: Testing environment.  Returns: Value of success rate normalized by DTW. """ success_rate = get_success_rate(action_list, env_output_list, environment) if not success_rate: return 0.0 return get_norm_dtw(action_list, env_output_list, environment) 
shuffle_exchange_model.LambadaModel.Lambada|Model def __init__(self, target, n_classes, label_smoothing) ->None: self.__target = target self.__n_classes = n_classes self.__y_one_hot = tf.one_hot(self.__target, self.__n_classes, dtype=tf .float32) self.__label_smoothing = label_smoothing 
craystack.codecs_test.rate|test|flatten def test_flatten_rate(): rng.seed(0) init_size = 500000 head_size = 250000 head, tail = cs.random_message(init_size, (head_size,)) tail_size = len(cs.flatten((np.array([2 ** 31]), tail))) tail_diff = init_size - tail_size rate = tail_diff / head_size assert abs(rate / ((5 + 31 + 15.5) / 32) - 1) < 0.001 
train_recons.train|recons def train_recons(original_imgs_path, validatioin_imgs_path, save_path, model_pre_path, ssim_weight, EPOCHES_set, BATCH_SIZE, debug=False, logging_period=1): if debug: from datetime import datetime start_time = datetime.now() EPOCHS = EPOCHES_set print('EPOCHES   : ', EPOCHS) print('BATCH_SIZE: ', BATCH_SIZE) num_val = len(validatioin_imgs_path) num_imgs = len(original_imgs_path) original_imgs_path = original_imgs_path[:num_imgs] mod = num_imgs % BATCH_SIZE print('Train images number %d.\n' % num_imgs) print('Train images samples %s.\n' % str(num_imgs / BATCH_SIZE)) if mod > 0: print('Train set has been trimmed %d samples...\n' % mod) original_imgs_path = original_imgs_path[:-mod] INPUT_SHAPE_OR = BATCH_SIZE, HEIGHT, WIDTH, CHANNELS with tf.Graph().as_default(), tf.Session() as sess: original = tf.placeholder(tf.float32, shape=INPUT_SHAPE_OR, name= 'original') source = original print('source  :', source.shape) print('original:', original.shape) dfn = DenseFuseNet(model_pre_path) generated_img = dfn.transform_recons(source) print('generate:', generated_img.shape) ssim_loss_value = SSIM_LOSS(original, generated_img) pixel_loss = tf.reduce_sum(tf.square(original - generated_img)) pixel_loss = pixel_loss / (BATCH_SIZE * HEIGHT * WIDTH) ssim_loss = 1 - ssim_loss_value loss = ssim_weight * ssim_loss + pixel_loss train_op = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss) sess.run(tf.global_variables_initializer()) saver = tf.train.Saver(keep_checkpoint_every_n_hours=1) step = 0 count_loss = 0 n_batches = int(len(original_imgs_path) // BATCH_SIZE) val_batches = int(len(validatioin_imgs_path) // BATCH_SIZE) if debug: elapsed_time = datetime.now() - start_time print( '\nElapsed time for preprocessing before actually train the model: %s' % elapsed_time) print('Now begin to train the model...\n') start_time = datetime.now() Loss_all = [i for i in range(EPOCHS * n_batches)] Loss_ssim = [i for i in range(EPOCHS * n_batches)] Loss_pixel = [i for i in range(EPOCHS * n_batches)] Val_ssim_data = [i for i in range(EPOCHS * n_batches)] Val_pixel_data = [i for i in range(EPOCHS * n_batches)] for epoch in range(EPOCHS): np.random.shuffle(original_imgs_path) for batch in range(n_batches): original_path = original_imgs_path[batch * BATCH_SIZE:batch * BATCH_SIZE + BATCH_SIZE] original_batch = get_train_images(original_path, crop_height=HEIGHT, crop_width=WIDTH, flag=False) original_batch = original_batch.transpose((3, 0, 1, 2)) sess.run(train_op, feed_dict={original: original_batch}) step += 1 if debug: is_last_step = (epoch == EPOCHS - 1 and batch == n_batches - 1) if is_last_step or step % logging_period == 0: elapsed_time = datetime.now() - start_time _ssim_loss, _loss, _p_loss = sess.run([ssim_loss, loss, pixel_loss], feed_dict={original: original_batch}) Loss_all[count_loss] = _loss Loss_ssim[count_loss] = _ssim_loss Loss_pixel[count_loss] = _p_loss print( 'epoch: %d/%d, step: %d,  total loss: %s, elapsed time: %s' % (epoch, EPOCHS, step, _loss, elapsed_time)) print('p_loss: %s, ssim_loss: %s ,w_ssim_loss: %s ' % (_p_loss, _ssim_loss, ssim_weight * _ssim_loss)) val_ssim_acc = 0 val_pixel_acc = 0 np.random.shuffle(validatioin_imgs_path) val_start_time = datetime.now() for v in range(val_batches): val_original_path = validatioin_imgs_path[v * BATCH_SIZE:v * BATCH_SIZE + BATCH_SIZE] val_original_batch = get_train_images( val_original_path, crop_height=HEIGHT, crop_width=WIDTH, flag=False) val_original_batch = val_original_batch.reshape([ BATCH_SIZE, 256, 256, 1]) val_ssim, val_pixel = sess.run([ssim_loss, pixel_loss], feed_dict={original: val_original_batch}) val_ssim_acc = val_ssim_acc + (1 - val_ssim) val_pixel_acc = val_pixel_acc + val_pixel Val_ssim_data[count_loss] = val_ssim_acc / val_batches Val_pixel_data[count_loss ] = val_pixel_acc / val_batches val_es_time = datetime.now() - val_start_time print( 'validation value, SSIM: %s, Pixel: %s, elapsed time: %s' % (val_ssim_acc / val_batches, val_pixel_acc / val_batches, val_es_time)) print( '------------------------------------------------------------------------------' ) count_loss += 1 saver.save(sess, save_path) loss_data = Loss_all[:count_loss] scio.savemat('./models/loss/DeepDenseLossData' + str(ssim_weight) + '.mat', {'loss': loss_data}) loss_ssim_data = Loss_ssim[:count_loss] scio.savemat('./models/loss/DeepDenseLossSSIMData' + str( ssim_weight) + '.mat', {'loss_ssim': loss_ssim_data}) loss_pixel_data = Loss_pixel[:count_loss] scio.savemat('./models/loss/DeepDenseLossPixelData.mat' + str( ssim_weight) + '', {'loss_pixel': loss_pixel_data}) validation_ssim_data = Val_ssim_data[:count_loss] scio.savemat('./models/val/Validation_ssim_Data.mat' + str( ssim_weight) + '', {'val_ssim': validation_ssim_data}) validation_pixel_data = Val_pixel_data[:count_loss] scio.savemat('./models/val/Validation_pixel_Data.mat' + str( ssim_weight) + '', {'val_pixel': validation_pixel_data}) if debug: elapsed_time = datetime.now() - start_time print('Done training! Elapsed time: %s' % elapsed_time) print('Model is saved to: %s' % save_path) 
model.load|vars def load_vars(infile, sess): if type(infile) == str: arr = np.fromfile(infile) print('%d weights read from %s.\n' % (arr.size, infile)) else: arr = infile model_vars = [var for var in tf.global_variables() if 'optimizer' not in var.name] p = 0 for v in model_vars: sh = v.shape.as_list() var = np.zeros(sh) if len(sh) > 1: for i in range(sh[-1]): if len(sh) > 2: for j in range(sh[-2]): l = sh[0] * sh[1] var[:, :, (j), (i)] = np.reshape(arr[p:p + l], sh[:2]) p += l else: var[:, (i)] = arr[p:p + sh[0]] p += sh[0] else: var = arr[p:p + sh[0]] p += sh[0] sess.run(tf.assign(v, var)) print('Assigned in total %d weights to model\n' % p) return arr 
cpplint._ClassInfo.Class|Info def __init__(self, name, class_or_struct, clean_lines, linenum): _BlockInfo.__init__(self, False) self.name = name self.starting_linenum = linenum self.is_derived = False self.check_namespace_indentation = True if class_or_struct == 'struct': self.access = 'public' self.is_struct = True else: self.access = 'private' self.is_struct = False self.class_indent = GetIndentLevel(clean_lines.raw_lines[linenum]) self.last_line = 0 depth = 0 for i in range(linenum, clean_lines.NumLines()): line = clean_lines.elided[i] depth += line.count('{') - line.count('}') if not depth: self.last_line = i break 
agents.GFootball.head def _head(self, core_output): policy_logits = self._policy_logits(core_output) baseline = tf.squeeze(self._baseline(core_output), axis=-1) new_action = tf.random.categorical(policy_logits, 1, dtype=tf.int32) new_action = tf.squeeze(new_action, 1, name='action') return AgentOutput(new_action, policy_logits, baseline) 
utils.imsave def imsave(images, size, path): images = merge(images, size) images = cv2.cvtColor(images.astype('uint8'), cv2.COLOR_RGB2BGR) return cv2.imwrite(path, images) 
env_config.goal|random|plus|reward def goal_plus_random_reward(path_history, next_pano, golden_path, end_of_episode, scan_info): """Rewards an agent based on the difference in DTW after going to nex_pano.  Args: path_history: See above. next_pano: See above. golden_path: See above. end_of_episode: See above. scan_info: See above.  Returns: A scalar float immediate reward for the transition current_pano --> next_pano. """ goal_rwd = distance_to_goal(path_history, next_pano, golden_path, end_of_episode, scan_info) random_rwd = np.random.uniform(-1, 1) return goal_rwd + random_rwd 
cpplint.Error def Error(filename, linenum, category, confidence, message): """Logs the fact we've found a lint error.  We log where the error was found, and also our confidence in the error, that is, how certain we are this is a legitimate style regression, and not a misidentification or a use that's sometimes justified.  False positives can be suppressed by the use of "cpplint(category)"  comments on the offending line.  These are parsed into _error_suppressions.  Args: filename: The name of the file containing the error. linenum: The number of the line containing the error. category: A string used to describe the "category" this bug falls under: "whitespace", say, or "runtime".  Categories may have a hierarchy separated by slashes: "whitespace/indent". confidence: A number from 1-5 representing a confidence score for the error, with 5 meaning that we are certain of the problem, and 1 meaning that it could be a legitimate construct. message: The error message. """ if _ShouldPrintError(category, confidence, linenum): _cpplint_state.IncrementErrorCount(category) if _cpplint_state.output_format == 'vs7': sys.stderr.write('%s(%s):  %s  [%s] [%d]\n' % (filename, linenum, message, category, confidence)) elif _cpplint_state.output_format == 'eclipse': sys.stderr.write('%s:%s: warning: %s  [%s] [%d]\n' % (filename, linenum, message, category, confidence)) else: sys.stderr.write('%s:%s:  %s  [%s] [%d]\n' % (filename, linenum, message, category, confidence)) 
noisematrix.NoiseMatrix.Matrix|Noise def __init__(self, name): self.name = name self.matrix = None self.description = None self.idx_to_label_name_map = None 
cnn_basenet.CNNBaseModel.layerbn.f def f2(): """  :return: """ return tf_layer.batch_norm(inputdata, is_training=False, center=True, scale=True, updates_collections=None, scope=name, reuse=True) 
texar.modules.decoders.rnn_decoders.AttentionRNNDecoder.output|dtype @property def output_dtype(self): """Types of output of one step. """ dtype = nest.flatten(self._initial_state)[0].dtype return AttentionRNNDecoderOutput(logits=nest.map_structure(lambda _: dtype, self._rnn_output_size()), sample_id=self._helper. sample_ids_dtype, cell_output=nest.map_structure(lambda _: dtype, self._cell.output_size), attention_scores=nest.map_structure(lambda _: dtype, self._alignments_size()), attention_context=nest. map_structure(lambda _: dtype, self._cell.state_size.attention)) 
t_wgan_sn_celeba_128.data|generator def data_generator(batch_size=32): X = [] while True: np.random.shuffle(imgs) for f in imgs: X.append(imread(f)) if len(X) == batch_size: X = np.array(X) yield X X = [] 
rasterize_triangles.rasterize def rasterize(world_space_vertices, attributes, triangles, camera_matrices, image_width, image_height, background_value): """Rasterizes a mesh and computes interpolated vertex attributes.  Applies projection matrices and then calls rasterize_clip_space().  Args: world_space_vertices: 3-D float32 tensor of xyz positions with shape [batch_size, vertex_count, 3]. attributes: 3-D float32 tensor with shape [batch_size, vertex_count, attribute_count]. Each vertex attribute is interpolated across the triangle using barycentric interpolation. triangles: 2-D int32 tensor with shape [triangle_count, 3]. Each triplet should contain vertex indices describing a triangle such that the triangle's normal points toward the viewer if the forward order of the triplet defines a clockwise winding of the vertices. Gradients with respect to this tensor are not available. camera_matrices: 3-D float tensor with shape [batch_size, 4, 4] containing model-view-perspective projection matrices. image_width: int specifying desired output image width in pixels. image_height: int specifying desired output image height in pixels. background_value: a 1-D float32 tensor with shape [attribute_count]. Pixels that lie outside all triangles take this value.  Returns: A 4-D float32 tensor with shape [batch_size, image_height, image_width, attribute_count], containing the interpolated vertex attributes at each pixel.  Raises: ValueError: An invalid argument to the method is detected. """ clip_space_vertices = camera_utils.transform_homogeneous(camera_matrices, world_space_vertices) return rasterize_clip_space(clip_space_vertices, attributes, triangles, image_width, image_height, background_value) 
avod.core.minibatch_samplers.balanced_positive_negative_sampler_test.BalancedPositiveNegativeSamplerTest.incorrect|raises|shape|with|test|label|error def test_raises_error_with_incorrect_label_shape(self): labels = tf.constant([[True, False, False]]) indicator = tf.constant([True, False, True]) sampler = (balanced_positive_negative_sampler. BalancedPositiveNegativeSampler()) with self.assertRaises(ValueError): sampler.subsample(indicator, 64, labels) 
construct_delta_patch.optimize def optimize(imgs, dog_imgs, loc, mask, img_name, num_steps=300, step_size= 500, func_name='resnet_model/final_dense:0', input_name='input_tensor:0'): with tf.Session(graph=tf.Graph()) as sess: graph = tf.get_default_graph() new_image_node = build_graph(sess, num_steps, step_size, dog_imgs, loc, mask) input_tensor = graph.get_tensor_by_name(input_name) new_image = sess.run(new_image_node, feed_dict={input_tensor: imgs})[0] logits = tf.nn.softmax(graph.get_tensor_by_name(func_name)) img_logit = list(sess.run([logits], feed_dict={input_name: imgs})[0][0] ) new_img_logit = list(sess.run([logits], feed_dict={input_name: [ new_image]})[0][0]) fx_node = graph.get_tensor_by_name(func_name) fx = sess.run([fx_node], feed_dict={input_name: imgs})[0][0] fxd = sess.run([fx_node], feed_dict={input_name: [new_image]})[0][0] l2 = np.linalg.norm(fxd - fx) print('Logit layer L2 distance = {}'.format(l2)) fname = img_name[:-4] + '_' + str(int(l2)) save_img(imgs[0], new_image, fname) 
texar.data.data.data_base.DataBase.hparams @property def hparams(self): """A :class:`~texar.HParams` instance of the data hyperparameters. """ return self._hparams 
layers.Clustering.Clustering def __init__(self, args): """ Initializing the cluster center matrix. """ self.args = args self.cluster_means = tf.Variable(tf.random_uniform([self.args. cluster_number, self.args.dimensions], -0.1 / self.args.dimensions, 0.1 / self.args.dimensions)) 
slac.agents.slac.model_distribution_network.Bernoulli.call def __call__(self, *inputs): if len(inputs) > 1: inputs = tf.concat(inputs, axis=-1) else: inputs, = inputs out = self.dense1(inputs) out = self.dense2(out) out = self.output_layer(out) logits = tf.squeeze(out, axis=-1) return tfd.Bernoulli(logits=logits) 
avod.core.trainer_utils.load|model|weights def load_model_weights(sess, checkpoint_dir): """Restores the model weights.  Loads the weights loaded from checkpoint dir onto the model. It ignores the missing weights since this is used to load the RPN weights onto AVOD.  Args: sess: A TensorFlow session checkpoint_dir: Path to the weights to be loaded """ init_fn = slim.assign_from_checkpoint_fn(checkpoint_dir, slim. get_model_variables(), ignore_missing_vars=True) init_fn(sess) 
utils.data_utils.dataLoaderUSR.Loader|data|USR def __init__(self, SCALE=4): dataset_name = 'USR-248' self.SCALE = SCALE self.lr_res_, self.low_res_folder_ = self.get_lr_info() train_dir = '/mnt/data2/ImageSR/USR-248/train/' val_dir = '/mnt/data2/ImageSR/USR-248/val/' self.num_train, self.train_lr_paths, self.train_hr_paths = (self. get_lr_hr_paths(train_dir)) print('Loaded {0} pairs of image-paths for training'.format(self.num_train) ) self.num_val, self.val_lr_paths, self.val_hr_paths = self.get_lr_hr_paths( val_dir) 
deeplab_resnet.image_reader.image|mirroring def image_mirroring(img, label): """ Randomly mirrors the images.  Args: img: Training image to mirror. label: Segmentation mask to mirror. """ distort_left_right_random = tf.random_uniform([1], 0, 1.0, dtype=tf.float32 )[0] mirror = tf.less(tf.stack([1.0, distort_left_right_random, 1.0]), 0.5) mirror = tf.boolean_mask([0, 1, 2], mirror) img = tf.reverse(img, mirror) label = tf.reverse(label, mirror) return img, label 
utils.get|sample|npy def get_sample_npy(label_path): label = sio.loadmat(label_path)['clas1'] with open('train.plk', 'rb') as op: lst = pickle.load(op) sample = 16 * np.ones([1079, 1024]) cnt = 0 for p in lst: cnt += 1 sample[p[0], p[1]] = label[p[0], p[1]] - 1 print(cnt) np.save('sample.npy', sample) 
ODE.ex|sol def sol_ex(t, param_initial_conditions): g = 9.81 v0 = param_initial_conditions['v0'] alpha = param_initial_conditions['alpha'] h = param_initial_conditions['h'] return v0 * np.cos(alpha) * t, -0.5 * g * t ** 2 + v0 * np.sin(alpha ) * t + h 
src.run_op.train|flag|relation def train_relation_flag(model): flags = np.zeros(model.qa.relation_size) for t in model.qa.train_data: flags[int(t.relation)] = 1 return flags 
deepMOT-master.models.DAN.SST.SST def __init__(self, phase, base, extras, selector, final_net, use_gpu=config ['cuda']): super(SST, self).__init__() self.phase = phase self.vgg = nn.ModuleList(base) self.extras = nn.ModuleList(extras) self.selector = nn.ModuleList(selector) self.stacker2_bn = nn.BatchNorm2d(int(config['final_net']['900'][0] / 2)) self.final_dp = nn.Dropout(0.5) self.final_net = nn.ModuleList(final_net) self.image_size = config['sst_dim'] self.max_object = config['max_object'] self.selector_channel = config['selector_channel'] self.false_objects_column = None self.false_objects_row = None self.false_constant = config['false_constant'] self.use_gpu = use_gpu 
models.layers.DenseLayer.Dense|Layer def __init__(self, in_channels, growth_rate, bias): super().__init__() self.add_module('norm', nn.BatchNorm2d(in_channels, track_running_stats =False)) self.add_module('relu', nn.ReLU(True)) self.add_module('conv', nn.Conv2d(in_channels, growth_rate, kernel_size =3, stride=1, padding=1, bias=bias)) self.add_module('drop', nn.Dropout(p=0.2)) 
cond_utils.model|ex|sdn|params def sdn_model_params_ex1(yy, iso): c = 0.01 init = 0.0 rg00100 = tf.get_variable('r_gain_param_%05d' % 100, [1], tf.float32, initializer=tf.constant_initializer(init / c)) rg00400 = tf.get_variable('r_gain_param_%05d' % 400, [1], tf.float32, initializer=tf.constant_initializer(init / c)) rg00800 = tf.get_variable('r_gain_param_%05d' % 800, [1], tf.float32, initializer=tf.constant_initializer(init / c)) rg01600 = tf.get_variable('r_gain_param_%05d' % 1600, [1], tf.float32, initializer=tf.constant_initializer(init / c)) rg03200 = tf.get_variable('r_gain_param_%05d' % 3200, [1], tf.float32, initializer=tf.constant_initializer(init / c)) r_gain = tf.cond(tf.equal(iso[0], 100), lambda : tf.exp(c * rg00100) * iso, lambda : tf.cond(tf.equal(iso[0], 400), lambda : tf.exp(c * rg00400) * iso, lambda : tf.cond(tf.equal(iso[0], 800), lambda : tf .exp(c * rg00800) * iso, lambda : tf.cond(tf.equal(iso[0], 1600), lambda : tf.exp(c * rg01600) * iso, lambda : tf.cond(tf.equal(iso[0 ], 3200), lambda : tf.exp(c * rg03200) * iso, lambda : tf.exp(c * rg00800) * iso))))) b1 = tf.get_variable('b1', [1], tf.float32, initializer=tf. constant_initializer(-3.0)) b1 = tf.nn.sigmoid(b1, 'sigmoid_b1') b2 = tf.get_variable('b2', [1], tf.float32, initializer=tf. constant_initializer(3.0)) b2 = tf.nn.sigmoid(b2, 'sigmoid_b2') scale = tf.sqrt(b1 * yy / r_gain + b2) return scale 
scheduling.launch def launch(jobs, interval=5, on_gpu=True, no_print=True): for i, job in enumerate(jobs): print('\nJob {} out of {}'.format(i + 1, len(jobs))) run_command(job, on_gpu, no_print) time.sleep(interval) 
texar.modules.classifiers.conv_classifiers.Conv1DClassifier.layers @property def layers(self): """A list of the layers. """ return self._encoder.layers 
gym_pycolab.prefab_parts.sprites.MazeWalker.virtual|position @property def virtual_position(self): """This `MazeWalker's "virtual position" (see class docstring).""" return self.Position(self._virtual_row, self._virtual_col) 
gan.factorVAE.FactorVAE.from|inference def inference_from(self, img): latents = [] for i in range(int(math.ceil(float(img.shape[0]) / self.batch_size))): sub_latents = self.sess.run(self.de_input_test_tf, feed_dict={self. real_image_pl: img[i * self.batch_size:(i + 1) * self.batch_size]}) latents.append(sub_latents) return np.vstack(latents) 
deeppoly_nodes.DeeppolyConv2dNodeIntermediate.get|arguments def get_arguments(self): """ facilitates putting together all the arguments for the transformers in the child classes  Return ------ output : tuple the 5 entries are: 1. the filter (numpy.ndarray) 2. the bias (numpy.ndarray) 3. the image_shape (numpy.ndarray) 4. length of a side of the square kernel (int) 5. number of filters (int) """ filter_size = (c_size_t * 2)(self.filters.shape[0], self.filters.shape[1]) numfilters = self.filters.shape[3] strides = (c_size_t * 2)(self.strides[0], self.strides[1]) return (self.filters, self.bias, self.image_shape, filter_size, numfilters, strides, self.padding == 'VALID', True, self.predecessors) 
utils.norm|scale def norm_scale(x): return (x - np.mean(x)) / np.mean(x) 
text_gcn-master.inits.zeros def zeros(shape, name=None): """All zeros.""" initial = tf.zeros(shape, dtype=tf.float32) return tf.Variable(initial, name=name) 
a3c.A3C.batch|queue|pull|from def pull_batch_from_queue(self): """ self explanatory:  take a rollout from the queue of the thread runner. """ rollout = self.runner.queue.get(timeout=600.0) while not rollout.terminal: try: rollout.extend(self.runner.queue.get_nowait()) except queue.Empty: break return rollout 
commons.basic_utils.to|pickle|save def save_to_pickle(data, pkl_filepath): with open(pkl_filepath, 'wb') as pkl_file: pickle.dump(data, pkl_file) 
nets.inception_resnet_v2_test.InceptionTest.Points|test|Build|End def testBuildEndPoints(self): batch_size = 5 height, width = 299, 299 num_classes = 1000 with self.test_session(): inputs = tf.random_uniform((batch_size, height, width, 3)) _, end_points = inception.inception_resnet_v2(inputs, num_classes) self.assertTrue('Logits' in end_points) logits = end_points['Logits'] self.assertListEqual(logits.get_shape().as_list(), [batch_size, num_classes]) self.assertTrue('AuxLogits' in end_points) aux_logits = end_points['AuxLogits'] self.assertListEqual(aux_logits.get_shape().as_list(), [batch_size, num_classes]) pre_pool = end_points['Conv2d_7b_1x1'] self.assertListEqual(pre_pool.get_shape().as_list(), [batch_size, 8, 8, 1536]) 
plyfile.make|d def make2d(array, cols=None, dtype=None): """ Make a 2D array from an array of arrays.  The `cols' and `dtype' arguments can be omitted if the array is not empty.  """ if (cols is None or dtype is None) and not len(array): raise RuntimeError('cols and dtype must be specified for empty array') if cols is None: cols = len(array[0]) if dtype is None: dtype = array[0].dtype return _np.fromiter(array, [('_', dtype, (cols,))], count=len(array))['_'] 
tensorflow_functions.matrix|analogy def matrix_analogy(ma, mb, mc, mM): tf.reset_default_graph() a = tf.placeholder(tf.float32, [len(ma), len(ma[0])]) b = tf.placeholder(tf.float32, [len(mb), len(mb[0])]) c = tf.placeholder(tf.float32, [len(mc), len(mc[0])]) M = tf.placeholder(tf.float32, [len(mM), len(mM[0])]) ag = tf.add(tf.subtract(c, a), b) nn = tf.matmul(ag, tf.transpose(M)) sess = tf.Session() sess.run(tf.global_variables_initializer()) x = sess.run(nn, {a: ma, b: mb, c: mc, M: mM}) sess.close() tf.reset_default_graph() return x 
torch_vae.Round.call def __call__(self, pic): return torch.round(pic) 
utils.image|yield|full def full_image_yield(image_path, label_path, window_size): data_real, data_imag = matReader(image_path) label = sio.loadmat(label_path)['clas1'] row, col = label.shape for i in range(row): if i % 10 == 0: print(i) point = [(i, j) for j in range(col)] yield row, col, get_image_data(data_real, point, label_path, window_size, row, col), get_image_data(data_imag, point, label_path, window_size, row, col), label[i] 
models.GaussianMixture.log_gradient.posterior def posterior(X): log_weighted, log_shift = self._sum_log_exp(X, self._weights, self._mu, self._log_var) prob = tf.exp(log_weighted - log_shift) prob = prob / tf.reduce_sum(prob, axis=0, keep_dims=True) return prob 
dataset.DataSet.files|copy def copy_files(self, train_dir, test_dir): """ Copy all the files in the training-set to train_dir and copy all the files in the test-set to test_dir.  For example, the normal directory structure for the different classes in the training-set is:  knifey-spoony/forky/ knifey-spoony/knifey/ knifey-spoony/spoony/  Normally the test-set is a sub-dir of the training-set:  knifey-spoony/forky/test/ knifey-spoony/knifey/test/ knifey-spoony/spoony/test/  But some APIs use another dir-structure for the training-set:  knifey-spoony/train/forky/ knifey-spoony/train/knifey/ knifey-spoony/train/spoony/  and for the test-set:  knifey-spoony/test/forky/ knifey-spoony/test/knifey/ knifey-spoony/test/spoony/  :param train_dir: Directory for the training-set e.g. 'knifey-spoony/train/' :param test_dir: Directory for the test-set e.g. 'knifey-spoony/test/' :return: Nothing. """  def _copy_files(src_paths, dst_dir, class_numbers): class_dirs = [os.path.join(dst_dir, class_name + '/') for class_name in self.class_names] for dir in class_dirs: if not os.path.exists(dir): os.makedirs(dir) for src, cls in zip(src_paths, class_numbers): shutil.copy(src=src, dst=class_dirs[cls]) _copy_files(src_paths=self.get_paths(test=False), dst_dir=train_dir, class_numbers=self.class_numbers) print('- Copied training-set to:', train_dir) _copy_files(src_paths=self.get_paths(test=True), dst_dir=test_dir, class_numbers=self.class_numbers_test) print('- Copied test-set to:', test_dir) 
official.resnet.resnet_main.resnet_model_fn.network|build def build_network(): network = resnet_model.resnet_v1(resnet_depth=params['resnet_depth'], num_classes=params['num_label_classes'], dropblock_size=params[ 'dropblock_size'], dropblock_keep_probs=dropblock_keep_probs, data_format=params['data_format']) return network(inputs=features, is_training=mode == tf.estimator. ModeKeys.TRAIN) 
bert-master.run_squad.FeatureWriter.feature|process def process_feature(self, feature): """Write a InputFeature to the TFRecordWriter as a tf.train.Example.""" self.num_features += 1  def create_int_feature(values): feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list (values))) return feature features = collections.OrderedDict() features['unique_ids'] = create_int_feature([feature.unique_id]) features['input_ids'] = create_int_feature(feature.input_ids) features['input_mask'] = create_int_feature(feature.input_mask) features['segment_ids'] = create_int_feature(feature.segment_ids) if self.is_training: features['start_positions'] = create_int_feature([feature. start_position]) features['end_positions'] = create_int_feature([feature.end_position]) impossible = 0 if feature.is_impossible: impossible = 1 features['is_impossible'] = create_int_feature([impossible]) tf_example = tf.train.Example(features=tf.train.Features(feature=features)) self._writer.write(tf_example.SerializeToString()) 
enas.ptb.ptb_enas_child.PTBEnasChild.model def _model(self, x, is_training, is_test, should_carry=True): if is_test: start_h = self.test_start_h num_steps = 1 batch_size = 1 else: start_h = self.start_h num_steps = self.bptt_steps batch_size = self.batch_size all_h = tf.TensorArray(tf.float32, size=num_steps, infer_shape=True) embedding = tf.nn.embedding_lookup(self.w_emb, x) if is_training:  def _gen_mask(shape, keep_prob): _mask = tf.random_uniform(shape, dtype=tf.float32) _mask = tf.floor(_mask + keep_prob) / keep_prob return _mask e_mask = _gen_mask([batch_size, num_steps], self.lstm_e_keep) first_e_mask = e_mask zeros = tf.zeros_like(e_mask) ones = tf.ones_like(e_mask) r = [tf.constant([[False]] * batch_size, dtype=tf.bool)] for step in range(1, num_steps): should_zero = tf.logical_and(tf.equal(x[:, :step], x[:, step: step + 1]), tf.equal(e_mask[:, :step], 0)) should_zero = tf.reduce_any(should_zero, axis=1, keep_dims=True) r.append(should_zero) r = tf.concat(r, axis=1) e_mask = tf.where(r, tf.zeros_like(e_mask), e_mask) e_mask = tf.reshape(e_mask, [batch_size, num_steps, 1]) embedding *= e_mask x_mask, h_mask = [], [] for layer_id in range(self.lstm_num_layers): x_mask.append(_gen_mask([batch_size, self.lstm_hidden_size], self.lstm_x_keep)) h_mask.append(_gen_mask([batch_size, self.lstm_hidden_size], self.lstm_h_keep)) h_mask.append(h_mask) o_mask = _gen_mask([batch_size, self.lstm_hidden_size], self. lstm_o_keep)  def condition(step, *args): return tf.less(step, num_steps)  def body(step, prev_h, all_h): with tf.variable_scope(self.name): next_h = [] for layer_id, (p_h, w_prev, w_skip) in enumerate(zip(prev_h, self.w_prev, self.w_skip)): with tf.variable_scope('layer_{}'.format(layer_id)): if layer_id == 0: inputs = embedding[:, (step), :] else: inputs = next_h[-1] if self.fixed_arc is None: curr_h = self._rhn_enas(inputs, p_h, w_prev, w_skip, is_training, x_mask=x_mask[layer_id] if is_training else None, s_mask=h_mask[layer_id] if is_training else None) else: curr_h = self._rhn_fixed(inputs, p_h, w_prev, w_skip, is_training, x_mask=x_mask[layer_id] if is_training else None, s_mask=h_mask[layer_id] if is_training else None) if self.lstm_l_skip: curr_h += inputs next_h.append(curr_h) out_h = next_h[-1] if is_training: out_h *= o_mask all_h = all_h.write(step, out_h) return step + 1, next_h, all_h loop_vars = [tf.constant(0, dtype=tf.int32), start_h, all_h] loop_outputs = tf.while_loop(condition, body, loop_vars, back_prop=True) next_h = loop_outputs[-2] all_h = loop_outputs[-1].stack() all_h_diff = (all_h[1:, :, :] - all_h[:-1, :, :]) ** 2 self.all_h_diff = tf.reduce_sum(all_h_diff) all_h = tf.transpose(all_h, [1, 0, 2]) all_h = tf.reshape(all_h, [batch_size * num_steps, self.lstm_hidden_size]) carry_states = [] reset_states = [] for layer_id, (s_h, n_h) in enumerate(zip(start_h, next_h)): reset_states.append(tf.assign(s_h, tf.zeros_like(s_h), use_locking= True)) carry_states.append(tf.assign(s_h, tf.stop_gradient(n_h), use_locking=True)) if should_carry: with tf.control_dependencies(carry_states): all_h = tf.identity(all_h) return all_h, reset_states 
texar.utils.shapes.get|batch|size def get_batch_size(tensor): """Returns a unit `Tensor` representing the batch size, i.e., the size of the 1st dimension of :attr:`tensor`. """ return tf.shape(tensor)[0] 
official.utils.export.export.tensor|build|receiver|input|fn|serving def build_tensor_serving_input_receiver_fn(shape, dtype=tf.float32, batch_size=1): """Returns a input_receiver_fn that can be used during serving.  This expects examples to come through as float tensors, and simply wraps them as TensorServingInputReceivers.  Arguably, this should live in tf.estimator.export. Testing here first.  Args: shape: list representing target size of a single example. dtype: the expected datatype for the input example batch_size: number of input tensors that will be passed for prediction  Returns: A function that itself returns a TensorServingInputReceiver. """  def serving_input_receiver_fn(): features = tf.placeholder(dtype=dtype, shape=[batch_size] + shape, name='input_tensor') return tf.estimator.export.TensorServingInputReceiver(features= features, receiver_tensors=features) return serving_input_receiver_fn 
pytorch_pretrained_bert.modeling_openai.MLP.forward def forward(self, x): h = self.act(self.c_fc(x)) h2 = self.c_proj(h) return self.dropout(h2) 
texar.utils.beam_search.get|invariants|shape|state def get_state_shape_invariants(tensor): """Returns the shape of the tensor but sets middle dims to None.""" shape = tensor.shape.as_list() for i in range(1, len(shape) - 1): shape[i] = None return tf.TensorShape(shape) 
gym_pycolab.envs.pycolab_env.PycolabEnv.format|observation def _format_observation(self, state): shape = list(self._observation_shape) shape[2] = 1 return np.reshape(state / 127.0, shape) 
xlnet-master.tpu_estimator._PaddingSignals.pad_features_and_labels.pad|nest def nest_pad(tensor_or_dict): return nest.map_structure(pad_single_tensor, tensor_or_dict) 
model.NBatchLogger.Logger|N|Batch def __init__(self, display=100): super(NBatchLogger, self).__init__() self.seen = 0 self.display = display 
neural_tangents.predict.cpu|jit def _jit_cpu(x):  def jit_cpu(f): if _is_on_cpu(x): return jit(f, backend='cpu') return jit(f) return jit_cpu 
data_utils.read|data def read_data(path, max_size=None): data_set = [] data = json.load(open(path, 'r')) counter = 0 for pair in data: post = pair[0][0] responses = pair[1] source_ids = [int(x) for x in post[0]] for response in responses: if not max_size or counter < max_size: counter += 1 if counter % 100000 == 0: print('    reading data pair %d' % counter) sys.stdout.flush() target_ids = [int(x) for x in response[0]] feat = response[1] desc_ids = [int(x) for x in response[2]] data_set.append([source_ids, target_ids, feat, desc_ids]) return data_set 
plato.agent.component.dialogue_policy.reinforcement_learning.reward_function.SlotFillingReward.initialize def initialize(self, **kwargs): """ Initialize parameters for turn penalty, success, and failure.  :param kwargs: turn penalty, failure penalty, and success reward :return: Nothing """ if 'turn_penalty' in kwargs: self.turn_penalty = kwargs['turn_penalty'] if 'failure_penalty' in kwargs: self.failure_penalty = kwargs['failure_penalty'] if 'success_reward' in kwargs: self.success_reward = kwargs['success_reward'] 
model.D|cnn def cnn1D(x, C=10, bn=True, tb=True, training=True): with tf.name_scope('encoder'): network = conv1d(x, [5, 1, 8], 'conv11', bn, tb, training) network = max_pool_2x1(network, 'pool1') network = conv1d(network, [5, 8, 16], 'conv21', bn, tb, training) network = max_pool_2x1(network, 'pool2') network = conv1d(network, [5, 16, 32], 'conv31', bn, tb, training) network = max_pool_2x1(network, 'pool3') network = conv1d(network, [5, 32, 64], 'conv41', bn, tb, training) network = max_pool_2x1(network, 'pool4') network = conv1d(network, [5, 64, 128], 'conv51', bn, tb, training) network = max_pool_2x1(network, 'pool5') network = conv1d(network, [5, 128, 128], 'conv61', bn, tb, training) network = max_pool_2x1(network, 'pool6') network = conv1d(network, [5, 128, 128], 'conv71', bn, tb, training) network = max_pool_2x1(network, 'pool7') network = conv1d(network, [5, 128, 128], 'conv81', bn, tb, training) network = max_pool_2x1(network, 'pool8') network = conv1d(network, [5, 128, 128], 'conv91', bn, tb, training) network = max_pool_2x1(network, 'pool9') network = conv1d(network, [5, 128, 128], 'conv101', bn, tb, training) network = conv1d(network, [5, 128, 256], 'conv102', bn, tb, training) network = max_pool_2x1(network, 'pool10') network = conv1d(network, [5, 256, 256], 'conv111', bn, tb, training) network = conv1d(network, [5, 256, 256], 'conv112', bn, tb, training) network = max_pool_2x1(network, 'pool11') network = conv1d(network, [5, 256, 256], 'conv121', bn, tb, training) network = conv1d(network, [5, 256, 256], 'conv122', bn, tb, training) network = max_pool_2x1(network, 'pool12') sz = network.get_shape().as_list() print('Final pooled size:') print(sz) network = tf.reshape(network, [-1, sz[1] * sz[2]]) print('First FC size:') print(network.get_shape().as_list()) keep_prob = tf.placeholder(tf.float32) act = 0 itype = 3 network = fc(network, [sz[1] * sz[2], 1024], 'fc1', bn, act, itype, tb, training) network = tf.nn.dropout(network, keep_prob) network = fc(network, [1024, 1024], 'fc2', bn, act, itype, tb, training ) network = tf.nn.dropout(network, keep_prob) network = fc(network, [1024, 1024], 'fc3', bn, act, itype, tb, training ) network = tf.nn.dropout(network, keep_prob) network = fc(network, [1024, 1024], 'fc4', bn, act, itype, tb, training ) network = tf.nn.dropout(network, keep_prob) network1 = fc(network, [1024, 64], 'fc5', bn, act, itype, tb, training) network = tf.nn.dropout(network1, keep_prob) network = fc(network, [64, C], 'fc_final', False, -1, itype, tb, training) return network, network1, keep_prob 
slac.utils.nest_utils.structure|map|distribution def map_distribution_structure(func, *dist_structure):  def _get_params(dist): return {k: v for k, v in dist.parameters.items() if isinstance(v, tf.Tensor)}  def _get_other_params(dist): return {k: v for k, v in dist.parameters.items() if not isinstance( v, tf.Tensor)}  def _func(*dist_list): for dist in dist_list[1:]: assert dist.__class__ == dist_list[0].__class__ dist_ctor = dist_list[0].__class__ dist_other_params_list = [_get_other_params(dist) for dist in dist_list ] for dist_other_params in dist_other_params_list[1:]: assert dist_other_params == dist_other_params_list[0] dist_other_params = dist_other_params_list[0] sig = inspect.signature(dist_ctor) dist_other_params = {k: v for k, v in dist_other_params.items() if k in sig.parameters} dist_params_list = [_get_params(dist) for dist in dist_list] values_list = [list(params.values()) for params in dist_params_list] values_list = list(zip(*values_list)) structure_list = [func(*values) for values in values_list] values_list = [tf.nest.flatten(structure) for structure in structure_list] values_list = list(zip(*values_list)) dist_params_list = [dict(zip(dist_params_list[0].keys(), values)) for values in values_list] dist_list = [dist_ctor(**params, **dist_other_params) for params in dist_params_list] dist_structure = tf.nest.pack_sequence_as(structure_list[0], dist_list) return dist_structure return tf.nest.map_structure(_func, *dist_structure) 
data_utils_table2text._build_batch.pad def _pad(s, maxlen): s_ = copy.deepcopy(s) s_ = s_[:maxlen] slen = len(s_) if slen < maxlen: s_ += [pad_id] * (maxlen - slen) return s_ 
ge.models.struc2vec.Struc2Vec.get|embeddings def get_embeddings(self): if self.w2v_model is None: print('model not train') return {} self._embeddings = {} for word in self.graph.nodes(): self._embeddings[word] = self.w2v_model.wv[word] return self._embeddings 
load_data.load|uci|dataset def load_uci_dataset(dataset, i): datapath = base_dir + dataset + '/' data = np.loadtxt(datapath + 'data.txt') index_features = np.loadtxt(datapath + 'index_features.txt').astype('int') index_target = np.loadtxt(datapath + 'index_target.txt').astype('int') X = data[:, (index_features.tolist())] y = data[:, (index_target.tolist())] index_train = np.loadtxt(datapath + 'index_train_{}.txt'.format(i)).astype( 'int') index_test = np.loadtxt(datapath + 'index_test_{}.txt'.format(i)).astype( 'int') X_train = X[index_train.tolist(),] y_train = y[index_train.tolist()] X_test = X[index_test.tolist(),] y_test = y[index_test.tolist()] std_X_train = np.std(X_train, 0) std_X_train[std_X_train == 0] = 1 mean_X_train = np.mean(X_train, 0) X_train = (X_train - mean_X_train) / std_X_train X_test = (X_test - mean_X_train) / std_X_train mean_y_train = np.mean(y_train) std_y_train = np.std(y_train) y_train = (y_train - mean_y_train) / std_y_train y_train = np.array(y_train, ndmin=2).reshape((-1, 1)) y_test = np.array(y_test, ndmin=2).reshape((-1, 1)) return X_train, X_test, np.squeeze(y_train), np.squeeze(y_test ), mean_y_train, std_y_train 
facenet-master.src.facenet.calculate|val def calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_target, nrof_folds=10, distance_metric=0, subtract_mean=False): assert embeddings1.shape[0] == embeddings2.shape[0] assert embeddings1.shape[1] == embeddings2.shape[1] nrof_pairs = min(len(actual_issame), embeddings1.shape[0]) nrof_thresholds = len(thresholds) k_fold = KFold(n_splits=nrof_folds, shuffle=False) val = np.zeros(nrof_folds) far = np.zeros(nrof_folds) indices = np.arange(nrof_pairs) for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)): if subtract_mean: mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0) else: mean = 0.0 dist = distance(embeddings1 - mean, embeddings2 - mean, distance_metric ) far_train = np.zeros(nrof_thresholds) for threshold_idx, threshold in enumerate(thresholds): _, far_train[threshold_idx] = calculate_val_far(threshold, dist [train_set], actual_issame[train_set]) if np.max(far_train) >= far_target: f = interpolate.interp1d(far_train, thresholds, kind='slinear') threshold = f(far_target) else: threshold = 0.0 val[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[ test_set], actual_issame[test_set]) val_mean = np.mean(val) far_mean = np.mean(far) val_std = np.std(val) return val_mean, val_std, far_mean 
AffineCouplingCondXYG.AffineCouplingCondXYG.det|jacobian|log|forward def _forward_log_det_jacobian(self, x, yy, nlf0=None, nlf1=None, iso=None, cam=None): if self._last_layer: x = tf.reshape(x, (-1, self.i0, self.i1, self.ic)) yy = tf.reshape(yy, (-1, self.i0, self.i1, self.ic)) if 2 * x.shape[1] == yy.shape[1]: yy = squeeze2d(yy, 2) x0 = x[:, :, :, :self.ic // 2] x0yy = tf.concat([x0, yy], axis=-1) _, log_scale = self._shift_and_log_scale_fn(x0yy, iso) log_scale = self.scale * tf.tanh(log_scale) if log_scale is None: return tf.constant(0.0, dtype=x.dtype, name='fldj') return -tf.reduce_sum(log_scale, axis=[1, 2, 3]) 
utils.Object|transform def transformObject(v, vn, chScale, chObjAz, chPosition): if chScale.size == 1: scaleMat = geometry.Scale(x=chScale[0], y=chScale[0], z=chScale[0])[ 0:3, 0:3] elif chScale.size == 2: scaleMat = geometry.Scale(x=chScale[0], y=chScale[0], z=chScale[1])[ 0:3, 0:3] else: scaleMat = geometry.Scale(x=chScale[0], y=chScale[1], z=chScale[2])[ 0:3, 0:3] chRotAzMat = geometry.RotateZ(a=chObjAz)[0:3, 0:3] chRotAzMatX = geometry.RotateX(a=0)[0:3, 0:3] transformation = ch.dot(ch.dot(chRotAzMat, chRotAzMatX), scaleMat) invTranspModel = ch.transpose(ch.inv(transformation)) vtransf = [] vntransf = [] for mesh_i, mesh in enumerate(v): vtransf = vtransf + [ch.dot(v[mesh_i], transformation) + chPosition] vndot = ch.dot(vn[mesh_i], invTranspModel) vndot = vndot / ch.sqrt(ch.sum(vndot ** 2, 1))[:, (None)] vntransf = vntransf + [vndot] return vtransf, vntransf 
gan.infogan_cr.INFOGAN_CR.metric|log def log_metric(self, epoch_id, batch_id, global_id): if self.metric_callbacks is not None: metric = {} for metric_callback in self.metric_callbacks: metric.update(metric_callback.evaluate(epoch_id, batch_id, global_id)) if not os.path.isfile(self.metric_path): self.METRIC_FIELD_NAMES = ['epoch_id', 'batch_id', 'global_id'] for k in metric: self.METRIC_FIELD_NAMES.append(k) with open(self.metric_path, 'wb') as csv_file: writer = csv.DictWriter(csv_file, fieldnames=self. METRIC_FIELD_NAMES) writer.writeheader() with open(self.metric_path, 'ab') as csv_file: writer = csv.DictWriter(csv_file, fieldnames=self. METRIC_FIELD_NAMES) data = {'epoch_id': epoch_id, 'batch_id': batch_id, 'global_id': global_id} metric_string = copy.deepcopy(metric) for k in metric_string: if isinstance(metric[k], (float, np.float32, np.float64)): metric_string[k] = '{0:.12f}'.format(metric_string[k]) data.update(metric_string) writer.writerow(data) for k in metric: if isinstance(metric[k], (int, long, float, complex, np.float32, np.float64)): summary = tf.Summary(value=[tf.Summary.Value(tag='metric/' + k, simple_value=metric[k])]) self.summary_writer.add_summary(summary, global_id) 
rnns.load|data def load_data(task, data): """this function loads the data, and sets the associated parameters (such as output and input dimensionality and batchsize) according to the specified task, which are either text, music, speech or digits """ if task == 'text': print('Task is text') if data == 'ptb': tr = 'simple-examples/data/ptb.train.txt' vld = 'simple-examples/data/ptb.valid.txt' test = 'simple-examples/data/ptb.test.txt' [X, fmap] = load_multiple_textdata([tr, vld, test]) Trainseq = X[0] Validseq = X[1] vslen = round(Validseq.shape[1]) vnum_steps = 2000 Validseq = Validseq[:, 0:vslen - vslen % vnum_steps + 1] Testseq = X[2] tslen = round(Testseq.shape[1]) tnum_steps = 2000 Testseq = Testseq[:, 0:tslen - tslen % tnum_steps + 1] mbatchsize = 200000 else: Tseq = 10000 [X, fmap] = load_textdata(data + '.txt') offset = 5000 Trainseq = X[:, offset:offset + Tseq] Testseq = X[:, offset + Tseq + 50:offset + 3 * Tseq + 50] Validseq = Testseq mbatchsize = Tseq - 1 elif task == 'music': print('Loading Music task ' + data + ' data') filename = data + '.pickle' if data == 'JSB Chorales': len_th = 99999 elif data == 'Piano-midi.de': len_th = 200 elif data == 'Nottingham': len_th = 200 elif data == 'MuseData': len_th = 200 else: len_th = 9000000000.0 dataset = load_musicdata(filename, len_th) d = {'data': dataset[0][0], 'lengths': dataset[0][1]} df_train = pd.DataFrame(d) d = {'data': dataset[1][0], 'lengths': dataset[1][1]} df_test = pd.DataFrame(d) d = {'data': dataset[2][0], 'lengths': dataset[2][1]} df_valid = pd.DataFrame(d) if data == 'JSB Chorales': iterator = 'SimpleDataIterator' batchsize = len(df_train) num_buckets = None if data == 'Piano-midi.de': iterator = 'SimpleDataIterator' batchsize = round(0.5 * len(df_train)) num_buckets = None elif data == 'Nottingham': iterator = 'SimpleDataIterator' batchsize = round(0.5 * len(df_train)) num_buckets = None elif data == 'MuseData': iterator = 'SimpleDataIterator' batchsize = round(0.5 * len(df_train)) num_buckets = None L1 = L2 = df_train['data'][0].shape[0] outstage = 'sigmoid' mapping_mode = 'seq2seq' num_steps = None elif task == 'digits': from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets('MNIST_data/', one_hot=True) Trainsize = mnist.train.images.shape[0] images = mnist.train.images[:Trainsize, :] Trainims = np.split(images.reshape(Trainsize * 28, 28), Trainsize, 0) Trainlabels = list(np.argmax(mnist.train.labels[:Trainsize, :], 1)) lengths = [28] * Trainsize d = {'data': Trainims, 'labels': Trainlabels, 'lengths': lengths} df_train = pd.DataFrame(d) Testsize = mnist.test.images.shape[0] images = mnist.test.images[:Testsize, :] Testims = np.split(images.reshape(Testsize * 28, 28), Testsize, 0) Testlabels = list(np.argmax(mnist.test.labels[:Testsize, :], 1)) lengths = [28] * Testsize d = {'data': Testims, 'labels': Testlabels, 'lengths': lengths} df_test = pd.DataFrame(d) Validsize = mnist.validation.images.shape[0] images = mnist.validation.images[:Validsize, :] Validims = np.split(images.reshape(Validsize * 28, 28), Validsize, 0) Validlabels = list(np.argmax(mnist.validation.labels[:Validsize, :], 1) ) lengths = [28] * Validsize d = {'data': Validims, 'labels': Validlabels, 'lengths': lengths} df_valid = pd.DataFrame(d) batchsize = 5000 L1 = Trainims[0].shape[0] L2 = np.max(Trainlabels) + 1 outstage = 'softmax' mapping_mode = 'seq2vec' num_steps = 28 iterator = 'SimpleDataIterator' num_buckets = None len_th = None elif task == 'speech': filehandle = open('timit39.pickle', 'rb') dataset = pickle.load(filehandle) d = {'data': dataset[0][0], 'labels': dataset[0][1], 'lengths': dataset[0][2]} df_train = pd.DataFrame(d) d = {'data': dataset[1][0], 'labels': dataset[1][1], 'lengths': dataset[1][2]} df_test = pd.DataFrame(d) d = {'data': dataset[2][0], 'labels': dataset[2][1], 'lengths': dataset[2][2]} df_valid = pd.DataFrame(d) batchsize = 200 L1 = dataset[0][0][0].shape[0] L2 = 39 outstage = 'softmax' mapping_mode = 'seq2seq' num_steps = None iterator = 'BucketedDataIterator' num_buckets = 5 len_th = None parameters = {'batchsize': batchsize, 'L1': L1, 'L2': L2, 'outstage': outstage, 'mapping_mode': mapping_mode, 'num_steps': num_steps, 'iterator': iterator, 'num_buckets': num_buckets, 'len_th': len_th} return {'Train': df_train, 'Test': df_test, 'Validation': df_valid }, parameters 
facenet-master.tmp.deepdream.main.render|multiscale def render_multiscale(t_obj, img0=img_noise, iter_n=10, step=1.0, octave_n= 3, octave_scale=1.4): t_score = tf.reduce_mean(t_obj) t_grad = tf.gradients(t_score, t_input)[0] img = img0.copy() for octave in range(octave_n): if octave > 0: hw = np.float32(img.shape[:2]) * octave_scale img = resize(img, np.int32(hw)) for _ in range(iter_n): g = calc_grad_tiled(img, t_grad) g /= g.std() + 1e-08 img += g * step showarray(visstd(img)) 
slac.agents.slac.model_distribution_network.Compressor.Compressor def __init__(self, base_depth, feature_size, name=None): super(Compressor, self).__init__(name=name) self.feature_size = feature_size conv = functools.partial(tf.keras.layers.Conv2D, padding='SAME', activation=tf.nn.leaky_relu) self.conv1 = conv(base_depth, 5, 2) self.conv2 = conv(2 * base_depth, 3, 2) self.conv3 = conv(4 * base_depth, 3, 2) self.conv4 = conv(8 * base_depth, 3, 2) self.conv5 = conv(8 * base_depth, 4, padding='VALID') 
archs.hydra.get|Bin def getBin(l=10): x_ = 2 n = 1 while x_ < l: x_ = x_ * 2 n += 1 numbers = [] for i in range(l): num = [] for j in list('{0:0b}'.format(i + 1).zfill(n)): num.append(int(j)) numbers.append(num) return numbers 
gen.iterative|all|lth|sparse|force|plot|resnet def plot_resnet20_sparse_iterative_lth_force_all(): common.lth_plot(network=common.RESNET20, is_iterative=True, prune_method=common.UNSTRUCTURED, min_max_y=(-0.03, 0.01), to_ignore=['reinit', 'lr_lottery'], force_single=True, force_all_single=True) 
embeddings.OpenKE.config.Config.Config.relation|predict def predict_relation(self, h, t, k): """This methods predict the relation id given head entity and tail entity.  Args: h (int): head entity id t (int): tail entity id k (int): top k relations  Returns: list: k possible relation ids """ self.init_link_prediction() if self.importName != None: self.restore_tensorflow() test_h = np.array([h] * self.relTotal) test_r = np.array(range(self.relTotal)) test_t = np.array([t] * self.relTotal) res = self.test_step(test_h, test_t, test_r).reshape(-1).argsort()[:k] print(res) return res 
vkge.training.util.scale|distribution def distribution_scale(log_sigma_square): """ Returns the scale (std dev) from embeddings for tensorflow distributions MultivariateNormalDiag function """ scale = tf.sqrt(tf.exp(log_sigma_square)) return scale 
utils.matches|d|compute|degree|cm def compute_3d_matches_degree_cm(gt_class_ids, gt_RTs, gt_scales, pred_boxes, pred_class_ids, pred_scores, pred_RTs, synset_names, thres_shift=5, thres_degree=5, bound_threshold=5): """Finds matches between prediction and ground truth instances. Returns: gt_match: 1-D array. For each GT box it has the index of the matched predicted box. pred_match: 1-D array. For each predicted box, it has the index of the matched ground truth box. overlaps: [pred_boxes, gt_boxes] IoU overlaps. """ if len(pred_boxes): pre_len = len(pred_boxes) pred_boxes = trim_zeros(pred_boxes) after_len = len(pred_boxes) assert pre_len == after_len pred_scores = pred_scores[:pred_boxes.shape[0]] indices = np.argsort(pred_scores)[::-1] pred_boxes = pred_boxes[indices] pred_class_ids = pred_class_ids[indices] pred_scores = pred_scores[indices] num_pred = len(pred_boxes) num_gt = len(gt_class_ids) overlaps = np.zeros((num_pred, num_gt), dtype=np.float32) for i in range(num_pred): for j in range(num_gt): overlaps[i, j] = compute_RT_upper_bound_symmetry(pred_RTs[i], gt_RTs[j], gt_class_ids[j], synset_names, thres_shift= thres_shift, thres_theta=thres_degree) pred_match = -1 * np.ones(num_pred) gt_match = -1 * np.ones(num_gt) for i in range(len(pred_boxes)): sorted_ixs = np.argsort(overlaps[i]) for j in sorted_ixs: if gt_match[j] > -1 or pred_class_ids[i] != gt_class_ids[j]: continue min_bound = overlaps[i, j] if min_bound > 1: break elif min_bound == 1.0: gt_match[j] = i pred_match[i] = j break else: assert False, 'Invalid overlaps {}'.format(min_bound) return gt_match, pred_match, pred_class_ids, pred_scores 
sidd_utils.sample|patches def sample_patches(input_image, gt_image, var_image, nlf, patch_height, patch_width, n_patches_per_image, n_target_ch, sampling='uniform'): H = input_image.shape[1] W = input_image.shape[2] if sampling == 'uniform': ii, jj, max_patches_per_image = sample_indices_uniform(H, W, patch_height, patch_width) if n_patches_per_image > max_patches_per_image: n_patches_per_image = max_patches_per_image else: ii, jj = sample_indices_random(H, W, patch_height, patch_width, n_patches_per_image) input_patches = np.zeros((n_patches_per_image, patch_height, patch_width, n_target_ch), dtype=float) gt_patches = np.zeros((n_patches_per_image, patch_height, patch_width, n_target_ch), dtype=float) var_patches = None nlfs_patches = [None] * n_patches_per_image offset = 0 if sampling == 'uniform': offset = int((max_patches_per_image - n_patches_per_image) / 2) for p_idx in np.arange(n_patches_per_image): i = ii[p_idx + offset] j = jj[p_idx + offset] if n_target_ch == 1: for ch in range(4): input_patches[p_idx] = np.expand_dims(input_image[:, i:i + patch_height, j:j + patch_width, (ch)], axis=3) gt_patches[p_idx] = np.expand_dims(gt_image[:, i:i + patch_height, j:j + patch_width, (ch)], axis=3) nlfs_patches[p_idx] = nlf else: input_patches[p_idx] = input_image[:, i:i + patch_height, j:j + patch_width, :] gt_patches[p_idx] = gt_image[:, i:i + patch_height, j:j + patch_width, :] nlfs_patches[p_idx] = nlf return input_patches, gt_patches, var_patches, nlfs_patches 
gpt.Block.forward def forward(self, x, sequence_mask): a = self.attn(x, sequence_mask) n = self.ln_1(x + a) m = self.mlp(n) h = self.ln_2(n + m) return h 
commons.measure.DropMaskType1.sample|theta def sample_theta(self, hparams): noise_shape = self.get_noise_shape() mask = np.random.uniform(size=noise_shape) p = hparams.drop_prob mask = np.float32(mask >= p) / (1 - p) theta_val = np.ones(shape=self.batch_dims) theta_val = theta_val * mask return theta_val 
texar.modules.decoders.transformer_decoders.TransformerDecoder.outputs|to|input|ids def _input_ids_to_outputs(self, input_ids, step, cache): """The function is called in beam-search decoding.  `inputs` should be of shape `[batch_size]`.  Returns outputs (i.e. logits) of shape `[batch_size, vocab_size]` and updated cache. """ _batch_size = shape_list(input_ids)[0] times = tf.ones([_batch_size], dtype=tf.int32) * step inputs = self.embedding(input_ids, times) outputs = self._self_attention_stack(tf.expand_dims(inputs, axis=1), memory=cache.get('memory'), cache=cache) outputs = self._output_layer(outputs) outputs = tf.squeeze(outputs, axis=[1]) return outputs, cache 
lambada.file|lambada def lambada_file(file: str) ->str: return os.path.join(cnf.lambada_data_dir, file) 
atomic_evaluate.AtomicGenerationEvaluator.result|print def print_result(self, split, epoch_losses): print('{} Loss: \t {}'.format(split, epoch_losses['total_micro'])) print('{} Perplexity: \t {}'.format(split, epoch_losses['ppl_micro'])) 
regression.misc.layers.Layer.n|out @property def n_out(self): return self._n_out 
functions.sequence|atomic|print def print_atomic_sequence(sequence_object): input_event = sequence_object['event'] category = sequence_object['effect_type'] print('Input Event:   {}'.format(input_event)) print('Target Effect: {}'.format(category)) print('') print('Candidate Sequences:') for beam in sequence_object['beams']: print(beam) print('') print('====================================================') print('') 
slac.utils.nest_utils.map_distribution_structure.get|other|params def _get_other_params(dist): return {k: v for k, v in dist.parameters.items() if not isinstance(v, tf.Tensor)} 
texar.data.data.multi_aligned_data_test.MultiAlignedDataTest.setUp.feature|bytes def _bytes_feature(value): """Returns a bytes_list from a string / byte. """ value = tf.compat.as_bytes(value, encoding='utf-8') return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value])) 
model.IAFLayer.up def up(self, input, **_): with arg_scope([conv2d]): self.qz_mean, self.qz_logsd, self.up_context, h = self.up_split(input) return self.up_merge(h, input) 
gpt.Attention.heads|split def split_heads(self, x, k=False): new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head) x = x.view(*new_x_shape) if k: return x.permute(0, 2, 3, 1) else: return x.permute(0, 2, 1, 3) 
seq2seq.Seq2seq.random_walk.feed|update|dict def _feed_dict_update(feed_dict, output_dict): batch_size = output_dict['dec_predict'].shape[0] inputs = output_dict['dec_predict'].T[1:].T inputs = np.concatenate([np.zeros([batch_size, 1]) + self.dec_start_id, inputs], axis=1) inp_lens = [] max_sent_len = inputs.shape[1] for s in inputs: for l in range(max_sent_len): if s[l] == self.dec_end_id: break inp_lens.append(l) feed_dict[self.enc_inputs] = inputs feed_dict[self.inp_lens] = np.array(inp_lens) return feed_dict 
RDF2Vec.get|Links def getLinks(net, source): if source not in net: return {} return net[source] 
modeling_transfo_xl_test.TransfoXLModelTest.TransfoXLModelTester.XL|Tester|Transfo|Model def __init__(self, parent, batch_size=13, seq_length=7, mem_len=30, clamp_len=15, is_training=True, use_labels=True, vocab_size=99, cutoffs =[10, 50, 80], d_model=32, d_embed=32, n_head=4, d_head=8, d_inner=128, div_val=2, n_layer=5, scope=None, seed=1): self.parent = parent self.batch_size = batch_size self.seq_length = seq_length self.mem_len = mem_len self.clamp_len = clamp_len self.is_training = is_training self.use_labels = use_labels self.vocab_size = vocab_size self.cutoffs = cutoffs self.d_model = d_model self.d_embed = d_embed self.n_head = n_head self.d_head = d_head self.d_inner = d_inner self.div_val = div_val self.n_layer = n_layer self.scope = scope self.seed = seed 
utils.psnr def psnr(img1, img2): img1 = np.float64(img1) img2 = np.float64(img2) mse = np.mean((img1 - img2) ** 2) if mse == 0: return 100 if np.max(img1) <= 1.0: PIXEL_MAX = 1.0 else: PIXEL_MAX = 255.0 return 20 * math.log10(PIXEL_MAX / math.sqrt(mse)) 
pytorch_pretrained_bert.tokenization_transfo_xl.LMOrderedIterator.iter def __iter__(self): return self.get_fixlen_iter() 
evaluate.load def load(saver, sess, ckpt_path): """Load trained weights.  Args: saver: TensorFlow saver object. sess: TensorFlow session. ckpt_path: path to checkpoint file with parameters. """ saver.restore(sess, ckpt_path) print('Restored model parameters from {}'.format(ckpt_path)) 
save_nlayer_weights.NLayerModel.N|Layer|Model def __init__(self, params, restore=None, session=None, use_log=False, image_size=28, image_channel=1): self.image_size = image_size self.num_channels = image_channel self.num_labels = 10 model = Sequential() model.add(Flatten(input_shape=(image_size, image_size, image_channel))) self.U = [] for param in params: self.U.append(Dense(param)) model.add(self.U[-1]) model.add(Activation('relu')) self.W = Dense(10) model.add(self.W) if use_log: model.add(Activation('softmax')) if restore: model.load_weights(restore) layer_outputs = [] for layer in model.layers: if isinstance(layer, Conv2D) or isinstance(layer, Dense): layer_outputs.append(K.function([model.layers[0].input], [layer .output])) self.layer_outputs = layer_outputs self.model = model 
scannet_block2scene_index.parse|fn def parse_fn(item): features = tf.parse_single_example(item, features={'index_label': tf. FixedLenFeature([], dtype=tf.string)}) index_label = tf.decode_raw(features['index_label'], tf.int32) index_label = tf.reshape(index_label, [-1, 1]) index_label = tf.cast(index_label, tf.float32) return index_label 
neural_tangents.stax.serial.kernel|fn def kernel_fn(kernels): for f in kernel_fns: kernels = f(kernels) return kernels 
borealisflows.layers.LeakyReLU.det|jacobian|log|forward def _forward_log_det_jacobian(self, x): I = tf.ones_like(x) J = tf.where(tf.greater_equal(x, 0), I, 1.0 / self.alpha * I) log_abs_det_J = tf.log(tf.abs(J)) return tf.reduce_sum(log_abs_det_J, axis=[1, 2, 3]) 
neural_tangents.predict.descent|mse|gradient|gp def gradient_descent_mse_gp(kernel_fn, x_train, y_train, x_test, get, diag_reg=0.0, compute_cov=False): """Predicts the gaussian embedding induced by gradient descent with mse loss.  This is equivalent to an infinite ensemble of networks after marginalizing out the initialization.  Args: kernel_fn: A kernel function that computes NNGP and NTK. x_train: A `np.ndarray`, representing the training data. y_train: A `np.ndarray`, representing the labels of the training data. x_test: A `np.ndarray`, representing the test data. get: string, the mode of the Gaussian process, either "nngp" or "ntk", or a tuple. diag_reg: A float, representing the strength of the regularization. compute_cov: A boolean. If `True` computing both `mean` and `variance` and only `mean` otherwise.  Returns: A function that predicts the gaussian parameters at t: predict(t) -> Gaussian(mean, variance). If compute_cov is False, only returns the mean. """ if get is None: get = 'nngp', 'ntk' if isinstance(get, str): return lambda t: gradient_descent_mse_gp(kernel_fn, x_train, y_train, x_test, diag_reg=diag_reg, get=(get,), compute_cov= compute_cov)(t)[0] _, get = canonicalize_get(get) normalization = y_train.size op_fn = _make_inv_expm1_fn(normalization) eigenspace = {} kdd, ktd, ktt = _get_matrices(kernel_fn, x_train, x_test, get, compute_cov) gp_inference_mat = _gp_inference_mat_jit_cpu if _is_on_cpu(kdd ) else _gp_inference_mat_jit  @_jit_cpu(kdd) def predict(t=None): """`t=None` is equivalent to infinite time and calls `gp_inference`.""" if t is None: return gp_inference_mat(kdd, ktd, ktt, y_train, get, diag_reg) if not eigenspace: for g in get: k = kdd.nngp if g == 'nngp' else kdd.ntk k_dd_plus_reg = _add_diagonal_regularizer(k, diag_reg) eigenspace[g] = _eigh(k_dd_plus_reg) out = {} if 'nngp' in get: evals, evecs = eigenspace['nngp'] op_evals = -op_fn(evals, t) pred_mean = _mean_prediction_einsum(evecs, op_evals, ktd.nngp, y_train) if compute_cov: op_evals_x2 = -op_fn(evals, 2 * t) pred_cov = ktt - np.einsum('mj,ji,i,ki,lk->ml', ktd.nngp, evecs, op_evals_x2, evecs, ktd.nngp, optimize=True) out['nngp'] = Gaussian(pred_mean, pred_cov ) if compute_cov else pred_mean if 'ntk' in get: evals, evecs = eigenspace['ntk'] op_evals = -op_fn(evals, t) pred_mean = _mean_prediction_einsum(evecs, op_evals, ktd.ntk, y_train) if compute_cov: term_1 = np.einsum('mi,i,ki,lk->ml', evecs, op_evals, evecs, ktd.ntk, optimize=True) pred_cov = np.einsum('ji,jk,kl->il', term_1, kdd.nngp, term_1, optimize=True) term_2 = np.einsum('mj,ji,i,ki,lk->ml', ktd.ntk, evecs, op_evals, evecs, ktd.nngp, optimize=True) term_2 += np.transpose(term_2) pred_cov += -term_2 + ktt out['ntk'] = Gaussian(pred_mean, pred_cov ) if compute_cov else pred_mean returntype = named_tuple_factory('Gaussians', get) return returntype(*tuple(out[g] for g in get)) return predict 
samplers.uniform_sampler.UniformSampler.sampling|negative def _negative_sampling(self, user_ids, pos_ids, neg_ids): neg_samples = np.random.choice(neg_ids, size=(len(pos_ids), self. n_negatives)) for i, uid, negatives in zip(range(len(user_ids)), user_ids, neg_samples): for j, neg in enumerate(negatives): while neg in self.user_items[uid]: neg_samples[i, j] = neg = np.random.choice(neg_ids) return neg_samples 
tf_utils.adamax.AdamaxOptimizer.apply|dense def _apply_dense(self, grad, var): lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype) beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype) beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype) if var.dtype.base_dtype == tf.float16: eps = 1e-07 else: eps = 1e-08 v = self.get_slot(var, 'v') v_t = v.assign(beta1_t * v + (1.0 - beta1_t) * grad) m = self.get_slot(var, 'm') m_t = m.assign(tf.maximum(beta2_t * m + eps, tf.abs(grad))) g_t = v_t / m_t var_update = state_ops.assign_sub(var, lr_t * g_t) return control_flow_ops.group(*[var_update, m_t, v_t]) 
texar.modules.decoders.tf_helpers.TrainingHelper.initialize def initialize(self, name=None): with ops.name_scope(name, 'TrainingHelperInitialize'): finished = math_ops.equal(0, self._sequence_length) all_finished = math_ops.reduce_all(finished) next_inputs = control_flow_ops.cond(all_finished, lambda : self. _zero_inputs, lambda : nest.map_structure(lambda inp: inp.read( 0), self._input_tas)) return finished, next_inputs 
celebA.gen.wgan_utils.conv|basic|d def conv2d_basic(x, W, bias): conv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME') return tf.nn.bias_add(conv, bias) 
applications.cambridge_restaurants.camrest_dst.CamRestDST.db|update|state def update_state_db(self, db_result, sys_req_slot_entropies=None): """ Updates the current database results in the dialogue state.  :param db_result: a dictionary containing the database query results :param sys_req_slot_entropies: entropy values for each slot :return: """ if db_result: self.DState.db_matches_ratio = len(db_result) self.DState.item_in_focus = db_result[0] if sys_req_slot_entropies: self.DState.system_requestable_slot_entropies = deepcopy( sys_req_slot_entropies) return self.DState 
SRGANs-Spectral-Regularization-GANs--master.source.miscs.random_samples.categorical|sample|from|distribution def sample_from_categorical_distribution(batch_probs): """Sample a batch of actions from a batch of action probabilities. Args: batch_probs (ndarray): batch of action probabilities BxA Returns: ndarray consisting of sampled action indices """ xp = chainer.cuda.get_array_module(batch_probs) return xp.argmax(xp.log(batch_probs) + xp.random.gumbel(size= batch_probs.shape), axis=1).astype(np.int32, copy=False) 
gym_daisy_custom.control.cpg_oscillator.OscillatorNetwork.Network|Oscillator def __init__(self, oscillators, weights, phase_biases): """ Network of oscillators. Implements the evolution of the CPGs  :param oscillators: Sequence of oscillators :param weights: the weights that couple the oscillators :param phase_biases: phase biases between the oscillators """ self.oscillators = oscillators self.weights = weights self.phase_biases = phase_biases self.n_oscillators = len(oscillators) 
fusion_addition.Strategy def Strategy(content, style): return content + style 
defense.make_defend_quilt.exists|index def index_exists(arr, index): if arr.ndim != len(index): return False return all(i > 0 for i in index) and all(index[i] < arr.shape[i] for i in range(arr.ndim)) 
learner_test.LearnerTest.two|n|test|target|step|steps|bellman def test_n_step_bellman_target_two_steps(self): targets = tf.function(learner.n_step_bellman_target)(rewards=np.array([ [1.0, 2.0, 3.0]], np.float32).T, done=np.array([[False, False, False]]).T, q_target=np.array([[100, 200, 300]], np.float32).T, gamma=0.9, n_steps=2) self.assertAllClose(targets, np.array([[1 + 0.9 * 2 + 0.9 ** 2 * 200, 2 + 0.9 * 3 + 0.9 ** 2 * 300, 3 + 0.9 * 300]]).T) 
waypts_policy.WaypointsMinJerkPolicy.make|traj def make_traj(self, waypts, t_max, robot, get_init_pos_fxn): assert robot.control_mode == 'torque' _, prev_ee_pos, prev_ee_quat, _ = get_init_pos_fxn() prev_fing_dist = robot.get_fing_dist() prev_ee_orient = quaternion_to_euler(prev_ee_quat) num_waypts = waypts.shape[0] steps_per_waypt = int(t_max / num_waypts) traj = np.zeros([t_max, 3 * 4 + 2]) ee_quat_traj = np.zeros([num_waypts * steps_per_waypt, 4]) t = 0 for wpt in range(num_waypts): ee_pos, ee_quat, fing_dist, gains = self.parse_waypoint(waypts[wpt]) ee_orient = quaternion_to_euler(ee_quat) qpos = robot.ee_pos_to_qpos(ee_pos, ee_quat, fing_dist) if qpos is None: ee_pos = prev_ee_pos ee_orient = prev_ee_orient fing_dist = prev_fing_dist Y, Yd, Ydd = plan_min_jerk_trajectory(prev_ee_pos, ee_pos, steps_per_waypt * robot.dt, robot.dt) th = plan_linear_orientation_trajectory(prev_ee_orient, ee_orient, steps_per_waypt) traj[t:t + steps_per_waypt, 0:3] = Y[:] traj[t:t + steps_per_waypt, 3:6] = Yd[:] traj[t:t + steps_per_waypt, 6:9] = Ydd[:] traj[t:t + steps_per_waypt, 9:12] = th[:] traj[t:t + steps_per_waypt, 12:] = gains[:] for substep in range(steps_per_waypt): ee_quat_traj[(t + substep), :] = euler_to_quaternion(th[substep]) t += steps_per_waypt prev_ee_pos = ee_pos prev_ee_orient = ee_orient prev_fing_dist = fing_dist if t + 1 < self.t_max: traj[t + 1:, :] = traj[(t), :] return traj, traj[:, 0:3], ee_quat_traj 
nets.mobilenet.mobilenet.make|divisible def _make_divisible(v, divisor, min_value=None): if min_value is None: min_value = divisor new_v = max(min_value, int(v + divisor / 2) // divisor * divisor) if new_v < 0.9 * v: new_v += divisor return new_v 
texar.modules.decoders.rnn_decoders.BasicRNNDecoder.RNN|Basic|Decoder def __init__(self, cell=None, cell_dropout_mode=None, vocab_size=None, output_layer=None, hparams=None): RNNDecoderBase.__init__(self, cell, vocab_size, output_layer, cell_dropout_mode, hparams) 
ODE.solve|for|input|data def solve_for_input_data(collocation_points, index, delta_t, multilevel=False): params_eq = {'gravity': 9.81, 'density': collocation_points['density'][ index], 'radius': collocation_points['radius'][index], 'mass': collocation_points['mass'][index], 'drag_coefficient': collocation_points['drag_coefficient'][index]} param_initial_conditions = {'h': collocation_points['h'][index], 'alpha': collocation_points['alpha'][index], 'v0': collocation_points['v0'][index]} time_, x, y = solve_object_ODE(delta_t, param_initial_conditions, params_eq ) max_x = (x[-1] - x[-2]) / (y[-1] - y[-2]) * -y[-2] + x[-2] max_x_ml = 0 if multilevel: time_ml, x_ml, y_ml = solve_object_ODE(2 * delta_t, param_initial_conditions, params_eq) max_x_ml = (x_ml[-1] - x_ml[-2]) / (y_ml[-1] - y_ml[-2]) * -y_ml[-2 ] + x_ml[-2] if max_x_ml < 0 or max_x_ml > 50 or math.isnan(max_x_ml ) or max_x < 0 or max_x > 50 or math.isnan(max_x): max_x_ml = 0 max_x = 0 if multilevel: if max_x != 0 and max_x_ml != 0: return [max_x, max_x_ml] else: print('Multilevel: bad Values') elif max_x != 0: return max_x else: print('Singlelevel: bad Values') 
src.adapter.adapter.loss|define def define_loss(self, is_training, method, dual_loss_alpha=1): """  :param is_training: :param method: loss_method see README.md :return: """ if 'dual' in method: self.dual_function = method.split('-')[-1] self.dual_learning(dual_loss_alpha) self.loss_method = method mm = method.split('-')[0] if mm == 'mse': if self.keep_prob != 1: self.adapter_output = tf.layers.dropout(self.adapter_output, 1.0 - self.keep_prob, training=is_training) temp = tf.reduce_mean((self.adapter_output - self.target) ** 2, 1) temp = tf.gather(temp, self.gather_index) adapter_loss = tf.reduce_mean(temp) self.adapter_loss = adapter_loss elif mm == 'gan': with tf.variable_scope('Discrimeter' + self.name): o_target, o_adapter = self.D(self.target, self.adapter_output, False) o_target = tf.gather(o_target, self.gather_index) o_adapter = tf.gather(o_adapter, self.gather_index) self.loss_D = tf.reduce_mean(-o_target + o_adapter, 0) self.loss_G = tf.reduce_mean(-o_adapter) error = (tf.reduce_mean(tf.ones_like(o_target) - tf.sigmoid( o_target), 0) + tf.reduce_mean(tf.sigmoid(o_adapter) - tf. zeros_like(o_adapter), 0)) / 2 self.D_acc = 1 - error else: print('loss method is error only accept mse dual gan ') 
utils.AP|compute|degree|cm|m def compute_degree_cm_mAP(final_results, synset_names, log_dir, degree_thresholds=[360], shift_thresholds=[100], iou_3d_thresholds=[0.1 ], iou_pose_thres=0.1, use_matches_for_pose=False): """Compute Average Precision at a set IoU threshold (default 0.5). Returns: mAP: Mean Average Precision precisions: List of precisions at different class score thresholds. recalls: List of recall values at different class score thresholds. overlaps: [pred_boxes, gt_boxes] IoU overlaps. """ num_classes = len(synset_names) degree_thres_list = list(degree_thresholds) + [360] num_degree_thres = len(degree_thres_list) shift_thres_list = list(shift_thresholds) + [100] num_shift_thres = len(shift_thres_list) iou_thres_list = list(iou_3d_thresholds) num_iou_thres = len(iou_thres_list) if use_matches_for_pose: assert iou_pose_thres in iou_thres_list iou_3d_aps = np.zeros((num_classes + 1, num_iou_thres)) iou_pred_matches_all = [np.zeros((num_iou_thres, 0)) for _ in range( num_classes)] iou_pred_scores_all = [np.zeros((num_iou_thres, 0)) for _ in range( num_classes)] iou_gt_matches_all = [np.zeros((num_iou_thres, 0)) for _ in range( num_classes)] pose_aps = np.zeros((num_classes + 1, num_degree_thres, num_shift_thres)) pose_pred_matches_all = [np.zeros((num_degree_thres, num_shift_thres, 0 )) for _ in range(num_classes)] pose_gt_matches_all = [np.zeros((num_degree_thres, num_shift_thres, 0)) for _ in range(num_classes)] pose_pred_scores_all = [np.zeros((num_degree_thres, num_shift_thres, 0) ) for _ in range(num_classes)] progress = 0 for progress, result in enumerate(final_results): print(progress, len(final_results)) gt_class_ids = result['gt_class_ids'].astype(np.int32) gt_RTs = np.array(result['gt_RTs']) gt_scales = np.array(result['gt_scales']) gt_handle_visibility = result['gt_handle_visibility'] pred_bboxes = np.array(result['pred_bboxes']) pred_class_ids = result['pred_class_ids'] pred_scales = result['pred_scales'] pred_scores = result['pred_scores'] pred_RTs = np.array(result['pred_RTs']) if len(gt_class_ids) == 0 and len(pred_class_ids) == 0: continue for cls_id in range(1, num_classes): cls_gt_class_ids = gt_class_ids[gt_class_ids == cls_id] if len( gt_class_ids) else np.zeros(0) cls_gt_scales = gt_scales[gt_class_ids == cls_id] if len( gt_class_ids) else np.zeros((0, 3)) cls_gt_RTs = gt_RTs[gt_class_ids == cls_id] if len(gt_class_ids ) else np.zeros((0, 4, 4)) cls_pred_class_ids = pred_class_ids[pred_class_ids == cls_id ] if len(pred_class_ids) else np.zeros(0) cls_pred_bboxes = pred_bboxes[(pred_class_ids == cls_id), : ] if len(pred_class_ids) else np.zeros((0, 4)) cls_pred_scores = pred_scores[pred_class_ids == cls_id] if len( pred_class_ids) else np.zeros(0) cls_pred_RTs = pred_RTs[pred_class_ids == cls_id] if len( pred_class_ids) else np.zeros((0, 4, 4)) cls_pred_scales = pred_scales[pred_class_ids == cls_id] if len( pred_class_ids) else np.zeros((0, 3)) if synset_names[cls_id] != 'mug': cls_gt_handle_visibility = np.ones_like(cls_gt_class_ids) else: cls_gt_handle_visibility = gt_handle_visibility[ gt_class_ids == cls_id] if len(gt_class_ids) else np.ones(0 ) iou_cls_gt_match, iou_cls_pred_match, _, iou_pred_indices = ( compute_3d_matches(cls_gt_class_ids, cls_gt_RTs, cls_gt_scales, cls_gt_handle_visibility, synset_names, cls_pred_bboxes, cls_pred_class_ids, cls_pred_scores, cls_pred_RTs, cls_pred_scales, iou_thres_list)) if len(iou_pred_indices): cls_pred_class_ids = cls_pred_class_ids[iou_pred_indices] cls_pred_RTs = cls_pred_RTs[iou_pred_indices] cls_pred_scores = cls_pred_scores[iou_pred_indices] cls_pred_bboxes = cls_pred_bboxes[iou_pred_indices] iou_pred_matches_all[cls_id] = np.concatenate(( iou_pred_matches_all[cls_id], iou_cls_pred_match), axis=-1) cls_pred_scores_tile = np.tile(cls_pred_scores, (num_iou_thres, 1)) iou_pred_scores_all[cls_id] = np.concatenate(( iou_pred_scores_all[cls_id], cls_pred_scores_tile), axis=-1) assert iou_pred_matches_all[cls_id].shape[1 ] == iou_pred_scores_all[cls_id].shape[1] iou_gt_matches_all[cls_id] = np.concatenate((iou_gt_matches_all [cls_id], iou_cls_gt_match), axis=-1) if use_matches_for_pose: thres_ind = list(iou_thres_list).index(iou_pose_thres) iou_thres_pred_match = iou_cls_pred_match[(thres_ind), :] cls_pred_class_ids = cls_pred_class_ids[ iou_thres_pred_match > -1] if len(iou_thres_pred_match ) > 0 else np.zeros(0) cls_pred_RTs = cls_pred_RTs[iou_thres_pred_match > -1] if len( iou_thres_pred_match) > 0 else np.zeros((0, 4, 4)) cls_pred_scores = cls_pred_scores[iou_thres_pred_match > -1 ] if len(iou_thres_pred_match) > 0 else np.zeros(0) cls_pred_bboxes = cls_pred_bboxes[iou_thres_pred_match > -1 ] if len(iou_thres_pred_match) > 0 else np.zeros((0, 4)) iou_thres_gt_match = iou_cls_gt_match[(thres_ind), :] cls_gt_class_ids = cls_gt_class_ids[iou_thres_gt_match > -1 ] if len(iou_thres_gt_match) > 0 else np.zeros(0) cls_gt_RTs = cls_gt_RTs[iou_thres_gt_match > -1] if len( iou_thres_gt_match) > 0 else np.zeros((0, 4, 4)) cls_gt_handle_visibility = cls_gt_handle_visibility[ iou_thres_gt_match > -1] if len(iou_thres_gt_match ) > 0 else np.zeros(0) RT_overlaps = compute_RT_overlaps(cls_gt_class_ids, cls_gt_RTs, cls_gt_handle_visibility, cls_pred_class_ids, cls_pred_RTs, synset_names) pose_cls_gt_match, pose_cls_pred_match = ( compute_match_from_degree_cm(RT_overlaps, cls_pred_class_ids, cls_gt_class_ids, degree_thres_list, shift_thres_list)) pose_pred_matches_all[cls_id] = np.concatenate(( pose_pred_matches_all[cls_id], pose_cls_pred_match), axis=-1) cls_pred_scores_tile = np.tile(cls_pred_scores, ( num_degree_thres, num_shift_thres, 1)) pose_pred_scores_all[cls_id] = np.concatenate(( pose_pred_scores_all[cls_id], cls_pred_scores_tile), axis=-1) assert pose_pred_scores_all[cls_id].shape[2 ] == pose_pred_matches_all[cls_id].shape[2 ], '{} vs. {}'.format(pose_pred_scores_all[cls_id].shape, pose_pred_matches_all[cls_id].shape) pose_gt_matches_all[cls_id] = np.concatenate(( pose_gt_matches_all[cls_id], pose_cls_gt_match), axis=-1) fig_iou = plt.figure() ax_iou = plt.subplot(111) plt.ylabel('AP') plt.ylim((0, 1)) plt.xlabel('3D IoU thresholds') iou_output_path = os.path.join(log_dir, 'IoU_3D_AP_{}-{}.png'.format( iou_thres_list[0], iou_thres_list[-1])) iou_dict_pkl_path = os.path.join(log_dir, 'IoU_3D_AP_{}-{}.pkl'.format( iou_thres_list[0], iou_thres_list[-1])) iou_dict = {} iou_dict['thres_list'] = iou_thres_list for cls_id in range(1, num_classes): class_name = synset_names[cls_id] print(class_name) for s, iou_thres in enumerate(iou_thres_list): iou_3d_aps[cls_id, s] = compute_ap_from_matches_scores( iou_pred_matches_all[cls_id][(s), :], iou_pred_scores_all[ cls_id][(s), :], iou_gt_matches_all[cls_id][(s), :]) ax_iou.plot(iou_thres_list, iou_3d_aps[(cls_id), :], label=class_name) iou_3d_aps[(-1), :] = np.mean(iou_3d_aps[1:-1, :], axis=0) ax_iou.plot(iou_thres_list, iou_3d_aps[(-1), :], label='mean') ax_iou.legend() fig_iou.savefig(iou_output_path) plt.close(fig_iou) iou_dict['aps'] = iou_3d_aps with open(iou_dict_pkl_path, 'wb') as f: cPickle.dump(iou_dict, f) if use_matches_for_pose: prefix = 'Pose_Only_' else: prefix = 'Pose_Detection_' pose_dict_pkl_path = os.path.join(log_dir, prefix + 'AP_{}-{}degree_{}-{}cm.pkl'.format(degree_thres_list[0], degree_thres_list[-2], shift_thres_list[0], shift_thres_list[-2])) pose_dict = {} pose_dict['degree_thres'] = degree_thres_list pose_dict['shift_thres_list'] = shift_thres_list for i, degree_thres in enumerate(degree_thres_list): for j, shift_thres in enumerate(shift_thres_list): for cls_id in range(1, num_classes): cls_pose_pred_matches_all = pose_pred_matches_all[cls_id][( i), (j), :] cls_pose_gt_matches_all = pose_gt_matches_all[cls_id][(i), (j), :] cls_pose_pred_scores_all = pose_pred_scores_all[cls_id][(i), (j), :] pose_aps[cls_id, i, j] = compute_ap_from_matches_scores( cls_pose_pred_matches_all, cls_pose_pred_scores_all, cls_pose_gt_matches_all) pose_aps[-1, i, j] = np.mean(pose_aps[1:-1, (i), (j)]) pose_dict['aps'] = pose_aps with open(pose_dict_pkl_path, 'wb') as f: cPickle.dump(pose_dict, f) for cls_id in range(1, num_classes): class_name = synset_names[cls_id] print(class_name) fig_iou = plt.figure() ax_iou = plt.subplot(111) plt.ylabel('Rotation thresholds/degree') plt.ylim((degree_thres_list[0], degree_thres_list[-2])) plt.xlabel('translation/cm') plt.xlim((shift_thres_list[0], shift_thres_list[-2])) plt.imshow(pose_aps[(cls_id), :-1, :-1], cmap='jet', interpolation= 'bilinear') output_path = os.path.join(log_dir, prefix + 'AP_{}_{}-{}degree_{}-{}cm.png'.format(class_name, degree_thres_list[0], degree_thres_list[-2], shift_thres_list[0 ], shift_thres_list[-2])) plt.colorbar() plt.savefig(output_path) plt.close(fig_iou) fig_pose = plt.figure() ax_pose = plt.subplot(111) plt.ylabel('Rotation thresholds/degree') plt.ylim((degree_thres_list[0], degree_thres_list[-2])) plt.xlabel('translation/cm') plt.xlim((shift_thres_list[0], shift_thres_list[-2])) plt.imshow(pose_aps[(-1), :-1, :-1], cmap='jet', interpolation='bilinear') output_path = os.path.join(log_dir, prefix + 'mAP_{}-{}degree_{}-{}cm.png'.format(degree_thres_list[0], degree_thres_list[-2], shift_thres_list[0], shift_thres_list[-2])) plt.colorbar() plt.savefig(output_path) plt.close(fig_pose) fig_rot = plt.figure() ax_rot = plt.subplot(111) plt.ylabel('AP') plt.ylim((0, 1.05)) plt.xlabel('translation/cm') for cls_id in range(1, num_classes): class_name = synset_names[cls_id] print(class_name) ax_rot.plot(shift_thres_list[:-1], pose_aps[(cls_id), (-1), :-1], label=class_name) ax_rot.plot(shift_thres_list[:-1], pose_aps[(-1), (-1), :-1], label='mean') output_path = os.path.join(log_dir, prefix + 'mAP_{}-{}cm.png'.format( shift_thres_list[0], shift_thres_list[-2])) ax_rot.legend() fig_rot.savefig(output_path) plt.close(fig_rot) fig_trans = plt.figure() ax_trans = plt.subplot(111) plt.ylabel('AP') plt.ylim((0, 1.05)) plt.xlabel('Rotation/degree') for cls_id in range(1, num_classes): class_name = synset_names[cls_id] print(class_name) ax_trans.plot(degree_thres_list[:-1], pose_aps[(cls_id), :-1, (-1)], label=class_name) ax_trans.plot(degree_thres_list[:-1], pose_aps[(-1), :-1, (-1)], label= 'mean') output_path = os.path.join(log_dir, prefix + 'mAP_{}-{}degree.png'. format(degree_thres_list[0], degree_thres_list[-2])) ax_trans.legend() fig_trans.savefig(output_path) plt.close(fig_trans) iou_aps = iou_3d_aps print('3D IoU at 25: {:.1f}'.format(iou_aps[-1, iou_thres_list.index( 0.25)] * 100)) print('3D IoU at 50: {:.1f}'.format(iou_aps[-1, iou_thres_list.index( 0.5)] * 100)) print('5 degree, 5cm: {:.1f}'.format(pose_aps[-1, degree_thres_list. index(5), shift_thres_list.index(5)] * 100)) print('5 degree, 100cm: {:.1f}'.format(pose_aps[-1, degree_thres_list. index(5), shift_thres_list.index(100)] * 100)) print('10 degree, 5cm: {:.1f}'.format(pose_aps[-1, degree_thres_list. index(10), shift_thres_list.index(5)] * 100)) print('10 degree, 10cm: {:.1f}'.format(pose_aps[-1, degree_thres_list. index(10), shift_thres_list.index(10)] * 100)) print('15 degree, 5cm: {:.1f}'.format(pose_aps[-1, degree_thres_list. index(15), shift_thres_list.index(5)] * 100)) print('15 degree, 10cm: {:.1f}'.format(pose_aps[-1, degree_thres_list. index(15), shift_thres_list.index(10)] * 100)) return iou_3d_aps, pose_aps 
nn_layer.Relu|WXA def WXA_Relu(X, A, W, b): """ :param W: (600,600) :param X:  (?,600,targets_num) :param A: (targets_num,targets_num) :param b: useless :return: """ X_shape_1 = tf.shape(X)[1] X_shape_2 = tf.shape(X)[2] X_shape_1_ = tf.shape(W)[1] X_trans = tf.transpose(X, [0, 2, 1]) W_X_trans = tf.einsum('ijk,kl->ijl', X_trans, W) W_X = tf.transpose(W_X_trans, [0, 2, 1]) W_X_A_relu = tf.nn.relu(tf.matmul(W_X, A)) return W_X_A_relu 
test_envs.test|space def test_space(gym_space, expected_size, expected_min, expected_max): """Test the shape and bounds of an action or observation space.  Parameters ---------- gym_space : gym.spaces.Box gym space object to be tested expected_size : int expected size expected_min : float or array_like expected minimum value(s) expected_max : float or array_like expected maximum value(s) """ assert gym_space.shape[0] == expected_size, '{}, {}'.format(gym_space. shape[0], expected_size) np.testing.assert_almost_equal(gym_space.high, expected_max, decimal=4) np.testing.assert_almost_equal(gym_space.low, expected_min, decimal=4) 
thumt.utils.hooks.EvaluationHook.run|after def after_run(self, run_context, run_values): stale_global_step = run_values.results if self._timer.should_trigger_for_step(stale_global_step + 1): global_step = run_context.session.run(self._global_step) if self._timer.should_trigger_for_step(global_step): self._timer.update_last_triggered_step(global_step) save_path = os.path.join(self._base_dir, 'model.ckpt') saver = _get_saver() tf.logging.info('Saving checkpoints for %d into %s.' % ( global_step, save_path)) saver.save(run_context.session, save_path, global_step=global_step) tf.logging.info('Validating model at step %d' % global_step) score = _evaluate(self._eval_fn, self._eval_input_fn, self. _eval_decode_fn, self._base_dir, self._session_config) tf.logging.info('%s at step %d: %f' % (self._metric, global_step, score)) _save_log(self._log_name, (self._metric, global_step, score)) checkpoint_filename = os.path.join(self._base_dir, 'checkpoint') all_checkpoints = _read_checkpoint_def(checkpoint_filename) records = _read_score_record(self._record_name) latest_checkpoint = all_checkpoints[-1] record = [latest_checkpoint, score] added, removed, records = _add_to_record(records, record, self. _max_to_keep) if added is not None: old_path = os.path.join(self._base_dir, added) new_path = os.path.join(self._save_path, added) old_files = tf.gfile.Glob(old_path + '*') tf.logging.info('Copying %s to %s' % (old_path, new_path)) for o_file in old_files: n_file = o_file.replace(old_path, new_path) tf.gfile.Copy(o_file, n_file, overwrite=True) if removed is not None: filename = os.path.join(self._save_path, removed) tf.logging.info('Removing %s' % filename) files = tf.gfile.Glob(filename + '*') for name in files: tf.gfile.Remove(name) _save_score_record(self._record_name, records) checkpoint_filename = checkpoint_filename.replace(self. _base_dir, self._save_path) _save_checkpoint_def(checkpoint_filename, [item[0] for item in records]) best_score = records[0][1] tf.logging.info('Best score at step %d: %f' % (global_step, best_score)) 
model.GEMSEC.feed|generator|dict def feed_dict_generator(self, a_random_walk, step, gamma): """ Method to generate random walk features, left and right handside matrices, proper time index and overlap vector. """ batch_inputs = batch_input_generator(a_random_walk, self.args. random_walk_length, self.args.window_size) batch_labels = batch_label_generator(a_random_walk, self.args. random_walk_length, self.args.window_size) feed_dict = {self.walker_layer.train_labels: batch_labels, self. walker_layer.train_inputs: batch_inputs, self.gamma: gamma, self. step: float(step)} return feed_dict 
tests.models.DCN_test.test|DCN @pytest.mark.parametrize( 'embedding_size,cross_num,hidden_size,sparse_feature_num', [(8, 0, (32, ), 2), ('auto', 1, (), 1), ('auto', 1, (32,), 3)]) def test_DCN(embedding_size, cross_num, hidden_size, sparse_feature_num): model_name = 'DCN' sample_size = SAMPLE_SIZE x, y, feature_dim_dict = get_test_data(sample_size, sparse_feature_num, sparse_feature_num) model = DCN(feature_dim_dict, embedding_size=embedding_size, cross_num= cross_num, dnn_hidden_units=hidden_size, dnn_dropout=0.5) check_model(model, model_name, x, y) 
avod.core.anchor_generator.AnchorGenerator.generate def generate(self, **params): """Generates a collection of bounding boxes to be used as anchors. """ return self._generate(**params) 
GPSig.low_rank_calculations.indices|draw def draw_indices(n, l, need_inv=False): """ Draws l indices from 0 to n-1 without replacement. Returns of a list of drawn and not drawn indices, and the inverse permutation """ idx = tf.random_shuffle(tf.range(n)) idx_sampled, idx_not_sampled = tf.split(idx, [l, n - l]) if need_inv: inv_map = tf.reverse(tf.nn.top_k(idx, k=n, sorted=True)[1], axis=[0]) return idx_sampled, idx_not_sampled, inv_map else: return idx_sampled, idx_not_sampled 
EndToEndClassification.EnvClassification.Models.RawPiczakCNN.RawPiczak.save def save(self, path, sess): """ Saves the model variables to the specified path.  Args: path (str): folder path where the checkpoint will be saved. sess (tf Session): the session. """ vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='model') saver = tf.train.Saver(vars) saver.save(sess, path) 
models.GaussianMixture.sample def sample(self, n_samples): n_samples = int(n_samples) if self.is_train: cat_probs = self._cat(n_samples) agg_mu = tf.reduce_sum(tf.expand_dims(cat_probs, 2) * self._mu, axis=1) agg_var = tf.reduce_sum(tf.expand_dims(cat_probs, 2) * self._var, axis=1) raw = tf.random_normal([n_samples, self.dim]) ret = agg_mu + tf.sqrt(agg_var) * raw else: cat_samples = self._cat.sample(n_samples) samples_raw_indices = array_ops.reshape(math_ops.range(0, n_samples ), cat_samples.get_shape().as_list()) partitioned_samples_indices = data_flow_ops.dynamic_partition(data= samples_raw_indices, partitions=cat_samples, num_partitions= self.n_components) samples_class = [None for _ in range(self.n_components)] for c in range(self.n_components): n_class = array_ops.size(partitioned_samples_indices[c]) raw = tf.random_normal([n_class, self.dim]) samples_class_c = self._mu[c] + raw * tf.sqrt(self._var[c]) samples_class[c] = samples_class_c ret = data_flow_ops.dynamic_stitch(indices= partitioned_samples_indices, data=samples_class) ret.set_shape((int(n_samples), self.dim)) return ret 
nets.nasnet.pnasnet_test.PNASNetTest.Layer|Non|Model|Existing|test|Build|Large def testBuildNonExistingLayerLargeModel(self): """Tests that the model is built correctly without unnecessary layers.""" inputs = tf.random_uniform((5, 331, 331, 3)) tf.train.create_global_step() with slim.arg_scope(pnasnet.pnasnet_large_arg_scope()): pnasnet.build_pnasnet_large(inputs, 1000) vars_names = [x.op.name for x in tf.trainable_variables()] self.assertIn('cell_stem_0/1x1/weights', vars_names) self.assertNotIn('cell_stem_1/comb_iter_0/right/1x1/weights', vars_names) 
utils.DD.deepcopy def __deepcopy__(self, memo): z = DD() for k, kv in self.items(): z[k] = copy.deepcopy(kv, memo) return z 
graph_gen.load|cpnet def load_cpnet(): global cpnet, concept2id, relation2id, id2relation, id2concept, cpnet_simple print('loading cpnet....') cpnet = nx.read_gpickle(config['paths']['conceptnet_en_graph']) print('Done') cpnet_simple = nx.Graph() for u, v, data in cpnet.edges(data=True): w = data['weight'] if 'weight' in data else 1.0 if cpnet_simple.has_edge(u, v): cpnet_simple[u][v]['weight'] += w else: cpnet_simple.add_edge(u, v, weight=w) 
utils.embeddings|prepare def prepare_embeddings(): """ Prepares fastText embeddings (available at https://fasttext.cc/docs/en/english-vectors.html) for use in the model. Function expects unarchived fastText embedding file. """ file_in = io.open(cnf.embedding_file, 'r', encoding='utf-8', newline= '\n', errors='ignore') n, d = list(map(int, file_in.readline().split(' '))) print('Preparing embedding with dimensions:', n, d) word_to_id = {'C_PAD': 0, 'C_UNK': 1} word_to_vec = [['0' for _ in range(d)], [str(random.random()) for _ in range(d)]] word_id = 2 with open(cnf.emb_vector_file, 'w') as vector_file: for line in file_in: tokens = line.rstrip().split() word_to_vec.append(tokens[1:]) word_to_id[tokens[0]] = word_id if word_id % 100000 == 0: vector_file.writelines([(' '.join(word) + '\n') for word in word_to_vec]) word_to_vec = [] print('Done with', word_id, 'word') word_id += 1 vector_file.writelines([(' '.join(word) + '\n') for word in word_to_vec]) with open(cnf.emb_word_dictionary, 'wb') as id_out: pickle.dump(word_to_id, id_out) print('Pickled word dictionary') vector_file = np.loadtxt(cnf.emb_vector_file, dtype=np.float) with open(cnf.emb_vector_file, 'wb') as emb_file_bin: pickle.dump(vector_file, emb_file_bin) print('Pickled embedding') 
pytorch_pretrained_bert.modeling_openai.Attention.forward def forward(self, x): x = self.c_attn(x) query, key, value = x.split(self.split_size, dim=2) query = self.split_heads(query) key = self.split_heads(key, k=True) value = self.split_heads(value) a = self._attn(query, key, value) a = self.merge_heads(a) a = self.c_proj(a) a = self.resid_dropout(a) return a 
struc2vec_flight.embeddings|plot def plot_embeddings(embeddings): X, Y = read_node_label('../data/flight/labels-brazil-airports.txt', skip_head=True) emb_list = [] for k in X: emb_list.append(embeddings[k]) emb_list = np.array(emb_list) model = TSNE(n_components=2) node_pos = model.fit_transform(emb_list) color_idx = {} for i in range(len(X)): color_idx.setdefault(Y[i][0], []) color_idx[Y[i][0]].append(i) for c, idx in color_idx.items(): plt.scatter(node_pos[idx, 0], node_pos[idx, 1], label=c) plt.legend() plt.show() 
plato.run_plato_rds.parse @click.command() @click.option('--config', default='') def parse(config): if config: run_data_parser.run(config) else: print_usage() 
imagenet.ImageNetDataset.len def __len__(self): return len(self.base) 
elpips.squeeze|maxpool|elpips def elpips_squeeze_maxpool(batch_size=1, n=1, dtype=tf.float32): """E-LPIPS-SQUEEZENET-MAXPOOL configuration with all input transformations and dropout with p=0.99. Returns the average result over n samples. Does not use average pooling since SqueezeNet would require re-training for that. Experimental! Warning: Some versions of TensorFlow might have bugs that make n > 1 problematic due to the tf.while_loop used internally when n > 1. """ config = Config() config.metric = 'squeeze_ensemble_maxpool' config.batch_size = batch_size config.average_over = n config.dtype = dtype return config 
train.main def main(): import os os.environ['CUDA_VISIBLE_DEVICES'] = '0' session_config = tf.ConfigProto(log_device_placement=True, inter_op_parallelism_threads=20, intra_op_parallelism_threads=20, allow_soft_placement=True) session_config.gpu_options.allow_growth = True steps_per_epoch = train_config['train_nums'] // train_config['batch_size'] run_config = tf.estimator.RunConfig(session_config=session_config, save_checkpoints_steps=steps_per_epoch, save_summary_steps=100, log_step_count_steps=100, keep_checkpoint_max=200) if train_config['finetuning'] is not None: ws = tf.estimator.WarmStartSettings(ckpt_to_initialize_from= train_config['finetuning']) model_dir = train_config['finetuning'] else: ws = None model_dir = train_config['checkpoint_path'] estimator = tf.estimator.Estimator(model_fn=model_fn, config=run_config, model_dir=model_dir, params={'batch_size': train_config[ 'batch_size'], 'train_nums': steps_per_epoch, 'lr': 4e-05, 'height': train_config['height'], 'width': train_config['width'], 'num_kps': train_config['num_kps'], 'paf': train_config['paf'], 'scale': train_config['input_scale']}, warm_start_from=ws) tf.logging.set_verbosity(tf.logging.INFO) tf.logging.info('start training') train_spec = tf.estimator.TrainSpec(input_fn=lambda : train_input_fn( parameters=train_config, epochs=200, mode='train')) eval_spec = tf.estimator.EvalSpec(input_fn=lambda : train_input_fn( parameters=train_config, epochs=1, mode='valid')) tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec) 
xlnet-master.prepro_utils.print def print_(*args): new_args = [] for arg in args: if isinstance(arg, list): s = [printable_text(i) for i in arg] s = ' '.join(s) new_args.append(s) else: new_args.append(printable_text(arg)) print(*new_args) 
bim.resnet_model_wrapper.ResNetModelWrapper.to|id|label def id_to_label(self, idx): return self.labels[idx] 
translate.translation_model.TranslationModel.train def train(self, baseline_steps=0, loss_function='xent', use_baseline=True, **kwargs): self.init_training(**kwargs) if (loss_function == 'reinforce' and use_baseline and baseline_steps > 0 and self.baseline_step.eval() < baseline_steps): utils.log('pre-training reinforce baseline') for i in range(baseline_steps - self.baseline_step.eval()): self.seq2seq_model.reinforce_step(next(self.batch_iterator), update_model=False, use_sgd=False, update_baseline=True) utils.log('starting training') while True: try: self.train_step(loss_function=loss_function, use_baseline= use_baseline, **kwargs) sys.stdout.flush() except (utils.FinishedTrainingException, KeyboardInterrupt): utils.log('exiting...') self.save() return except utils.EvalException: self.save() step, score = self.training.scores[-1] self.manage_best_checkpoints(step, score) except utils.CheckpointException: self.save() 
t_wgan_sn_celeba_128.spectral|norm def spectral_norm(w, r=5): w_shape = K.int_shape(w) in_dim = np.prod(w_shape[:-1]).astype(int) out_dim = w_shape[-1] w = K.reshape(w, (in_dim, out_dim)) u = K.ones((1, in_dim)) for i in range(r): v = K.l2_normalize(K.dot(u, w)) u = K.l2_normalize(K.dot(v, K.transpose(w))) return K.sum(K.dot(K.dot(u, w), K.transpose(v))) 
texar.data.data.scalar_data_test.ScalarDataTest.default|test|setting def test_default_setting(self): """Tests the logics of ScalarData. """ self._run_and_test(self._int_hparams) self._run_and_test(self._float_hparams) 
classification.ops.fisher_blocks.EigenCorrectedKroneckerProductFB.register|and|basis|output|input|eigen def _register_input_and_output_eigen_basis(self, damping): """ Initialize the input and output Eigen basis with the damping term. :param damping: float :return: None """ self._input_damping, self._output_damping = _compute_pi_adjusted_damping( self._input_factor.get_cov(), self._output_factor.get_cov(), damping ** 0.5) self._input_factor.register_eigen_basis(self._input_damping) self._output_factor.register_eigen_basis(self._output_damping) 
train_dataset.Dataset.Darc|create def createDarc(self): """Loads a dataset of images from disk and stores it in the Darc format. Darc is similar in idea to HDF5 but much simpler and should handle multi-process parallelization issues better.""" print("Reading dataset '{}' from disk.".format(self.getCacheKey())) archive = darc.DataArchive(self.getDarcPath(), 'w') judge_files = cached_listdir(os.path.join(self.full_path, 'judge')) judge_files = [os.path.join(self.full_path, 'judge', file) for file in judge_files if os.path.splitext(file)[1] == '.npy'] judge_files = sorted(judge_files) p0_files = cached_listdir(os.path.join(self.full_path, 'p0')) p0_files = [os.path.join(self.full_path, 'p0', file) for file in p0_files if os.path.splitext(file)[1] == '.png'] p0_files = sorted(p0_files) p1_files = cached_listdir(os.path.join(self.full_path, 'p1')) p1_files = [os.path.join(self.full_path, 'p1', file) for file in p1_files if os.path.splitext(file)[1] == '.png'] p1_files = sorted(p1_files) ref_files = cached_listdir(os.path.join(self.full_path, 'ref')) ref_files = [os.path.join(self.full_path, 'ref', file) for file in ref_files if os.path.splitext(file)[1] == '.png'] ref_files = sorted(ref_files)  def handle_task(judge_path, p0_path, p1_path, ref_path): judge = np.load(judge_path)[0] p0 = load_image_uint(p0_path) p1 = load_image_uint(p1_path) ref = load_image_uint(ref_path) assert len(p0.shape) >= 3 and p0.shape[2] >= 3 assert len(p1.shape) >= 3 and p1.shape[2] >= 3 assert len(ref.shape) >= 3 and ref.shape[2] >= 3 p0 = p0[:, :, 0:3] p1 = p1[:, :, 0:3] ref = ref[:, :, 0:3] if self.dataset_mode == '2afc' and p0.shape[0:2] != (self.load_size, self.load_size): p0 = skimage.transform.resize(p0, [self.load_size, self. load_size, 3], mode='reflect', anti_aliasing=False) p1 = skimage.transform.resize(p1, [self.load_size, self. load_size, 3], mode='reflect', anti_aliasing=False) ref = skimage.transform.resize(ref, [self.load_size, self. load_size, 3], mode='reflect', anti_aliasing=False) return judge, p0, p1, ref, judge_path, p0_path, p1_path, ref_path if not all(len(collection) == len(judge_files) for collection in ( p0_files, p1_files, ref_files)): raise Exception('Dataset files missing!') with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor: postponed_image_sets = [] for i in range(len(judge_files)): judge_path, p0_path, p1_path, ref_path = judge_files[i], p0_files[i ], p1_files[i], ref_files[i] postponed_image_sets.append((judge_path, p0_path, p1_path, ref_path)) queued_image_sets = [] i = 0 while queued_image_sets or postponed_image_sets: while len(queued_image_sets) < 20 and postponed_image_sets: judge_path, p0_path, p1_path, ref_path = postponed_image_sets[0 ] queued_image_sets.append(executor.submit(handle_task, judge_path, p0_path, p1_path, ref_path)) postponed_image_sets.pop(0) judge, p0, p1, ref, judge_path, p0_path, p1_path, ref_path = ( queued_image_sets[0].result()) queued_image_sets.pop(0) print('{}/{}'.format(i, len(judge_files))) p_tensor = np.stack([p0, p1, ref]) archive.append(p_tensor, chunks=[1, -1, -1, -1], name='{}_p'. format(i)) judge_tensor = np.asarray(judge).reshape([1]) archive.append(judge_tensor, name='{}_judge'.format(i)) i += 1 archive.close() 
pvae.pixelvae_bbans_two_layer.vae|view def vae_view(head): return ag_tuple((np.reshape(head[:latent1_size], latent1_shape), np. reshape(head[latent1_size:latent1_size + latent2_size], latent2_shape), np.reshape(head[latent1_size + latent2_size:], ( batch_size,)))) 
pytorch_pretrained_bert.modeling_gpt2.MLP.forward def forward(self, x): h = self.act(self.c_fc(x)) h2 = self.c_proj(h) return h2 
neural_tangents.stax.In|Fan|Sum @_layer def FanInSum(): """Layer construction function for a fan-in sum layer.  Based on `jax.experimental.stax.FanInSum`. """ init_fn, apply_fn = ostax.FanInSum  def kernel_fn(kernels): is_gaussian = all(ker.is_gaussian for ker in kernels) if not is_gaussian: raise NotImplementedError( '`FanInSum` layer is only implemented for the case if all input layers guaranteed to be mean-zero gaussian, i.e. having all `is_gaussianset to `True`.' ) marginal, cross = kernels[0].marginal, kernels[0].cross shape1, shape2 = kernels[0].shape1, kernels[0].shape2 if not all(k.marginal == marginal and k.cross == cross for k in kernels ): raise NotImplementedError( '`FanInSum` layer is only implemented for the case if all input layers output the same typeof covariance matrices, i.e. having all matching `marginal` and `cross` attributes' ) n_kernels = len(kernels) n_height_width = sum(ker.is_height_width for ker in kernels) if n_height_width == n_kernels: is_height_width = True elif n_height_width >= n_kernels / 2: is_height_width = True for i in range(n_kernels): if not kernels[i].is_height_width: kernels[i] = _flip_height_width(kernels[i]) else: is_height_width = False for i in range(n_kernels): if kernels[i].is_height_width: kernels[i] = _flip_height_width(kernels[i]) if not all([(k.shape1 == shape1 and k.shape2 == shape2) for k in kernels]): raise ValueError('All shapes should be equal in FanInSum.') kers = tuple(None if all(ker[i] is None for ker in kernels) else sum(ker[i] for ker in kernels) for i in range(4)) return Kernel(*(kers + (is_gaussian, is_height_width, marginal, cross, None, None))) return init_fn, apply_fn, kernel_fn 
avod.core.box_list.BoxList.coordinates|and|sizes|center|get def get_center_coordinates_and_sizes(self, scope=None): """Computes the center coordinates, height and width of the boxes.  Args: scope: name scope of the function.  Returns: a list of 4 1-D tensors [ycenter, xcenter, height, width]. """ with tf.name_scope(scope, 'get_center_coordinates_and_sizes'): box_corners = self.get() ymin, xmin, ymax, xmax = tf.unstack(tf.transpose(box_corners)) width = xmax - xmin height = ymax - ymin ycenter = ymin + height / 2.0 xcenter = xmin + width / 2.0 return [ycenter, xcenter, height, width] 
data_gen.omni_gen.Img|read def readImg(path, size=[28, 28], rgb=False): img = cv2.imread(path) img = cv2.resize(img, (size[0], size[1])).astype(float) if np.max(img) > 1.0: img /= 255.0 if not rgb: return img[:, :, :1] elif len(img.shape) == 3: if img.shape[-1] != 3: print('ASFASFASFAS') print(img.shape) print(path) return img else: return np.reshape([img, img, img], [size[0], size[1], 3]) 
neural_tangents.stax.pad|one|side def _pad_one_side(x, pads, axes, mode): """Pad an array on one side. See `Returns` section for details.  Args: x: `np.ndarray` to pad, e.g. a 4D `NHWC` image. pads: tuple of integers, the convolutional filters spatial shape (e.g. `(3, 3)` for a 2D convolution). axes: tuple of non-negative integers, the axes to apply padding of sizes `pads` to. mode: a string, padding mode, for all options see https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html.  Returns: A `np.ndarray` of the same dimensionality as `x` padded to a potentially larger shape with `pads` applied at `axes`, where positive values in `pads` are applied on the left (start), and negative on the right (end). """ axis_pads = [((p, 0) if p >= 0 else (0, -p)) for p in pads] pads = [(0, 0)] * x.ndim for i in range(len(axes)): pads[axes[i]] = axis_pads[i] x = np.pad(x, pads, mode) return x 
PlotFunc.f def f(x, r, d, L): e = np.log2(1 / r) / L c = e - d print(c) return (x + 1) * r / (1 - 2 ** (c / x)) 
model.to|id|class|theta def class_id_to_theta(class_id):  def my_func(class_id): if class_id in [1, 2, 4]: return np.float32(2 * math.pi / 6) else: return np.float32(0) return tf.py_func(my_func, [class_id], tf.float32) 
bow_seq2seq.BowSeq2seq.train|dec|step def dec_train_step(self, sess, batch_dict): """Single step training on decoder""" max_len = batch_dict['enc_inputs'].shape[1] feed_dict = {self.dec_bow: batch_dict['dec_bow'], self.dec_bow_len: batch_dict['dec_bow_len'], self.dec_inputs: batch_dict['dec_inputs' ], self.dec_targets: batch_dict['dec_targets'], self.dec_lens: batch_dict['dec_lens'], self.max_len: max_len, self.drop_out: 0.3} dec_train_output = sess.run(self.dec_train_output, feed_dict=feed_dict) return dec_train_output 
borealisflows.noise_flow_model.NoiseFlow.loss def loss(self, x, y, nlf0=None, nlf1=None, iso=None, cam=None, reuse=False): nll, sd_z = self._loss(x=x, y=y, nlf0=nlf0, nlf1=nlf1, iso=iso, cam=cam, reuse=reuse) return tf.reduce_mean(nll), sd_z 
convert_minigo.getMinigoWeightsV2.conv|add def add_conv(number, with_gamma=True): number = tensor_number(number) weight_names.append('conv2d{}/kernel:0'.format(number)) weight_names.append('conv2d{}/bias:0'.format(number)) if with_gamma: weight_names.append('batch_normalization{}/gamma:0'.format(number)) weight_names.append('batch_normalization{}/beta:0'.format(number)) weight_names.append('batch_normalization{}/moving_mean:0'.format(number)) weight_names.append('batch_normalization{}/moving_variance:0'.format( number)) 
cpplint.Derived|Is|Function def IsDerivedFunction(clean_lines, linenum): """Check if current line contains an inherited function.  Args: clean_lines: A CleansedLines instance containing the file. linenum: The number of the line to check. Returns: True if current line contains a function with "override" virt-specifier. """ for i in xrange(linenum, max(-1, linenum - 10), -1): match = Match('^([^()]*\\w+)\\(', clean_lines.elided[i]) if match: line, _, closing_paren = CloseExpression(clean_lines, i, len( match.group(1))) return closing_paren >= 0 and Search('\\boverride\\b', line[ closing_paren:]) return False 
lm_torchtext._main.epoch|run def _run_epoch(sess, data_iter, epoch, is_train=False, verbose=False): start_time = time.time() loss = 0.0 iters = 0 state = sess.run(initial_state) fetches = {'mle_loss': mle_loss, 'final_state': final_state} if is_train: fetches['train_op'] = train_op mode = (tf.estimator.ModeKeys.TRAIN if is_train else tf.estimator. ModeKeys.EVAL) epoch_size = (len(train) // batch_size - 1) // num_steps for step, data_batch in enumerate(data_iter): feed_dict = {inputs: data_batch.text, targets: data_batch.target, global_step: epoch, tx.global_mode(): mode} for i, (c, h) in enumerate(initial_state): feed_dict[c] = state[i].c feed_dict[h] = state[i].h rets = sess.run(fetches, feed_dict) loss += rets['mle_loss'] state = rets['final_state'] iters += num_steps ppl = np.exp(loss / iters) if verbose and step % (epoch_size // 10) == 10: print('%.3f perplexity: %.3f speed: %.0f wps' % (step * 1.0 / epoch_size, ppl, iters * batch_size / (time.time() - start_time))) ppl = np.exp(loss / iters) return ppl 
layers.rlngru_layer.slice|step def _step_slice(m_, x_, xx_, h_, U, Ux, s1, s2, s3, s4): x_ = rln(x_, s1) xx_ = rln(xx_, s2) preact = tensor.dot(h_, U) preact = rln(preact, s3) preact += x_ r = tensor.nnet.sigmoid(_slice(preact, 0, dim)) u = tensor.nnet.sigmoid(_slice(preact, 1, dim)) preactx = tensor.dot(h_, Ux) preactx = rln(preactx, s4) preactx = preactx * r preactx = preactx + xx_ h = tensor.tanh(preactx) h = u * h_ + (1.0 - u) * h h = m_[:, (None)] * h + (1.0 - m_)[:, (None)] * h_ return h 
utils.batcher.get|batch def get_batch(self): if self.pos + self.bsz >= len(self.idx): bidx = self.idx[self.pos:] idx = self.rand.permutation(len(self.idx)) self.idx = np.int32(idx) self.pos = 0 if len(bidx) < self.bsz: self.pos = self.bsz - len(bidx) bidx2 = self.idx[0:self.pos] bidx = np.concatenate((bidx, bidx2)) else: bidx = self.idx[self.pos:self.pos + self.bsz] self.pos = self.pos + self.bsz return [self.data[bidx[i]] for i in range(len(bidx))], self.labels[bidx, ...] 
tf_util.for|conv|d|batch|norm def batch_norm_for_conv3d(inputs, is_training, bn_decay, scope, is_dist=False): """ Batch normalization on 3D convolutional maps.  Args: inputs:      Tensor, 5D BDHWC input maps is_training: boolean tf.Varialbe, true indicates training phase bn_decay:    float or float tensor variable, controling moving average weight scope:       string, variable scope is_dist:     true indicating distributed training scheme Return: normed:      batch-normalized maps """ if is_dist: return batch_norm_dist_template(inputs, is_training, scope, [0, 1, 2, 3], bn_decay) else: return batch_norm_template(inputs, is_training, scope, [0, 1, 2, 3], bn_decay) 
models.lenet def lenet5(num_classes, activation_fn=tf.nn.relu, w_init=slim.initializers. variance_scaling_initializer(), name='lenet5'):  @with_end_points def net(inputs, train=True): with slim.arg_scope([slim.conv2d], activation_fn=activation_fn, weights_initializer=w_init, padding='VALID'), slim.arg_scope([ slim.conv2d, slim.max_pool2d], data_format='NCHW'): net = slim.conv2d(inputs, 32, 5) net = slim.max_pool2d(net, 2) net = slim.conv2d(net, 64, 5) net = slim.max_pool2d(net, 2) net = slim.flatten(net) with slim.arg_scope([slim.fully_connected], activation_fn= activation_fn, weights_initializer=w_init): net = slim.fully_connected(net, 512) logits = slim.fully_connected(net, num_classes, activation_fn= None, scope='logits') return logits return tf.make_template(name, net) 
cleverhans.devtools.tests.docscrape.SphinxDocString.str|indent def _str_indent(self, doc, indent=4): out = [] for line in doc: out += [' ' * indent + line] return out 
darkflow.dark.convolution.conv_select_layer.setup def setup(self, ksize, c, n, stride, pad, batch_norm, activation, keep_idx, real_n): self.batch_norm = bool(batch_norm) self.activation = activation self.keep_idx = keep_idx self.stride = stride self.ksize = ksize self.pad = pad self.wshape = dict({'biases': [real_n], 'kernel': [ksize, ksize, c, real_n]}) if self.batch_norm: self.wshape.update({'moving_variance': [real_n], 'moving_mean': [ real_n], 'gamma': [real_n]}) self.h['is_training'] = {'shape': (), 'feed': True, 'dfault': False} 
modeling_openai_test.OpenAIGPTModelTest.OpenAIGPTModelTester.head|openai|lm|create def create_openai_lm_head(self, config, input_ids, token_type_ids, position_ids, mc_labels, lm_labels, mc_token_ids): model = OpenAIGPTLMHeadModel(config) model.eval() loss = model(input_ids, position_ids, token_type_ids, lm_labels) lm_logits = model(input_ids, position_ids, token_type_ids) outputs = {'loss': loss, 'lm_logits': lm_logits} return outputs 
word_chatbot.WordChatbot.data|dir @data_dir.setter def data_dir(self, value): self._data_dir = value 
utils.symmetry|d|compute|projection|RT def compute_RT_projection_2d_symmetry(RT_1, RT_2, class_id, handle_visibility, mesh_vertices, intrinsics, synset_names, num_rotation=20 ): """ :param RT_1: [4, 4]. homogeneous affine transformation :param RT_2: [4, 4]. homogeneous affine transformation :param vertices: [3, N]. :param intrinsics: [4, 4] :return: mean 2d projection distance in pixel  synset_names = ['BG',  # 0 'bottle',  # 1 'bowl',  # 2 'camera',  # 3 'can',  # 4 'laptop',  # 5 'mug'  # 6 ] """ if RT_1 is None or RT_2 is None: return -1 try: assert np.array_equal(RT_1[(3), :], RT_2[(3), :]) assert np.array_equal(RT_1[(3), :], np.array([0, 0, 0, 1])) except AssertionError: print(RT_1[(3), :], RT_2[(3), :]) exit() RT_1[:3, :3] = RT_1[:3, :3] / np.cbrt(np.linalg.det(RT_1[:3, :3])) R1 = RT_1[:3, :3] RT_2[:3, :3] = RT_2[:3, :3] / np.cbrt(np.linalg.det(RT_2[:3, :3])) R2 = RT_2[:3, :3] try: assert np.abs(np.linalg.det(R1) - 1) < 0.01 assert np.abs(np.linalg.det(R2) - 1) < 0.01 except AssertionError: print(np.linalg.det(R1), np.linalg.det(R2)) vertices = np.copy(mesh_vertices) / 1000 assert np.amax(vertices) < 0.5, np.amax(vertices) assert np.amax(vertices) > 0, np.amax(vertices) assert np.amin(vertices) < 0, np.amin(vertices) assert np.amin(vertices) > -0.5, np.amin(vertices) assert vertices.shape[0] == 3 num_vertices = vertices.shape[1] coords_3d_1 = transform_coordinates_3d(vertices, RT_1) projected_1 = calculate_2d_projections(coords_3d_1, intrinsics) coords_3d_2 = transform_coordinates_3d(vertices, RT_2) projected_2 = calculate_2d_projections(coords_3d_2, intrinsics) dists = np.linalg.norm(projected_1 - projected_2, axis=1) assert len(dists) == num_vertices min_mean_dist = np.mean(dists) if synset_names[class_id] in ['bottle', 'can', 'bowl'] or synset_names[ class_id] == 'mug' and handle_visibility == 0:  def y_rotation_matrix(theta): return np.array([[np.cos(theta), 0, np.sin(theta)], [0, 1, 0], [-np.sin(theta), 0, np.cos(theta)]]) for i in range(1, num_rotation): theta = 2 * math.pi * i / float(num_rotation) coords_3d_2 = transform_coordinates_3d(y_rotation_matrix(theta) @ vertices, RT_2) projected_2 = calculate_2d_projections(coords_3d_2, intrinsics) dists = np.linalg.norm(projected_1 - projected_2, axis=1) assert len(dists) == num_vertices min_mean_dist = min(min_mean_dist, np.mean(dists)) elif synset_names[class_id] in ['phone']: y_180_RT = np.diag([-1.0, 1.0, -1.0]) coords_3d_2 = transform_coordinates_3d(y_180_RT @ vertices, RT_2) projected_2 = calculate_2d_projections(coords_3d_2, intrinsics) dists = np.linalg.norm(projected_1 - projected_2, axis=1) assert len(dists) == num_vertices min_mean_dist = min(min_mean_dist, np.mean(dists)) elif synset_names[class_id] in ['eggbox', 'glue']: z_180_RT = np.diag([-1.0, -1.0, 1.0]) coords_3d_2 = transform_coordinates_3d(z_180_RT @ vertices, RT_2) projected_2 = calculate_2d_projections(coords_3d_2, intrinsics) dists = np.linalg.norm(projected_1 - projected_2, axis=1) assert len(dists) == num_vertices min_mean_dist = min(min_mean_dist, np.mean(dists)) else: min_mean_dist = min_mean_dist return min_mean_dist 
gym_pycolab.examples.warehouse_manager.BoxSprite.update def update(self, actions, board, layers, backdrop, things, the_plot): del backdrop, things rows, cols = self.position if actions == 0: if layers['P'][rows + 1, cols]: self._north(board, the_plot) elif actions == 1: if layers['P'][rows - 1, cols]: self._south(board, the_plot) elif actions == 2: if layers['P'][rows, cols + 1]: self._west(board, the_plot) elif actions == 3: if layers['P'][rows, cols - 1]: self._east(board, the_plot) 
nmt.attention_utils.AttentionWrapperState.clone.same|shape|with def with_same_shape(old, new): """Check and set new tensor's shape.""" if isinstance(old, ops.Tensor) and isinstance(new, ops.Tensor): return tensor_util.with_same_shape(old, new) return new 
gym_pycolab.tests.test_things.TestMazeWalker.real|update def real_update(self, actions, board, layers, backdrop, things, the_plot): if isinstance(actions, str): direction = actions elif isinstance(actions, dict): direction = actions.get(self.character, None) else: direction = None if direction == 'nw': result = self._northwest(board, the_plot) elif direction == 'n': result = self._north(board, the_plot) elif direction == 'ne': result = self._northeast(board, the_plot) elif direction == 'e': result = self._east(board, the_plot) elif direction == 'se': result = self._southeast(board, the_plot) elif direction == 's': result = self._south(board, the_plot) elif direction == 'sw': result = self._southwest(board, the_plot) elif direction == 'w': result = self._west(board, the_plot) else: result = self._stay(board, the_plot) the_plot['walk_result_{}'.format(self.character)] = result 
sw_loader.Dataset.Dataset def __init__(self, jsonl_path, mode=None): self.mode = mode self.raw = [] self.lst = [] self.refs = [] if mode == 'test': lst = json.load(open(jsonl_path, 'r')) for item in lst: context = item['context'] dialog = [] for utts in context: p = utts.find(':') dialog.append(((utts[p - 1] == 'A') * 2 - 1, utts[p + 2:-1], 0) ) if dialog[0][1][-1] == '>': dialog = dialog[1:] if len(dialog) == 0: continue responses = [] for resp in item['responses']: responses.append(resp) spk = (item['speaker'] == 'A') * 2 - 1 dialog.append((spk, responses[0], 0)) responses = responses[1:] responses = [' '.join(WordPunctTokenizer().tokenize(resp)) for resp in responses] if len(responses) == 0: continue self.raw.append(dialog) self.lst.append((len(self.raw) - 1, 0, len(dialog))) self.refs.append(responses) return from collections import Counter self.ct = Counter() self.topics = [] with open(jsonl_path, 'r') as f: for idx, item in enumerate(reader(f)): utts = item['utts'] self.topics.append(item['topic']) self.raw.append([(int(speaker == 'A') * 2 - 1, sentence, _) for speaker, sentence, _ in utts]) lst = [(idx, start, start + wnd_sz) for start in range(0, len( utts) - wnd_sz)] + [(idx, 0, end) for end in range(2, min( wnd_sz + 1, len(utts)))] self.lst += lst self.refs = [['none']] * len(self.lst) 
hbaselines.envs.hac.envs.Pendulum.observation|space @property def observation_space(self): return gym.spaces.Box(low=0, high=1, shape=(2 * len(self.sim.data.qpos) + len(self.sim.data.qvel),), dtype=np.float32) 
hbaselines.goal_conditioned.policy.FeedForwardPolicy.get|action def get_action(self, obs, context, apply_noise, random_actions): """See parent class.""" obs = self._get_obs(obs, context, axis=1) if random_actions: action = np.array([self.ac_space.sample()]) else: action = self.sess.run(self.actor_tf, {self.obs_ph: obs}) if apply_noise: if apply_noise: action += np.random.normal(0, self.noise, action.shape) action = np.clip(action, self.ac_space.low, self.ac_space.high) return action 
GPSig.kernels.SequentialPoly.Poly|Sequential def __init__(self, input_dim, gamma=1, degree=3, **kwargs): Sequential.__init__(self, input_dim, **kwargs) self.gamma = Parameter(gamma, transform=transforms.positive, dtype= settings.float_type) self.degree = Parameter(degree, dtype=settings.float_type, trainable=False) self._base_kern = self._poly 
bert-master.run_classifier.InputExample.Example|Input def __init__(self, guid, text_a, text_b=None, label=None): """Constructs a InputExample.  Args: guid: Unique id for the example. text_a: string. The untokenized text of the first sequence. For single sequence tasks, only this sequence must be specified. text_b: (Optional) string. The untokenized text of the second sequence. Only must be specified for sequence pair tasks. label: (Optional) string. The label of the example. This should be specified for train and dev examples, but not for test examples. """ self.guid = guid self.text_a = text_a self.text_b = text_b self.label = label 
CRPM_Net.CRPM_Net.Cs|CNN|inference def inference_Cs_CNN(self): self.label = tf.placeholder(tf.int32, shape=[None], name='image_label') self.up_h_convs = OrderedDict() dw_h_convs = self.down_conv() x = dw_h_convs[self.layers - 1] flat_real = tf.reshape(x[0], [tf.shape(self.input_real)[0], -1]) flat_imag = tf.reshape(x[1], [tf.shape(self.input_real)[0], -1]) fc_real, fc_imag = complex_cross_fc(flat_real, flat_imag, 'fc', [self. features_root * 4, self.num_label], False) logits_real = tf.expand_dims(fc_real, -1) logits_imag = tf.expand_dims(fc_imag, -1) self.real = logits_real self.imag = logits_imag self.mold = tf.sqrt(tf.add(tf.square(logits_real), tf.square(logits_imag))) self.phase = tf.atan(tf.div(logits_imag, tf.add(logits_real, tf. constant(1e-08)))) self.logits = tf.concat((logits_real, logits_imag, self.mold, self. phase), -1) with tf.variable_scope('mold_phase'): self.weight = tf.get_variable(name='weight_model', shape=[4, 1], initializer=tf.contrib.layers.xavier_initializer(), dtype=tf. float32) self.logits = tf.reshape(tf.matmul(tf.reshape(self.logits, [-1, 4]), self.weight), [tf.shape(self.input_real)[0], self.num_label]) self.out = tf.argmax(self.logits, 1) 
train_scannet.string|log def log_string(out_str): LOG_FOUT.write(out_str + '\n') LOG_FOUT.flush() print(out_str) 
texar.modules.memory.memory_network.MemNetRNNLike._build.unsqueeze def _unsqueeze(x): return x if x is None else tf.expand_dims(x, 1) 
pytorch_pretrained_bert.modeling_gpt2.Block.forward def forward(self, x, layer_past=None): a, present = self.attn(self.ln_1(x), layer_past=layer_past) x = x + a m = self.mlp(self.ln_2(x)) x = x + m return x, present 
borealisflows.layers.conv|d def conv2d(name, x, width, filter_size=[3, 3], stride=[1, 1], pad='SAME', do_weightnorm=False, context1d=None, skip=1, edge_bias=False): with tf.variable_scope(name, reuse=tf.AUTO_REUSE): if edge_bias and pad == 'SAME': x = add_edge_padding(x, filter_size) pad = 'VALID' n_in = int(x.get_shape()[3]) stride_shape = [1] + stride + [1] filter_shape = filter_size + [n_in, width] w = tf.get_variable('W', filter_shape, tf.float32, initializer= default_initializer(width / 512 * 0.05)) if do_weightnorm: w = tf.nn.l2_normalize(w, [0, 1, 2]) if skip == 1: x = tf.nn.conv2d(x, w, stride_shape, pad, data_format='NHWC') else: assert stride[0] == 1 and stride[1] == 1 x = tf.nn.atrous_conv2d(x, w, skip, pad) x += tf.get_variable('b', [1, 1, 1, width], initializer=tf. zeros_initializer()) return x 
utils.training.rate|learning|adjust def adjust_learning_rate(optimizer, lr): for param_group in optimizer.param_groups: param_group['lr'] = lr return lr 
models.attacks.CarliniWagnerL0.generate def generate(self, x, **kwargs): """ Return a tensor that constructs adversarial examples for the given input. Generate uses tf.py_func in order to operate over tensors. :param x: (required) A tensor with the inputs. :param y: (optional) A tensor with the true labels for an untargeted attack. If None (and y_target is None) then use the original labels the classifier assigns. :param y_target: (optional) A tensor with the target labels for a targeted attack. :param confidence: Confidence of adversarial examples: higher produces examples with larger l2 distortion, but more strongly classified as adversarial. :param learning_rate: The learning rate for the attack algorithm. Smaller values produce better results but are slower to converge. :param max_iterations: The maximum number of iterations. Setting this to a larger value will produce lower distortion results. Using only a few iterations requires a larger learning rate, and will produce larger distortion results. :param abort_early: If true, allows early aborts if gradient descent is unable to make progress (i.e., gets stuck in a local minimum). :param initial_const: The initial tradeoff-constant to use to tune the relative importance of size of the pururbation and confidence of classification. A smaller value of this constant gives lower distortion results. :param largest_const: When the tradeoff-constant exceeds this value, the attack terminates. Larger values gives lower distortion results. :param const_factor: How much to increase the tradeoff-constant by on each iteration of the attack if the prior iteration failed. :param clip_min: (optional float) Minimum input component value :param clip_max: (optional float) Maximum input component value """ import tensorflow as tf from .attacks_tf import CarliniWagnerL0 as CWL0 self.parse_params(**kwargs) labels, nb_classes = self.get_or_guess_labels(x, kwargs) attack = CWL0(self.sess, self.model, self.confidence, 'y_target' in kwargs, self.learning_rate, self.max_iterations, self.abort_early, self.initial_const, self.largest_const, self.const_factor, self. clip_min, self.clip_max, nb_classes, 3)  def cw_wrap(x_val, y_val): return np.array(attack.attack(x_val, y_val), dtype=np.float32) wrap = tf.py_func(cw_wrap, [x, labels], tf.float32) return wrap 
mac_testing.train|validation|set def train_set_validation(data_loader, z_train_prep_ops, train_error_list, losses, epoch, sess, args): """ run an epoch over train set and write logs :param data_loader: :param z_train_prep_ops: :param train_error_list: :param losses: :param epoch: :param sess: :param args: :return: """ sess.run(data_loader.iter.initializer, feed_dict=data_loader.train_dict) n_loss_acc, accuracy_acc = 0, 0 while True: try: sess.run(data_loader.get_next_op) except tf.errors.OutOfRangeError: train_acc_factor = args.batch_size / args.train_set_size train_error_nested = n_loss_acc * train_acc_factor test_summary_string = sess.run(logger.ops['train_loss'], feed_dict={logger.acc_loss_pl: train_error_nested}) logger.writer.add_summary(test_summary_string, epoch) print('train error nested:  {:.5}'.format(train_error_nested)) if losses.n_correct is not None: accuracy = accuracy_acc / args.train_set_size print('train accuracy:      {:.3}'.format(accuracy)) train_error_list.append(accuracy) train_acc_string = sess.run(logger.ops['train_accuracy'], feed_dict={logger.acc_loss_pl: accuracy}) logger.writer.add_summary(train_acc_string, epoch) else: train_error_list.append(train_error_nested) break sess.run(z_train_prep_ops) if losses.n_correct is None: nlu = sess.run(losses.nested) else: nlu, acc = sess.run([losses.nested, losses.n_correct]) accuracy_acc += acc n_loss_acc = n_loss_acc + nlu 
utils.ssm_psnr_utils.ssim|compute def compute_ssim(X, Y): """ Compute the structural similarity per single channel (given two images) """ K1 = 0.01 K2 = 0.03 sigma = 1.5 win_size = 5 ux = gaussian_filter(X, sigma) uy = gaussian_filter(Y, sigma) uxx = gaussian_filter(X * X, sigma) uyy = gaussian_filter(Y * Y, sigma) uxy = gaussian_filter(X * Y, sigma) N = win_size ** X.ndim unbiased_norm = N / (N - 1) vx = (uxx - ux * ux) * unbiased_norm vy = (uyy - uy * uy) * unbiased_norm vxy = (uxy - ux * uy) * unbiased_norm R = 255 C1 = (K1 * R) ** 2 C2 = (K2 * R) ** 2 sim = (2 * ux * uy + C1) * (2 * vxy + C2) D = (ux ** 2 + uy ** 2 + C1) * (vx + vy + C2) SSIM = sim / D mssim = SSIM.mean() return mssim 
mac_testing.args|log def log_args(log_dir, args): """ print and save all args """ if not os.path.exists(log_dir): os.makedirs(log_dir) with open(os.path.join(log_dir, 'args_log'), 'w') as f: lines = ['  {:<25}- {}\n'.format(key, val) for key, val in vars( args).items()] f.writelines(lines) for line in lines: print(line.rstrip()) print('-------------------------------------------') 
avod.core.box_list_ops_test.BoxListOpsTest.test|height|width def test_height_width(self): corners = tf.constant([[0.0, 0.0, 10.0, 20.0], [1.0, 2.0, 3.0, 4.0]]) exp_output_heights = [10.0, 2.0] exp_output_widths = [20.0, 2.0] boxes = box_list.BoxList(corners) heights, widths = box_list_ops.height_width(boxes) with self.test_session() as sess: output_heights, output_widths = sess.run([heights, widths]) self.assertAllClose(output_heights, exp_output_heights) self.assertAllClose(output_widths, exp_output_widths) 
generalization_plot.calculate|variance def calculate_variance(X, epoch): var = [np.cov(X[epoch == e].T).trace() for e in np.unique(epoch)] return np.sum(var) 
vkge.models.similarities.dot|product def dot_product(x1, x2, axis=1): """ Dot Product.  .. math:: L = \\sum_i x1_i x2_i  :param x1: First term. :param x2: Second term. :param axis: Reduction Indices. :return: Similarity Value. """ similarity = tf.reduce_sum(x1 * x2, axis=axis) return similarity 
nets.vgg_test.VGG19Test.Classes|test|No def testNoClasses(self): batch_size = 5 height, width = 224, 224 num_classes = None with self.test_session(): inputs = tf.random_uniform((batch_size, height, width, 3)) net, end_points = vgg.vgg_19(inputs, num_classes) expected_names = ['vgg_19/conv1/conv1_1', 'vgg_19/conv1/conv1_2', 'vgg_19/pool1', 'vgg_19/conv2/conv2_1', 'vgg_19/conv2/conv2_2', 'vgg_19/pool2', 'vgg_19/conv3/conv3_1', 'vgg_19/conv3/conv3_2', 'vgg_19/conv3/conv3_3', 'vgg_19/conv3/conv3_4', 'vgg_19/pool3', 'vgg_19/conv4/conv4_1', 'vgg_19/conv4/conv4_2', 'vgg_19/conv4/conv4_3', 'vgg_19/conv4/conv4_4', 'vgg_19/pool4', 'vgg_19/conv5/conv5_1', 'vgg_19/conv5/conv5_2', 'vgg_19/conv5/conv5_3', 'vgg_19/conv5/conv5_4', 'vgg_19/pool5', 'vgg_19/fc6', 'vgg_19/fc7'] self.assertSetEqual(set(end_points.keys()), set(expected_names)) self.assertTrue(net.op.name.startswith('vgg_19/fc7')) 
generate.generate def generate(infrared_path, visible_path, model_path, model_pre_path, ssim_weight, index, IS_VIDEO, IS_RGB, type='addition', output_path=None): if IS_VIDEO: print('video_addition') _handler_video(infrared_path, visible_path, model_path, model_pre_path, ssim_weight, output_path=output_path) elif IS_RGB: print('RGB - addition') _handler_rgb(infrared_path, visible_path, model_path, model_pre_path, ssim_weight, index, output_path=output_path) print('RGB - l1') _handler_rgb_l1(infrared_path, visible_path, model_path, model_pre_path, ssim_weight, index, output_path=output_path) elif type == 'addition': print('addition') _handler(infrared_path, visible_path, model_path, model_pre_path, ssim_weight, index, output_path=output_path) elif type == 'l1': print('l1') _handler_l1(infrared_path, visible_path, model_path, model_pre_path, ssim_weight, index, output_path=output_path) 
amb_measure_utils.blur|np def blur_np(hparams, x): size = hparams.blur_filter_size gaussian_filter = get_gaussian_filter(hparams.blur_radius, size) gaussian_filter = np.reshape(gaussian_filter, [1, size, size, 1]) x_blurred = ndimage.filters.convolve(x, gaussian_filter, mode='constant') return x_blurred 
m_phate.train.BatchTraceHistory.on|batch|end def on_batch_end(self, epoch, logs): self.trace.append(np.array([data.T for data in self.trace_model.predict (self.trace_data)]).reshape(-1, self.trace_data.shape[0])) if self.save_weights: self.weights.append(self.trace_model.layers[1].get_weights()[0]) return super().on_epoch_end(epoch, logs) 
calculation_helper.SecondOrderRandomWalker.walks|simulate def simulate_walks(self, num_walks, walk_length): """ Repeatedly simulate random walks from each node. """ G = self.G walks = [] nodes = list(G.nodes()) for walk_iter in range(num_walks): print(' ') print('Random walk series ' + str(walk_iter + 1) + '. initiated.') print(' ') random.shuffle(nodes) for node in tqdm(nodes): walks.append(self.node2vec_walk(walk_length=walk_length, start_node=node)) return walks, self.count_frequency_values(walks) 
classification.ops.fisher_factors.InverseProvidingFactor.get|inverse|damped def get_damped_inverse(self, damping): return self._inverses_by_damping[damping] 
models.attacks.Attack.generate|np def generate_np(self, x_val, **kwargs): """ Generate adversarial examples and return them as a NumPy array. Sub-classes *should not* implement this method unless they must perform special handling of arguments. :param x_val: A NumPy array with the original inputs. :param **kwargs: optional parameters used by child classes. :return: A NumPy array holding the adversarial examples. """ if self.sess is None: raise ValueError('Cannot use `generate_np` when no `sess` was provided' ) fixed = dict((k, v) for k, v in kwargs.items() if k in self. structural_kwargs) feedable = dict((k, v) for k, v in kwargs.items() if k in self. feedable_kwargs) if len(fixed) + len(feedable) < len(kwargs): warnings.warn( 'Supplied extra keyword arguments that are not used in the graph computation. They have been ignored.' ) if not all(isinstance(value, collections.Hashable) for value in fixed. values()): hash_key = None else: hash_key = tuple(sorted(fixed.items())) if hash_key not in self.graphs: self.construct_graph(fixed, feedable, x_val, hash_key) x, new_kwargs, x_adv = self.graphs[hash_key] feed_dict = {x: x_val} for name in feedable: feed_dict[new_kwargs[name]] = feedable[name] return self.sess.run(x_adv, feed_dict) 
parse.TestParse.datasrc|test def test_datasrc(self): num_chunks = 3 chunks = [] for x in range(num_chunks): filename = '/tmp/parse-unittest-chunk' + str(x) + '.gz' chunk_file = gzip.open(filename, 'w', 1) chunk_file.write(bytes(x)) chunk_file.close() chunks.append(filename) ds = FileDataSrc(list(chunks)) counts = {} for _ in range(200): data = ds.next() if data in counts: counts[data] += 1 else: counts[data] = 1 for x in range(num_chunks): self.assertGreater(counts[bytes(x)], 3) self.assertEqual(len(counts.keys()), num_chunks) for c in chunks: os.remove(c) 
han2one.con|onehot def con2onehot(s): res = np.zeros(len(congseng)) if s in congseng: res[congseng.index(s)] = 1 return res 
deeplab_resnet.image_reader_segment.labeled|image|read|list def read_labeled_image_list(data_dir, data_list): """Reads txt file containing paths to images and ground truths.  Args: data_dir: path to the directory with images and masks. data_list: path to the file with lines of the form '/path/to/image /path/to/attn /path/to/sal /path/to/image-labels'.  Returns: Four lists with all file names for images, attention, saliencies and image-labels, respectively. """ f = open(data_list, 'r') images = [] attns = [] sals = [] catgs = [] for line in f: image, attn, sal, catg = line.strip('\n').split(' ') images.append(data_dir + image) attns.append(data_dir + attn) sals.append(data_dir + sal) catgs.append(data_dir + catg) return images, attns, sals, catgs 
train_eval.get|control|timestep def get_control_timestep(py_env): try: control_timestep = py_env.dt except AttributeError: control_timestep = py_env.control_timestep() return control_timestep 
craystack.codecs.buckets|logistic|mixture|create def _create_logistic_mixture_buckets(means, log_scales, logit_probs, coding_prec, bin_prec, bin_lb, bin_ub): inv_stdv = np.exp(-log_scales) buckets = np.linspace(bin_lb, bin_ub, (1 << bin_prec) + 1) buckets = np.broadcast_to(buckets, means.shape + ((1 << bin_prec) + 1,)) cdfs = inv_stdv[..., np.newaxis] * (buckets - means[..., np.newaxis]) cdfs[..., 0] = -np.inf cdfs[..., -1] = np.inf cdfs = sigmoid(cdfs) prob_cpts = cdfs[(...), 1:] - cdfs[(...), :-1] mixture_probs = util.softmax(logit_probs, axis=1) probs = np.sum(prob_cpts * mixture_probs[..., np.newaxis], axis=1) return _cumulative_buckets_from_probs(probs, coding_prec) 
nets.inception_v3_test.InceptionV3Test.Network|test|Build|Classification def testBuildClassificationNetwork(self): batch_size = 5 height, width = 299, 299 num_classes = 1000 inputs = tf.random_uniform((batch_size, height, width, 3)) logits, end_points = inception.inception_v3(inputs, num_classes) self.assertTrue(logits.op.name.startswith( 'InceptionV3/Logits/SpatialSqueeze')) self.assertListEqual(logits.get_shape().as_list(), [batch_size, num_classes]) self.assertTrue('Predictions' in end_points) self.assertListEqual(end_points['Predictions'].get_shape().as_list(), [ batch_size, num_classes]) 
regression.controller.distributions.EigenMultivariateNormal.mean @property def mean(self): """The mean of the MatrixVariateNormal distribution.""" return self._mean 
src.util.result_util.mail|send def send_mail(to_user, sub, content, send_file=None): print('Send email to {}'.format(to_user)) me = 'Result' + '<' + mail_user + '@' + mail_postfix + '>' msg = MIMEMultipart() part = MIMEText(content) msg.attach(part) msg['Subject'] = sub msg['From'] = me msg['To'] = to_user + '@' + mail_postfix if send_file != None: part = MIMEApplication(open(send_file, 'rb').read()) part.add_header('Content-Disposition', 'attachment', filename=send_file ) msg.attach(part) try: s = smtplib.SMTP() s.connect(mail_host) s.login(mail_user, mail_pass) s.sendmail(me, [msg['To']], msg.as_string()) s.close() return True except Exception as err: print(str(err)) return False 
src.pose_decode.plot|pose def plot_pose(img_orig, joint_list, person_to_joint_assoc, bool_fast_plot=True ): canvas = img_orig.copy() person_num = person_to_joint_assoc.shape[0] joints = np.zeros((person_num, 14, 3), dtype=np.float32) for person, person_joint_info in enumerate(person_to_joint_assoc): for limb_type in range(NUM_LIMBS): joint_indices = person_joint_info[ joint_to_limb_heatmap_relationship[limb_type]].astype(int) if -1 in joint_indices or len(joint_indices) < 2: continue joint_coords = joint_list[(joint_indices), 0:2] joint_types = joint_list[joint_indices, -1] for index, joint_type in enumerate(joint_types): x = joint_coords[index][0] y = joint_coords[index][1] v = 1 joints[(person), (int(joint_type)), :] = [x, y, v] for joint in joint_coords: cv2.circle(canvas, tuple(joint[0:2].astype(int)), 3, (255, 255, 255), thickness=-1) cv2.line(canvas, tuple(joint_coords[0].astype(int)), tuple( joint_coords[1].astype(int)), color=colors[person % len( colors)], thickness=2) return canvas, joints 
imagenet_to_gcs.main def main(argv): tf.logging.set_verbosity(tf.logging.INFO) if FLAGS.gcs_upload and FLAGS.project is None: raise ValueError('GCS Project must be provided.') if FLAGS.gcs_upload and FLAGS.gcs_output_path is None: raise ValueError('GCS output path must be provided.') elif FLAGS.gcs_upload and not FLAGS.gcs_output_path.startswith('gs://'): raise ValueError('GCS output path must start with gs://') if FLAGS.local_scratch_dir is None: raise ValueError('Scratch directory path must be provided.') raw_data_dir = FLAGS.raw_data_dir if raw_data_dir is None: raw_data_dir = os.path.join(FLAGS.local_scratch_dir, 'raw_data') tf.logging.info('Downloading data to raw_data_dir: %s' % raw_data_dir) download_dataset(raw_data_dir) training_records, validation_records = convert_to_tf_records(raw_data_dir) if FLAGS.gcs_upload: upload_to_gcs(training_records, validation_records) 
classification.ops.fisher_factors.InverseProvidingFactor.get|eigendecomp def get_eigendecomp(self): return self._eigendecomp 
deeppoly_nodes.DeeppolyTanhNodeFirst.transformer def transformer(self, nn, man, element, nlb, nub, use_area_heuristic): """ transformer for the first layer of a neural network, if that first layer is fully connected with tanh  Arguments --------- man : ElinaManagerPtr man to which element belongs element : ElinaAbstract0Ptr abstract element onto which the transformer gets applied  Return ------ output : ElinaAbstract0Ptr abstract element after the transformer """ ffn_handle_first_tanh_layer(man, element, *self.get_arguments()) return element 
avod.datasets.kitti.kitti_aug.compute|pca def compute_pca(image_set): """Calculates and returns PCA of a set of images  Args: image_set: List of images read with cv2.imread in np.uint8 format  Returns: PCA for the set of images """ assert image_set[0].dtype == np.uint8 reshaped_data = np.concatenate([image for pixels in image_set for image in pixels]) reshaped_data = (reshaped_data / 255.0).astype(np.float32) covariance = np.cov(reshaped_data.T) e_vals, e_vecs = np.linalg.eigh(covariance) pca = np.sqrt(e_vals) * e_vecs return pca 
texar.data.data_utils._download.progress def _progress(count, block_size, total_size): percent = float(count * block_size) / float(total_size) * 100.0 sys.stdout.write('\r>> Downloading %s %.1f%%' % (filename, percent)) sys.stdout.flush() 
sidd_utils.nll|normal def nll_normal(mu, vr, data): """Negative log likelihood of data given mean (mu) and variance (vr) of a univariate normal distribution NLL is computed pixel-wise (i.e., each pixel is a data point""" n = np.prod(data.shape) nll = n / 2 * np.log(2 * np.pi * vr) + np.sum((data - mu) ** 2) / (2 * vr) return nll 
model_unet.UNET.model|build def build_model(self): self.patches_labeled = tf.placeholder(tf.float32, [F.batch_size, self. patch_shape[0], self.patch_shape[1], self.patch_shape[2], F.num_mod ], name='real_images_l') self.labels = tf.placeholder(tf.uint8, [F.batch_size, self.patch_shape[ 0], self.patch_shape[1], self.patch_shape[2]], name='image_labels') self.labels_1hot = tf.one_hot(self.labels, depth=F.num_classes) self.phase = tf.placeholder(tf.bool) self._logits_labeled, self._probdist = self.network_dis(self. patches_labeled, reuse=False) self.Val_output = tf.argmax(self._probdist, axis=-1) class_weights = tf.constant([[0.33, 1.5, 0.83, 1.33]]) weights = tf.reduce_sum(class_weights * self.labels_1hot, axis=-1) unweighted_losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits= self._logits_labeled, labels=self.labels_1hot) weighted_losses = unweighted_losses * weights self.u_loss = tf.reduce_mean(weighted_losses) t_vars = tf.trainable_variables() self.u_vars = [var for var in t_vars if 'u_' in var.name] self.saver = tf.train.Saver() 
nets.resnet_v2_test.ResnetCompleteNetworkTest.test|Points|Classification|End def testClassificationEndPoints(self): global_pool = True num_classes = 10 inputs = create_test_input(2, 224, 224, 3) with slim.arg_scope(resnet_utils.resnet_arg_scope()): logits, end_points = self._resnet_small(inputs, num_classes, global_pool=global_pool, spatial_squeeze=False, scope='resnet') self.assertTrue(logits.op.name.startswith('resnet/logits')) self.assertListEqual(logits.get_shape().as_list(), [2, 1, 1, num_classes]) self.assertTrue('predictions' in end_points) self.assertListEqual(end_points['predictions'].get_shape().as_list(), [ 2, 1, 1, num_classes]) self.assertTrue('global_pool' in end_points) self.assertListEqual(end_points['global_pool'].get_shape().as_list(), [ 2, 1, 1, 32]) 
regression.controller.sample.OutSample.sample|p def p_sample(self, h): """ Sample from prior distribution.  :param h: Tensor of shape [n_particles, batch_size, n]. Representing the output of former forwarding process.  :return: A tensor of shape [n_particles, batch_size]. """ return NotImplementedError 
hbaselines.envs.mixed_autonomy.envs.FlowEnv.reset def reset(self): """Reset the environment.""" self.step_number = 0 return self.wrapped_env.reset() 
train_run.Weights|save def saveWeights(sess): linear_weight_as_dict = sess.run(model.network.linear_weight_as_dict) linear_weight_as_dict = {key: np.maximum(0.0, value) for key, value in linear_weight_as_dict.items()} np.save('./saves/{}-latest.npy'.format(opt.name), linear_weight_as_dict) 
SRGANs-Spectral-Regularization-GANs--master.source.inception.download.inception|copy def copy_inception(sess, model): """Copy weights and params from the graph in the given TensorFlow session to the Chainer chain.""" print('Copying first layers ...') copy_conv(sess, 'conv', model.conv) copy_bn(sess, 'conv/batchnorm', model.bn_conv) copy_conv(sess, 'conv_1', model.conv_1) copy_bn(sess, 'conv_1/batchnorm', model.bn_conv_1) copy_conv(sess, 'conv_2', model.conv_2) copy_bn(sess, 'conv_2/batchnorm', model.bn_conv_2) copy_conv(sess, 'conv_3', model.conv_3) copy_bn(sess, 'conv_3/batchnorm', model.bn_conv_3) copy_conv(sess, 'conv_4', model.conv_4) copy_bn(sess, 'conv_4/batchnorm', model.bn_conv_4) for m in ['mixed', 'mixed_1', 'mixed_2']: print('Copying ', m, '...') copy_conv(sess, '{}/conv'.format(m), getattr(model, m).conv.conv) copy_bn(sess, '{}/conv/batchnorm'.format(m), getattr(model, m).conv .bn_conv) for t in ['tower', 'tower_1', 'tower_2']: copy_conv(sess, '{}/{}/conv'.format(m, t), getattr(getattr( model, m), t).conv) copy_bn(sess, '{}/{}/conv/batchnorm'.format(m, t), getattr( getattr(model, m), t).bn_conv) if t == 'tower' or t == 'tower_1': copy_conv(sess, '{}/{}/conv_1'.format(m, t), getattr( getattr(model, m), t).conv_1) copy_bn(sess, '{}/{}/conv_1/batchnorm'.format(m, t), getattr(getattr(model, m), t).bn_conv_1) if t == 'tower_1': copy_conv(sess, '{}/{}/conv_2'.format(m, t), getattr( getattr(model, m), t).conv_2) copy_bn(sess, '{}/{}/conv_2/batchnorm'.format(m, t), getattr(getattr(model, m), t).bn_conv_2) for m in ['mixed_3']: print('Copying ', m, '...') copy_conv(sess, '{}/conv'.format(m), getattr(model, m).conv.conv) copy_bn(sess, '{}/conv/batchnorm'.format(m), getattr(model, m).conv .bn_conv) for t in ['tower']: copy_conv(sess, '{}/{}/conv'.format(m, t), getattr(getattr( model, m), t).conv) copy_bn(sess, '{}/{}/conv/batchnorm'.format(m, t), getattr( getattr(model, m), t).bn_conv) copy_conv(sess, '{}/{}/conv_1'.format(m, t), getattr(getattr( model, m), t).conv_1) copy_bn(sess, '{}/{}/conv_1/batchnorm'.format(m, t), getattr( getattr(model, m), t).bn_conv_1) copy_conv(sess, '{}/{}/conv_2'.format(m, t), getattr(getattr( model, m), t).conv_2) copy_bn(sess, '{}/{}/conv_2/batchnorm'.format(m, t), getattr( getattr(model, m), t).bn_conv_2) for m in ['mixed_4', 'mixed_5', 'mixed_6', 'mixed_7']: print('Copying ', m, '...') copy_conv(sess, '{}/conv'.format(m), getattr(model, m).conv.conv) copy_bn(sess, '{}/conv/batchnorm'.format(m), getattr(model, m).conv .bn_conv) for t in ['tower', 'tower_1', 'tower_2']: copy_conv(sess, '{}/{}/conv'.format(m, t), getattr(getattr( model, m), t).conv) copy_bn(sess, '{}/{}/conv/batchnorm'.format(m, t), getattr( getattr(model, m), t).bn_conv) if t == 'tower' or t == 'tower_1': copy_conv(sess, '{}/{}/conv_1'.format(m, t), getattr( getattr(model, m), t).conv_1) copy_bn(sess, '{}/{}/conv_1/batchnorm'.format(m, t), getattr(getattr(model, m), t).bn_conv_1) copy_conv(sess, '{}/{}/conv_2'.format(m, t), getattr( getattr(model, m), t).conv_2) copy_bn(sess, '{}/{}/conv_2/batchnorm'.format(m, t), getattr(getattr(model, m), t).bn_conv_2) if t == 'tower_1': copy_conv(sess, '{}/{}/conv_3'.format(m, t), getattr( getattr(model, m), t).conv_3) copy_bn(sess, '{}/{}/conv_3/batchnorm'.format(m, t), getattr(getattr(model, m), t).bn_conv_3) copy_conv(sess, '{}/{}/conv_4'.format(m, t), getattr( getattr(model, m), t).conv_4) copy_bn(sess, '{}/{}/conv_4/batchnorm'.format(m, t), getattr(getattr(model, m), t).bn_conv_4) for m in ['mixed_8']: print('Copying ', m, '...') for t in ['tower', 'tower_1']: copy_conv(sess, '{}/{}/conv'.format(m, t), getattr(getattr( model, m), t).conv) copy_bn(sess, '{}/{}/conv/batchnorm'.format(m, t), getattr( getattr(model, m), t).bn_conv) copy_conv(sess, '{}/{}/conv_1'.format(m, t), getattr(getattr( model, m), t).conv_1) copy_bn(sess, '{}/{}/conv_1/batchnorm'.format(m, t), getattr( getattr(model, m), t).bn_conv_1) if t == 'tower_1': copy_conv(sess, '{}/{}/conv_2'.format(m, t), getattr( getattr(model, m), t).conv_2) copy_bn(sess, '{}/{}/conv_2/batchnorm'.format(m, t), getattr(getattr(model, m), t).bn_conv_2) copy_conv(sess, '{}/{}/conv_3'.format(m, t), getattr( getattr(model, m), t).conv_3) copy_bn(sess, '{}/{}/conv_3/batchnorm'.format(m, t), getattr(getattr(model, m), t).bn_conv_3) for m in ['mixed_9', 'mixed_10']: print('Copying ', m, '...') copy_conv(sess, '{}/conv'.format(m), getattr(model, m).conv.conv) copy_bn(sess, '{}/conv/batchnorm'.format(m), getattr(model, m).conv .bn_conv) for t in ['tower', 'tower_1', 'tower_2']: copy_conv(sess, '{}/{}/conv'.format(m, t), getattr(getattr( model, m), t).conv) copy_bn(sess, '{}/{}/conv/batchnorm'.format(m, t), getattr( getattr(model, m), t).bn_conv) if t == 'tower' or t == 'tower_1': copy_conv(sess, '{}/{}/mixed/conv'.format(m, t), getattr( getattr(model, m), t).mixed.conv.conv) copy_bn(sess, '{}/{}/mixed/conv/batchnorm'.format(m, t), getattr(getattr(model, m), t).mixed.conv.bn_conv) copy_conv(sess, '{}/{}/mixed/conv_1'.format(m, t), getattr( getattr(model, m), t).mixed.conv_1.conv_1) copy_bn(sess, '{}/{}/mixed/conv_1/batchnorm'.format(m, t), getattr(getattr(model, m), t).mixed.conv_1.bn_conv_1) if t == 'tower_1': copy_conv(sess, '{}/{}/conv_1'.format(m, t), getattr( getattr(model, m), t).conv_1) copy_bn(sess, '{}/{}/conv_1/batchnorm'.format(m, t), getattr(getattr(model, m), t).bn_conv_1) print('Copying logit...') w = sess.graph.get_operation_by_name('softmax/logits/MatMul').inputs[1 ].eval() b = sess.graph.get_tensor_by_name('softmax/biases:0').eval() assert w.T.shape == model.logit.W.shape assert b.shape == model.logit.b.shape model.logit.W.data = w.T model.logit.b.data = b 
user_parameters_custom.customised|args|add def add_customised_args(parser, task_name): """ loading keywords arguments to parser by task name :param parser: :param task_name: supported choices are listed in `SUPPORTED_ARG_SECTIONS` :return: parser with updated actions """ task_name = task_name.upper() if task_name in SUPPORTED_ARG_SECTIONS: return SUPPORTED_ARG_SECTIONS[task_name](parser) else: raise NotImplementedError 
avod.builders.config_builder_util.proto|to|obj def proto_to_obj(config): """Hack to convert proto config into an object so repeated fields can be overwritten  Args: config: proto config  Returns: config_obj: object with same fields as the config """ all_fields = list(config.DESCRIPTOR.fields_by_name) config_obj = ConfigObj() for field in all_fields: field_value = eval('config.{}'.format(field)) setattr(config_obj, field, field_value) return config_obj 
regression.controller.sample.NormalOutSample.ps def ps(self, n_particles): if not hasattr(self, '_p_prec'): self._p_samples(n_particles) assert_same_num = tf.assert_equal(n_particles, tf.shape(self._p_prec)[0 ], message='n_particles must be the same with the previous one') with tf.control_dependencies([assert_same_num]): return self._p_prec 
alig.test.Test.Up|set def setUp(self): np.random.seed(1234) torch.set_default_dtype(torch.float64) self.max_lr = 3 self.momentum = 0.9 self.iterations = 5 self.n_samples = 10 self.h1 = 5 self.h2 = 8 self.n_classes = 3 self.x = np.random.normal(0, 1, size=(self.n_samples, self.h1)) self.y = np.random.randint(0, self.n_classes, size=(self.n_samples,)) self.w1 = np.random.normal(0, 1, size=(self.h1, self.h2)) self.b1 = np.random.normal(0, 1, size=(self.h2,)) self.w2 = np.random.normal(0, 1, size=(self.h2, self.n_classes)) self.b2 = np.random.normal(0, 1, size=(self.n_classes,)) self.setup_th() self.setup_tf() 
pytorch_pretrained_bert.modeling_transfo_xl.AdaptiveEmbedding.forward def forward(self, inp): if self.div_val == 1: embed = self.emb_layers[0](inp) if self.d_proj != self.d_embed: embed = F.linear(embed, self.emb_projs[0]) else: param = next(self.parameters()) inp_flat = inp.view(-1) emb_flat = torch.zeros([inp_flat.size(0), self.d_proj], dtype=param .dtype, device=param.device) for i in range(len(self.cutoffs)): l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1] mask_i = (inp_flat >= l_idx) & (inp_flat < r_idx) indices_i = mask_i.nonzero().squeeze() if indices_i.numel() == 0: continue inp_i = inp_flat.index_select(0, indices_i) - l_idx emb_i = self.emb_layers[i](inp_i) emb_i = F.linear(emb_i, self.emb_projs[i]) emb_flat.index_copy_(0, indices_i, emb_i) embed_shape = inp.size() + (self.d_proj,) embed = emb_flat.view(embed_shape) embed.mul_(self.emb_scale) return embed 
cnn_helpers.x|make|no|conv|bias|stride def make_conv_9x9_stride_2_no_bias(op_name, in_tensor, filters, padding= 'VALID', weight_decay=0.0005, stddev=0.1): return make_conv_no_bias(op_name, in_tensor, 9, 9, filters, (1, 2, 2, 1 ), padding, weight_decay, stddev) 
embeddings.numberbatch_to_npy.pos|del def del_pos(s): """ Deletes part-of-speech encoding from an entity string, if present. :param s: Entity string. :return: Entity string with part-of-speech encoding removed. """ if s.endswith('/n') or s.endswith('/a') or s.endswith('/v') or s.endswith( '/r'): s = s[:-2] return s 
cnn.train.softmax|Y def softmaxY(Y): newY = [] for y in Y: tmpY = [0] * hidden_dim tmpY[y] = 1 newY.append(tmpY) return np.asarray(newY) 
classification.controller.weight_container.WeightContainer.get|weight def _get_weight(self): """ Return weight of the current weight container. :return: Tensor with self.shape. """ return self._weight 
main.eval|netowrk|relation def eval_relation_netowrk(eval_set, batch_size, device, model, num_choice): dataset_loader = data.DataLoader(eval_set, batch_size=batch_size, num_workers=0, shuffle=True, collate_fn=collate_csqa_paths) cnt_correct = 0 for k, (statements, correct_labels, cpt_paths, rel_paths, qa_pairs ) in enumerate(tqdm(dataset_loader, desc='Eval Batch')): statements = statements.to(device) correct_labels = correct_labels.to(device) flat_statements = [] flat_qa_pairs = [] assert len(statements) == len(cpt_paths) for i in range(len(statements)): cur_statement = statements[i][0] cur_qa_pairs = qa_pairs[i] flat_statements.extend(cur_statement) flat_qa_pairs.extend(cur_qa_pairs) flat_statements = torch.stack(flat_statements).to(device) flat_logits = model(flat_statements, flat_qa_pairs) assert len(flat_statements) == len(statements) * num_choice for j, correct in enumerate(correct_labels): max_logit = None pred = 0 for i in range(num_choice): cur_logit = flat_logits[j * num_choice + i] if max_logit is None: max_logit = cur_logit pred = i if max_logit < cur_logit: max_logit = cur_logit pred = i if correct[0] == pred: cnt_correct += 1 acc = cnt_correct / len(eval_set) return acc 
mnist.gen.wganlib.conv2d.Conv|D def Conv2D(name, input_dim, output_dim, filter_size, inputs, he_init=True, mask_type=None, stride=1, weightnorm=None, biases=True, gain=1.0): """ inputs: tensor of shape (batch size, num channels, height, width) mask_type: one of None, 'a', 'b'  returns: tensor of shape (batch size, num channels, height, width) """ with tf.name_scope(name) as scope: if mask_type is not None: mask_type, mask_n_channels = mask_type mask = np.ones((filter_size, filter_size, input_dim, output_dim ), dtype='float32') center = filter_size // 2 mask[center + 1:, :, :, :] = 0.0 mask[(center), center + 1:, :, :] = 0.0 for i in xrange(mask_n_channels): for j in xrange(mask_n_channels): if (mask_type == 'a' and i >= j or mask_type == 'b' and i > j): mask[(center), (center), i::mask_n_channels, j:: mask_n_channels] = 0.0  def uniform(stdev, size): return np.random.uniform(low=-stdev * np.sqrt(3), high=stdev * np.sqrt(3), size=size).astype('float32') fan_in = input_dim * filter_size ** 2 fan_out = output_dim * filter_size ** 2 / stride ** 2 if mask_type is not None: fan_in /= 2.0 fan_out /= 2.0 if he_init: filters_stdev = np.sqrt(4.0 / (fan_in + fan_out)) else: filters_stdev = np.sqrt(2.0 / (fan_in + fan_out)) if _weights_stdev is not None: filter_values = uniform(_weights_stdev, (filter_size, filter_size, input_dim, output_dim)) else: filter_values = uniform(filters_stdev, (filter_size, filter_size, input_dim, output_dim)) filter_values *= gain filters = lib.param(name + '.Filters', filter_values) if weightnorm == None: weightnorm = _default_weightnorm if weightnorm: norm_values = np.sqrt(np.sum(np.square(filter_values), axis=(0, 1, 2))) target_norms = lib.param(name + '.g', norm_values) with tf.name_scope('weightnorm') as scope: norms = tf.sqrt(tf.reduce_sum(tf.square(filters), reduction_indices=[0, 1, 2])) filters = filters * (target_norms / norms) if mask_type is not None: with tf.name_scope('filter_mask'): filters = filters * mask result = tf.nn.conv2d(input=inputs, filter=filters, strides=[1, 1, stride, stride], padding='SAME', data_format='NCHW') if biases: _biases = lib.param(name + '.Biases', np.zeros(output_dim, dtype='float32')) result = tf.nn.bias_add(result, _biases, data_format='NCHW') return result 
avod.core.evaluator_utils.iou|native|with|run|script|kitti def run_kitti_native_script_with_05_iou(checkpoint_name, score_threshold, global_step): """Runs the kitti native code script.""" eval_script_dir = avod.root_dir( ) + '/data/outputs/' + checkpoint_name + '/predictions' make_script = eval_script_dir + '/kitti_native_eval/run_eval_05_iou.sh' script_folder = eval_script_dir + '/kitti_native_eval/' results_dir = avod.top_dir() + '/scripts/offline_eval/results_05_iou/' score_threshold = round(score_threshold, 3) subprocess.call([make_script, script_folder, str(score_threshold), str( global_step), str(checkpoint_name), str(results_dir)]) 
conlleval.report def report(counts, out=None): out = '' overall, by_type = metrics(counts) c = counts out += 'processed %d tokens with %d phrases; ' % (c.token_counter, c. found_correct) out += 'found: %d phrases; correct: %d.\n' % (c.found_guessed, c. correct_chunk) if c.token_counter > 0: out += 'accuracy: %6.2f%%; ' % (100.0 * c.correct_tags / c. token_counter) out += 'precision: %6.2f%%; ' % (100.0 * overall.prec) out += 'recall: %6.2f%%; ' % (100.0 * overall.rec) out += 'FB1: %6.2f\n' % (100.0 * overall.fscore) for i, m in sorted(by_type.items()): out += '%17s: ' % i out += 'precision: %6.2f%%; ' % (100.0 * m.prec) out += 'recall: %6.2f%%; ' % (100.0 * m.rec) out += 'FB1: %6.2f  %d\n' % (100.0 * m.fscore, c.t_found_guessed[i]) return out 
craystack.codecs.ppf|from|cumulative|buckets def _ppf_from_cumulative_buckets(c_buckets): *shape, n = np.shape(c_buckets) cumulative_buckets = np.reshape(c_buckets, (-1, n))  def ppf(cfs): cfs = np.ravel(cfs) ret = np.array([(np.searchsorted(bucket, cf, 'right') - 1) for bucket, cf in zip(cumulative_buckets, cfs)]) return np.reshape(ret, shape) return ppf 
xlnet-master.tpu_estimator.TPUInfeedOutfeedSessionHookForPrediction.infeed|controller|create def _create_infeed_controller(self, name, target, args): return _OpSignalOnceQueueContext(name=name, target=target, args=args) 
trainer.Trainer.eval|run def eval_run(self): mean_p, var_p = self.session.run(self.p_target.model_mean_and_variance()) mean_q, var_q = self.session.run(self.model.q_approx. model_mean_and_variance()) var_diff = np.mean((var_q - var_p) ** 2) var_ratio = np.mean(var_q / var_p) mean_diff = np.mean((mean_q - mean_p) ** 2) q_mu, p_mu = self.session.run([self.model.q_approx._mu, self.p_target._mu]) from scipy.spatial import distance dist = distance.cdist(p_mu, q_mu, 'euclidean') diff = np.min(dist, axis=1).mean() with open(os.path.join(self.train_dir, 'results.log'), 'a') as f: f.write('method' + ',' + self.config.method + ',' + 'alpha' + ',' + repr(self.config.alpha) + ',' + 'scale' + ',' + repr(self. config.scale) + ',' + 'seed' + ',' + repr(self.config.seed) + ',' + 'mode_shift' + ',' + repr(diff) + ',' + 'mean' + ',' + repr(mean_diff) + ',' + 'var_diff' + ',' + repr(var_diff) + ',' + 'var_ratio' + ',' + repr(var_ratio) + '\n') 
gym_pycolab.envs.pycolab_grid_worlds_env.PycolabConnect5Env.game|make def _make_game(self): self._setup() return connect_four.make_game(shape=self._shape, connect_n=5) 
utils_test.TPUEncodeTest.Up|set def setUp(self): super(TPUEncodeTest, self).setUp() self.data = tf.random.uniform([128], maxval=100000, dtype=tf.int32 ), tf.cast(tf.random.uniform([128], maxval=65535, dtype=tf.int32), tf.uint16), tf.cast(tf.random.uniform([64, 84, 84, 4], maxval=256, dtype=tf.int32), tf.uint8), tf.cast(tf.random.uniform([1], maxval= 256, dtype=tf.int32), tf.uint8), tf.cast(tf.random.uniform([100, 128, 1, 1, 1], maxval=256, dtype=tf.int32), tf.uint8), tf.cast(tf. random.uniform([128, 100, 1, 1, 1], maxval=256, dtype=tf.int32), tf .uint8) 
bot_model.BaryOT.get|stats def get_stats(self, config): """print outs""" stats = OrderedDict() if self.d_loss is not None: stats['loss/disc'] = self.d_loss if self.g_loss is not None: stats['loss/gen'] = self.g_loss return stats 
observation_test.UtilsTest.bits|test|packed @parameterized.parameters((False, False), (False, True), (True, False), ( True, True)) def test_packed_bits(self, stacked, enable_sides_swap): env = gym.make('gfootball:GFootball-11_vs_11_easy_stochastic-SMM-v0', stacked=stacked, enable_sides_swap=enable_sides_swap) env.reset() for _ in range(10): obs, _, done, _ = env.step(env.action_space.sample()) baseline_obs = tf.cast(np.array(obs), tf.float32) packed_obs = observation.PackedBitsObservation.observation(env, obs) packed_obs = tf.convert_to_tensor(packed_obs) tpu_obs = observation.unpackbits(utils.tpu_encode(packed_obs)) non_tpu_obs = observation.unpackbits(packed_obs) self.assertAllEqual(baseline_obs, tpu_obs[(...), :obs.shape[-1]]) self.assertAllEqual(baseline_obs, non_tpu_obs[(...), :obs.shape[-1]]) self.assertAllEqual(tf.math.reduce_sum(tpu_obs[(...), obs.shape[-1] :]), 0) self.assertAllEqual(tf.math.reduce_sum(non_tpu_obs[(...), obs.shape [-1]:]), 0) if done: env.reset() env.close() 
nets.mobilenet_v1_test.MobilenetV1Test.Stride|All|Upto|Conv|Check|d|Output|test|Build|Points|And|End def testOutputStride8BuildAndCheckAllEndPointsUptoConv2d_13(self): batch_size = 5 height, width = 224, 224 output_stride = 8 inputs = tf.random_uniform((batch_size, height, width, 3)) with slim.arg_scope([slim.conv2d, slim.separable_conv2d], normalizer_fn =slim.batch_norm): _, end_points = mobilenet_v1.mobilenet_v1_base(inputs, output_stride=output_stride, final_endpoint='Conv2d_13_pointwise') _, explicit_padding_end_points = mobilenet_v1.mobilenet_v1_base(inputs, output_stride=output_stride, final_endpoint= 'Conv2d_13_pointwise', use_explicit_padding=True) endpoints_shapes = {'Conv2d_0': [batch_size, 112, 112, 32], 'Conv2d_1_depthwise': [batch_size, 112, 112, 32], 'Conv2d_1_pointwise': [batch_size, 112, 112, 64], 'Conv2d_2_depthwise': [batch_size, 56, 56, 64], 'Conv2d_2_pointwise': [batch_size, 56, 56, 128], 'Conv2d_3_depthwise': [batch_size, 56, 56, 128], 'Conv2d_3_pointwise': [batch_size, 56, 56, 128], 'Conv2d_4_depthwise': [batch_size, 28, 28, 128], 'Conv2d_4_pointwise': [batch_size, 28, 28, 256], 'Conv2d_5_depthwise': [batch_size, 28, 28, 256], 'Conv2d_5_pointwise': [batch_size, 28, 28, 256], 'Conv2d_6_depthwise': [batch_size, 28, 28, 256], 'Conv2d_6_pointwise': [batch_size, 28, 28, 512], 'Conv2d_7_depthwise': [batch_size, 28, 28, 512], 'Conv2d_7_pointwise': [batch_size, 28, 28, 512], 'Conv2d_8_depthwise': [batch_size, 28, 28, 512], 'Conv2d_8_pointwise': [batch_size, 28, 28, 512], 'Conv2d_9_depthwise': [batch_size, 28, 28, 512], 'Conv2d_9_pointwise': [batch_size, 28, 28, 512], 'Conv2d_10_depthwise': [batch_size, 28, 28, 512], 'Conv2d_10_pointwise': [batch_size, 28, 28, 512], 'Conv2d_11_depthwise': [batch_size, 28, 28, 512], 'Conv2d_11_pointwise': [batch_size, 28, 28, 512], 'Conv2d_12_depthwise': [batch_size, 28, 28, 512], 'Conv2d_12_pointwise': [batch_size, 28, 28, 1024], 'Conv2d_13_depthwise': [batch_size, 28, 28, 1024], 'Conv2d_13_pointwise': [batch_size, 28, 28, 1024]} self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys()) for endpoint_name, expected_shape in endpoints_shapes.items(): self.assertTrue(endpoint_name in end_points) self.assertListEqual(end_points[endpoint_name].get_shape().as_list( ), expected_shape) self.assertItemsEqual(endpoints_shapes.keys(), explicit_padding_end_points.keys()) for endpoint_name, expected_shape in endpoints_shapes.items(): self.assertTrue(endpoint_name in explicit_padding_end_points) self.assertListEqual(explicit_padding_end_points[endpoint_name]. get_shape().as_list(), expected_shape) 
nets.nasnet.nasnet_utils_test.NasnetUtilsTest.test|Index|Get|Channel def testGetChannelIndex(self): data_formats = ['NHWC', 'NCHW'] for data_format in data_formats: index = nasnet_utils.get_channel_index(data_format) correct_index = 3 if data_format == 'NHWC' else 1 self.assertEqual(index, correct_index) 
utils.params_utils.HParams.hparam|pop def pop_hparam(self, name): value = getattr(self, name) self.del_hparam(name) return value 
deepctr.models.nffm.NFFM def NFFM(feature_dim_dict, embedding_size=4, dnn_hidden_units=(128, 128), l2_reg_embedding=1e-05, l2_reg_linear=1e-05, l2_reg_dnn=0, dnn_dropout= 0, init_std=0.0001, seed=1024, include_linear=True, use_bn=True, reduce_sum=False, task='binary'): """Instantiates the Field-aware Neural Factorization Machine architecture.  :param feature_dim_dict: dict,to indicate sparse field and dense field like {'sparse':{'field_1':4,'field_2':3,'field_3':2},'dense':['field_4','field_5']} :param embedding_size: positive integer,sparse feature embedding_size :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of deep net :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector :param l2_reg_linear: float. L2 regularizer strength applied to linear part. :param l2_reg_dnn: float . L2 regularizer strength applied to DNN :param init_std: float,to use as the initialize std of embedding vector :param seed: integer ,to use as random seed. :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate. :param include_linear: bool,whether include linear term or not :param use_bn: bool,whether use bn after ffm out or not :param reduce_sum: bool,whether apply reduce_sum on cross vector :param task: str, ``"binary"`` for  binary logloss or  ``"regression"`` for regression loss :return: A Keras model instance. """ check_feature_config_dict(feature_dim_dict) if 'sequence' in feature_dim_dict and len(feature_dim_dict['sequence'] ) > 0: raise ValueError('now sequence input is not supported in NFFM') sparse_input_dict, dense_input_dict = create_singlefeat_inputdict( feature_dim_dict) sparse_embedding, dense_embedding, linear_embedding = ( create_embedding_dict(feature_dim_dict, embedding_size, init_std, seed, l2_reg_embedding, l2_reg_linear)) embed_list = [] for i, j in itertools.combinations(feature_dim_dict['sparse'], 2): i_input = sparse_input_dict[i.name] if i.hash_flag: i_input = Hash(i.dimension)(i_input) j_input = sparse_input_dict[j.name] if j.hash_flag: j_input = Hash(j.dimension)(j_input) element_wise_prod = multiply([sparse_embedding[i.name][j.name]( i_input), sparse_embedding[j.name][i.name](j_input)]) if reduce_sum: element_wise_prod = Lambda(lambda element_wise_prod: K.sum( element_wise_prod, axis=-1))(element_wise_prod) embed_list.append(element_wise_prod) for i, j in itertools.combinations(feature_dim_dict['dense'], 2): element_wise_prod = multiply([dense_embedding[i.name][j.name]( dense_input_dict[i.name]), dense_embedding[j.name][i.name]( dense_input_dict[j.name])]) if reduce_sum: element_wise_prod = Lambda(lambda element_wise_prod: K.sum( element_wise_prod, axis=-1))(element_wise_prod) embed_list.append(Lambda(lambda x: K.expand_dims(x, axis=1))( element_wise_prod)) for i in feature_dim_dict['sparse']: i_input = sparse_input_dict[i.name] if i.hash_flag: i_input = Hash(i.dimension)(i_input) for j in feature_dim_dict['dense']: element_wise_prod = multiply([sparse_embedding[i.name][j.name]( i_input), dense_embedding[j.name][i.name](dense_input_dict[ j.name])]) if reduce_sum: element_wise_prod = Lambda(lambda element_wise_prod: K.sum( element_wise_prod, axis=-1))(element_wise_prod) embed_list.append(element_wise_prod) ffm_out = tf.keras.layers.Flatten()(concat_fun(embed_list, axis=1)) if use_bn: ffm_out = tf.keras.layers.BatchNormalization()(ffm_out) ffm_out = DNN(dnn_hidden_units, l2_reg=l2_reg_dnn, dropout_rate=dnn_dropout )(ffm_out) final_logit = Dense(1, use_bias=False)(ffm_out) linear_emb_list = get_embedding_vec_list(linear_embedding, sparse_input_dict, feature_dim_dict['sparse']) linear_logit = get_linear_logit(linear_emb_list, dense_input_dict, l2_reg_linear) if include_linear: final_logit = add([final_logit, linear_logit]) output = PredictionLayer(task)(final_logit) inputs_list = get_inputs_list([sparse_input_dict, dense_input_dict]) model = Model(inputs=inputs_list, outputs=output) return model 
svae_dc.collect_env_experience.main def main(args): x_buf, xi_1toT_buf, bads_1toT_buf, rwd_buf, rnd_params_buf = ( collect_episodes(args) if args.num_procs <= 1 else main_multiproc(args) ) save_path = args.output_prefix if not os.path.exists(save_path): os.makedirs(save_path) outfl = os.path.join(save_path, 'episodes{:d}K_seed{:d}_{:s}.npz'. format(int(args.num_episodes / 1000), args.seed, args.env_name)) kwargs = {'x_buf': x_buf, 'xi_1toT_buf': xi_1toT_buf, 'bads_1toT_buf': bads_1toT_buf, 'rwd_buf': rwd_buf} if rnd_params_buf is not None: kwargs['rnd_params_buf'] = rnd_params_buf np.savez(outfl, **kwargs) if args.num_episodes < 20: print('rwd_buf', rwd_buf) print('goodness', 1 - bads_1toT_buf.mean(axis=1)) print('rnd_params_buf', rnd_params_buf) 
nmt.estimator.eval|runner|create def create_eval_runner(hparams): hparams.tgt_sos_id, hparams.tgt_eos_id = 1, 2 eval_steps = int(math.ceil(hparams.examples_to_infer / hparams. infer_batch_size)) return low_level_runner.EvalLowLevelRunner(eval_steps, hparams) 
gan_task_generate_latent_trans.GANTask.main def main(self): import os import tensorflow as tf from gan.load_data import load_celeba from gan.latent import UniformLatent, JointLatent from gan.network import Decoder, InfoGANDiscriminator, CrDiscriminator from gan.infogan_cr import INFOGAN_CR import numpy as np import imageio data = load_celeba('data/celeba') _, height, width, depth = data.shape latent_list = [] for i in range(self._config['uniform_reg_dim']): latent_list.append(UniformLatent(in_dim=1, out_dim=1, low=-1.0, high=1.0, q_std=1.0, apply_reg=True)) if self._config['uniform_not_reg_dim'] > 0: latent_list.append(UniformLatent(in_dim=self._config[ 'uniform_not_reg_dim'], out_dim=self._config[ 'uniform_not_reg_dim'], low=-1.0, high=1.0, q_std=1.0, apply_reg=False)) latent = JointLatent(latent_list=latent_list) decoder = Decoder(output_width=width, output_height=height, output_depth=depth) infoGANDiscriminator = InfoGANDiscriminator(output_length=latent. reg_out_dim) crDiscriminator = CrDiscriminator(output_length=latent.num_reg_latent) checkpoint_dir = os.path.join(self._work_dir, 'checkpoint') if not os.path.exists(checkpoint_dir): os.makedirs(checkpoint_dir) sample_dir = os.path.join(self._work_dir, 'sample') if not os.path.exists(sample_dir): os.makedirs(sample_dir) time_path = os.path.join(self._work_dir, 'time.txt') metric_path = os.path.join(self._work_dir, 'metric.csv') run_config = tf.ConfigProto() with tf.Session(config=run_config) as sess: metric_callbacks = [] gan = INFOGAN_CR(sess=sess, checkpoint_dir=checkpoint_dir, sample_dir=sample_dir, time_path=time_path, epoch=self._config[ 'epoch'], batch_size=self._config['batch_size'], data=data, vis_freq=self._config['vis_freq'], vis_num_sample=self._config[ 'vis_num_sample'], vis_num_rep=self._config['vis_num_rep'], latent=latent, decoder=decoder, infoGANDiscriminator= infoGANDiscriminator, crDiscriminator=crDiscriminator, gap_start=self._config['gap_start'], gap_decrease_times=self. _config['gap_decrease_times'], gap_decrease=self._config[ 'gap_decrease'], gap_decrease_batch=self._config[ 'gap_decrease_batch'], cr_coe_start=self._config['cr_coe_start' ], cr_coe_increase_times=self._config['cr_coe_increase_times'], cr_coe_increase=self._config['cr_coe_increase'], cr_coe_increase_batch=self._config['cr_coe_increase_batch'], info_coe_de=self._config['info_coe_de'], info_coe_infod=self. _config['info_coe_infod'], metric_callbacks=metric_callbacks, metric_freq=self._config['metric_freq'], metric_path= metric_path, output_reverse=self._config['output_reverse'], de_lr=self._config['de_lr'], infod_lr=self._config['infod_lr'], crd_lr=self._config['crd_lr'], summary_freq=self._config[ 'summary_freq']) gan.build() gan.load() NUM_SAMPLE = 15 NUM_REP = 100 GAP = 2 height, width, depth = gan.image_dims latents = gan.latent.uniformly_sample(NUM_SAMPLE, NUM_REP) for latent_i, latent in enumerate(latents): folder = os.path.join(self._work_dir, 'latent_trans', 'latent{}'.format(latent_i)) if not os.path.exists(folder): os.makedirs(folder) samples = gan.sample_from(latent) for i in range(NUM_REP): image = np.zeros((height, NUM_SAMPLE * width + (NUM_SAMPLE - 1) * GAP, depth)) for j in range(NUM_SAMPLE): id_ = j * NUM_REP + i col_start = j * (width + GAP) image[:, col_start:col_start + width, :] = samples[id_] v_min = image.min() - gan.EPS v_max = image.max() + gan.EPS image = (image - v_min) / (v_max - v_min) * 255.0 image = image.astype(np.uint8) for j in range(NUM_SAMPLE - 1): col_start = (j + 1) * width + j * GAP image[:, col_start:col_start + GAP, :] = 255 file_path = os.path.join(folder, 'sample{}.png'.format(i)) imageio.imwrite(file_path, image) 
model.fpn|build|delta|graph|bins|coords def build_fpn_coords_bins_delta_graph(rois, feature_maps, image_shape, pool_size, num_classes, num_bins): """Builds the computation graph of the coordinate map head of Feature Pyramid Network.  rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized coordinates. feature_maps: List of feature maps from different layers of the pyramid, [P2, P3, P4, P5]. Each has a different resolution. image_shape: [height, width, depth] pool_size: The width of the square feature map generated from ROI Pooling. num_classes: number of classes, which determines the depth of the results  Returns: Coordinate maps [batch, roi_count, height, width, num_classes, 3] """ x = PyramidROIAlign([pool_size, pool_size], image_shape, name= 'roi_align_coord')([rois] + feature_maps) x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding='same'), name= 'mrcnn_coord_conv1')(x) x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_bn1')(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding='same'), name= 'mrcnn_coord_conv2')(x) x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_bn2')(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding='same'), name= 'mrcnn_coord_conv3')(x) x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_bn3')(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding='same'), name= 'mrcnn_coord_conv4')(x) x = KL.TimeDistributed(BatchNorm(axis=-1), name='mrcnn_coord_bn4')(x) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2DTranspose(256, (2, 2), strides=2, activation='relu'), name='mrcnn_coord_deconv')(x) x1 = KL.TimeDistributed(KL.Conv2D(3 * num_bins * num_classes, (1, 1), strides=1), name='mrcnn_coord_conv_bins')(x) x2 = KL.TimeDistributed(KL.Conv2D(3 * num_bins * num_classes, (1, 1), strides=1), name='mrcnn_coord_conv_delta')(x) x1 = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], tf.shape(t)[1], tf.shape(t)[2], tf.shape(t)[3], -1, 3, num_bins]), name= 'mrcnn_coord_bins_reshape')(x1) x2 = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], tf.shape(t)[1], tf.shape(t)[2], tf.shape(t)[3], -1, 3, num_bins]), name= 'mrcnn_coord_delta_reshape')(x2) x1 = KL.Activation('softmax', name='mrcnn_coord_bins')(x1) x2 = KL.Activation('sigmoid', name='mrcnn_coord_delta')(x2) mrcnn_coord_x_bin = KL.Lambda(lambda x: x[:, :, :, :, :, (0), :], name= 'mrcnn_coord_x_bin')(x1) mrcnn_coord_y_bin = KL.Lambda(lambda x: x[:, :, :, :, :, (1), :], name= 'mrcnn_coord_y_bin')(x1) mrcnn_coord_z_bin = KL.Lambda(lambda x: x[:, :, :, :, :, (2), :], name= 'mrcnn_coord_z_bin')(x1) mrcnn_coord_x_delta = KL.Lambda(lambda x: x[:, :, :, :, :, (0), :], name='mrcnn_coord_x_delta')(x2) mrcnn_coord_y_delta = KL.Lambda(lambda x: x[:, :, :, :, :, (1), :], name='mrcnn_coord_y_delta')(x2) mrcnn_coord_z_delta = KL.Lambda(lambda x: x[:, :, :, :, :, (2), :], name='mrcnn_coord_z_delta')(x2) return (mrcnn_coord_x_bin, mrcnn_coord_y_bin, mrcnn_coord_z_bin, mrcnn_coord_x_delta, mrcnn_coord_y_delta, mrcnn_coord_z_delta) 
defense.defend|crop def defend_crop(x, crop_size=90, ensemble_size=30): x_size = tf.to_float(x.shape[1]) frac = crop_size / x_size start_fraction_max = (x_size - crop_size) / x_size  def randomizing_crop(x): start_x = tf.random_uniform((), 0, start_fraction_max) start_y = tf.random_uniform((), 0, start_fraction_max) return tf.image.crop_and_resize([x], boxes=[[start_y, start_x, start_y + frac, start_x + frac]], box_ind=[0], crop_size=[ crop_size, crop_size]) return tf.concat([randomizing_crop(x) for _ in range(ensemble_size)], axis=0) 
bert.tokenization.BasicTokenizer.tokenize def tokenize(self, text): """Tokenizes a piece of text.""" text = convert_to_unicode(text) text = self._clean_text(text) text = self._tokenize_chinese_chars(text) orig_tokens = whitespace_tokenize(text) split_tokens = [] for token in orig_tokens: if self.do_lower_case: token = token.lower() token = self._run_strip_accents(token) split_tokens.extend(self._run_split_on_punc(token)) output_tokens = whitespace_tokenize(' '.join(split_tokens)) return output_tokens 
test_envs.TestUR5.test|init def test_init(self): """Ensure that all variables are being initialized properly.""" self.assertEqual(self.env.name, 'ur5.xml') self.assertEqual(self.env.observation_space.shape[0], 6) self.assertEqual(self.env.action_space.shape[0], 3) np.testing.assert_array_almost_equal((self.env.action_space.high - self .env.action_space.low) / 2, [3.15, 5.0, 3.15]) np.testing.assert_array_almost_equal((self.env.action_space.high + self .env.action_space.low) / 2, [0.0, 0.0, 0.0]) self.assertEqual(len(self.env.context_range), 3) np.testing.assert_array_almost_equal(self.env.end_goal_thresholds, [( 0.17453293) for _ in range(3)]) self.assertEqual(self.env.max_actions, 600) self.assertEqual(self.env.visualize, False) self.assertEqual(self.env.viewer, None) self.assertEqual(self.env.num_frames_skip, 1) np.testing.assert_array_almost_equal(self.env.context_space.low, [- 3.141593, -0.785398, -0.785398]) np.testing.assert_array_almost_equal(self.env.context_space.high, [ 3.141593, 0.0, 0.785398]) 
csqa_dataset.collate|graphs|csqa def collate_csqa_graphs(samples): statements, correct_labels, graph_data = map(list, zip(*samples)) flat_graph_data = [] for gd in graph_data: flat_graph_data.extend(gd) batched_graph = dgl.batch(flat_graph_data) sents_vecs = torch.stack(statements) return sents_vecs, torch.Tensor([[i] for i in correct_labels] ), batched_graph 
