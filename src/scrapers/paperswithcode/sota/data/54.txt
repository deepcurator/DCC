Learning discriminative representations for unseen person images is critical
for person Re-Identification (ReID). Most of current approaches learn deep
representations in classification tasks, which essentially minimize the
empirical classification risk on the training set. As shown in our experiments,
such representations commonly focus on several body parts discriminative to the
training set, rather than the entire human body. Inspired by the structural
risk minimization principle in SVM, we revise the traditional deep
representation learning procedure to minimize both the empirical classification
risk and the representation learning risk. The representation learning risk is
evaluated by the proposed part loss, which automatically generates several
parts for an image, and computes the person classification loss on each part
separately. Compared with traditional global classification loss,
simultaneously considering multiple part loss enforces the deep network to
focus on the entire human body and learn discriminative representations for
different parts. Experimental results on three datasets, i.e., Market1501,
CUHK03, VIPeR, show that our representation outperforms the existing deep
representations.