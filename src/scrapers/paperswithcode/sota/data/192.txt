We propose a viewpoint invariant model for 3D human pose estimation from a
single depth image. To achieve this, our discriminative model embeds local
regions into a learned viewpoint invariant feature space. Formulated as a
multi-task learning problem, our model is able to selectively predict partial
poses in the presence of noise and occlusion. Our approach leverages a
convolutional and recurrent network architecture with a top-down error feedback
mechanism to self-correct previous pose estimates in an end-to-end manner. We
evaluate our model on a previously published depth dataset and a newly
collected human pose dataset containing 100K annotated depth images from
extreme viewpoints. Experiments show that our model achieves competitive
performance on frontal views while achieving state-of-the-art performance on
alternate viewpoints.