Convolutional neural networks are capable of learning powerful
representational spaces, which are necessary for tackling complex learning
tasks. However, due to the model capacity required to capture such
representations, they are often susceptible to overfitting and therefore
require proper regularization in order to generalize well. In this paper, we
show that the simple regularization technique of randomly masking out square
regions of input during training, which we call cutout, can be used to improve
the robustness and overall performance of convolutional neural networks. Not
only is this method extremely easy to implement, but we also demonstrate that
it can be used in conjunction with existing forms of data augmentation and
other regularizers to further improve model performance. We evaluate this
method by applying it to current state-of-the-art architectures on the
CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results
of 2.56%, 15.20%, and 1.30% test error respectively. Code is available at
this https URL