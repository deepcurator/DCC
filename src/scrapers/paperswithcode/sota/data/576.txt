Who did what to whom is a major focus in natural language understanding,
which is right the aim of semantic role labeling (SRL). Although SRL is
naturally essential to text comprehension tasks, it is surprisingly ignored in
previous work. This paper thus makes the first attempt to let SRL enhance text
comprehension and inference through specifying verbal arguments and their
corresponding semantic roles. In terms of deep learning models, our embeddings
are enhanced by semantic role labels for more fine-grained semantics. We show
that the salient labels can be conveniently added to existing models and
significantly improve deep learning models in challenging text comprehension
tasks. Extensive experiments on benchmark machine reading comprehension and
inference datasets verify that the proposed semantic learning helps our system
reach new state-of-the-art.