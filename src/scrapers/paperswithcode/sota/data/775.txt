Recurrent neural network grammars (RNNG) are a recently proposed
probabilistic generative modeling family for natural language. They show
state-of-the-art language modeling and parsing performance. We investigate what
information they learn, from a linguistic perspective, through various
ablations to the model and the data, and by augmenting the model with an
attention mechanism (GA-RNNG) to enable closer inspection. We find that
explicit modeling of composition is crucial for achieving the best performance.
Through the attention mechanism, we find that headedness plays a central role
in phrasal representation (with the model's latent attention largely agreeing
with predictions made by hand-crafted head rules, albeit with some important
differences). By training grammars without nonterminal labels, we find that
phrasal representations depend minimally on nonterminals, providing support for
the endocentricity hypothesis.