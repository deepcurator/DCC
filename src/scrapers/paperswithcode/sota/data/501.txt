To answer the question in machine comprehension (MC) task, the models need to
establish the interaction between the question and the context. To tackle the
problem that the single-pass model cannot reflect on and correct its answer, we
present Ruminating Reader. Ruminating Reader adds a second pass of attention
and a novel information fusion component to the Bi-Directional Attention Flow
model (BiDAF). We propose novel layer structures that construct an query-aware
context vector representation and fuse encoding representation with
intermediate representation on top of BiDAF model. We show that a multi-hop
attention mechanism can be applied to a bi-directional attention structure. In
experiments on SQuAD, we find that the Reader outperforms the BiDAF baseline by
a substantial margin, and matches or surpasses the performance of all other
published systems.