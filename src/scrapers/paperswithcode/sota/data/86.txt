Much recent progress in Vision-to-Language problems has been achieved through
a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural
Networks (RNNs). This approach does not explicitly represent high-level
semantic concepts, but rather seeks to progress directly from image features to
text. In this paper we first propose a method of incorporating high-level
concepts into the successful CNN-RNN approach, and show that it achieves a
significant improvement on the state-of-the-art in both image captioning and
visual question answering. We further show that the same mechanism can be used
to incorporate external knowledge, which is critically important for answering
high level visual questions. Specifically, we design a visual question
answering model that combines an internal representation of the content of an
image with information extracted from a general knowledge base to answer a
broad range of image-based questions. It particularly allows questions to be
asked about the contents of an image, even when the image itself does not
contain a complete answer. Our final model achieves the best reported results
on both image captioning and visual question answering on several benchmark
datasets.