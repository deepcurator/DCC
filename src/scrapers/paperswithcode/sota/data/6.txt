This paper presents a Deep convolutional network model for Identity-Aware
Transfer (DIAT) of facial attributes. Given the source input image and the
reference attribute, DIAT aims to generate a facial image that owns the
reference attribute as well as keeps the same or similar identity to the input
image. In general, our model consists of a mask network and an attribute
transform network which work in synergy to generate a photo-realistic facial
image with the reference attribute. Considering that the reference attribute
may be only related to some parts of the image, the mask network is introduced
to avoid the incorrect editing on attribute irrelevant region. Then the
estimated mask is adopted to combine the input and transformed image for
producing the transfer result. For joint training of transform network and mask
network, we incorporate the adversarial attribute loss, identity-aware adaptive
perceptual loss, and VGG-FACE based identity loss. Furthermore, a denoising
network is presented to serve for perceptual regularization to suppress the
artifacts in transfer result, while an attribute ratio regularization is
introduced to constrain the size of attribute relevant region. Our DIAT can
provide a unified solution for several representative facial attribute transfer
tasks, e.g., expression transfer, accessory removal, age progression, and
gender transfer, and can be extended for other face enhancement tasks such as
face hallucination. The experimental results validate the effectiveness of the
proposed method. Even for the identity-related attribute (e.g., gender), our
DIAT can obtain visually impressive results by changing the attribute while
retaining most identity-aware features.