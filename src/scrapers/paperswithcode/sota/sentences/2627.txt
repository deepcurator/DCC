In this work, we extend this approach
to multiple languages and show the effectiveness of cross-lingual pretraining.